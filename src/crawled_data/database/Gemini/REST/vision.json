{
    "base_url": "https://ai.google.dev/gemini-api/docs/vision?lang=rest",
    "crawl_date": "2025-01-01T18:50:54.196997",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/vision?lang=rest",
            "title": "Explore vision capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore vision capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore vision capabilities with the Gemini API Python Node.js Go REST The Gemini API is able to process images and videos, enabling a multitude of\n exciting developer use cases. Some of Gemini's vision capabilities include\n the ability to: Caption and answer questions about images Transcribe and reason over PDFs, including long documents up to 2 million token context window Describe, segment, and extract information from videos,\nincluding both visual frames and audio, up to 90 minutes long Detect objects in an image and return bounding box coordinates for them This tutorial demonstrates some possible ways to prompt the Gemini API with\nimages and video input, provides code examples,\nand outlines prompting best practices with multimodal vision capabilities.\nAll output is text-only. Before you begin: Set up your project and API key Before calling the Gemini API, you need to set up your project and configure\nyour API key. Expand to view how to set up your project and API key Tip: For complete setup instructions, see the Gemini API quickstart . Get and secure your API key You need an API key to call the Gemini API. If you don't already have one,\ncreate a key in Google AI Studio. Get an API key It's strongly recommended that you do not check an API key into your version\ncontrol system. This tutorial assumes that you're accessing your API key as an environment\nvariable. Prompting with images In this tutorial, you will upload images using the File API or as inline data\nand generate content based on those images. Technical details (images) Gemini 1.5 Pro and 1.5 Flash support a maximum of 3,600 image files. Images must be in one of the following image data MIME types: PNG - image/png JPEG - image/jpeg WEBP - image/webp HEIC - image/heic HEIF - image/heif Each image is equivalent to 258 tokens. While there are no specific limits to the number of pixels in an image besides\nthe model's context window, larger images are scaled down to a maximum\nresolution of 3072x3072 while preserving their original aspect ratio, while\nsmaller images are scaled up to 768x768 pixels. There is no cost reduction\nfor images at lower sizes, other than bandwidth, or performance improvement\nfor images at higher resolution. For best results: Rotate images to the correct orientation before uploading. Avoid blurry images. If using a single image, place the text prompt after the image. Image input For total image payload size less than 20MB, we recommend either uploading\nbase64 encoded images or directly uploading locally stored image files. Base64 encoded images You can upload public image URLs by encoding them as Base64 payloads.\nWe recommend using the httpx library to fetch the image URLs.\nThe following code example shows how to do this: IMG_PATH = /path/to/your/image1.jpeg if [[ \" $( base64 --version 2>&1 ) \" = * \"FreeBSD\" * ]] ; then B64FLAGS = \"--input\" else B64FLAGS = \"-w0\" fi curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= $GOOGLE_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d '{ \"contents\": [{ \"parts\":[ {\"text\": \"Caption this image.\"}, { \"inline_data\": { \"mime_type\":\"image/jpeg\", \"data\": \"' \\$ ( base64 \\$ B64FLAGS \\$ IMG_PATH ) '\" } } ] }] }' 2 > /dev/null Multiple images To prompt with multiple images in Base64 encoded format, you can do the\nfollowing: IMAGE_PATH_1 = /path/to/your/image1.jpeg IMAGE_PATH_2 = /path/to/your/image2.jpeg if [[ \" $( base64 --version 2>&1 ) \" = * \"FreeBSD\" * ]] ; then B64FLAGS = \"--input\" else B64FLAGS = \"-w0\" fi curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= $GOOGLE_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d '{ \"contents\": [{ \"parts\":[ { \"inline_data\": { \"mime_type\": \"image/jpeg\", \"data\": \"' \\$ ( base64 $B64FLAGS \" $IMAGE_PATH_1 \" ) '\" } }, { \"inline_data\": { \"mime_type\": \"image/png\", \"data\": \"' \\$ ( base64 $B64FLAGS \" $IMAGE_PATH_2 \" ) '\" } }, { \"text\": \"Generate a list of all the objects contained in both images.\" } ] }] }' 2 > /dev/null Upload an image and generate content When the combination of files and system instructions that you intend to send is\nlarger than 20 MB in size, use the File API to upload those files. Use the media.upload method of the File API to upload an image of any size. Note: The File API lets you store up to 20 GB of files per project, with a\nper-file maximum size of 2 GB. Files are stored for 48 hours. They can be\naccessed in that period with your API key, but cannot be downloaded from the\nAPI. It is available at no cost in all regions where the Gemini API is\navailable. After uploading the file, you can make GenerateContent requests that reference\nthe File API URI. Select the generative model and provide it with a text prompt\nand the uploaded image. MIME_TYPE = $( file -b --mime-type \" ${ IMG_PATH_2 } \" ) NUM_BYTES = $( wc -c < \" ${ IMG_PATH_2 } \" ) DISPLAY_NAME = TEXT tmp_header_file = upload-header.tmp # Initial resumable request defining metadata. # The upload url is in the response headers dump them to a file. curl \" ${ BASE_URL } /upload/v1beta/files?key= ${ GOOGLE_API_KEY } \" \\ -D upload-header.tmp \\ -H \"X-Goog-Upload-Protocol: resumable\" \\ -H \"X-Goog-Upload-Command: start\" \\ -H \"X-Goog-Upload-Header-Content-Length: ${ NUM_BYTES } \" \\ -H \"X-Goog-Upload-Header-Content-Type: ${ MIME_TYPE } \" \\ -H \"Content-Type: application/json\" \\ -d \"{'file': {'display_name': ' ${ DISPLAY_NAME } '}}\" 2 > /dev/null upload_url = $( grep -i \"x-goog-upload-url: \" \" ${ tmp_header_file } \" | cut -d \" \" -f2 | tr -d \"\\r\" ) rm \" ${ tmp_header_file } \" # Upload the actual bytes. curl \" ${ upload_url } \" \\ -H \"Content-Length: ${ NUM_BYTES } \" \\ -H \"X-Goog-Upload-Offset: 0\" \\ -H \"X-Goog-Upload-Command: upload, finalize\" \\ --data-binary \"@ ${ IMG_PATH_2 } \" 2 > /dev/null > file_info.json file_uri = $( jq \".file.uri\" file_info.json ) echo file_uri = $file_uri # Now generate content using that file curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= $GOOGLE_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d '{ \"contents\": [{ \"parts\":[ {\"text\": \"Can you tell me about the instruments in this photo?\"}, {\"file_data\": {\"mime_type\": \"image/jpeg\", \"file_uri\": ' $file_uri '} }] }] }' 2 > /dev/null > response.json\n\ncat response.json echo jq \".candidates[].content.parts[].text\" response.json files.sh Verify image file upload and get metadata You can verify the API successfully stored the uploaded file and get its\nmetadata by calling files.get . Only the name (and by extension, the uri ) are unique. name = $( jq \".file.name\" file_info.json ) # Get the file of interest to check state curl https://generativelanguage.googleapis.com/v1beta/files/ $name > file_info.json # Print some information about the file you got name = $( jq \".file.name\" file_info.json ) echo name = $name file_uri = $( jq \".file.uri\" file_info.json ) echo file_uri = $file_uri files .sh OpenAI Compatibility You can access Gemini's image understanding capabilities using the\nOpenAI libraries. This lets you integrate Gemini into existing\nOpenAI workflows by updating three lines of code and using\nyour Gemini API key. See the Image understanding example for code demonstrating how to send images encoded as Base64 payloads. Capabilities This section outlines specific vision capabilities of the Gemini model,\nincluding object detection and bounding box coordinates. Get a bounding box for an object Gemini models are trained to return bounding box coordinates as relative widths\nor heights in the range of [0, 1]. These values are then scaled by 1000 and\nconverted to integers. Effectively, the coordinates represent the bounding box\non a 1000x1000 pixel version of the image. Therefore, you'll need to\nconvert these coordinates back to the dimensions of your original\nimage to accurately map the bounding boxes. IMG_PATH = /path/to/your/image1.jpeg if [[ \" $( base64 --version 2>&1 ) \" = * \"FreeBSD\" * ]] ; then B64FLAGS = \"--input\" else B64FLAGS = \"-w0\" fi curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?key= $GOOGLE_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d \"{ \\\"contents\\\": [{ \\\"parts\\\":[ { \\\"inline_data\\\": { \\\"mime_type\\\": \\\"image/jpeg\\\", \\\"data\\\": \\\"'\\$(base64 $B64FLAGS \\\" $IMG_PATH \\\")'\\\" } }, { \\\"text\\\": \\\"Generate bounding boxes for each of the objects in this image in [y_min, x_min, y_max, x_max] format.\\\" } ] }] }\" 2 > /dev/null The model returns bounding box coordinates in the format [ymin, xmin, ymax, xmax] . To convert these normalized coordinates\nto the pixel coordinates of your original image, follow these steps: Divide each output coordinate by 1000. Multiply the x-coordinates by the original image width. Multiply the y-coordinates by the original image height. Prompting with video In this tutorial, you will upload a video using the File API and generate\ncontent based on those images. Note: The File API is required to upload video files, due to their size.\nHowever, the File API is only available for Python, Node.js, Go, and REST. Technical details (video) Gemini 1.5 Pro and Flash support up to approximately an hour of video data. Video must be in one of the following video format MIME types: video/mp4 video/mpeg video/mov video/avi video/x-flv video/mpg video/webm video/wmv video/3gpp The File API service extracts image frames from videos at 1 frame per second\n(FPS) and audio at 1Kbps, single channel, adding timestamps every second.\nThese rates are subject to change in the future for improvements in inference. Note: The details of fast action sequences may be lost at the 1 FPS frame\nsampling rate. Consider slowing down high-speed clips for improved inference\nquality. Individual frames are 258 tokens, and audio is 32 tokens per second. With\nmetadata, each second of video becomes ~300 tokens, which means a 1M context\nwindow can fit slightly less than an hour of video. To ask questions about time-stamped locations, use the format MM:SS , where\nthe first two digits represent minutes and the last two digits represent\nseconds. For best results: Use one video per prompt. If using a single video, place the text prompt after the video. Upload a video and generate content Note: The File API lets you store up to 20 GB of files per project, with a\nper-file maximum size of 2 GB. Files are stored for 48 hours. They can be\naccessed in that period with your API key, but they cannot be downloaded\nusing any API. It is available at no cost in all regions where the Gemini\nAPI is available. MIME_TYPE = $( file -b --mime-type \" ${ VIDEO_PATH } \" ) NUM_BYTES = $( wc -c < \" ${ VIDEO_PATH } \" ) DISPLAY_NAME = VIDEO_PATH # Initial resumable request defining metadata. # The upload url is in the response headers dump them to a file. curl \" ${ BASE_URL } /upload/v1beta/files?key= ${ GOOGLE_API_KEY } \" \\ -D upload-header.tmp \\ -H \"X-Goog-Upload-Protocol: resumable\" \\ -H \"X-Goog-Upload-Command: start\" \\ -H \"X-Goog-Upload-Header-Content-Length: ${ NUM_BYTES } \" \\ -H \"X-Goog-Upload-Header-Content-Type: ${ MIME_TYPE } \" \\ -H \"Content-Type: application/json\" \\ -d \"{'file': {'display_name': ' ${ DISPLAY_NAME } '}}\" 2 > /dev/null upload_url = $( grep -i \"x-goog-upload-url: \" \" ${ tmp_header_file } \" | cut -d \" \" -f2 | tr -d \"\\r\" ) rm \" ${ tmp_header_file } \" # Upload the actual bytes. curl \" ${ upload_url } \" \\ -H \"Content-Length: ${ NUM_BYTES } \" \\ -H \"X-Goog-Upload-Offset: 0\" \\ -H \"X-Goog-Upload-Command: upload, finalize\" \\ --data-binary \"@ ${ VIDEO_PATH } \" 2 > /dev/null > file_info.json file_uri = $( jq \".file.uri\" file_info.json ) echo file_uri = $file_uri state = $( jq \".file.state\" file_info.json ) echo state = $state # Ensure the state of the video is 'ACTIVE' while [[ \"( $state )\" = * \"PROCESSING\" * ]] ; do echo \"Processing video...\" sleep 5 # Get the file of interest to check state curl https://generativelanguage.googleapis.com/v1beta/files/ $name > file_info.json state = $( jq \".file.state\" file_info.json ) done # Now generate content using that file curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= $GOOGLE_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d '{ \"contents\": [{ \"parts\":[ {\"text\": \"Describe this video clip\"}, {\"file_data\":{\"mime_type\": \"video/mp4\", \"file_uri\": ' $file_uri '}}] }] }' 2 > /dev/null > response.json\n\ncat response.json echo jq \".candidates[].content.parts[].text\" response.json files.sh List files You can list all files uploaded using the File API and their URIs using files.list . echo \"My files: \" curl \"https://generativelanguage.googleapis.com/v1beta/files?key= $GOOGLE_API_KEY \" files.sh Delete files Files uploaded using the File API are automatically deleted after 2 days. You\ncan also manually delete them using files.delete . curl --request \"DELETE\" https://generativelanguage.googleapis.com/v1beta/files/ $name ?key = $GOOGLE_API_KEY files .sh What's next This guide shows how to upload image and video files using the File API and\nthen generate text outputs from image and video inputs. To learn more,\nsee the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started building with Gemini&#39;s multimodal capabilities in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/vision"
            ],
            "timestamp": "2025-01-01T18:50:53.388062",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/vision",
            "title": "Explore vision capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore vision capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore vision capabilities with the Gemini API Python Node.js Go REST The Gemini API is able to process images and videos, enabling a multitude of\n exciting developer use cases. Some of Gemini's vision capabilities include\n the ability to: Caption and answer questions about images Transcribe and reason over PDFs, including long documents up to 2 million token context window Describe, segment, and extract information from videos,\nincluding both visual frames and audio, up to 90 minutes long Detect objects in an image and return bounding box coordinates for them This tutorial demonstrates some possible ways to prompt the Gemini API with\nimages and video input, provides code examples,\nand outlines prompting best practices with multimodal vision capabilities.\nAll output is text-only. What's next This guide shows how to upload image and video files using the File API and\nthen generate text outputs from image and video inputs. To learn more,\nsee the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started building with Gemini&#39;s multimodal capabilities in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision"
            ],
            "timestamp": "2025-01-01T18:50:54.171341",
            "status_code": 200
        }
    ]
}