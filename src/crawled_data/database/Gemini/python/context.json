# Context caching 

{
    "base_url": "https://ai.google.dev/gemini-api/docs/caching?lang=python",
    "crawl_date": "2024-12-29T12:35:10.259679",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/caching?lang=python",
            "title": "Context caching  |  Gemini API  |  Google AI for Developers",
            "text_content": "Context caching  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Context caching Python Node.js Go REST In a typical AI workflow, you might pass the same input tokens over and over to\na model. Using the Gemini API context caching feature, you can pass some content\nto the model once, cache the input tokens, and then refer to the cached tokens\nfor subsequent requests. At certain volumes, using cached tokens is lower cost\nthan passing in the same corpus of tokens repeatedly. When you cache a set of tokens, you can choose how long you want the cache to\nexist before the tokens are automatically deleted. This caching duration is\ncalled the time to live (TTL). If not set, the TTL defaults to 1 hour. The\ncost for caching depends on the input token size and how long you want the\ntokens to persist. Context caching supports both Gemini 1.5 Pro and Gemini 1.5 Flash. Note: Context caching is only available for stable models with fixed versions\n(for example, gemini-1.5-pro-001 ). You must include the version postfix (for\nexample, the -001 in gemini-1.5-pro-001 ). When to use context caching Context caching is particularly well suited to scenarios where a substantial\ninitial context is referenced repeatedly by shorter requests. Consider using\ncontext caching for use cases such as: Chatbots with extensive system instructions Repetitive analysis of lengthy video files Recurring queries against large document sets Frequent code repository analysis or bug fixing How caching reduces costs Context caching is a paid feature designed to reduce overall operational costs.\nBilling is based on the following factors: Cache token count: The number of input tokens cached, billed at a\nreduced rate when included in subsequent prompts. Storage duration: The amount of time cached tokens are stored (TTL),\nbilled based on the TTL duration of cached token count. There are no minimum\nor maximum bounds on the TTL. Other factors: Other charges apply, such as for non-cached input tokens\nand output tokens. For up-to-date pricing details, refer to the Gemini API pricing\npage . To learn how to count tokens, see the Token\nguide . How to use context caching This section assumes that you've installed a Gemini SDK (or have curl installed)\nand that you've configured an API key, as shown in the quickstart . Generate content using a cache The following example shows how to generate content using a cached system\ninstruction and video file. import os import google.generativeai as genai from google.generativeai import caching import datetime import time # Get your API key from https://aistudio.google.com/app/apikey # and access your API key as an environment variable. # To authenticate from a Colab, see # https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb genai . configure ( api_key = os . environ [ 'API_KEY' ]) # Download video file # curl -O https://storage.googleapis.com/generativeai-downloads/data/Sherlock_Jr_FullMovie.mp4 path_to_video_file = 'Sherlock_Jr_FullMovie.mp4' # Upload the video using the Files API video_file = genai . upload_file ( path = path_to_video_file ) # Wait for the file to finish processing while video_file . state . name == 'PROCESSING' : print ( 'Waiting for video to be processed.' ) time . sleep ( 2 ) video_file = genai . get_file ( video_file . name ) print ( f 'Video processing complete: { video_file . uri } ' ) # Create a cache with a 5 minute TTL cache = caching . CachedContent . create ( model = 'models/gemini-1.5-flash-001' , display_name = 'sherlock jr movie' , # used to identify the cache system_instruction = ( 'You are an expert video analyzer, and your job is to answer ' 'the user \\' s query based on the video file you have access to.' ), contents = [ video_file ], ttl = datetime . timedelta ( minutes = 5 ), ) # Construct a GenerativeModel which uses the created cache. model = genai . GenerativeModel . from_cached_content ( cached_content = cache ) # Query the model response = model . generate_content ([( 'Introduce different characters in the movie by describing ' 'their personality, looks, and names. Also list the timestamps ' 'they were introduced for the first time.' )]) print ( response . usage_metadata ) # The output should look something like this: # # prompt_token_count: 696219 # cached_content_token_count: 696190 # candidates_token_count: 214 # total_token_count: 696433 print ( response . text ) List caches It's not possible to retrieve or view cached content, but you can retrieve\ncache metadata ( name , model , display_name , usage_metadata , create_time , update_time , and expire_time ). To list metadata for all uploaded caches, use CachedContent.list() : for c in caching . CachedContent . list (): print ( c ) Update a cache You can set a new ttl or expire_time for a cache. Changing anything else\nabout the cache isn't supported. The following example shows how to update the ttl of a cache using CachedContent.update() . import datetime cache . update ( ttl = datetime . timedelta ( hours = 2 )) Delete a cache The caching service provides a delete operation for manually removing content\nfrom the cache. The following example shows how to delete a cache using CachedContent.delete() . cache . delete () Additional considerations Keep the following considerations in mind when using context caching: The minimum input token count for context caching is 32,768, and the maximum is the same as the maximum for the given model. (For more on\ncounting tokens, see the Token guide ). The model doesn't make any distinction between cached tokens and regular\ninput tokens. Cached content is simply a prefix to the prompt. There are no special rate or usage limits on context caching; the standard\nrate limits for GenerateContent apply, and token limits include cached\ntokens. The number of cached tokens is returned in the usage_metadata from the\ncreate, get, and list operations of the cache service, and also in GenerateContent when using the cache. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-10-16 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use Context Caching in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/caching?lang=python",
                "https://ai.google.dev/gemini-api/docs/caching?lang=python",
                "https://ai.google.dev/gemini-api/docs/caching?lang=python",
                "https://ai.google.dev/gemini-api/docs/caching?lang=python",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching?lang=python"
            ],
            "timestamp": "2024-12-29T12:35:09.619288",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/caching",
            "title": "Context caching  |  Gemini API  |  Google AI for Developers",
            "text_content": "Context caching  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Context caching Python Node.js Go REST In a typical AI workflow, you might pass the same input tokens over and over to\na model. Using the Gemini API context caching feature, you can pass some content\nto the model once, cache the input tokens, and then refer to the cached tokens\nfor subsequent requests. At certain volumes, using cached tokens is lower cost\nthan passing in the same corpus of tokens repeatedly. When you cache a set of tokens, you can choose how long you want the cache to\nexist before the tokens are automatically deleted. This caching duration is\ncalled the time to live (TTL). If not set, the TTL defaults to 1 hour. The\ncost for caching depends on the input token size and how long you want the\ntokens to persist. Context caching supports both Gemini 1.5 Pro and Gemini 1.5 Flash. Note: Context caching is only available for stable models with fixed versions\n(for example, gemini-1.5-pro-001 ). You must include the version postfix (for\nexample, the -001 in gemini-1.5-pro-001 ). When to use context caching Context caching is particularly well suited to scenarios where a substantial\ninitial context is referenced repeatedly by shorter requests. Consider using\ncontext caching for use cases such as: Chatbots with extensive system instructions Repetitive analysis of lengthy video files Recurring queries against large document sets Frequent code repository analysis or bug fixing How caching reduces costs Context caching is a paid feature designed to reduce overall operational costs.\nBilling is based on the following factors: Cache token count: The number of input tokens cached, billed at a\nreduced rate when included in subsequent prompts. Storage duration: The amount of time cached tokens are stored (TTL),\nbilled based on the TTL duration of cached token count. There are no minimum\nor maximum bounds on the TTL. Other factors: Other charges apply, such as for non-cached input tokens\nand output tokens. For up-to-date pricing details, refer to the Gemini API pricing\npage . To learn how to count tokens, see the Token\nguide . How to use context caching This section assumes that you've installed a Gemini SDK (or have curl installed)\nand that you've configured an API key, as shown in the quickstart . Additional considerations Keep the following considerations in mind when using context caching: The minimum input token count for context caching is 32,768, and the maximum is the same as the maximum for the given model. (For more on\ncounting tokens, see the Token guide ). The model doesn't make any distinction between cached tokens and regular\ninput tokens. Cached content is simply a prefix to the prompt. There are no special rate or usage limits on context caching; the standard\nrate limits for GenerateContent apply, and token limits include cached\ntokens. The number of cached tokens is returned in the usage_metadata from the\ncreate, get, and list operations of the cache service, and also in GenerateContent when using the cache. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-10-16 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use Context Caching in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching"
            ],
            "timestamp": "2024-12-29T12:35:10.233566",
            "status_code": 200
        }
    ]
}