{
    "base_url": "https://ai.google.dev/gemini-api/docs/text-generation?lang=python",
    "base_path": "/gemini-api/docs/text-generation",
    "crawl_date": "2024-12-15T00:54:29.049024",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/text-generation?lang=python",
            "title": "Generate text using the Gemini API  |  Google AI for Developers",
            "text_content": "Generate text using the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Models Gemini API docs API Reference SDKs Pricing Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Generate text using the Gemini API Python Node.js Go REST The Gemini API can generate text output when provided text, images, video, and\naudio as input. This guide shows you how to generate text using the generateContent and streamGenerateContent methods. To learn about working with Gemini's vision and audio capabilities,\nrefer to the Vision and Audio guides. Before you begin: Set up your project and API key Before calling the Gemini API, you need to set up your project and configure\nyour API key. Expand to view how to set up your project and API key Tip: For complete setup instructions, see the Gemini API quickstart . Get and secure your API key You need an API key to call the Gemini API. If you don't already have one,\ncreate a key in Google AI Studio. Get an API key It's strongly recommended that you do not check an API key into your version\ncontrol system. You should store your API key in a secrets store such as Google Cloud Secret Manager . This tutorial assumes that you're accessing your API key as an environment\nvariable. Install the SDK package and configure your API key Note: This section shows setup steps for a local Python environment. To install\n      dependencies and configure your API key for Colab, see the Authentication quickstart notebook The Python SDK for the Gemini API is contained in the google-generativeai package. Install the dependency using pip: pip install -U google-generativeai Import the package and configure the service with your API key: import os import google.generativeai as genai genai . configure ( api_key = os . environ [ 'API_KEY' ]) Generate text from text-only input The simplest way to generate text using the Gemini API is to provide the model\nwith a single text-only input, as shown in this example: import google.generativeai as genai model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) response = model . generate_content ( \"Write a story about a magic backpack.\" ) print ( response . text ) text_generation . py In this case, the prompt (\"Write a story about a magic backpack\") doesn't\ninclude any output examples, system instructions, or formatting information.\nIt's a zero-shot approach. For some use cases, a one-shot or few-shot prompt\nmight produce output that's more aligned with user expectations. In some cases,\nyou might also want to provide system instructions to help the model\nunderstand the task or follow specific guidelines. Generate text from text-and-image input The Gemini API supports multimodal inputs that combine text with media files.\nThe following example shows how to generate text from text-and-image input: import google.generativeai as genai import PIL.Image model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) organ = PIL . Image . open ( media / \"organ.jpg\" ) response = model . generate_content ([ \"Tell me about this instrument\" , organ ]) print ( response . text ) text_generation . py As with text-only prompting, multimodal prompting can involve various approaches\nand refinements. Depending on the output from this example, you might want to\nadd steps to the prompt or be more specific in your instructions. To learn more,\nsee File prompting strategies . Generate a text stream By default, the model returns a response after completing the entire text\ngeneration process. You can achieve faster interactions by not waiting for the\nentire result, and instead use streaming to handle partial results. The following example shows how to implement streaming using the streamGenerateContent method to\ngenerate text from a text-only input prompt. import google.generativeai as genai model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) response = model . generate_content ( \"Write a story about a magic backpack.\" , stream = True ) for chunk in response : print ( chunk . text ) print ( \"_\" * 80 ) text_generation . py Build an interactive chat You can use the Gemini API to build interactive chat experiences for your users.\nUsing the chat feature of the API lets you collect multiple rounds of questions\nand responses, allowing users to step incrementally toward answers or get help\nwith multipart problems. This feature is ideal for applications that require\nongoing communication, such as chatbots, interactive tutors, or customer support\nassistants. The following code example shows a basic chat implementation: import google.generativeai as genai model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) chat = model . start_chat ( history = [ { \"role\" : \"user\" , \"parts\" : \"Hello\" }, { \"role\" : \"model\" , \"parts\" : \"Great to meet you. What would you like to know?\" }, ] ) response = chat . send_message ( \"I have 2 dogs in my house.\" ) print ( response . text ) response = chat . send_message ( \"How many paws are in my house?\" ) print ( response . text ) chat . py Enable chat streaming You can also use streaming with chat, as shown in the following example: import google.generativeai as genai model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) chat = model . start_chat ( history = [ { \"role\" : \"user\" , \"parts\" : \"Hello\" }, { \"role\" : \"model\" , \"parts\" : \"Great to meet you. What would you like to know?\" }, ] ) response = chat . send_message ( \"I have 2 dogs in my house.\" , stream = True ) for chunk in response : print ( chunk . text ) print ( \"_\" * 80 ) response = chat . send_message ( \"How many paws are in my house?\" , stream = True ) for chunk in response : print ( chunk . text ) print ( \"_\" * 80 ) print ( chat . history ) chat . py Configure text generation Every prompt you send to the model includes parameters that\ncontrol how the model generates responses. You can use GenerationConfig to\nconfigure these parameters. If you don't configure the parameters, the model\nuses default options, which can vary by model. The following example shows how to configure several of the available options. import google.generativeai as genai model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) response = model . generate_content ( \"Tell me a story about a magic backpack.\" , generation_config = genai . types . GenerationConfig ( # Only one candidate for now. candidate_count = 1 , stop_sequences = [ \"x\" ], max_output_tokens = 20 , temperature = 1.0 , ), ) print ( response . text ) configure_model_parameters . py candidateCount specifies the number of generated responses to return.\nCurrently, this value can only be set to 1. If unset, this will default to 1. stopSequences specifies the set of character sequences (up to 5) that will\nstop output generation. If specified, the API will stop at the first appearance\nof a stop_sequence . The stop sequence won't be included as part of the\nresponse. maxOutputTokens sets the maximum number of tokens to include in a candidate. temperature controls the randomness of the output. Use higher values for more\ncreative responses, and lower values for more deterministic responses. Values\ncan range from [0.0, 2.0]. You can also configure individual calls to generateContent : response = model . generate_content ( 'Write a story about a magic backpack.' , generation_config = genai . GenerationConfig ( max_output_tokens = 1000 , temperature = 0.1 , ) ) Any values set on the individual call override values on the model constructor. What's next Now that you have explored the basics of the Gemini API, you might want to\ntry: Vision understanding : Learn how to use\nGemini's native vision understanding to process images and videos. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Audio understanding : Learn how to use\nGemini's native audio understanding to process audio files. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started building chat and text generation apps with the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/text-generation?lang=python",
                "https://ai.google.dev/gemini-api/docs/text-generation?lang=python",
                "https://ai.google.dev/gemini-api/docs/text-generation?lang=python",
                "https://ai.google.dev/gemini-api/docs/text-generation?lang=python",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation?lang=python"
            ],
            "timestamp": "2024-12-15T00:54:28.632516",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/text-generation",
            "title": "Generate text using the Gemini API  |  Google AI for Developers",
            "text_content": "Generate text using the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Models Gemini API docs API Reference SDKs Pricing Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Generate text using the Gemini API Python Node.js Go REST The Gemini API can generate text output when provided text, images, video, and\naudio as input. This guide shows you how to generate text using the generateContent and streamGenerateContent methods. To learn about working with Gemini's vision and audio capabilities,\nrefer to the Vision and Audio guides. What's next Now that you have explored the basics of the Gemini API, you might want to\ntry: Vision understanding : Learn how to use\nGemini's native vision understanding to process images and videos. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Audio understanding : Learn how to use\nGemini's native audio understanding to process audio files. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started building chat and text generation apps with the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation"
            ],
            "timestamp": "2024-12-15T00:54:29.045450",
            "status_code": 200
        }
    ]
}

{
    "base_url": "https://ai.google.dev/gemini-api/docs/models/gemini",
    "base_path": "/gemini-api/docs/models/gemini",
    "crawl_date": "2024-12-15T00:56:03.252051",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/models/gemini",
            "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
            "text_content": "Gemini models  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Models Gemini API docs API Reference SDKs Pricing Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Gemini models 2.0 Flash experiment Our newest multimodal model, with next generation features and improved\n      capabilities Input audio, images, video, and text — get text, image, and audio responses Features low-latency conversational interactions with our Multimodal Live API 1.5 Flash spark Our most balanced multimodal model with great performance\n      for most tasks Input audio, images, video, and text, get text responses Generate code, extract data, edit text, and more Best for tasks balancing performance and cost 1.5 Pro Our best performing multimodal model with features for a wide variety of reasoning tasks Input audio, images, video, and text, get text responses Generate code, extract data, edit text, and more For when you need a boost in performance Model variants The Gemini API offers different models that are optimized for specific use\ncases. Here's a brief overview of Gemini variants that are available: Model variant Input(s) Output Optimized for Gemini 2.0 Flash gemini-2.0-flash-exp Audio, images, videos, and text Text, images (coming soon), and audio (coming soon) Next generation features, speed, and multimodal generation for a diverse variety of tasks Gemini 1.5 Flash gemini-1.5-flash Audio, images, videos, and text Text Fast and versatile performance across a diverse variety of tasks Gemini 1.5 Flash-8B gemini-1.5-flash-8b Audio, images, videos, and text Text High volume and lower intelligence tasks Gemini 1.5 Pro gemini-1.5-pro Audio, images, videos, and text Text Complex reasoning tasks requiring more intelligence (Deprecated on 2/15/2025) Gemini 1.0 Pro gemini-1.0-pro Text Text Natural language tasks, multi-turn text and code chat, and code\n      generation Text Embedding text-embedding-004 Text Text embeddings Measuring the relatedness of text strings AQA aqa Text Text Providing source-grounded answers to questions (Experimental) Gemini 2.0 Flash Important: Gemini 2.0 Flash is available as an experimental preview,\n    and some supported features are not currently available. If you need a model\n    for production-level code, use one of our 1.5 models instead. Gemini 2.0 Flash delivers next-gen features and improved capabilities,\n    including superior speed, native tool use, multimodal generation, and a 1M token\n    context window. Learn more about Gemini 2.0 Flash in our overview page . Try in Google AI Studio Model details Property Description id_card Model code models/gemini-2.0-flash-exp save Supported data types Inputs Audio, images, video, and text Output Audio (coming soon), images (coming soon), and text token_auto Token limits [*] Input token limit 1,048,576 Output token limit 8,192 swap_driving_apps_wheel Rate limits [**] 10 RPM 4 million TPM 1,500 RPD handyman Capabilities Structured outputs Supported Caching Not supported Tuning Not supported Function calling Supported Code execution Supported Search Supported Image generation Supported Native tool use Supported Audio generation Supported 123 Versions Read the model version patterns for more details. Latest: gemini-2.0-flash-exp calendar_month Latest update December 2024 Gemini 1.5 Flash Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across\n    diverse tasks. Try in Google AI Studio Model details Property Description id_card Model code models/gemini-1.5-flash save Supported data types Inputs Audio, images, video, and text Output Text token_auto Token limits [*] Input token limit 1,048,576 Output token limit 8,192 movie_info Audio/visual specs Maximum number of images per prompt 3,600 Maximum video length 1 hour Maximum audio length Approximately 9.5 hours swap_driving_apps_wheel Rate limits [**] Free: 15 RPM 1 million TPM 1,500 RPD Pay-as-you-go: 2,000 RPM 4 million TPM handyman Capabilities System instructions Supported JSON mode Supported JSON schema Supported Adjustable safety settings Supported Caching Supported Tuning Supported Function calling Supported Code execution Supported Bidirectional streaming Not supported 123 Versions Read the model version patterns for more details. Latest: gemini-1.5-flash-latest Latest stable: gemini-1.5-flash Stable: gemini-1.5-flash-001 gemini-1.5-flash-002 calendar_month Latest update September 2024 Gemini 1.5 Flash-8B Gemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. Try in Google AI Studio Model details Property Description id_card Model code models/gemini-1.5-flash-8b save Supported data types Inputs Audio, images, video, and text Output Text token_auto Token limits [*] Input token limit 1,048,576 Output token limit 8,192 movie_info Audio/visual specs Maximum number of images per prompt 3,600 Maximum video length 1 hour Maximum audio length Approximately 9.5 hours swap_driving_apps_wheel Rate limits [**] Free: 15 RPM 1 million TPM 1,500 RPD Pay-as-you-go: 4,000 RPM 4 million TPM handyman Capabilities System instructions Supported JSON mode Supported JSON schema Supported Adjustable safety settings Supported Caching Supported Tuning Supported Function calling Supported Code execution Supported Bidirectional streaming Not supported 123 Versions Read the model version patterns for more details. Latest: gemini-1.5-flash-8b-latest Latest stable: gemini-1.5-flash-8b Stable: gemini-1.5-flash-8b-001 calendar_month Latest update October 2024 Gemini 1.5 Pro Gemini 1.5 Pro is a mid-size multimodal model that is optimized for\n    a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data\n    at once, including 2 hours of video, 19 hours of audio, codebases with\n    60,000 lines of code, or 2,000 pages of text. Try in Google AI Studio Model details Property Description id_card Model code models/gemini-1.5-pro save Supported data types Inputs Audio, images, video, and text Output Text token_auto Token limits [*] Input token limit 2,097,152 Output token limit 8,192 movie_info Audio/visual specs Maximum number of images per prompt 7,200 Maximum video length 2 hours Maximum audio length Approximately 19 hours swap_driving_apps_wheel Rate limits [**] Free: 2 RPM 32,000 TPM 50 RPD Pay-as-you-go: 1,000 RPM 4 million TPM handyman Capabilities System instructions Supported JSON mode Supported JSON schema Supported Adjustable safety settings Supported Caching Supported Tuning Not supported Function calling Supported Code execution Supported Bidirectional streaming Not supported 123 Versions Read the model version patterns for more details. Latest: gemini-1.5-pro-latest Latest stable: gemini-1.5-pro Stable: gemini-1.5-pro-001 gemini-1.5-pro-002 calendar_month Latest update September 2024 (Deprecated) Gemini 1.0 Pro Important: 1.0 Pro is deprecated and will be removed on\n  February 15, 2025. Use 1.5 Pro or 1.5 Flash instead. Gemini 1.0 Pro is an NLP model that handles tasks like multi-turn text and\n    code chat, and code generation. Try in Google AI Studio Model details Property Description id_card Model code models/gemini-1.0-pro save Supported data types Input Text Output Text swap_driving_apps_wheel Rate limits [**] Free: 15 RPM 32,000 TPM 1,500 RPD Pay-as-you-go: 360 RPM 120,000 TPM 30,000 RPD handyman Capabilities System instructions Not supported JSON mode Not supported JSON schema Not supported Adjustable safety settings Supported Caching Not supported Tuning Supported Function calling Supported Function calling configuration Not supported Code execution Not supported Bidirectional streaming Not supported 123 Versions Latest: gemini-1.0-pro-latest Latest stable: gemini-1.0-pro Stable: gemini-1.0-pro-001 calendar_month Latest update February 2024 Note: gemini-pro is an alias for gemini-1.0-pro . Text Embedding and Embedding Text Embedding Text embeddings are used to measure the relatedness of strings and are widely used in\n    many AI applications. text-embedding-004 achieves a stronger retrieval performance and outperforms existing models with comparable dimensions, on the standard MTEB embedding benchmarks. Model details Property Description id_card Model code Gemini API models/text-embedding-004 save Supported data types Input Text Output Text embeddings token_auto Token limits [*] Input token limit 2,048 Output dimension size 768 swap_driving_apps_wheel Rate limits [**] 1,500 requests per minute encrypted Adjustable safety settings Not supported calendar_month Latest update April 2024 Embedding Note: Text Embedding is the newer version of the Embedding model. If\n    you're creating a new project, use Text Embedding. You can use the Embedding model to generate text embeddings for\n    input text. The Embedding model is optimized for creating embeddings with 768 dimensions\n    for text of up to 2,048 tokens. Embedding model details Property Description id_card Model code models/embedding-001 save Supported data types Input Text Output Text embeddings token_auto Token limits [*] Input token limit 2,048 Output dimension size 768 swap_driving_apps_wheel Rate limits [**] 1,500 requests per minute encrypted Adjustable safety settings Not supported calendar_month Latest update December 2023 AQA You can use the AQA model to perform Attributed Question-Answering (AQA)–related tasks over a document, corpus, or a set of passages. The AQA\n    model returns answers to questions that are grounded in provided sources,\n    along with estimating answerable probability. Model details Property Description id_card Model code models/aqa save Supported data types Input Text Output Text language Supported language English token_auto Token limits [*] Input token limit 7,168 Output token limit 1,024 swap_driving_apps_wheel Rate limits [**] 1,500 requests per minute encrypted Adjustable safety settings Supported calendar_month Latest update December 2023 See the examples to explore the capabilities of these model\nvariations. [*]   A token is equivalent to about 4 characters for Gemini models. 100 tokens\n  are about 60-80 English words. [**] RPM: Requests per minute TPM: Tokens per minute RPD: Requests per day TPD: Tokens per day Due to capacity limitations, specified maximum rate limits are not\n  guaranteed. Model version name patterns Gemini models are available in either preview or stable versions. In your\ncode, you can use one of the following model name formats to specify which model\nand version you want to use. Latest: Points to the cutting-edge version of the model for a specified\ngeneration and variation. The underlying model is updated regularly and might\nbe a preview version. Only exploratory testing apps and prototypes should\nuse this alias. To specify the latest version, use the following pattern: <model>-<generation>-<variation>-latest . For example, gemini-1.0-pro-latest . Latest stable: Points to the most recent stable version released for the\nspecified model generation and variation. To specify the latest stable version, use the following pattern: <model>-<generation>-<variation> . For example, gemini-1.0-pro . Stable: Points to a specific stable model. Stable models don't change.\nMost production apps should use a specific stable model. To specify a stable version, use the following pattern: <model>-<generation>-<variation>-<version> . For example, gemini-1.0-pro-001 . Experimental: Points to an experimental model available in Preview,\nas defined in the Terms ,\nmeaning it is not for production use. We release experimental models\nto gather feedback, get our latest updates into the hands of developers\nquickly, and highlight the pace of innovation happening at Google. What\nwe learn from experimental launches informs how we release models more\nwidely. An experimental model can be swapped for another without prior\nnotice. We don't guarantee that an experimental model will become a\nstable model in the future. To specify an experimental version, use the following pattern: <model>-<generation>-<variation>-<version> . For example, gemini-exp-1121 . Available languages Gemini models are trained to work with the following languages: Arabic ( ar ) Bengali ( bn ) Bulgarian ( bg ) Chinese simplified and traditional ( zh ) Croatian ( hr ) Czech ( cs ) Danish ( da ) Dutch ( nl ) English ( en ) Estonian ( et ) Finnish ( fi ) French ( fr ) German ( de ) Greek ( el ) Hebrew ( iw ) Hindi ( hi ) Hungarian ( hu ) Indonesian ( id ) Italian ( it ) Japanese ( ja ) Korean ( ko ) Latvian ( lv ) Lithuanian ( lt ) Norwegian ( no ) Polish ( pl ) Portuguese ( pt ) Romanian ( ro ) Russian ( ru ) Serbian ( sr ) Slovak ( sk ) Slovenian ( sl ) Spanish ( es ) Swahili ( sw ) Swedish ( sv ) Thai ( th ) Turkish ( tr ) Ukrainian ( uk ) Vietnamese ( vi ) Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn about Google&#39;s most advanced AI models, the Gemini model family, including Gemini 1.5 Flash, Gemini 1.5 Pro, and more",
            "links": [
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/models/gemini"
            ],
            "timestamp": "2024-12-15T00:56:02.718124",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
            "title": "Gemini 2.0 (experimental)  |  Gemini API  |  Google AI for Developers",
            "text_content": "Gemini 2.0 (experimental)  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Models Gemini API docs API Reference SDKs Pricing Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Gemini 2.0 (experimental) Gemini 2.0 Flash is now available as an experimental preview release through\nthe Gemini Developer API and Google AI Studio. The model introduces new features\nand enhanced core capabilities: Multimodal Live API: This new API helps you create real-time vision and\naudio streaming applications with tool use. Speed and performance: Gemini 2.0 has a significantly improved time to\nfirst token (TTFT) over 1.5 Flash. Quality: Better performance across most benchmarks than Gemini 1.5 Pro. Improved agentic capabilities: Gemini 2.0 delivers improvements to\nmultimodal understanding, coding, complex instruction following, and\nfunction calling. New modalities: Gemini 2.0 introduces native image generation and\ncontrollable text-to-speech capabilities. To provide a better developer experience, we're also shipping a new SDK . For Gemini 2.0 technical details, see Gemini models . Note: Image and audio generation are in private experimental release, under\nallowlist. All other features are public experimental. Google Gen AI SDK (experimental) The new Google Gen AI SDK provides a unified interface to Gemini 2.0 through\nboth the Gemini Developer API and the Gemini API on Vertex AI. With a few\nexceptions, code that runs on one platform will run on both. This means that you\ncan prototype an application using the Developer API and then migrate the\napplication to Vertex AI without rewriting your code. The Gen AI SDK also supports the Gemini 1.5 models. The new SDK is available in Python and Go, with Java and JavaScript coming soon. You can start using the SDK as shown below. Install the new SDK: pip install google-genai Then import the library, initialize a client, and generate content: from google import genai client = genai . Client ( api_key = \"YOUR_API_KEY\" ) response = client . models . generate_content ( model = 'gemini-2.0-flash-exp' , contents = 'How does AI work?' ) print ( response . text ) (Optional) Set environment variables Alternatively, you can initialize the client using environment variables. First\nset the appropriate values and export the variables: # Replace `YOUR_API_KEY` with your API key. export GOOGLE_API_KEY = YOUR_API_KEY Then you can initialize the client without any args: client = genai . Client () Python developers can also try out the Getting Started\nnotebook in the Cookbook . Multimodal Live API To try a tutorial that lets you use your voice and camera to talk to Gemini\n  through the Multimodal Live API, see the websocket-demo-app tutorial . The Multimodal Live API enables low-latency bidirectional voice and video\ninteractions with Gemini. Using the Multimodal Live API, you can provide end\nusers with the experience of natural, human-like voice conversations, and with\nthe ability to interrupt the model's responses using voice commands. The model\ncan process text, audio, and video input, and it can provide text and audio\noutput. The Multimodal Live API is available in the Gemini API as the BidiGenerateContent method and is built on WebSockets . from google import genai client = genai . Client ( http_options = { 'api_version' : 'v1alpha' }) model_id = \"gemini-2.0-flash-exp\" config = { \"response_modalities\" : [ \"TEXT\" ]} async with client . aio . live . connect ( model = model_id , config = config ) as session : message = \"Hello? Gemini, are you there?\" print ( \"> \" , message , \" \\n \" ) await session . send ( message , end_of_turn = True ) async for response in session . receive (): print ( response . text ) Key capabilities: Multimodality: The model can see, hear, and speak. Low-latency real-time interaction: Provides fast responses. Session memory: The model retains memory of all interactions within a\nsingle session, recalling previously heard or seen information. Support for function calling, code execution, and Search as a tool: Enables integration with external services and data sources. Automated voice activity detection (VAD): The model can accurately\nrecognize when the user begins and stops speaking. This allows for natural,\nconversational interactions and empowers users to interrupt the model at any\ntime. Language: English only Limitations: Both audio inputs and audio outputs negatively impact the model's ability to\nuse function calling. To learn more the API's capabilities and limitations, see the Multimodal Live API reference guide . You can try the Multimodal Live API in Google AI Studio . To start developing,\nyou can try the web console (written in React). For Python developers, try the starter code\n( notebook ,\nand .py file ).\nYou may find the notebook easiest to get started with, but the live API works\nbest when run from your terminal. Search as a tool Using Grounding with Google Search, you can improve the accuracy and recency of\nresponses from the model. Starting with Gemini 2.0, Google Search is available\nas a tool. This means that the model can decide when to use Google Search. The\nfollowing example shows how to configure Search as a tool. from google import genai from google.genai.types import Tool , GenerateContentConfig , GoogleSearch client = genai . Client () model_id = \"gemini-2.0-flash-exp\" google_search_tool = Tool ( google_search = GoogleSearch () ) response = client . models . generate_content ( model = model_id , contents = \"When is the next total solar eclipse in the United States?\" , config = GenerateContentConfig ( tools = [ google_search_tool ], response_modalities = [ \"TEXT\" ], ) ) for each in response . candidates [ 0 ] . content . parts : print ( each . text ) # Example response: # The next total solar eclipse visible in the contiguous United States will be on ... # To get grounding metadata as web content. print ( response . candidates [ 0 ] . grounding_metadata . search_entry_point . rendered_content ) The Search-as-a-tool functionality also enables multi-turn searches and\nmulti-tool queries (for example, combining Grounding with Google Search and code\nexecution). Search as a tool enables complex prompts and workflows that require planning,\nreasoning, and thinking: Grounding to enhance factuality and recency and provide more accurate answers Retrieving artifacts from the web to do further analysis on Finding relevant images, videos, or other media to assist in multimodal\nreasoning or generation tasks Coding, technical troubleshooting, and other specialized tasks Finding region-specific information or assisting in translating content\naccurately Finding relevant websites for further browsing You can get started by trying the Search tool notebook . Improved tools Gemini 2.0 introduces improvements to function calling and tools that provide\nbetter support for agentic experiences. Compositional function calling Gemini 2.0 supports a new function calling capability: compositional function calling . Compositional function calling enables the\nGemini API to invoke multiple user-defined functions automatically in the\nprocess of generating a response. For example, to respond to the prompt \"Get the temperature in my current location\" , the Gemini API might invoke both\na get_current_location() function and a get_weather() function that takes\nthe location as a parameter. Compositional function calling with code execution requires bidirectional\nstreaming and is only supported by the new Multimodal Live API. Here's an\nexample showing how you might use compositional function calling, code\nexecution, and the Multimodal Live API together: Note: The run() function declaration, which handles the asynchronous websocket\nsetup, is omitted for brevity. turn_on_the_lights_schema = { 'name' : 'turn_on_the_lights' } turn_off_the_lights_schema = { 'name' : 'turn_off_the_lights' } prompt = \"\"\" Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights? \"\"\" tools = [ { 'code_execution' : {}}, { 'function_declarations' : [ turn_on_the_lights_schema , turn_off_the_lights_schema ]} ] await run ( prompt , tools = tools , modality = \"AUDIO\" ) Python developers can try this out in the Live API Tool Use\nnotebook . Multi-tool use With Gemini 2.0, you can enable multiple tools at the same time, and the model\nwill decide when to call them. Here's an example that enables two tools,\nGrounding with Google Search and code execution, in a request using the\nMultimodal Live API. Note: The run() function declaration, which handles the asynchronous websocket\nsetup, is omitted for brevity. prompt = \"\"\" Hey, I need you to do three things for me. 1. Turn on the lights. 2. Then compute the largest prime palindrome under 100000. 3. Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024. Thanks! \"\"\" tools = [ { 'google_search' : {}}, { 'code_execution' : {}}, { 'function_declarations' : [ turn_on_the_lights_schema , turn_off_the_lights_schema ]} ] await run ( prompt , tools = tools , modality = \"AUDIO\" ) Python developers can try this out in the Live API Tool Use\nnotebook . Bounding box detection In this experimental launch, we are providing developers with a powerful tool\nfor object detection and localization within images and video. By accurately\nidentifying and delineating objects with bounding boxes, developers can unlock a\nwide range of applications and enhance the intelligence of their projects. Key Benefits: Simple: Integrate object detection capabilities into your applications\nwith ease, regardless of your computer vision expertise. Customizable: Produce bounding boxes based on custom instructions (e.g. \"I\nwant to see bounding boxes of all the green objects in this image\"), without\nhaving to train a custom model. Technical Details: Input: Your prompt and associated images or video frames. Output: Bounding boxes in the [y_min, x_min, y_max, x_max] format. The\ntop left corner is the origin. The x and y axis go horizontally and\nvertically, respectively. Coordinate values are normalized to 0-1000 for every\nimage. Visualization: AI Studio users will see bounding boxes plotted within the\nUI. Vertex AI users should visualize their bounding boxes through custom\nvisualization code. For Python developers, try the 2D spatial understanding\nnotebook or the experimental 3D pointing\nnotebook . Speech generation (early access/allowlist) Gemini 2.0 supports a new multimodal generation capability: text to speech.\nUsing the text-to-speech capability, you can prompt the model to generate high\nquality audio output that sounds like a human voice ( say \"hi everyone\" ), and\nyou can further refine the output by steering the voice. Image generation (early access/allowlist) Gemini 2.0 supports the ability to output text with in-line images. This lets\nyou use Gemini to conversationally edit images or generate multimodal outputs\n(for example, a blog post with text and images in a single turn). Previously\nthis would have required stringing together multiple models. Image generation is available as a private experimental release. It supports the\nfollowing modalities and capabilities: Text to image Example prompt: \"Generate an image of the Eiffel tower with fireworks in\nthe background.\" Text to image(s) and text (interleaved) Example prompt: \"Generate an illustrated recipe for a paella.\" Image(s) and text to image(s) and text (interleaved) Example prompt: (With an image of a furnished room) \"What other color\nsofas would work in my space? can you update the image?\" Image editing (text and image to image) Example prompt: \"Edit this image to make it look like a cartoon\" Example prompt: [image of a cat] + [image of a pillow] + \"Create a cross\nstitch of my cat on this pillow.\" Multi-turn image editing (chat) Example prompts: [upload an image of a blue car.] \"Turn this car into a\nconvertible.\" \"Now change the color to yellow.\" Watermarking (synthID) Limitations: Generation of people and editing of uploaded images of people are not allowed. For best performance, use the following languages: EN, es-MX, ja-JP, zh-CN,\nhi-IN. Image generation does not support audio or video inputs. Image generation may not always trigger: The model may output text only. Try asking for image outputs explicitly\n(e.g. \"generate an image\", \"provide images as you go along\", \"update the\nimage\"). The model may stop generating partway through. Try again or try a different\nprompt. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-13 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2"
            ],
            "timestamp": "2024-12-15T00:56:03.246153",
            "status_code": 200
        }
    ]
}

{
    "base_url": "https://ai.google.dev/gemini-api/docs/models/experimental-models",
    "base_path": "/gemini-api/docs/models/experimental-models",
    "crawl_date": "2024-12-15T00:57:02.204693",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/models/experimental-models",
            "title": "Experimental models  |  Gemini API  |  Google AI for Developers",
            "text_content": "Experimental models  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Models Gemini API docs API Reference SDKs Pricing Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Experimental models In addition to the base models , the Gemini API\noffers experimental models available in Preview, as defined in the Terms , meaning it is not for\nproduction use. We release experimental models to gather feedback, get our\nlatest updates into the hands of developers quickly, and highlight the pace of\ninnovation happening at Google. What we learn from experimental launches informs\nhow we release models more widely. An experimental model can be swapped for\nanother without prior notice. We don't guarantee that an experimental model will\nbecome a stable model in the future. Use an experimental model Important: Support for an experimental model can change at any time. The Gemini API's experimental models are available for all users. You can use\nthe experimental models either in your code directly using the Gemini API, or\nyou can use an experimental model in Google AI Studio : Gemini API To use an experimental model, specify the model code when you initialize the\ngenerative model. For example: model = genai . GenerativeModel ( model_name = \"gemini-2.0-flash-exp\" ) AI Studio Select the model code of the experimental model you want to use in the Model drop-down menu in the Settings pane. Experimental models are\nlabeled Preview in the drop-down menu. Available models Model code Base model Highlights Release date gemini-2.0-flash-exp Gemini 2.0 Flash Next generation features, superior speed, native tool use, and multimodal generation December 11, 2024 gemini-exp-1206 Gemini Quality improvements, celebrate 1 year of Gemini December 6th, 2024 gemini-exp-1121 Gemini Improved coding, reasoning, and vision capabilities November 21, 2024 learnlm-1.5-pro-experimental LearnLM 1.5 Pro Experimental Inputs: Audio, images, videos, and text Output: Text November 19, 2024 Note: Support for individual experimental models is not permanent or long-term,\nand can be turned off without prior warning. Experimental models cannot\nbe used in production code. Provide feedback You can provide feedback on the Gemini API's experimental models using our developer forum . Previous models As new versions or stable releases become available, we remove and replace\nexperimental models. You can find the previous experimental models we released in\nthe following section along with the replacement version: Model code Base model Replacement version gemini-exp-1114 Gemini gemini-exp-1114 gemini-1.5-pro-exp-0827 Gemini 1.5 Pro gemini-1.5-pro-002 gemini-1.5-pro-exp-0801 Gemini 1.5 Pro gemini-1.5-pro-002 gemini-1.5-flash-8b-exp-0924 Gemini 1.5 Flash-8B gemini-1.5-flash-8b gemini-1.5-flash-8b-exp-0827 Gemini 1.5 Flash-8B gemini-1.5-flash-8b Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models"
            ],
            "timestamp": "2024-12-15T00:57:02.199303",
            "status_code": 200
        }
    ]
}

{
    "base_url": "https://ai.google.dev/gemini-api/docs/caching?lang=python",
    "base_path": "/gemini-api/docs/caching",
    "crawl_date": "2024-12-15T00:57:45.738938",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/caching?lang=python",
            "title": "Context caching  |  Gemini API  |  Google AI for Developers",
            "text_content": "Context caching  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Models Gemini API docs API Reference SDKs Pricing Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Context caching Python Node.js Go REST In a typical AI workflow, you might pass the same input tokens over and over to\na model. Using the Gemini API context caching feature, you can pass some content\nto the model once, cache the input tokens, and then refer to the cached tokens\nfor subsequent requests. At certain volumes, using cached tokens is lower cost\nthan passing in the same corpus of tokens repeatedly. When you cache a set of tokens, you can choose how long you want the cache to\nexist before the tokens are automatically deleted. This caching duration is\ncalled the time to live (TTL). If not set, the TTL defaults to 1 hour. The\ncost for caching depends on the input token size and how long you want the\ntokens to persist. Context caching supports both Gemini 1.5 Pro and Gemini 1.5 Flash. Note: Context caching is only available for stable models with fixed versions\n(for example, gemini-1.5-pro-001 ). You must include the version postfix (for\nexample, the -001 in gemini-1.5-pro-001 ). When to use context caching Context caching is particularly well suited to scenarios where a substantial\ninitial context is referenced repeatedly by shorter requests. Consider using\ncontext caching for use cases such as: Chatbots with extensive system instructions Repetitive analysis of lengthy video files Recurring queries against large document sets Frequent code repository analysis or bug fixing How caching reduces costs Context caching is a paid feature designed to reduce overall operational costs.\nBilling is based on the following factors: Cache token count: The number of input tokens cached, billed at a\nreduced rate when included in subsequent prompts. Storage duration: The amount of time cached tokens are stored (TTL),\nbilled based on the TTL duration of cached token count. There are no minimum\nor maximum bounds on the TTL. Other factors: Other charges apply, such as for non-cached input tokens\nand output tokens. For up-to-date pricing details, refer to the Gemini API pricing\npage . To learn how to count tokens, see the Token\nguide . How to use context caching This section assumes that you've installed a Gemini SDK (or have curl installed)\nand that you've configured an API key, as shown in the quickstart . Generate content using a cache The following example shows how to generate content using a cached system\ninstruction and video file. import os import google.generativeai as genai from google.generativeai import caching import datetime import time # Get your API key from https://aistudio.google.com/app/apikey # and access your API key as an environment variable. # To authenticate from a Colab, see # https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb genai . configure ( api_key = os . environ [ 'API_KEY' ]) # Download video file # curl -O https://storage.googleapis.com/generativeai-downloads/data/Sherlock_Jr_FullMovie.mp4 path_to_video_file = 'Sherlock_Jr_FullMovie.mp4' # Upload the video using the Files API video_file = genai . upload_file ( path = path_to_video_file ) # Wait for the file to finish processing while video_file . state . name == 'PROCESSING' : print ( 'Waiting for video to be processed.' ) time . sleep ( 2 ) video_file = genai . get_file ( video_file . name ) print ( f 'Video processing complete: { video_file . uri } ' ) # Create a cache with a 5 minute TTL cache = caching . CachedContent . create ( model = 'models/gemini-1.5-flash-001' , display_name = 'sherlock jr movie' , # used to identify the cache system_instruction = ( 'You are an expert video analyzer, and your job is to answer ' 'the user \\' s query based on the video file you have access to.' ), contents = [ video_file ], ttl = datetime . timedelta ( minutes = 5 ), ) # Construct a GenerativeModel which uses the created cache. model = genai . GenerativeModel . from_cached_content ( cached_content = cache ) # Query the model response = model . generate_content ([( 'Introduce different characters in the movie by describing ' 'their personality, looks, and names. Also list the timestamps ' 'they were introduced for the first time.' )]) print ( response . usage_metadata ) # The output should look something like this: # # prompt_token_count: 696219 # cached_content_token_count: 696190 # candidates_token_count: 214 # total_token_count: 696433 print ( response . text ) List caches It's not possible to retrieve or view cached content, but you can retrieve\ncache metadata ( name , model , display_name , usage_metadata , create_time , update_time , and expire_time ). To list metadata for all uploaded caches, use CachedContent.list() : for c in caching . CachedContent . list (): print ( c ) Update a cache You can set a new ttl or expire_time for a cache. Changing anything else\nabout the cache isn't supported. The following example shows how to update the ttl of a cache using CachedContent.update() . import datetime cache . update ( ttl = datetime . timedelta ( hours = 2 )) Delete a cache The caching service provides a delete operation for manually removing content\nfrom the cache. The following example shows how to delete a cache using CachedContent.delete() . cache . delete () Additional considerations Keep the following considerations in mind when using context caching: The minimum input token count for context caching is 32,768, and the maximum is the same as the maximum for the given model. (For more on\ncounting tokens, see the Token guide ). The model doesn't make any distinction between cached tokens and regular\ninput tokens. Cached content is simply a prefix to the prompt. There are no special rate or usage limits on context caching; the standard\nrate limits for GenerateContent apply, and token limits include cached\ntokens. The number of cached tokens is returned in the usage_metadata from the\ncreate, get, and list operations of the cache service, and also in GenerateContent when using the cache. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-10-16 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use Context Caching in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/caching?lang=python",
                "https://ai.google.dev/gemini-api/docs/caching?lang=python",
                "https://ai.google.dev/gemini-api/docs/caching?lang=python",
                "https://ai.google.dev/gemini-api/docs/caching?lang=python",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching?lang=python"
            ],
            "timestamp": "2024-12-15T00:57:45.314902",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/caching",
            "title": "Context caching  |  Gemini API  |  Google AI for Developers",
            "text_content": "Context caching  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Models Gemini API docs API Reference SDKs Pricing Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Context caching Python Node.js Go REST In a typical AI workflow, you might pass the same input tokens over and over to\na model. Using the Gemini API context caching feature, you can pass some content\nto the model once, cache the input tokens, and then refer to the cached tokens\nfor subsequent requests. At certain volumes, using cached tokens is lower cost\nthan passing in the same corpus of tokens repeatedly. When you cache a set of tokens, you can choose how long you want the cache to\nexist before the tokens are automatically deleted. This caching duration is\ncalled the time to live (TTL). If not set, the TTL defaults to 1 hour. The\ncost for caching depends on the input token size and how long you want the\ntokens to persist. Context caching supports both Gemini 1.5 Pro and Gemini 1.5 Flash. Note: Context caching is only available for stable models with fixed versions\n(for example, gemini-1.5-pro-001 ). You must include the version postfix (for\nexample, the -001 in gemini-1.5-pro-001 ). When to use context caching Context caching is particularly well suited to scenarios where a substantial\ninitial context is referenced repeatedly by shorter requests. Consider using\ncontext caching for use cases such as: Chatbots with extensive system instructions Repetitive analysis of lengthy video files Recurring queries against large document sets Frequent code repository analysis or bug fixing How caching reduces costs Context caching is a paid feature designed to reduce overall operational costs.\nBilling is based on the following factors: Cache token count: The number of input tokens cached, billed at a\nreduced rate when included in subsequent prompts. Storage duration: The amount of time cached tokens are stored (TTL),\nbilled based on the TTL duration of cached token count. There are no minimum\nor maximum bounds on the TTL. Other factors: Other charges apply, such as for non-cached input tokens\nand output tokens. For up-to-date pricing details, refer to the Gemini API pricing\npage . To learn how to count tokens, see the Token\nguide . How to use context caching This section assumes that you've installed a Gemini SDK (or have curl installed)\nand that you've configured an API key, as shown in the quickstart . Additional considerations Keep the following considerations in mind when using context caching: The minimum input token count for context caching is 32,768, and the maximum is the same as the maximum for the given model. (For more on\ncounting tokens, see the Token guide ). The model doesn't make any distinction between cached tokens and regular\ninput tokens. Cached content is simply a prefix to the prompt. There are no special rate or usage limits on context caching; the standard\nrate limits for GenerateContent apply, and token limits include cached\ntokens. The number of cached tokens is returned in the usage_metadata from the\ncreate, get, and list operations of the cache service, and also in GenerateContent when using the cache. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-10-16 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use Context Caching in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching"
            ],
            "timestamp": "2024-12-15T00:57:45.734933",
            "status_code": 200
        }
    ]
}

{
    "base_url": "https://ai.google.dev/gemini-api/docs/document-processing?lang=python",
    "base_path": "/gemini-api/docs/document-processing",
    "crawl_date": "2024-12-15T00:58:52.799846",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/document-processing?lang=python",
            "title": "Explore document processing capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore document processing capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Models Gemini API docs API Reference SDKs Pricing Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore document processing capabilities with the Gemini API Python Node.js Go REST The Gemini API can process and run inference on PDF documents passed to it. When\na PDF is uploaded, the Gemini API can: Describe or answer questions about the content Summarize the content Extrapolate from the content This tutorial demonstrates some possible ways to prompt the Gemini API with\nprovided PDF documents. All output is text-only. Before you begin: Set up your project and API key Before calling the Gemini API, you need to set up your project and configure\nyour API key. Expand to view how to set up your project and API key Tip: For complete setup instructions, see the Gemini API quickstart . Get and secure your API key You need an API key to call the Gemini API. If you don't already have one,\ncreate a key in Google AI Studio. Get an API key It's strongly recommended that you do not check an API key into your version\ncontrol system. You should store your API key in a secrets store such as Google Cloud Secret Manager . This tutorial assumes that you're accessing your API key as an environment\nvariable. Install the SDK package and configure your API key Note: This section shows setup steps for a local Python environment. To install\n      dependencies and configure your API key for Colab, see the Authentication quickstart notebook The Python SDK for the Gemini API is contained in the google-generativeai package. Install the dependency using pip: pip install -U google-generativeai Import the package and configure the service with your API key: import os import google.generativeai as genai genai . configure ( api_key = os . environ [ 'API_KEY' ]) Technical details Gemini 1.5 Pro and 1.5 Flash support a maximum of 3,600 document pages. Document\npages must be in one of the following text data MIME types: PDF - application/pdf JavaScript - application/x-javascript , text/javascript Python - application/x-python , text/x-python TXT - text/plain HTML - text/html CSS - text/css Markdown - text/md CSV - text/csv XML - text/xml RTF - text/rtf Each document page is equivalent to 258 tokens. While there are no specific limits to the number of pixels in a document besides\nthe model's context window, larger pages are scaled down to a maximum resolution\nof 3072x3072 while preserving their original aspect ratio, while smaller pages\nare scaled up to 768x768 pixels. There is no cost reduction for pages at lower\nsizes, other than bandwidth, or performance improvement for pages at higher\nresolution. For best results: Rotate pages to the correct orientation before uploading. Avoid blurry pages. If using a single page, place the text prompt after the page. Upload a document and generate content You can use the File API to upload a document of any size. Always use the File\nAPI when the total request size (including the files, text prompt, system\ninstructions, etc.) is larger than 20 MB. Note: The File API lets you store up to 20 GB of files per project, with a\nper-file maximum size of 2 GB. Files are stored for 48 hours. They can be\naccessed in that period with your API key, but cannot be downloaded from the\nAPI. The File API is available at no cost in all regions where the Gemini API is\navailable. Call media.upload to upload a file using the\nFile API. The following code uploads a document file and then uses the file in a\ncall to models.generateContent . import google.generativeai as genai model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) sample_pdf = genai . upload_file ( media / \"test.pdf\" ) response = model . generate_content ([ \"Give me a summary of this pdf file.\" , sample_pdf ]) print ( response . text ) files . py Get metadata for a file You can verify the API successfully stored the uploaded file and get its\nmetadata by calling files.get . Only the name (and by extension, the uri ) are unique. import google.generativeai as genai myfile = genai . upload_file ( media / \"poem.txt\" ) file_name = myfile . name print ( file_name ) # \"files/*\" myfile = genai . get_file ( file_name ) print ( myfile ) files . py Upload one or more locally stored files Alternatively, you can upload one or more locally stored files. When the combination of files and system instructions that you intend to send is\nlarger than 20MB in size, use the File API to upload those files, as previously\nshown. Smaller files can instead be called locally from the Gemini API: import PyPDF2 def extract_text_from_pdf ( pdf_path ): with open ( pdf_path , 'rb' ) as pdf_file : pdf_reader = PyPDF2 . PdfReader ( pdf_file ) extracted_text = \"\" for page in pdf_reader . pages : text = page . extract_text () if text : extracted_text += text return extracted_text sample_file_2 = extract_text_from_pdf ( 'example-1.pdf' ) sample_file_3 = extract_text_from_pdf ( 'example-2.pdf' ) Prompt with multiple documents You can provide the Gemini API with any combination of documents and text that\nfit within the model's context window. This example provides one short text\nprompt and three documents previously uploaded: # Choose a Gemini model. model = genai . GenerativeModel ( model_name = \"gemini-1.5-flash\" ) prompt = \"Summarize the differences between the thesis statements for these documents.\" response = model . generate_content ([ prompt , sample_file , sample_file_2 , sample_file_3 ]) print ( response . text ) List files You can list all files uploaded using the File API and their URIs using files.list . import google.generativeai as genai print ( \"My files:\" ) for f in genai . list_files (): print ( \"  \" , f . name ) files . py Delete files Files uploaded using the File API are automatically deleted after 2 days. You\ncan also manually delete them using files.delete . import google.generativeai as genai myfile = genai . upload_file ( media / \"poem.txt\" ) myfile . delete () try : # Error. model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) result = model . generate_content ([ myfile , \"Describe this file.\" ]) except google . api_core . exceptions . PermissionDenied : pass files . py What's next This guide shows how to use generateContent and\nto generate text outputs from processed documents. To learn more,\nsee the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use the Gemini API to process documents like PDFs",
            "links": [
                "https://ai.google.dev/gemini-api/docs/document-processing?lang=python",
                "https://ai.google.dev/gemini-api/docs/document-processing?lang=python",
                "https://ai.google.dev/gemini-api/docs/document-processing?lang=python",
                "https://ai.google.dev/gemini-api/docs/document-processing?lang=python",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing?lang=python"
            ],
            "timestamp": "2024-12-15T00:58:52.120865",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/document-processing",
            "title": "Explore document processing capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore document processing capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Models Gemini API docs API Reference SDKs Pricing Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore document processing capabilities with the Gemini API Python Node.js Go REST The Gemini API can process and run inference on PDF documents passed to it. When\na PDF is uploaded, the Gemini API can: Describe or answer questions about the content Summarize the content Extrapolate from the content This tutorial demonstrates some possible ways to prompt the Gemini API with\nprovided PDF documents. All output is text-only. What's next This guide shows how to use generateContent and\nto generate text outputs from processed documents. To learn more,\nsee the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use the Gemini API to process documents like PDFs",
            "links": [
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing"
            ],
            "timestamp": "2024-12-15T00:58:52.795973",
            "status_code": 200
        }
    ]
}

{
    "base_url": "https://ai.google.dev/gemini-api/docs/audio?lang=python",
    "base_path": "/gemini-api/docs/audio",
    "crawl_date": "2024-12-15T00:59:55.411239",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/audio?lang=python",
            "title": "Explore audio capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore audio capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Models Gemini API docs API Reference SDKs Pricing Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore audio capabilities with the Gemini API Python Node.js Go REST Gemini can respond to prompts about audio. For example, Gemini can: Describe, summarize, or answer questions about audio content. Provide a transcription of the audio. Provide answers or a transcription about a specific segment of the audio. Note: You can't generate audio output with the Gemini API. This guide demonstrates different ways to interact with audio files and audio\ncontent using the Gemini API. Supported audio formats Gemini supports the following audio format MIME types: WAV - audio/wav MP3 - audio/mp3 AIFF - audio/aiff AAC - audio/aac OGG Vorbis - audio/ogg FLAC - audio/flac Technical details about audio Gemini imposes the following rules on audio: Gemini represents each second of audio as 25 tokens; for example,\none minute of audio is represented as 1,500 tokens. Gemini can only infer responses to English-language speech. Gemini can \"understand\" non-speech components, such as birdsong or sirens. The maximum supported length of audio data in a single prompt is 9.5 hours.\nGemini doesn't limit the number of audio files in a single prompt; however,\nthe total combined length of all audio files in a single prompt cannot exceed\n9.5 hours. Gemini downsamples audio files to a 16 Kbps data resolution. If the audio source contains multiple channels, Gemini combines those channels\ndown to a single channel. Before you begin: Set up your project and API key Before calling the Gemini API, you need to set up your project and configure\nyour API key. Expand to view how to set up your project and API key Tip: For complete setup instructions, see the Gemini API quickstart . Get and secure your API key You need an API key to call the Gemini API. If you don't already have one,\ncreate a key in Google AI Studio. Get an API key It's strongly recommended that you do not check an API key into your version\ncontrol system. You should store your API key in a secrets store such as Google Cloud Secret Manager . This tutorial assumes that you're accessing your API key as an environment\nvariable. Install the SDK package and configure your API key Note: This section shows setup steps for a local Python environment. To install\n      dependencies and configure your API key for Colab, see the Authentication quickstart notebook The Python SDK for the Gemini API is contained in the google-generativeai package. Install the dependency using pip: pip install -U google-generativeai Import the package and configure the service with your API key: import os import google.generativeai as genai genai . configure ( api_key = os . environ [ 'API_KEY' ]) Make an audio file available to Gemini You can make an audio file available to Gemini in either of the following ways: Upload the audio file prior to making the prompt request. Provide the audio file as inline data to the prompt request. Upload an audio file and generate content You can use the File API to upload an audio file of any size. Always use the\nFile API when the total request size (including the files, text prompt, system\ninstructions, etc.) is larger than 20 MB. Note: The File API lets you store up to 20 GB of files per project, with a\nper-file maximum size of 2 GB. Files are stored for 48 hours. They can be\naccessed in that period with your API key, but cannot be downloaded from the\nAPI. The File API is available at no cost in all regions where the Gemini API is\navailable. Call media.upload to upload a file using the\nFile API. The following code uploads an audio file and then uses the file in a\ncall to models.generateContent . import google.generativeai as genai myfile = genai . upload_file ( media / \"sample.mp3\" ) print ( f \" { myfile =} \" ) model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) result = model . generate_content ([ myfile , \"Describe this audio clip\" ]) print ( f \" { result . text =} \" ) files . py Get metadata for a file You can verify the API successfully stored the uploaded file and get its\nmetadata by calling files.get . import google.generativeai as genai myfile = genai . upload_file ( media / \"poem.txt\" ) file_name = myfile . name print ( file_name ) # \"files/*\" myfile = genai . get_file ( file_name ) print ( myfile ) files . py List uploaded files You can upload multiple audio files (and other kinds of files).\nThe following code generates a list of all the files uploaded: import google.generativeai as genai print ( \"My files:\" ) for f in genai . list_files (): print ( \"  \" , f . name ) files . py Delete uploaded files Files are automatically deleted after 48 hours. Optionally, you can manually\ndelete an uploaded file. For example: import google.generativeai as genai myfile = genai . upload_file ( media / \"poem.txt\" ) myfile . delete () try : # Error. model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) result = model . generate_content ([ myfile , \"Describe this file.\" ]) except google . api_core . exceptions . PermissionDenied : pass files . py Provide the audio file as inline data in the request Instead of uploading an audio file, you can pass audio data in the\nsame call that contains the prompt. Then, pass that downloaded small audio file along with the prompt to Gemini: # Initialize a Gemini model appropriate for your use case. model = genai . GenerativeModel ( 'models/gemini-1.5-flash' ) # Create the prompt. prompt = \"Please summarize the audio.\" # Load the samplesmall.mp3 file into a Python Blob object containing the audio # file's bytes and then pass the prompt and the audio to Gemini. response = model . generate_content ([ prompt , { \"mime_type\" : \"audio/mp3\" , \"data\" : pathlib . Path ( 'samplesmall.mp3' ) . read_bytes () } ]) # Output Gemini's response to the prompt and the inline audio. print ( response . text ) Note the following about providing audio as inline data: The maximum request size is 20 MB, which includes text prompts,\nsystem instructions, and files provided inline. If your file's\nsize will make the total request size exceed 20 MB, then use the File API to upload files for use in requests. If you're using an audio sample multiple times, it is more efficient\nto use the File API . More ways to work with audio This section provides a few additional ways to get more from audio. Get a transcript of the audio file To get a transcript, just ask for it in the prompt. For example: # Initialize a Gemini model appropriate for your use case. model = genai . GenerativeModel ( model_name = \"gemini-1.5-flash\" ) # Create the prompt. prompt = \"Generate a transcript of the speech.\" # Pass the prompt and the audio file to Gemini. response = model . generate_content ([ prompt , audio_file ]) # Print the transcript. print ( response . text ) Refer to timestamps in the audio file A prompt can specify timestamps of the form MM:SS to refer to particular\nsections in an audio file. For example, the following prompt requests\na transcript that: Starts at 2 minutes 30 seconds from the beginning of the file. Ends at 3 minutes 29 seconds from the beginning of the file. # Create a prompt containing timestamps. prompt = \"Provide a transcript of the speech from 02:30 to 03:29.\" Count tokens Call the countTokens method to get a\ncount of the number of tokens in the audio file. For example: model . count_tokens ([ audio_file ]) What's next This guide shows how to upload audio files using the File API and then generate\ntext outputs from audio inputs. To learn more, see the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs/audio?lang=python",
                "https://ai.google.dev/gemini-api/docs/audio?lang=python",
                "https://ai.google.dev/gemini-api/docs/audio?lang=python",
                "https://ai.google.dev/gemini-api/docs/audio?lang=python",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio?lang=python"
            ],
            "timestamp": "2024-12-15T00:59:54.741729",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/audio",
            "title": "Explore audio capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore audio capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Models Gemini API docs API Reference SDKs Pricing Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore audio capabilities with the Gemini API Python Node.js Go REST Gemini can respond to prompts about audio. For example, Gemini can: Describe, summarize, or answer questions about audio content. Provide a transcription of the audio. Provide answers or a transcription about a specific segment of the audio. Note: You can't generate audio output with the Gemini API. This guide demonstrates different ways to interact with audio files and audio\ncontent using the Gemini API. Supported audio formats Gemini supports the following audio format MIME types: WAV - audio/wav MP3 - audio/mp3 AIFF - audio/aiff AAC - audio/aac OGG Vorbis - audio/ogg FLAC - audio/flac Technical details about audio Gemini imposes the following rules on audio: Gemini represents each second of audio as 25 tokens; for example,\none minute of audio is represented as 1,500 tokens. Gemini can only infer responses to English-language speech. Gemini can \"understand\" non-speech components, such as birdsong or sirens. The maximum supported length of audio data in a single prompt is 9.5 hours.\nGemini doesn't limit the number of audio files in a single prompt; however,\nthe total combined length of all audio files in a single prompt cannot exceed\n9.5 hours. Gemini downsamples audio files to a 16 Kbps data resolution. If the audio source contains multiple channels, Gemini combines those channels\ndown to a single channel. What's next This guide shows how to upload audio files using the File API and then generate\ntext outputs from audio inputs. To learn more, see the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio"
            ],
            "timestamp": "2024-12-15T00:59:55.406508",
            "status_code": 200
        }
    ]
}

{
    "base_url": "https://ai.google.dev/gemini-api/docs/vision?lang=python",
    "base_path": "/gemini-api/docs/vision",
    "crawl_date": "2024-12-15T01:00:24.079971",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/vision?lang=python",
            "title": "Explore vision capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore vision capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Models Gemini API docs API Reference SDKs Pricing Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore vision capabilities with the Gemini API Python Node.js Go REST View on ai.google.dev Try a Colab notebook View notebook on GitHub The Gemini API is able to process images and videos, enabling a multitude of\n exciting developer use cases. Some of Gemini's vision capabilities include\n the ability to: Caption and answer questions about images Transcribe and reason over PDFs, including long documents up to 2 million token context window Describe, segment, and extract information from videos,\nincluding both visual frames and audio, up to 90 minutes long Detect objects in an image and return bounding box coordinates for them This tutorial demonstrates some possible ways to prompt the Gemini API with\nimages and video input, provides code examples,\nand outlines prompting best practices with multimodal vision capabilities.\nAll output is text-only. Before you begin: Set up your project and API key Before calling the Gemini API, you need to set up your project and configure\nyour API key. Expand to view how to set up your project and API key Tip: For complete setup instructions, see the Gemini API quickstart . Get and secure your API key You need an API key to call the Gemini API. If you don't already have one,\ncreate a key in Google AI Studio. Get an API key It's strongly recommended that you do not check an API key into your version\ncontrol system. You should store your API key in a secrets store such as Google Cloud Secret Manager . This tutorial assumes that you're accessing your API key as an environment\nvariable. Install the SDK package and configure your API key Note: This section shows setup steps for a local Python environment. To install\n      dependencies and configure your API key for Colab, see the Authentication quickstart notebook The Python SDK for the Gemini API is contained in the google-generativeai package. Install the dependency using pip: pip install -U google-generativeai Import the package and configure the service with your API key: import os import google.generativeai as genai genai . configure ( api_key = os . environ [ 'API_KEY' ]) Prompting with images In this tutorial, you will upload images using the File API or as inline data\nand generate content based on those images. Technical details (images) Gemini 1.5 Pro and 1.5 Flash support a maximum of 3,600 image files. Images must be in one of the following image data MIME types: PNG - image/png JPEG - image/jpeg WEBP - image/webp HEIC - image/heic HEIF - image/heif Each image is equivalent to 258 tokens. While there are no specific limits to the number of pixels in an image besides\nthe model's context window, larger images are scaled down to a maximum\nresolution of 3072x3072 while preserving their original aspect ratio, while\nsmaller images are scaled up to 768x768 pixels. There is no cost reduction\nfor images at lower sizes, other than bandwidth, or performance improvement\nfor images at higher resolution. For best results: Rotate images to the correct orientation before uploading. Avoid blurry images. If using a single image, place the text prompt after the image. Image input For total image payload size less than 20MB, we recommend either uploading\nbase64 encoded images or directly uploading locally stored image files. Base64 encoded images You can upload public image URLs by encoding them as Base64 payloads.\nWe recommend using the httpx library to fetch the image URLs.\nThe following code example shows how to do this: import httpx import os import base64 model = genai . GenerativeModel ( model_name = \"gemini-1.5-pro\" ) image_path = \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Palace_of_Westminster_from_the_dome_on_Methodist_Central_Hall.jpg/2560px-Palace_of_Westminster_from_the_dome_on_Methodist_Central_Hall.jpg\" image = httpx . get ( image_path ) prompt = \"Caption this image.\" response = model . generate_content ([{ 'mime_type' : 'image/jpeg' , 'data' : base64 . b64encode ( image . content ) . decode ( 'utf-8' )}, prompt ]) print ( response . text ) Multiple images To prompt with multiple images in Base64 encoded format, you can do the\nfollowing: import httpx import os import base64 model = genai . GenerativeModel ( model_name = \"gemini-1.5-pro\" ) image_path_1 = \"path/to/your/image1.jpeg\" # Replace with the actual path to your first image image_path_2 = \"path/to/your/image2.jpeg\" # Replace with the actual path to your second image image_1 = httpx . get ( image_path_1 ) image_2 = httpx . get ( image_path_2 ) prompt = \"Generate a list of all the objects contained in both images.\" response = model . generate_content ([ { 'mime_type' : 'image/jpeg' , 'data' : base64 . b64encode ( image_1 . content ) . decode ( 'utf-8' )}, { 'mime_type' : 'image/jpeg' , 'data' : base64 . b64encode ( image_2 . content ) . decode ( 'utf-8' )}, prompt ]) print ( response . text ) Upload one or more locally stored image files Alternatively, you can upload one or more locally stored image files. import PIL.Image import os import google.generativeai as genai image_path_1 = \"path/to/your/image1.jpeg\" # Replace with the actual path to your first image image_path_2 = \"path/to/your/image2.jpeg\" # Replace with the actual path to your second image sample_file_1 = PIL . Image . open ( image_path_1 ) sample_file_2 = PIL . Image . open ( image_path_2 ) #Choose a Gemini model. model = genai . GenerativeModel ( model_name = \"gemini-1.5-pro\" ) prompt = \"Write an advertising jingle based on the items in both images.\" response = model . generate_content ([ prompt , sample_file_1 , sample_file_2 ]) print ( response . text ) Note that these inline data calls don't include many of the features available\nthrough the File API, such as getting file metadata, listing ,\nor deleting files . Large image payloads When the combination of files and system instructions that you intend to send is\nlarger than 20 MB in size, use the File API to upload those files. Use the media.upload method of the File API to upload an image of any size. Note: The File API lets you store up to 20 GB of files per project, with a\nper-file maximum size of 2 GB. Files are stored for 48 hours. They can be\naccessed in that period with your API key, but cannot be downloaded from the\nAPI. It is available at no cost in all regions where the Gemini API is\navailable. After uploading the file, you can make GenerateContent requests that reference\nthe File API URI. Select the generative model and provide it with a text prompt\nand the uploaded image. import google.generativeai as genai myfile = genai . upload_file ( media / \"Cajun_instruments.jpg\" ) print ( f \" { myfile =} \" ) model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) result = model . generate_content ( [ myfile , \" \\n\\n \" , \"Can you tell me about the instruments in this photo?\" ] ) print ( f \" { result . text =} \" ) files . py OpenAI Compatibility You can access Gemini's image understanding capabilities using the\nOpenAI libraries. This lets you integrate Gemini into existing\nOpenAI workflows by updating three lines of code and using\nyour Gemini API key. See the Image understanding example for code demonstrating how to send images encoded as Base64 payloads. Capabilities This section outlines specific vision capabilities of the Gemini model,\nincluding object detection and bounding box coordinates. Get a bounding box for an object Gemini models are trained to return bounding box coordinates as relative widths\nor heights in the range of [0, 1]. These values are then scaled by 1000 and\nconverted to integers. Effectively, the coordinates represent the bounding box\non a 1000x1000 pixel version of the image. Therefore, you'll need to\nconvert these coordinates back to the dimensions of your original\nimage to accurately map the bounding boxes. # Choose a Gemini model. model = genai . GenerativeModel ( model_name = \"gemini-1.5-pro\" ) prompt = \"Return a bounding box for each of the objects in this image in [ymin, xmin, ymax, xmax] format.\" response = model . generate_content ([ sample_file_1 , prompt ]) print ( response . text ) The model returns bounding box coordinates in the format [ymin, xmin, ymax, xmax] . To convert these normalized coordinates\nto the pixel coordinates of your original image, follow these steps: Divide each output coordinate by 1000. Multiply the x-coordinates by the original image width. Multiply the y-coordinates by the original image height. To explore more detailed examples of generating bounding box coordinates and\nvisualizing them on images, we encourage you to review our Object Detection cookbook example . Prompting with video In this tutorial, you will upload a video using the File API and generate\ncontent based on those images. Note: The File API is required to upload video files, due to their size.\nHowever, the File API is only available for Python, Node.js, Go, and REST. Technical details (video) Gemini 1.5 Pro and Flash support up to approximately an hour of video data. Video must be in one of the following video format MIME types: video/mp4 video/mpeg video/mov video/avi video/x-flv video/mpg video/webm video/wmv video/3gpp The File API service extracts image frames from videos at 1 frame per second\n(FPS) and audio at 1Kbps, single channel, adding timestamps every second.\nThese rates are subject to change in the future for improvements in inference. Note: The details of fast action sequences may be lost at the 1 FPS frame\nsampling rate. Consider slowing down high-speed clips for improved inference\nquality. Individual frames are 258 tokens, and audio is 32 tokens per second. With\nmetadata, each second of video becomes ~300 tokens, which means a 1M context\nwindow can fit slightly less than an hour of video. To ask questions about time-stamped locations, use the format MM:SS , where\nthe first two digits represent minutes and the last two digits represent\nseconds. For best results: Use one video per prompt. If using a single video, place the text prompt after the video. Upload a video file using the File API Note: The File API lets you store up to 20 GB of files per project, with a\nper-file maximum size of 2 GB. Files are stored for 48 hours. They can be\naccessed in that period with your API key, but they cannot be downloaded\nusing any API. It is available at no cost in all regions where the Gemini\nAPI is available. The File API accepts video file formats directly. This example uses the\nshort NASA film \"Jupiter's Great Red Spot Shrinks and Grows\" .\nCredit: Goddard Space Flight Center (GSFC)/David Ladd (2018). \"Jupiter's Great Red Spot Shrinks and Grows\" is in the public domain and does\nnot show identifiable people.\n( NASA image and media usage guidelines. ) Start by retrieving the short video: wget https://storage.googleapis.com/generativeai-downloads/images/GreatRedSpot.mp4 Upload the video using the File API and print the URI. # Upload the video and print a confirmation. video_file_name = \"GreatRedSpot.mp4\" print ( f \"Uploading file...\" ) video_file = genai . upload_file ( path = video_file_name ) print ( f \"Completed upload: { video_file . uri } \" ) Verify file upload and check state Verify the API has successfully received the files by calling the files.get method. Note: Video files have a State field in the File API. When a video is\nuploaded, it will be in the PROCESSING state until it is ready for inference. Only ACTIVE files can be used for model inference. import time # Check whether the file is ready to be used. while video_file . state . name == \"PROCESSING\" : print ( '.' , end = '' ) time . sleep ( 10 ) video_file = genai . get_file ( video_file . name ) if video_file . state . name == \"FAILED\" : raise ValueError ( video_file . state . name ) Prompt with a video and text Once the uploaded video is in the ACTIVE state, you can make GenerateContent requests that specify the File API URI for that video. Select the generative\nmodel and provide it with the uploaded video and a text prompt. # Create the prompt. prompt = \"Summarize this video. Then create a quiz with answer key based on the information in the video.\" # Choose a Gemini model. model = genai . GenerativeModel ( model_name = \"gemini-1.5-pro\" ) # Make the LLM request. print ( \"Making LLM inference request...\" ) response = model . generate_content ([ video_file , prompt ], request_options = { \"timeout\" : 600 }) # Print the response, rendering any Markdown Markdown ( response . text ) Refer to timestamps in the content You can use timestamps of the form HH:MM:SS to refer to specific moments in the\nvideo. # Create the prompt. prompt = \"What are the examples given at 01:05 and 01:19 supposed to show us?\" # Choose a Gemini model. model = genai . GenerativeModel ( model_name = \"gemini-1.5-pro\" ) # Make the LLM request. print ( \"Making LLM inference request...\" ) response = model . generate_content ([ video_file , prompt ], request_options = { \"timeout\" : 600 }) print ( response . text ) Transcribe video and provide visual descriptions The Gemini models can transcribe and provide visual descriptions of video content\nby processing both the audio track and visual frames.\nFor visual descriptions, the model samples the video at a rate of 1 frame\nper second . This sampling rate may affect the level of detail in the\ndescriptions, particularly for videos with rapidly changing visuals. # Create the prompt. prompt = \"Transcribe the audio from this video, giving timestamps for salient events in the video. Also provide visual descriptions.\" # Choose a Gemini model. model = genai . GenerativeModel ( model_name = \"gemini-1.5-pro\" ) # Make the LLM request. print ( \"Making LLM inference request...\" ) response = model . generate_content ([ video_file , prompt ], request_options = { \"timeout\" : 600 }) print ( response . text ) List files You can list all files uploaded using the File API and their URIs using files.list . import google.generativeai as genai print ( \"My files:\" ) for f in genai . list_files (): print ( \"  \" , f . name ) files . py Delete files Files uploaded using the File API are automatically deleted after 2 days. You\ncan also manually delete them using files.delete . import google.generativeai as genai myfile = genai . upload_file ( media / \"poem.txt\" ) myfile . delete () try : # Error. model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) result = model . generate_content ([ myfile , \"Describe this file.\" ]) except google . api_core . exceptions . PermissionDenied : pass files . py What's next This guide shows how to upload image and video files using the File API and\nthen generate text outputs from image and video inputs. To learn more,\nsee the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started building with Gemini&#39;s multimodal capabilities in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/vision?lang=python",
                "https://ai.google.dev/gemini-api/docs/vision?lang=python",
                "https://ai.google.dev/gemini-api/docs/vision?lang=python",
                "https://ai.google.dev/gemini-api/docs/vision?lang=python",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision?lang=python"
            ],
            "timestamp": "2024-12-15T01:00:23.530375",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/vision",
            "title": "Explore vision capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore vision capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Models Gemini API docs API Reference SDKs Pricing Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore vision capabilities with the Gemini API Python Node.js Go REST The Gemini API is able to process images and videos, enabling a multitude of\n exciting developer use cases. Some of Gemini's vision capabilities include\n the ability to: Caption and answer questions about images Transcribe and reason over PDFs, including long documents up to 2 million token context window Describe, segment, and extract information from videos,\nincluding both visual frames and audio, up to 90 minutes long Detect objects in an image and return bounding box coordinates for them This tutorial demonstrates some possible ways to prompt the Gemini API with\nimages and video input, provides code examples,\nand outlines prompting best practices with multimodal vision capabilities.\nAll output is text-only. What's next This guide shows how to upload image and video files using the File API and\nthen generate text outputs from image and video inputs. To learn more,\nsee the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started building with Gemini&#39;s multimodal capabilities in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision"
            ],
            "timestamp": "2024-12-15T01:00:24.075763",
            "status_code": 200
        }
    ]
}

