{
    "base_url": "https://ai.google.dev/gemini-api/docs",
    "crawl_date": "2024-12-27T22:47:43.854318",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs",
            "title": "Gemini API  |  Google AI for Developers",
            "text_content": "Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Get started with the Gemini API Get a Gemini API Key The Gemini API and Google AI Studio help you start working with Google's latest\nmodels and turn your ideas into applications that scale. Python import google.generativeai as genai genai . configure ( api_key = \" YOUR_API_KEY \" ) model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) response = model . generate_content ( \"Explain how AI works\" ) print ( response . text ) Node.js const { GoogleGenerativeAI } = require ( \"@google/generative-ai\" ); const genAI = new GoogleGenerativeAI ( \" YOUR_API_KEY \" ); const model = genAI . getGenerativeModel ({ model : \"gemini-1.5-flash\" }); const prompt = \"Explain how AI works\" ; const result = await model . generateContent ( prompt ); console . log ( result . response . text ()); REST curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= YOUR_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d '{ \"contents\": [{ \"parts\":[{\"text\": \"Write a story about a magic backpack.\"}] }] }' Meet the models Try Gemini in Google AI Studio 2.0 Flash experiment Our newest multimodal model, with next generation features and improved\n          capabilities 1.5 Flash-8B Our fastest and most cost-efficient multimodal model with great performance\n          for high-frequency tasks 1.5 Pro Our best performing multimodal model with features for a wide variety of reasoning tasks Explore the API Explore long context Input millions of tokens to Gemini models and derive understanding from unstructured images, videos, and documents. Solve tasks with fine-tuning Modify the behavior of Gemini models to adapt to specific tasks, recognize data, and solve problems. Tune models with your own data to make production deployments more robust and reliable. Generate structured outputs Constrain Gemini to respond with JSON, a structured data format suitable for automated processing. Start building with the Gemini API Get started Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Gemini API Developer Docs and API Reference",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs"
            ],
            "timestamp": "2024-12-27T22:47:00.874600",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/gemini-for-research",
            "title": "Gemini API  |  Google AI for Developers",
            "text_content": "Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Accelerate discovery with Gemini for Research Get a Gemini API Key Gemini models can be used to advance foundational research across disciplines.\nHere are ways that you can explore Gemini for your research: Fine-tuning : You can fine-tune Gemini models for a variety of modalities to\nadvance your research. Learn more . Analyze and control model outputs : For further analysis, you can examine a\nresponse candidate generated by the model using tools like Logprobs and CitationMetadata . You can also configure options for model generation and\noutputs, such as responseSchema , topP , and topK . Learn more . Multimodal inputs : Gemini can process images, audio, and videos, enabling a\nmultitude of exciting research directions. Learn more . Long-context capabilities : Gemini 1.5 Flash comes with a 1-million-token\ncontext window, and Gemini 1.5 Pro comes with a 2-million-token context window. Learn more . Grow with Google : Quickly access Gemini models through the API and Google AI\nStudio for production use cases. If you're looking for a Google Cloud-based\nplatform, Vertex AI can provide additional supporting infrastructure. To support academic research and drive cutting-edge research, Google provides\naccess to Gemini API credits for scientists and academic researchers through the Gemini Academic Program . Get started with Gemini The Gemini API and Google AI Studio help you start working with Google's latest\nmodels and turn your ideas into applications that scale. Python import google.generativeai as genai genai . configure ( api_key = \" YOUR_API_KEY \" ) model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) response = model . generate_content ( \"How large is the universe?\" ) print ( response . text ) Node.js const { GoogleGenerativeAI } = require ( \"@google/generative-ai\" ); const genAI = new GoogleGenerativeAI ( \" YOUR_API_KEY \" ); const model = genAI . getGenerativeModel ({ model : \"gemini-1.5-flash\" }); const prompt = \"How large is the universe?\" ; const result = await model . generateContent ( prompt ); console . log ( result . response . text ()); REST curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= YOUR_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d '{ \"contents\": [{ \"parts\":[{\"text\": \"How large is the universe?\"}] }] }' Featured academics \"Our research investigates Gemini as a visual language model (VLM) and its agentic behaviors in diverse environments from robustness and safety perspectives. So far, we have evaluated Gemini's robustness against distractions such as pop-up windows when VLM agents perform computer tasks, and have leveraged Gemini to analyze social interaction, temporal events as well as risk factors based on video input.\" \"Gemini Pro and Flash, with their long context window, have been helping us in OK-Robot, our open-vocabulary mobile manipulation project. Gemini enables complex natural language queries and commands over the robot's \"memory\": in this case, previous observations made by the robot over a long operation duration. Mahi Shafiullah and I are also using Gemini to decompose tasks into code that the robot can execute in the real world.\" Gemini Academic Program Qualified academic researchers (such as faculty, staff, and PhD students) in supported\ncountries can apply to receive Gemini API\ncredits and higher rate limits for research projects. This support enables\nhigher throughput for scientific experiments and advances research. We are particularly interested in the research areas in the following section,\nbut we welcome applications from diverse scientific disciplines: Evaluations and benchmarks : Community-endorsed evaluation methods that\ncan provide a strong performance signal in areas such as factuality, safety,\ninstruction following, reasoning, and planning. Accelerating scientific discovery to benefit humanity : Potential\napplications of AI in interdisciplinary scientific research, including areas\nsuch as rare and neglected diseases, experimental biology, materials science,\nand sustainability. Embodiment and interactions : Utilizing large language models to\ninvestigate novel interactions within the fields of embodied AI, ambient\ninteractions, robotics, and human-computer interaction. Emergent capabilities : Exploring new agentic capabilities required to\nenhance reasoning and planning, and how capabilities can be expanded during\ninference (e.g., by utilizing Gemini Flash). Multimodal interaction and understanding : Identifying gaps and\nopportunities for multimodal foundational models for analysis, reasoning,\nand planning across a variety of tasks. Eligibility: Only individuals (faculty members, researchers or equivalent)\naffiliated with a valid academic institution, or academic research organization\ncan apply. Note that API access and credits will be granted and removed\nat Google's discretion. We review applications on a monthly basis. Start researching with the Gemini API Apply now Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Accelerate discovery with Gemini for Research",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research"
            ],
            "timestamp": "2024-12-27T22:47:01.716391",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/sdks",
            "title": "Google Gen AI SDKs  |  Gemini API  |  Google AI for Developers",
            "text_content": "Google Gen AI SDKs  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Generative AI SDKs Overview Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models SDKs Send feedback Google Gen AI SDKs The new Google Gen AI SDK provides a unified interface to Gemini 2.0\nthrough both the Gemini Developer API and the Gemini Enterprise API (\nVertex AI). With a few exceptions, code that runs on one platform will\nrun on both. The Gen AI SDK also supports the Gemini 1.5 models. Python The Google Gen AI SDK for Python is available on PyPI and GitHub: google-genai on PyPI python-genai on GitHub Or try out the Getting Started\nnotebook . To learn more, see the Python SDK reference (opens in a new tab) . Quickstart 1. Install SDK bash\npip install google-genai 2. Import libraries from google import genai from google.genai import types 3. Create a client client = genai . Client ( api_key = 'GEMINI_API_KEY' ) 4. Generate content response = client . models . generate_content ( model = 'gemini-1.5-flash' , contents = 'How does RLHF work?' ) print ( response . text ) Go The Google Gen AI SDK for Go is available on go.dev and GitHub: go-genai on go.dev go-genai on GitHub Quickstart 1. Import libraries import \"google.golang.org/genai\" 2. Create a client client , err := genai . NewClient ( ctx , & genai . ClientConfig { APIKey : apiKey , Backend : genai . BackendGoogleAI , }) 3. Generate content // Call the GenerateContent method result , err := client . Models . GenerateContent ( ctx , \"gemini-2.0-flash-exp\" , genai . Text ( \"Tell me about New York?\" ), nil ) Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-23 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/sdks"
            ],
            "timestamp": "2024-12-27T22:47:02.806494",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/quickstart",
            "title": "Gemini API quickstart  |  Google AI for Developers",
            "text_content": "Gemini API quickstart  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Gemini API quickstart This quickstart shows you how to install your SDK of choice and then make your first\nGemini API request. Python Node.js REST Go Install the Gemini API library Make your first request Get a Gemini API key in Google AI Studio Use the generateContent method\nto send a request to the Gemini API. What's next Now that you made your first API request, you might want to explore the following\nguides which showcase Gemini in action: Text generation Vision Long context Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started with the Gemini API for Developers",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/quickstart"
            ],
            "timestamp": "2024-12-27T22:47:03.811802",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/api-key",
            "title": "Get a Gemini API key  |  Google AI for Developers",
            "text_content": "Get a Gemini API key  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Get a Gemini API key To use the Gemini API, you need an API key.\nYou can create a key with a few clicks in Google AI Studio. Get a Gemini API key in Google AI Studio Set up your API key For initial testing, you can hard code an API key, but this should only be\ntemporary since it is not secure. The rest of this section goes through how to\nset up your API key locally as an environment variable with different operating\nsystems. Linux/macOS - Bash Bash is a common Linux and macOS terminal configuration. You can check if\nyou have a configuration file for it by running the following command: ~/.bashrc If the response is \"No such file or directory\", you will need to create this\nfile and open it by running the following commands, or use zsh : touch ~/.bashrc open ~/.bashrc Next, you need to set you API key by adding the following export command: export GEMINI_API_KEY = <YOUR_API_KEY_HERE> After saving the file, apply the changes by running: source ~/.bashrc macOS - Zsh Zsh is a common Linux and macOS terminal configuration. You can check if\nyou have a configuration file for it by running the following command: ~/.zshrc If the response is \"No such file or directory\", you will need to create this\nfile and open it by running the following commands, or use bash : touch ~/.zshrc open ~/.zshrc Next, you need to set you API key by adding the following export command: export GEMINI_API_KEY = <YOUR_API_KEY_HERE> After saving the file, apply the changes by running: source ~/.zshrc Windows Search for \"Environment Variables\" in the system settings Edit either \"User variables\" (for current user) or \"System variables\"\n(for all users - use with caution). Create the variable and add export GEMINI_API_KEY=your_key_here Apply the changes Send your first Gemini API request You can use a curl command to verify your setup: curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= ${ GEMINI_API_KEY } \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d '{ \"contents\": [{ \"parts\":[{\"text\": \"Write a story about a magic backpack.\"}] }] }' Keep your API key secure It's important to keep your Gemini API key secure. Here are a few things to keep\nin mind when using your Gemini API key: The Google AI Gemini API uses API keys for authorization. If others get access\nto your Gemini API key, they can make calls using your project's quota,\nwhich could result in lost quota or additional charges for billed projects,\nin addition to accessing tuned models and files. Adding API key restrictions can help limit the surface area usable through each API key. You're responsible for keeping your Gemini API key secure. Do NOT check Gemini API keys into source control. Client-side applications (Android, Swift, web, and Dart/Flutter) risk\nexposing API keys. We don't recommend using the Google AI client SDKs\nin production apps to call the Google AI Gemini API directly from your\nmobile and web apps. For some general best practices, you can also review this support article . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-26 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get your Gemini API key and start building in less than 5 minutes.",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/api-key"
            ],
            "timestamp": "2024-12-27T22:47:05.277108",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/changelog",
            "title": "Release notes  |  Gemini API  |  Google AI for Developers",
            "text_content": "Release notes  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Release notes This page documents updates to the Gemini API. December 19, 2024 Model updates: Released Gemini 2.0 Flash Thinking Mode for public preview. Thinking Mode is\na test-time compute model that lets you see the model's thought process\nwhile it generates a response, and produces responses with stronger\nreasoning capabilities. Read more about Gemini 2.0 Flash Thinking Mode in our overview\npage . December 11, 2024 Model updates: Released Gemini 2.0 Flash Experimental for public preview. Gemini 2.0 Flash Experimental's partial list of features includes: Twice as fast as Gemini 1.5 Pro Bidirectional streaming with our Multimodal Live API Multimodal response generation in the form of text, images, and speech Native tool use with multi-turn reasoning to use features like code\nexecution, Search, function calling, and more Read more about Gemini 2.0 Flash in our overview\npage . November 21, 2024 Model updates: Released gemini-exp-1121 , an even more powerful experimental Gemini API model. Model updates: Updated the gemini-1.5-flash-latest and gemini-1.5-flash model aliases\nto use gemini-1.5-flash-002 . Change to top_k parameter: The gemini-1.5-flash-002 model supports top_k values between 1 and 41 (exclusive).\nValues greater than 40 will be changed to 40. November 14, 2024 Model updates: Released gemini-exp-1114 , a powerful experimental Gemini API model. November 8, 2024 API updates Added support for Gemini in the OpenAI libraries / REST API. October 31, 2024 API updates Added support for Grounding with Google Search . October 3, 2024 Model updates: Released gemini-1.5-flash-8b-001 , a stable version of our smallest Gemini\nAPI model. September 24, 2024 Model updates: Released gemini-1.5-pro-002 and gemini-1.5-flash-002 , two new stable\nversions of Gemini 1.5 Pro and 1.5 Flash, for general availability. Updated the gemini-1.5-pro-latest model code to use gemini-1.5-pro-002 and the gemini-1.5-flash-latest model code to use gemini-1.5-flash-002 . Released gemini-1.5-flash-8b-exp-0924 to replace gemini-1.5-flash-8b-exp-0827 . Released the civic integrity safety filter for the Gemini API and AI Studio. Released support for two new parameters for Gemini 1.5 Pro and 1.5 Flash in\nPython and NodeJS: frequencyPenalty and presencePenalty . September 19, 2024 AI Studio updates: Added thumb-up and thumb-down buttons to model responses, to enable users to\nprovide feedback on the quality of a response. API updates: Added support for Google Cloud credits, which can now be used towards\nGemini API usage. September 17, 2024 AI Studio updates: Added an Open in Colab button that exports a prompt – and the\ncode to run it – to a Colab notebook. The feature doesn't yet support\nprompting with tools (JSON mode, function calling, or code execution). September 13, 2024 AI Studio updates: Added support for compare mode, which lets you compare responses across\n models and prompts to find the best fit for your use case. August 30, 2024 Model updates Gemini 1.5 Flash supports supplying JSON schema through model configuration . August 27, 2024 Model updates: Released the following experimental models : gemini-1.5-pro-exp-0827 gemini-1.5-flash-exp-0827 gemini-1.5-flash-8b-exp-0827 August 9, 2024 API updates Added support for PDF processing . August 5, 2024 Model updates Fine-tuning support released for Gemini 1.5 Flash. August 1, 2024 Model updates Released gemini-1.5-pro-exp-0801 , a new experimental version of Gemini 1.5 Pro . July 12, 2024 Model updates Support for Gemini 1.0 Pro Vision removed from Google AI services and tools. June 27, 2024 Model updates General availability release for Gemini 1.5 Pro's 2M context window. API updates Added support for code execution . June 18, 2024 API updates Added support for context caching . June 12, 2024 Model updates Gemini 1.0 Pro Vision deprecated. May 23, 2024 Model updates Gemini 1.5 Pro ( gemini-1.5-pro-001 ) is generally available (GA). Gemini 1.5 Flash ( gemini-1.5-flash-001 ) is generally available (GA). May 14, 2024 API updates Introduced a 2M context window for Gemini 1.5 Pro (waitlist). Introduced pay-as-you-go billing for Gemini 1.0\nPro, with Gemini 1.5 Pro and Gemini 1.5 Flash billing coming soon. Introduced increased rate limits for the upcoming paid tier of Gemini 1.5\nPro. Added built-in video support to the File API . Added plain text support to the File API . Added support for parallel function calling, which returns more than one\ncall at a time. May 10, 2024 Model updates Released Gemini 1.5 Flash ( gemini-1.5-flash-latest ) in preview. April 9, 2024 Model updates Released Gemini 1.5 Pro ( gemini-1.5-pro-latest ) in preview. Released a new text embedding model, text-embeddings-004 , which supports elastic embedding sizes under 768. API updates Released the File API for temporarily storing\nmedia files for use in prompting. Added support for prompting with text, image, and audio data, also\nknown as multimodal prompting. To learn more, see Prompting with media . Released System instructions in\nbeta. Added Function calling mode ,\nwhich defines the execution behavior for function calling. Added support for the response_mime_type configuration option, which lets\nyou request responses in JSON format . March 19, 2024 Added support for tuning Gemini 1.0 Pro in Google AI Studio or with the Gemini API. December 13 2023 4 new models: gemini-pro: New text model for a wide variety of tasks. Balances capability\nand efficiency. gemini-pro-vision: New multimodal model for a wide variety of tasks.\nBalances capability and efficiency. embedding-001: New embeddings model. aqa: A new specially tuned model that is trained to answer questions\n  using text passages for grounding generated answers. See Gemini models for more details. API version updates: v1: The stable API channel. v1beta: Beta channel. This channel has features that may be under\ndevelopment. See the API versions topic for more details. API updates GenerateContent is a single unified endpoint for chat and text. Streaming available through the StreamGenerateContent method. Multimodal capability: Image is a new supported modality New beta features: Function Calling Semantic Retriever Attributed Question Answering (AQA) Updated candidate count: Gemini models only return 1 candidate. Different Safety Settings and SafetyRating categories. See safety settings for more details. Tuning models is not yet supported for Gemini models (Work in progress). Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-19 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Keep track of updates to the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/changelog"
            ],
            "timestamp": "2024-12-27T22:47:06.297564",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/models/gemini",
            "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
            "text_content": "Gemini models  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Gemini models 2.0 Flash experiment Our newest multimodal model, with next generation features and improved\n      capabilities Input audio, images, video, and text — get text, image, and audio responses Features low-latency conversational interactions with our Multimodal Live API 1.5 Flash spark Our most balanced multimodal model with great performance\n      for most tasks Input audio, images, video, and text, get text responses Generate code, extract data, edit text, and more Best for tasks balancing performance and cost 1.5 Pro Our best performing multimodal model with features for a wide variety of reasoning tasks Input audio, images, video, and text, get text responses Generate code, extract data, edit text, and more For when you need a boost in performance Model variants The Gemini API offers different models that are optimized for specific use\ncases. Here's a brief overview of Gemini variants that are available: Model variant Input(s) Output Optimized for Gemini 2.0 Flash gemini-2.0-flash-exp Audio, images, videos, and text Text, images (coming soon), and audio (coming soon) Next generation features, speed, and multimodal generation for a diverse variety of tasks Gemini 1.5 Flash gemini-1.5-flash Audio, images, videos, and text Text Fast and versatile performance across a diverse variety of tasks Gemini 1.5 Flash-8B gemini-1.5-flash-8b Audio, images, videos, and text Text High volume and lower intelligence tasks Gemini 1.5 Pro gemini-1.5-pro Audio, images, videos, and text Text Complex reasoning tasks requiring more intelligence Gemini 1.0 Pro gemini-1.0-pro (Deprecated on 2/15/2025) Text Text Natural language tasks, multi-turn text and code chat, and code\n      generation Text Embedding text-embedding-004 Text Text embeddings Measuring the relatedness of text strings AQA aqa Text Text Providing source-grounded answers to questions Gemini 2.0 Flash (Experimental) Important: Gemini 2.0 Flash is available as an experimental preview,\n    and some supported features are not currently available. If you need a model\n    for production-level code, use one of our 1.5 models instead. Gemini 2.0 Flash delivers next-gen features and improved capabilities,\n    including superior speed, native tool use, multimodal generation, and a 1M token\n    context window. Learn more about Gemini 2.0 Flash in our overview page . Try in Google AI Studio Model details Property Description id_card Model code models/gemini-2.0-flash-exp save Supported data types Inputs Audio, images, video, and text Output Audio (coming soon), images (coming soon), and text token_auto Token limits [*] Input token limit 1,048,576 Output token limit 8,192 swap_driving_apps_wheel Rate limits [**] 10 RPM 4 million TPM 1,500 RPD handyman Capabilities Structured outputs Supported Caching Not supported Tuning Not supported Function calling Supported Code execution Supported Search Supported Image generation Supported Native tool use Supported Audio generation Supported Multimodal Live API Supported 123 Versions Read the model version patterns for more details. Latest: gemini-2.0-flash-exp calendar_month Latest update December 2024 cognition_2 Knowledge cutoff August 2024 Gemini 1.5 Flash Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across\n    diverse tasks. Try in Google AI Studio Model details Property Description id_card Model code models/gemini-1.5-flash save Supported data types Inputs Audio, images, video, and text Output Text token_auto Token limits [*] Input token limit 1,048,576 Output token limit 8,192 movie_info Audio/visual specs Maximum number of images per prompt 3,600 Maximum video length 1 hour Maximum audio length Approximately 9.5 hours swap_driving_apps_wheel Rate limits [**] Free: 15 RPM 1 million TPM 1,500 RPD Pay-as-you-go: 2,000 RPM 4 million TPM handyman Capabilities System instructions Supported JSON mode Supported JSON schema Supported Adjustable safety settings Supported Caching Supported Tuning Supported Function calling Supported Code execution Supported Bidirectional streaming Not supported 123 Versions Read the model version patterns for more details. Latest: gemini-1.5-flash-latest Latest stable: gemini-1.5-flash Stable: gemini-1.5-flash-001 gemini-1.5-flash-002 calendar_month Latest update September 2024 Gemini 1.5 Flash-8B Gemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. Try in Google AI Studio Model details Property Description id_card Model code models/gemini-1.5-flash-8b save Supported data types Inputs Audio, images, video, and text Output Text token_auto Token limits [*] Input token limit 1,048,576 Output token limit 8,192 movie_info Audio/visual specs Maximum number of images per prompt 3,600 Maximum video length 1 hour Maximum audio length Approximately 9.5 hours swap_driving_apps_wheel Rate limits [**] Free: 15 RPM 1 million TPM 1,500 RPD Pay-as-you-go: 4,000 RPM 4 million TPM handyman Capabilities System instructions Supported JSON mode Supported JSON schema Supported Adjustable safety settings Supported Caching Supported Tuning Supported Function calling Supported Code execution Supported Bidirectional streaming Not supported 123 Versions Read the model version patterns for more details. Latest: gemini-1.5-flash-8b-latest Latest stable: gemini-1.5-flash-8b Stable: gemini-1.5-flash-8b-001 calendar_month Latest update October 2024 Gemini 1.5 Pro Gemini 1.5 Pro is a mid-size multimodal model that is optimized for\n    a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data\n    at once, including 2 hours of video, 19 hours of audio, codebases with\n    60,000 lines of code, or 2,000 pages of text. Try in Google AI Studio Model details Property Description id_card Model code models/gemini-1.5-pro save Supported data types Inputs Audio, images, video, and text Output Text token_auto Token limits [*] Input token limit 2,097,152 Output token limit 8,192 movie_info Audio/visual specs Maximum number of images per prompt 7,200 Maximum video length 2 hours Maximum audio length Approximately 19 hours swap_driving_apps_wheel Rate limits [**] Free: 2 RPM 32,000 TPM 50 RPD Pay-as-you-go: 1,000 RPM 4 million TPM handyman Capabilities System instructions Supported JSON mode Supported JSON schema Supported Adjustable safety settings Supported Caching Supported Tuning Not supported Function calling Supported Code execution Supported Bidirectional streaming Not supported 123 Versions Read the model version patterns for more details. Latest: gemini-1.5-pro-latest Latest stable: gemini-1.5-pro Stable: gemini-1.5-pro-001 gemini-1.5-pro-002 calendar_month Latest update September 2024 Gemini 1.0 Pro (Deprecated) Important: 1.0 Pro is deprecated and will be removed on\n  February 15, 2025. Use 1.5 Pro or 1.5 Flash instead. Gemini 1.0 Pro is an NLP model that handles tasks like multi-turn text and\n    code chat, and code generation. Try in Google AI Studio Model details Property Description id_card Model code models/gemini-1.0-pro save Supported data types Input Text Output Text swap_driving_apps_wheel Rate limits [**] Free: 15 RPM 32,000 TPM 1,500 RPD Pay-as-you-go: 360 RPM 120,000 TPM 30,000 RPD handyman Capabilities System instructions Not supported JSON mode Not supported JSON schema Not supported Adjustable safety settings Supported Caching Not supported Tuning Supported Function calling Supported Function calling configuration Not supported Code execution Not supported Bidirectional streaming Not supported 123 Versions Latest: gemini-1.0-pro-latest Latest stable: gemini-1.0-pro Stable: gemini-1.0-pro-001 calendar_month Latest update February 2024 Note: gemini-pro is an alias for gemini-1.0-pro . Text Embedding and Embedding Text Embedding Text embeddings are used to measure the relatedness of strings and are widely used in\n    many AI applications. text-embedding-004 achieves a stronger retrieval performance and outperforms existing models with comparable dimensions, on the standard MTEB embedding benchmarks. Model details Property Description id_card Model code Gemini API models/text-embedding-004 save Supported data types Input Text Output Text embeddings token_auto Token limits [*] Input token limit 2,048 Output dimension size 768 swap_driving_apps_wheel Rate limits [**] 1,500 requests per minute encrypted Adjustable safety settings Not supported calendar_month Latest update April 2024 Embedding Note: Text Embedding is the newer version of the Embedding model. If\n    you're creating a new project, use Text Embedding. You can use the Embedding model to generate text embeddings for\n    input text. The Embedding model is optimized for creating embeddings with 768 dimensions\n    for text of up to 2,048 tokens. Embedding model details Property Description id_card Model code models/embedding-001 save Supported data types Input Text Output Text embeddings token_auto Token limits [*] Input token limit 2,048 Output dimension size 768 swap_driving_apps_wheel Rate limits [**] 1,500 requests per minute encrypted Adjustable safety settings Not supported calendar_month Latest update December 2023 AQA You can use the AQA model to perform Attributed Question-Answering (AQA)–related tasks over a document, corpus, or a set of passages. The AQA\n    model returns answers to questions that are grounded in provided sources,\n    along with estimating answerable probability. Model details Property Description id_card Model code models/aqa save Supported data types Input Text Output Text language Supported language English token_auto Token limits [*] Input token limit 7,168 Output token limit 1,024 swap_driving_apps_wheel Rate limits [**] 1,500 requests per minute encrypted Adjustable safety settings Supported calendar_month Latest update December 2023 See the examples to explore the capabilities of these model\nvariations. [*]   A token is equivalent to about 4 characters for Gemini models. 100 tokens\n  are about 60-80 English words. [**] RPM: Requests per minute TPM: Tokens per minute RPD: Requests per day TPD: Tokens per day Due to capacity limitations, specified maximum rate limits are not\n  guaranteed. Model version name patterns Gemini models are available in either preview or stable versions. In your\ncode, you can use one of the following model name formats to specify which model\nand version you want to use. Latest: Points to the cutting-edge version of the model for a specified\ngeneration and variation. The underlying model is updated regularly and might\nbe a preview version. Only exploratory testing apps and prototypes should\nuse this alias. To specify the latest version, use the following pattern: <model>-<generation>-<variation>-latest . For example, gemini-1.0-pro-latest . Latest stable: Points to the most recent stable version released for the\nspecified model generation and variation. To specify the latest stable version, use the following pattern: <model>-<generation>-<variation> . For example, gemini-1.0-pro . Stable: Points to a specific stable model. Stable models don't change.\nMost production apps should use a specific stable model. To specify a stable version, use the following pattern: <model>-<generation>-<variation>-<version> . For example, gemini-1.0-pro-001 . Experimental: Points to an experimental model available in Preview,\nas defined in the Terms ,\nmeaning it is not for production use. We release experimental models\nto gather feedback, get our latest updates into the hands of developers\nquickly, and highlight the pace of innovation happening at Google. What\nwe learn from experimental launches informs how we release models more\nwidely. An experimental model can be swapped for another without prior\nnotice. We don't guarantee that an experimental model will become a\nstable model in the future. To specify an experimental version, use the following pattern: <model>-<generation>-<variation>-<version> . For example, gemini-exp-1121 . Available languages Gemini models are trained to work with the following languages: Arabic ( ar ) Bengali ( bn ) Bulgarian ( bg ) Chinese simplified and traditional ( zh ) Croatian ( hr ) Czech ( cs ) Danish ( da ) Dutch ( nl ) English ( en ) Estonian ( et ) Finnish ( fi ) French ( fr ) German ( de ) Greek ( el ) Hebrew ( iw ) Hindi ( hi ) Hungarian ( hu ) Indonesian ( id ) Italian ( it ) Japanese ( ja ) Korean ( ko ) Latvian ( lv ) Lithuanian ( lt ) Norwegian ( no ) Polish ( pl ) Portuguese ( pt ) Romanian ( ro ) Russian ( ru ) Serbian ( sr ) Slovak ( sk ) Slovenian ( sl ) Spanish ( es ) Swahili ( sw ) Swedish ( sv ) Thai ( th ) Turkish ( tr ) Ukrainian ( uk ) Vietnamese ( vi ) Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-16 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn about Google&#39;s most advanced AI models, the Gemini model family, including Gemini 1.5 Flash, Gemini 1.5 Pro, and more",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/models/gemini"
            ],
            "timestamp": "2024-12-27T22:47:07.334340",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
            "title": "Gemini 2.0 Flash (experimental)  |  Gemini API  |  Google AI for Developers",
            "text_content": "Gemini 2.0 Flash (experimental)  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Gemini 2.0 Flash (experimental) Gemini 2.0 Flash is now available as an experimental preview release through\nthe Gemini Developer API and Google AI Studio. The model introduces new features\nand enhanced core capabilities: Multimodal Live API: This new API helps you create real-time vision and\naudio streaming applications with tool use. Speed and performance: Gemini 2.0 has a significantly improved time to\nfirst token (TTFT) over 1.5 Flash. Quality: Better performance across most benchmarks than Gemini 1.5 Pro. Improved agentic capabilities: Gemini 2.0 delivers improvements to\nmultimodal understanding, coding, complex instruction following, and\nfunction calling. New modalities: Gemini 2.0 introduces native image generation and\ncontrollable text-to-speech capabilities. To provide a better developer experience, we're also shipping a new SDK . For Gemini 2.0 technical details, see Gemini models . Note: Native image and audio generation are in private experimental release, under\nallowlist. All other features are public experimental. Google Gen AI SDK (experimental) The new Google Gen AI SDK provides a unified interface to Gemini 2.0 through both\nthe Gemini Developer API and the Gemini API on Vertex AI. With a few exceptions,\ncode that runs on one platform will run on both. The new SDK is available in\nPython and Go, with Java and JavaScript coming soon. Install the new SDK: pip install google-genai Then import the library, initialize a client, and generate content: from google import genai client = genai . Client ( api_key = \"GEMINI_API_KEY\" ) response = client . models . generate_content ( model = 'gemini-2.0-flash-exp' , contents = 'How does AI work?' ) print ( response . text ) Python developers can also try out the Getting Started\nnotebook in the Gemini Cookbook . Multimodal Live API To try a tutorial that lets you use your voice and camera to talk to Gemini\n  through the Multimodal Live API, see the Web Console Demo project . The Multimodal Live API enables low-latency bidirectional voice and video\ninteractions with Gemini. Using the Multimodal Live API, you can provide end\nusers with the experience of natural, human-like voice conversations, and with\nthe ability to interrupt the model's responses using voice commands. The model\ncan process text, audio, and video input, and it can provide text and audio\noutput. You can try the Multimodal Live API in Google AI Studio . To learn more the API's\ncapabilities and limitations, see the Multimodal Live API reference\nguide . Search as a tool Using Grounding with Google Search, you can improve the accuracy and recency of\nresponses from the model. Starting with Gemini 2.0, Google Search is available\nas a tool. This means that the model can decide when to use Google Search. The\nfollowing example shows how to configure Search as a tool. from google import genai from google.genai.types import Tool , GenerateContentConfig , GoogleSearch client = genai . Client () model_id = \"gemini-2.0-flash-exp\" google_search_tool = Tool ( google_search = GoogleSearch () ) response = client . models . generate_content ( model = model_id , contents = \"When is the next total solar eclipse in the United States?\" , config = GenerateContentConfig ( tools = [ google_search_tool ], response_modalities = [ \"TEXT\" ], ) ) for each in response . candidates [ 0 ] . content . parts : print ( each . text ) # Example response: # The next total solar eclipse visible in the contiguous United States will be on ... # To get grounding metadata as web content. print ( response . candidates [ 0 ] . grounding_metadata . search_entry_point . rendered_content ) The Search-as-a-tool functionality also enables multi-turn searches and\nmulti-tool queries (for example, combining Grounding with Google Search and code\nexecution). Search as a tool enables complex prompts and workflows that require planning,\nreasoning, and thinking: Grounding to enhance factuality and recency and provide more accurate answers Retrieving artifacts from the web to do further analysis on Finding relevant images, videos, or other media to assist in multimodal\nreasoning or generation tasks Coding, technical troubleshooting, and other specialized tasks Finding region-specific information or assisting in translating content\naccurately Finding relevant websites for further browsing You can get started by trying the Search tool notebook . Improved tool use Gemini 2.0 introduces improvements to function calling and tools that provide\nbetter support for agentic experiences. Compositional function calling Gemini 2.0 supports a new function calling capability: compositional function calling . Compositional function calling enables the\nGemini API to invoke multiple user-defined functions automatically in the\nprocess of generating a response. For example, to respond to the prompt \"Get the temperature in my current location\" , the Gemini API might invoke both\na get_current_location() function and a get_weather() function that takes\nthe location as a parameter. Compositional function calling with code execution requires bidirectional\nstreaming and is only supported by the new Multimodal Live API. Here's an\nexample showing how you might use compositional function calling, code\nexecution, and the Multimodal Live API together: Note: The run() function declaration, which handles the asynchronous websocket\nsetup, is omitted for brevity. turn_on_the_lights_schema = { 'name' : 'turn_on_the_lights' } turn_off_the_lights_schema = { 'name' : 'turn_off_the_lights' } prompt = \"\"\" Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights? \"\"\" tools = [ { 'code_execution' : {}}, { 'function_declarations' : [ turn_on_the_lights_schema , turn_off_the_lights_schema ]} ] await run ( prompt , tools = tools , modality = \"AUDIO\" ) Python developers can try this out in the Live API Tool Use\nnotebook . Multi-tool use With Gemini 2.0, you can enable multiple tools at the same time, and the model\nwill decide when to call them. Here's an example that enables two tools,\nGrounding with Google Search and code execution, in a request using the\nMultimodal Live API. Note: The run() function declaration, which handles the asynchronous websocket\nsetup, is omitted for brevity. prompt = \"\"\" Hey, I need you to do three things for me. 1. Turn on the lights. 2. Then compute the largest prime palindrome under 100000. 3. Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024. Thanks! \"\"\" tools = [ { 'google_search' : {}}, { 'code_execution' : {}}, { 'function_declarations' : [ turn_on_the_lights_schema , turn_off_the_lights_schema ]} ] await run ( prompt , tools = tools , modality = \"AUDIO\" ) Python developers can try this out in the Live API Tool Use\nnotebook . Bounding box detection In this experimental launch, we are providing developers with a powerful tool\nfor object detection and localization within images and video. By accurately\nidentifying and delineating objects with bounding boxes, developers can unlock a\nwide range of applications and enhance the intelligence of their projects. Key Benefits: Simple: Integrate object detection capabilities into your applications\nwith ease, regardless of your computer vision expertise. Customizable: Produce bounding boxes based on custom instructions (e.g. \"I\nwant to see bounding boxes of all the green objects in this image\"), without\nhaving to train a custom model. Technical Details: Input: Your prompt and associated images or video frames. Output: Bounding boxes in the [y_min, x_min, y_max, x_max] format. The\ntop left corner is the origin. The x and y axis go horizontally and\nvertically, respectively. Coordinate values are normalized to 0-1000 for every\nimage. Visualization: AI Studio users will see bounding boxes plotted within the\nUI. Vertex AI users should visualize their bounding boxes through custom\nvisualization code. For Python developers, try the 2D spatial understanding\nnotebook or the experimental 3D pointing\nnotebook . Speech generation (early access/allowlist) Gemini 2.0 supports a new multimodal generation capability: text to speech.\nUsing the text-to-speech capability, you can prompt the model to generate high\nquality audio output that sounds like a human voice ( say \"hi everyone\" ), and\nyou can further refine the output by steering the voice. Image generation (early access/allowlist) Gemini 2.0 supports the ability to output text with in-line images. This lets\nyou use Gemini to conversationally edit images or generate multimodal outputs\n(for example, a blog post with text and images in a single turn). Previously\nthis would have required stringing together multiple models. Image generation is available as a private experimental release. It supports the\nfollowing modalities and capabilities: Text to image Example prompt: \"Generate an image of the Eiffel tower with fireworks in\nthe background.\" Text to image(s) and text (interleaved) Example prompt: \"Generate an illustrated recipe for a paella.\" Image(s) and text to image(s) and text (interleaved) Example prompt: (With an image of a furnished room) \"What other color\nsofas would work in my space? can you update the image?\" Image editing (text and image to image) Example prompt: \"Edit this image to make it look like a cartoon\" Example prompt: [image of a cat] + [image of a pillow] + \"Create a cross\nstitch of my cat on this pillow.\" Multi-turn image editing (chat) Example prompts: [upload an image of a blue car.] \"Turn this car into a\nconvertible.\" \"Now change the color to yellow.\" Watermarking All generated images include a SynthID watermark. Limitations: Generation of people and editing of uploaded images of people are not allowed. For best performance, use the following languages: EN, es-MX, ja-JP, zh-CN,\nhi-IN. Image generation does not support audio or video inputs. Image generation may not always trigger: The model may output text only. Try asking for image outputs explicitly\n(e.g. \"generate an image\", \"provide images as you go along\", \"update the\nimage\"). The model may stop generating partway through. Try again or try a different\nprompt. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-24 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2"
            ],
            "timestamp": "2024-12-27T22:47:08.086530",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/thinking-mode",
            "title": "Gemini 2.0 Flash Thinking Mode  |  Gemini API  |  Google AI for Developers",
            "text_content": "Gemini 2.0 Flash Thinking Mode  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Gemini 2.0 Flash Thinking Mode Gemini 2.0 Flash Thinking Mode is an experimental model that's trained to\ngenerate the \"thinking process\" the model goes through as part of its response.\nAs a result, Thinking Mode is capable of stronger reasoning capabilities in its\nresponses than the Gemini 2.0 Flash Experimental model. Using Thinking Models Thinking models are available in Google AI\nStudio ,\nand through the Gemini API. One of the main considerations when using a model that\nreturns the thinking process is how much information you want to expose to\nend users, as the thinking process can be quite verbose. Note: We have setup gemini-2.0-flash-thinking-exp as an alias to the latest\nthinking model. Use this alias to get the latest Flash thinking model, or specify\nthe full model name. Sending a basic request Python This example uses the new Google Genai SDK which is useful in this context for parsing out \"thoughts\" programmatically. from google import genai client = genai . Client ( api_key = 'GEMINI_API_KEY' , http_options = { 'api_version' : 'v1alpha' }) response = client . models . generate_content ( model = 'gemini-2.0-flash-thinking-exp' , contents = 'Explain how RLHF works in simple terms.' ) # Usually the first part is the thinking process, but it's not guaranteed print ( response . candidates [ 0 ] . content . parts [ 0 ] . text ) print ( response . candidates [ 0 ] . content . parts [ 1 ] . text ) Working with thoughts On a standard request, the model responds with two parts, the thoughts and the\nmodel response. You can check programmatically if a part is a thought or not by\nseeing if the part.thought field is set to True . Python To use the new thought parameter, you need to use the v1alpha version of\nthe Gemini API along with the new Google Genai SDK . from google import genai client = genai . Client ( api_key = 'GEMINI_API_KEY' , http_options = { 'api_version' : 'v1alpha' }) response = client . models . generate_content ( model = 'gemini-2.0-flash-thinking-exp' , contents = 'Explain how RLHF works in simple terms.' ) for part in response . candidates [ 0 ] . content . parts : if part . thought == True : print ( f \"Model Thought: \\n { part . text } \\n \" ) else : print ( f \" \\n Model Response: \\n { part . text } \\n \" ) Streaming model thinking Thinking models generally take longer to response than standard models. To\nstream the model thinking, you can use the generate_content_stream method. Python To use the new thought parameter, you need to use the v1alpha version of\nthe Gemini API along with the new Google Genai SDK . from google import genai client = genai . Client ( api_key = 'GEMINI_API_KEY' , http_options = { 'api_version' : 'v1alpha' }) for chunk in client . models . generate_content_stream ( model = 'gemini-2.0-flash-thinking-exp' , contents = 'What is your name?' ): for part in chunk . candidates [ 0 ] . content . parts : if part . thought == True : print ( f \"Model Thought Chunk: \\n { part . text } \\n \" ) else : print ( f \" \\n Model Response: \\n { part . text } \\n \" ) Multi-turn thinking conversations During multi-turn conversations, the model will by default not pass the thoughts\nfrom the previous turn to the next turn, but you can still see the thoughts on\nthe most recent turn. Python The new Google Genai SDK provides the ability\nto create a multi-turn chat session which is helpful to manage the state of\na conversation. import asyncio from google import genai client = genai . Client ( api_key = 'GEMINI_API_KEY' , http_options = { 'api_version' : 'v1alpha' }) async def main (): chat = client . aio . chats . create ( model = 'gemini-2.0-flash-thinking-exp' ) response = await chat . send_message ( 'What is your name?' ) print ( response . text ) response = await chat . send_message ( 'What did you just say before this?' ) print ( response . text ) asyncio . run ( main ()) Limitations Thinking Mode is an experimental model and has the following limitations: 32k token input limit Text and image input only 8k token output limit Text only output No built-in tool usage like Search or code execution What's next? Try Thinking Mode for yourself with our Colab\nnotebook ,\nor open Google AI\nStudio and try prompting the model for yourself. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-23 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/thinking-mode"
            ],
            "timestamp": "2024-12-27T22:47:08.997781",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/models/experimental-models",
            "title": "Experimental models  |  Gemini API  |  Google AI for Developers",
            "text_content": "Experimental models  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Experimental models In addition to the base models , the Gemini API\noffers experimental models available in Preview, as defined in the Terms , meaning it is not for\nproduction use. We release experimental models to gather feedback, get our\nlatest updates into the hands of developers quickly, and highlight the pace of\ninnovation happening at Google. What we learn from experimental launches informs\nhow we release models more widely. An experimental model can be swapped for\nanother without prior notice. We don't guarantee that an experimental model will\nbecome a stable model in the future. Use an experimental model Important: Support for an experimental model can change at any time. The Gemini API's experimental models are available for all users. You can use\nthe experimental models either in your code directly using the Gemini API, or\nyou can use an experimental model in Google AI Studio : Gemini API To use an experimental model, specify the model code when you initialize the\ngenerative model. For example: model = genai . GenerativeModel ( model_name = \"gemini-2.0-flash-exp\" ) Google AI Studio Select the model code of the experimental model you want to use in the Model drop-down menu in the Settings pane. Experimental models are\nlabeled Preview in the drop-down menu. Available models Model code Base model Highlights Release date gemini-2.0-flash-thinking-exp-1219 Gemini 2.0 Flash Thinking Mode Reasoning for complex problems, features a new Thinking mode December 19, 2024 gemini-2.0-flash-exp Gemini 2.0 Flash Next generation features, superior speed, native tool use, and multimodal generation December 11, 2024 gemini-exp-1206 Gemini Quality improvements, celebrate 1 year of Gemini December 6th, 2024 learnlm-1.5-pro-experimental LearnLM 1.5 Pro Experimental Inputs: Audio, images, videos, and text Output: Text November 19, 2024 Note: Support for individual experimental models is not permanent or long-term,\nand can be turned off without prior warning. Experimental models cannot\nbe used in production code. Provide feedback You can provide feedback on the Gemini API's experimental models using our developer forum . Previous models As new versions or stable releases become available, we remove and replace\nexperimental models. You can find the previous experimental models we released in\nthe following section along with the replacement version: Model code Base model Replacement version gemini-exp-1121 Gemini N/A gemini-exp-1114 Gemini N/A gemini-1.5-pro-exp-0827 Gemini 1.5 Pro gemini-1.5-pro-002 gemini-1.5-pro-exp-0801 Gemini 1.5 Pro gemini-1.5-pro-002 gemini-1.5-flash-8b-exp-0924 Gemini 1.5 Flash-8B gemini-1.5-flash-8b gemini-1.5-flash-8b-exp-0827 Gemini 1.5 Flash-8B gemini-1.5-flash-8b Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-19 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models"
            ],
            "timestamp": "2024-12-27T22:47:09.774338",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/text-generation",
            "title": "Generate text using the Gemini API  |  Google AI for Developers",
            "text_content": "Generate text using the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Generate text using the Gemini API Python Node.js Go REST The Gemini API can generate text output when provided text, images, video, and\naudio as input. This guide shows you how to generate text using the generateContent and streamGenerateContent methods. To learn about working with Gemini's vision and audio capabilities,\nrefer to the Vision and Audio guides. What's next Now that you have explored the basics of the Gemini API, you might want to\ntry: Vision understanding : Learn how to use\nGemini's native vision understanding to process images and videos. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Audio understanding : Learn how to use\nGemini's native audio understanding to process audio files. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started building chat and text generation apps with the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/text-generation"
            ],
            "timestamp": "2024-12-27T22:47:10.673299",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/vision",
            "title": "Explore vision capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore vision capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore vision capabilities with the Gemini API Python Node.js Go REST The Gemini API is able to process images and videos, enabling a multitude of\n exciting developer use cases. Some of Gemini's vision capabilities include\n the ability to: Caption and answer questions about images Transcribe and reason over PDFs, including long documents up to 2 million token context window Describe, segment, and extract information from videos,\nincluding both visual frames and audio, up to 90 minutes long Detect objects in an image and return bounding box coordinates for them This tutorial demonstrates some possible ways to prompt the Gemini API with\nimages and video input, provides code examples,\nand outlines prompting best practices with multimodal vision capabilities.\nAll output is text-only. What's next This guide shows how to upload image and video files using the File API and\nthen generate text outputs from image and video inputs. To learn more,\nsee the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started building with Gemini&#39;s multimodal capabilities in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/vision"
            ],
            "timestamp": "2024-12-27T22:47:11.575127",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/audio",
            "title": "Explore audio capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore audio capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore audio capabilities with the Gemini API Python Node.js Go REST Gemini can respond to prompts about audio. For example, Gemini can: Describe, summarize, or answer questions about audio content. Provide a transcription of the audio. Provide answers or a transcription about a specific segment of the audio. Note: You can't generate audio output with the Gemini API. This guide demonstrates different ways to interact with audio files and audio\ncontent using the Gemini API. Supported audio formats Gemini supports the following audio format MIME types: WAV - audio/wav MP3 - audio/mp3 AIFF - audio/aiff AAC - audio/aac OGG Vorbis - audio/ogg FLAC - audio/flac Technical details about audio Gemini imposes the following rules on audio: Gemini represents each second of audio as 25 tokens; for example,\none minute of audio is represented as 1,500 tokens. Gemini can only infer responses to English-language speech. Gemini can \"understand\" non-speech components, such as birdsong or sirens. The maximum supported length of audio data in a single prompt is 9.5 hours.\nGemini doesn't limit the number of audio files in a single prompt; however,\nthe total combined length of all audio files in a single prompt cannot exceed\n9.5 hours. Gemini downsamples audio files to a 16 Kbps data resolution. If the audio source contains multiple channels, Gemini combines those channels\ndown to a single channel. What's next This guide shows how to upload audio files using the File API and then generate\ntext outputs from audio inputs. To learn more, see the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/audio"
            ],
            "timestamp": "2024-12-27T22:47:12.377661",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/long-context",
            "title": "Long context  |  Gemini API  |  Google AI for Developers",
            "text_content": "Long context  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Long context Gemini 1.5 Flash comes standard with a 1-million-token context window, and\nGemini 1.5 Pro comes with a 2-million-token context window. Historically, large\nlanguage models (LLMs) were significantly limited by the amount of text (or\ntokens) that could be passed to the model at one time. The Gemini 1.5 long\ncontext window, with near-perfect retrieval\n(>99%) ,\nunlocks many new use cases and developer paradigms. The code you already use for cases like text\ngeneration or multimodal\ninputs will work out of the box with long context. Throughout this guide, you briefly explore the basics of the context window, how\ndevelopers should think about long context, various real world use cases for\nlong context, and ways to optimize the usage of long context. What is a context window? The basic way you use the Gemini 1.5 models is by passing information (context)\nto the model, which will subsequently generate a response. An analogy for the\ncontext window is short term memory. There is a limited amount of information\nthat can be stored in someone's short term memory, and the same is true for\ngenerative models. You can read more about how models work under the hood in our generative models\nguide . Getting started with long context Most generative models created in the last few years were only capable of\nprocessing 8,000 tokens at a time. Newer models pushed this further by accepting\n32,000 tokens or 128,000 tokens. Gemini 1.5 is the first model capable of\naccepting 1 million tokens, and now 2 million tokens with Gemini 1.5\nPro . In practice, 1 million tokens would look like: 50,000 lines of code (with the standard 80 characters per line) All the text messages you have sent in the last 5 years 8 average length English novels Transcripts of over 200 average length podcast episodes Even though the models can take in more and more context, much of the\nconventional wisdom about using large language models assumes this inherent\nlimitation on the model, which as of 2024, is no longer the case. Some common strategies to handle the limitation of small context windows\nincluded: Arbitrarily dropping old messages / text from the context window as new text\ncomes in Summarizing previous content and replacing it with the summary when the\ncontext window gets close to being full Using RAG with semantic search to move data out of the context window and\ninto a vector database Using deterministic or generative filters to remove certain text /\ncharacters from prompts to save tokens While many of these are still relevant in certain cases, the default place to\nstart is now just putting all of the tokens into the context window. Because\nGemini 1.5 models were purpose-built with a long context window, they are much\nmore capable of in-context learning. For example, with only instructional\nmaterials (a 500-page reference grammar, a dictionary, and ≈ 400 extra parallel\nsentences) all provided in context, Gemini 1.5 Pro and Gemini 1.5 Flash are capable of learning to translate from English to Kalamang— a Papuan language with fewer than 200 speakers and\ntherefore almost no online presence—with quality similar to a person who learned\nfrom the same materials. This example underscores how you can start to think about what is possible with\nlong context and the in-context learning capabilities of Gemini 1.5. Long context use cases While the standard use case for most generative models is still text input, the\nGemini 1.5 model family enables a new paradigm of multimodal use cases. These\nmodels can natively understand text, video, audio, and images. They are\naccompanied by the Gemini API that takes in multimodal file\ntypes for\nconvenience. Long form text Text has proved to be the layer of intelligence underpinning much of the\nmomentum around LLMs. As mentioned earlier, much of the practical limitation of\nLLMs was because of not having a large enough context window to do certain\ntasks. This led to the rapid adoption of retrieval augmented generation (RAG)\nand other techniques which dynamically provide the model with relevant\ncontextual information. Now, with larger and larger context windows (currently\nup to 2 million on Gemini 1.5 Pro), there are new techniques becoming available\nwhich unlock new use cases. Some emerging and standard use cases for text based long context include: Summarizing large corpuses of text Previous summarization options with smaller context models would require\na sliding window or another technique to keep state of previous sections\nas new tokens are passed to the model Question and answering Historically this was only possible with RAG given the limited amount of\ncontext and models' factual recall being low Agentic workflows Text is the underpinning of how agents keep state of what they have done\nand what they need to do; not having enough information about the world\nand the agent's goal is a limitation on the reliability of agents Many-shot in-context learning is one of the\nmost unique capabilities unlocked by long context models. Research has shown\nthat taking the common \"single shot\" or \"multi-shot\" example paradigm, where the\nmodel is presented with one or a few examples of a task, and scaling that up to\nhundreds, thousands, or even hundreds of thousands of examples, can lead to\nnovel model capabilities. This many-shot approach has also been shown to perform\nsimilarly to models which were fine-tuned for a specific task. For use cases\nwhere a Gemini model's performance is not yet sufficient for a production\nrollout, you can try the many-shot approach. As you might explore later in the\nlong context optimization section, context caching makes this type of high input\ntoken workload much more economically feasible and even lower latency in some\ncases. Long form video Video content's utility has long been constrained by the lack of accessibility\nof the medium itself. It was hard to skim the content, transcripts often failed\nto capture the nuance of a video, and most tools don't process image, text, and\naudio together. With Gemini 1.5, the long-context text capabilities translate to\nthe ability to reason and answer questions about multimodal inputs with\nsustained performance. Gemini 1.5 Flash, when tested on the needle in a video\nhaystack problem with 1M tokens, obtained >99.8% recall of the video in the\ncontext window, and 1.5 Pro reached state of the art performance on the Video-MME benchmark . Some emerging and standard use cases for video long context include: Video question and answering Video memory, as shown with Google's Project Astra Video captioning Video recommendation systems, by enriching existing metadata with new\nmultimodal understanding Video customization, by looking at a corpus of data and associated video\nmetadata and then removing parts of videos that are not relevant to the\nviewer Video content moderation Real-time video processing When working with videos, it is important to consider how the videos are\nprocessed into tokens , which affects\nbilling and usage limits. You can learn more about prompting with video files in\nthe Prompting\nguide . Long form audio The Gemini 1.5 models were the first natively multimodal large language models\nthat could understand audio. Historically, the typical developer workflow would\ninvolve stringing together multiple domain specific models, like a\nspeech-to-text model and a text-to-text model, in order to process audio. This\nled to additional latency required by performing multiple round-trip requests\nand decreased performance usually attributed to disconnected architectures of\nthe multiple model setup. On standard audio-haystack evaluations, Gemini 1.5 Pro is able to find the\nhidden audio in 100% of the tests and Gemini 1.5 Flash is able to find it in\n98.7% of the\ntests .\nGemini 1.5 Flash accepts up to 9.5 hours of audio in a single\nrequest and\nGemini 1.5 Pro can accept up to 19 hours of audio using the 2-million-token\ncontext window. Further, on a test set of 15-minute audio clips, Gemini 1.5 Pro\narchives a word error rate (WER) of ~5.5%, much lower than even specialized\nspeech-to-text models, without the added complexity of extra input segmentation\nand pre-processing. Some emerging and standard use cases for audio context include: Real-time transcription and translation Podcast / video question and answering Meeting transcription and summarization Voice assistants You can learn more about prompting with audio files in the Prompting\nguide . Long context optimizations The primary optimization when working with long context and the Gemini 1.5\nmodels is to use context\ncaching . Beyond the previous\nimpossibility of processing lots of tokens in a single request, the other main\nconstraint was the cost. If you have a \"chat with your data\" app where a user\nuploads 10 PDFs, a video, and some work documents, you would historically have\nto work with a more complex retrieval augmented generation (RAG) tool /\nframework in order to process these requests and pay a significant amount for\ntokens moved into the context window. Now, you can cache the files the user\nuploads and pay to store them on a per hour basis. The input / output cost per\nrequest with Gemini\n1.5 Flash for example is ~4x less than the standard input / output cost, so if\nthe user chats with their data enough, it becomes a huge cost saving for you as\nthe developer. Long context limitations In various sections of this guide, we talked about how Gemini 1.5 models achieve\nhigh performance across various needle-in-a-haystack retrieval evals. These\ntests consider the most basic setup, where you have a single needle you are\nlooking for. In cases where you might have multiple \"needles\" or specific pieces\nof information you are looking for, the model does not perform with the same\naccuracy. Performance can vary to a wide degree depending on the context. This\nis important to consider as there is an inherent tradeoff between getting the\nright information retrieved and cost. You can get ~99% on a single query, but\nyou have to pay the input token cost every time you send that query. So for 100\npieces of information to be retrieved, if you needed 99% performance, you would\nlikely need to send 100 requests. This is a good example of where context\ncaching can significantly reduce the cost associated with using Gemini models\nwhile keeping the performance high. FAQs Do I lose model performance when I add more tokens to a query? Generally, if you don't need tokens to be passed to the model, it is best to\navoid passing them. However, if you have a large chunk of tokens with some\ninformation and want to ask questions about that information, the model is\nhighly capable of extracting that information (up to 99% accuracy in many\ncases). How does Gemini 1.5 Pro perform on the standard needle-in-a-haystack test? Gemini 1.5 Pro achieves 100% recall up to 530k tokens and >99.7% recall up to\n1M\ntokens . How can I lower my cost with long-context queries? If you have a similar set of tokens / context that you want to re-use many\ntimes, context caching can help reduce the costs\nassociated with asking questions about that information. How can I get access to the 2-million-token context window? All developers now have access to the 2-million-token context window with Gemini\n1.5 Pro. Does the context length affect the model latency? There is some fixed amount of latency in any given request, regardless of the\nsize, but generally longer queries will have higher latency (time to first\ntoken). Do the long context capabilities differ between Gemini 1.5 Flash and Gemini 1.5 Pro? Yes, some of the numbers were mentioned in different sections of this guide, but\ngenerally Gemini 1.5 Pro is more performant on most long context use cases. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-09-28 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn about how to get started building with long context (1 million and 2 million context window) on Gemini 1.5 Pro and Gemini 1.5 Flash",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/long-context"
            ],
            "timestamp": "2024-12-27T22:47:13.097783",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/code-execution",
            "title": "Code execution  |  Gemini API  |  Google AI for Developers",
            "text_content": "Code execution  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Code execution Python Node.js Go REST The Gemini API code execution feature enables the model to generate and run\nPython code and learn iteratively from the results until it arrives at a\nfinal output. You can use this code execution capability to build applications\nthat benefit from code-based reasoning and that produce text output. For\nexample, you could use code execution in an application that solves equations or\nprocesses text. Code execution is available in both AI Studio and the Gemini API. In AI Studio,\nyou can enable code execution under Advanced settings . The Gemini API\nprovides code execution as a tool, similar to function calling . After you add\ncode execution as a tool, the model decides when to use it. Note: The code execution environment includes the NumPy and SymPy libraries. You can't install your own libraries. Get started with code execution This section assumes that you've completed the setup and configuration steps\nshown in the quickstart . Code execution versus function calling Code execution and function calling are\nsimilar features: Code execution lets the model run code in the API backend in a fixed, isolated\nenvironment. Function calling lets you run the functions that the model requests, in\nwhatever environment you want. In general you should prefer to use code execution if it can handle your use\ncase. Code execution is simpler to use (you just enable it) and resolves in a\nsingle GenerateContent request (thus incurring a single charge). Function\ncalling takes an additional GenerateContent request to send back the output\nfrom each function call (thus incurring multiple charges). For most cases, you should use function calling if you have your own functions\nthat you want to run locally, and you should use code execution if you'd like\nthe API to write and run Python code for you and return the result. Billing There's no additional charge for enabling code execution from the\nGemini API. You'll be billed at the current rate of input and output tokens . Here are a few other things to know about billing for code execution: You're only billed once for the input tokens you pass to the model, and\nyou're billed for the final output tokens returned to you by the model. Tokens representing generated code are counted as output tokens. Code execution results are also counted as output tokens. Limitations The model can only generate and execute code. It can't return other artifacts\nlike media files. The feature doesn't support file I/O or use cases that involve non-text output\n(for example, data plots or a CSV file upload). Code execution can run for a maximum of 30 seconds before timing out. In some cases, enabling code execution can lead to regressions in other areas\nof model output (for example, writing a story). There is some variation in the ability of the different models to use code\nexecution successfully. Gemini 1.5 Pro is the best performing\nmodel, based on our testing. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use the Gemini API context caching feature.",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/code-execution"
            ],
            "timestamp": "2024-12-27T22:47:14.414938",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/structured-output",
            "title": "Generate structured output with the Gemini API  |  Google AI for Developers",
            "text_content": "Generate structured output with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Generate structured output with the Gemini API Python Node.js Go Dart (Flutter) Android Swift Web REST Gemini generates unstructured text by default, but some applications require\nstructured text. For these use cases, you can constrain Gemini to respond with\nJSON, a structured data format suitable for automated processing. You can also\nconstrain the model to respond with one of the options specified in an enum. Here are a few use cases that might require structured output from the model: Build a database of companies by pulling company information out of\nnewspaper articles. Pull standardized information out of resumes. Extract ingredients from recipes and display a link to a grocery website for\neach ingredient. In your prompt, you can ask Gemini to produce JSON-formatted output, but note\nthat the model is not guaranteed to produce JSON and nothing but JSON.\nFor a more deterministic response, you can pass a specific JSON schema in a responseSchema field so that Gemini always responds with an expected structure. This guide shows you how to generate JSON using the generateContent method through the SDK\nof your choice or using the REST API directly. The examples show text-only\ninput, although Gemini can also produce JSON responses to multimodal requests\nthat include images , videos , and audio . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to generate structured output with the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/structured-output"
            ],
            "timestamp": "2024-12-27T22:47:15.587452",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/function-calling",
            "title": "Intro to function calling with the Gemini API  |  Google AI for Developers",
            "text_content": "Intro to function calling with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Intro to function calling with the Gemini API Using the Gemini API function calling feature, you can provide custom function\ndefinitions to the model. The model doesn't directly invoke these functions, but\ninstead generates structured output that specifies a function name and suggested\narguments. You can then use the function name and arguments to call an external\nAPI, and you can incorporate the resulting API output into a further query to\nthe model, enabling the model to provide a more comprehensive response and take\nadditional actions. Function calling empowers users to interact with real-time information and\nservices like databases, customer relationship management systems, and document\nrepositories. The feature also enhances the model's ability to provide relevant\nand contextual answers. Function calling is best for interacting with external\nsystems. If your use case requires the model to perform computation but doesn't\ninvolve external systems or APIs, you should consider using code execution instead. For a working example of function calling, see the \"light bot\" notebook . Beta: The function calling feature is in Beta release. For more information,\n  see the API versions page. How function calling works You use the function calling feature by adding structured query data describing\nprograming interfaces, called function declarations , to a model prompt. The\nfunction declarations provide the name of the API function, explain its purpose,\nany parameters it supports, and descriptions of those parameters. After you pass\na list of function declarations in a query to the model, it analyzes function\ndeclarations and the rest of the query to determine how to use the declared\nAPI in response to the request. The model then returns an object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond\nto the user's question. You can then take the recommended function call\nparameters, call the actual API, get a response, and provide that response to\nthe user or take further action. Note that the model doesn't actually call the\ndeclared functions. Instead, you use the returned schema object parameters to\ncall the function. The Gemini API also supports parallel function calling, where\nthe model recommends multiple API function calls based on a single request. Function declarations When you implement function calling in a prompt, you create a tools object,\nwhich contains one or more function declarations . You define functions\nusing JSON, specifically with a select subset of the OpenAPI schema format. A single function declaration can include the following parameters: name (string): The unique identifier for the function within the API\ncall. description (string): A comprehensive explanation of the function's\npurpose and capabilities. parameters (object): Defines the input data required by the function. type (string): Specifies the overall data type, such as object . properties (object): Lists individual parameters, each with: type (string): The data type of the parameter, such as string , integer , boolean . description (string): A clear explanation of the parameter's\npurpose and expected format. required (array): An array of strings listing the parameter names\nthat are mandatory for the function to operate. For code examples of a function declaration using cURL commands, see the Function calling examples . For examples of\n creating function declarations using the Gemini API SDKs, see the Function calling tutorial . Best practices for function declarations Accurately defining your functions is essential when integrating them into your\nrequests. Each function relies on specific parameters that guide its behavior\nand interaction with the model. The following listing provides guidance on\ndefining the parameters of an individual function in a functions_declarations array. name : Use clear, descriptive names without space, period\n( . ), or dash ( - ) characters. Instead, use underscore ( _ ) characters\nor camel case. description : Provide detailed, clear, and specific function\ndescriptions, providing examples if necessary. For example, instead of find theaters , use find theaters based on location and optionally movie\ntitle that is currently playing in theaters. Avoid overly broad or ambiguous\ndescriptions. properties > type : Use strongly typed parameters to reduce\nmodel hallucinations. For example, if the parameter values are from a finite\nset, use an enum field instead of listing the values in the description\n(e.g., \"type\": \"enum\", \"values\": [\"now_playing\", \"upcoming\"] ). If the\nparameter value is always an integer, set the type to integer rather than number . properties > description : Provide concrete examples and constraints.\nFor example, instead of the location to search , use The city and state,\ne.g. San Francisco, CA or a zip code e.g. 95616 . For more best practices when using function calling, see the Best Practices section. Function calling mode You can use the function calling mode parameter to modify the execution\nbehavior of the feature. There are three modes available: AUTO : The default model behavior. The model decides to predict either a\nfunction call or a natural language response. ANY : The model is constrained to always predict a function call. If allowed_function_names is not provided, the model picks from all of the\navailable function declarations. If allowed_function_names is provided,\nthe model picks from the set of allowed functions. NONE : The model won't predict a function call. In this case, the model\nbehavior is the same as if you don't pass any function declarations. The usage of the ANY mode (\"forced function calling\") is\nsupported for Gemini 1.5 Pro and Gemini 1.5 Flash models only. You can also pass a set of allowed_function_names that, when provided, limits\nthe functions that the model will call. You should only include allowed_function_names when the mode is ANY . Function names should match\nfunction declaration names. With the mode set to ANY and the allowed_function_names set, the model will predict a function call from the\nset of function names provided. Key Point: If you set the mode to ANY and provide allowed_function_names ,\nthe model picks from the set of allowed functions. If you set the mode to ANY and don't provide allowed_function_names , the model picks from all of the\navailable functions. The following code snippet from an example request shows how to\nset the mode to ANY and specify a list of allowed functions: \"tool_config\" : { \"function_calling_config\" : { \"mode\" : \"ANY\" , \"allowed_function_names\" : [ \"find_theaters\" , \"get_showtimes\" ] }, } Function calling examples This section provides example prompts for function calling using cURL commands.\nThe examples include single turn and multiple-turn scenarios, and enabling\ndifferent function calling modes. When using cURL commands with this feature, the function and parameter\ninformation is included in the tools element. Each function declaration in the tools element contains the function name, and you specify the parameters using\nan OpenAPI compatible schema ,\nand a function description. Single-turn example Single-turn is when you call the language model one time. With function calling,\na single-turn use case might be when you provide the model a natural language\nquery and a list of functions. In this case, the model uses the function\ndeclaration, which includes the function name, parameters, and description, to\npredict which function to call and the arguments to call it with. The following curl sample is an example of passing in a description of a\nfunction that returns information about where a movie is playing. Several\nfunction declarations are included in the request, such as find_movies and find_theaters . Single-turn function calling example request curl https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"contents\": {\n      \"role\": \"user\",\n      \"parts\": {\n        \"text\": \"Which theaters in Mountain View show Barbie movie?\"\n    }\n  },\n  \"tools\": [\n    {\n      \"function_declarations\": [\n        {\n          \"name\": \"find_movies\",\n          \"description\": \"find movie titles currently playing in theaters based on any description, genre, title words, etc.\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"description\": {\n                \"type\": \"string\",\n                \"description\": \"Any kind of description including category or genre, title words, attributes, etc.\"\n              }\n            },\n            \"required\": [\n              \"description\"\n            ]\n          }\n        },\n        {\n          \"name\": \"find_theaters\",\n          \"description\": \"find theaters based on location and optionally movie title which is currently playing in theaters\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"movie\": {\n                \"type\": \"string\",\n                \"description\": \"Any movie title\"\n              }\n            },\n            \"required\": [\n              \"location\"\n            ]\n          }\n        },\n        {\n          \"name\": \"get_showtimes\",\n          \"description\": \"Find the start times for movies playing in a specific theater\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"movie\": {\n                \"type\": \"string\",\n                \"description\": \"Any movie title\"\n              },\n              \"theater\": {\n                \"type\": \"string\",\n                \"description\": \"Name of the theater\"\n              },\n              \"date\": {\n                \"type\": \"string\",\n                \"description\": \"Date for requested showtime\"\n              }\n            },\n            \"required\": [\n              \"location\",\n              \"movie\",\n              \"theater\",\n              \"date\"\n            ]\n          }\n        }\n      ]\n    }\n  ]\n}' The response to this curl example might be similar to the following. Single-turn function calling curl example response [{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"functionCall\": {\n              \"name\": \"find_theaters\",\n              \"args\": {\n                \"movie\": \"Barbie\",\n                \"location\": \"Mountain View, CA\"\n              }\n            }\n          }\n        ]\n      },\n      \"finishReason\": \"STOP\",\n      \"safetyRatings\": [\n        {\n          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        }\n      ]\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 9,\n    \"totalTokenCount\": 9\n  }\n}] Single-turn example using ANY mode The following curl example is similar to the single-turn example , but it sets\nthe mode to ANY : \"tool_config\" : { \"function_calling_config\" : { \"mode\" : \"ANY\" }, } Single-turn function calling using ANY mode (request) curl https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"contents\": {\n      \"role\": \"user\",\n      \"parts\": {\n        \"text\": \"What movies are showing in North Seattle tonight?\"\n    }\n  },\n  \"tools\": [\n    {\n      \"function_declarations\": [\n        {\n          \"name\": \"find_movies\",\n          \"description\": \"find movie titles currently playing in theaters based on any description, genre, title words, etc.\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"description\": {\n                \"type\": \"string\",\n                \"description\": \"Any kind of description including category or genre, title words, attributes, etc.\"\n              }\n            },\n            \"required\": [\n              \"description\"\n            ]\n          }\n        },\n        {\n          \"name\": \"find_theaters\",\n          \"description\": \"find theaters based on location and optionally movie title which is currently playing in theaters\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"movie\": {\n                \"type\": \"string\",\n                \"description\": \"Any movie title\"\n              }\n            },\n            \"required\": [\n              \"location\"\n            ]\n          }\n        },\n        {\n          \"name\": \"get_showtimes\",\n          \"description\": \"Find the start times for movies playing in a specific theater\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"movie\": {\n                \"type\": \"string\",\n                \"description\": \"Any movie title\"\n              },\n              \"theater\": {\n                \"type\": \"string\",\n                \"description\": \"Name of the theater\"\n              },\n              \"date\": {\n                \"type\": \"string\",\n                \"description\": \"Date for requested showtime\"\n              }\n            },\n            \"required\": [\n              \"location\",\n              \"movie\",\n              \"theater\",\n              \"date\"\n            ]\n          }\n        }\n      ]\n    }\n  ],\n  \"tool_config\": {\n    \"function_calling_config\": {\n      \"mode\": \"ANY\"\n    },\n  }\n}' The response might be similar to the following: Single-turn function calling using ANY mode (response) {\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"functionCall\": {\n              \"name\": \"find_movies\",\n              \"args\": {\n                \"description\": \"\",\n                \"location\": \"North Seattle, WA\"\n              }\n            }\n          }\n        ],\n        \"role\": \"model\"\n      },\n      \"finishReason\": \"STOP\",\n      \"index\": 0,\n      \"safetyRatings\": [\n        {\n          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          \"probability\": \"NEGLIGIBLE\"\n        }\n      ]\n    }\n  ],\n  \"promptFeedback\": {\n    \"safetyRatings\": [\n      {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      }\n    ]\n  }\n} Single-turn example using ANY mode and allowed functions The following curl example is similar to the single-turn example , but it sets\nthe mode to ANY and includes a list of allowed\nfunctions: \"tool_config\" : { \"function_calling_config\" : { \"mode\" : \"ANY\" , \"allowed_function_names\" : [ \"find_theaters\" , \"get_showtimes\" ] }, } Single-turn function calling using ANY mode and allowed functions (request) curl https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"contents\": {\n      \"role\": \"user\",\n      \"parts\": {\n        \"text\": \"What movies are showing in North Seattle tonight?\"\n    }\n  },\n  \"tools\": [\n    {\n      \"function_declarations\": [\n        {\n          \"name\": \"find_movies\",\n          \"description\": \"find movie titles currently playing in theaters based on any description, genre, title words, etc.\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"description\": {\n                \"type\": \"string\",\n                \"description\": \"Any kind of description including category or genre, title words, attributes, etc.\"\n              }\n            },\n            \"required\": [\n              \"description\"\n            ]\n          }\n        },\n        {\n          \"name\": \"find_theaters\",\n          \"description\": \"find theaters based on location and optionally movie title which is currently playing in theaters\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"movie\": {\n                \"type\": \"string\",\n                \"description\": \"Any movie title\"\n              }\n            },\n            \"required\": [\n              \"location\"\n            ]\n          }\n        },\n        {\n          \"name\": \"get_showtimes\",\n          \"description\": \"Find the start times for movies playing in a specific theater\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"movie\": {\n                \"type\": \"string\",\n                \"description\": \"Any movie title\"\n              },\n              \"theater\": {\n                \"type\": \"string\",\n                \"description\": \"Name of the theater\"\n              },\n              \"date\": {\n                \"type\": \"string\",\n                \"description\": \"Date for requested showtime\"\n              }\n            },\n            \"required\": [\n              \"location\",\n              \"movie\",\n              \"theater\",\n              \"date\"\n            ]\n          }\n        }\n      ]\n    }\n  ],\n  \"tool_config\": {\n    \"function_calling_config\": {\n      \"mode\": \"ANY\",\n      \"allowed_function_names\": [\"find_theaters\", \"get_showtimes\"]\n    },\n  }\n}' The model can't predict the find_movies function, because it's not on the list\nof allowed functions, so it predicts a different function instead. The response\nmight be similar to the following: Single-turn function calling using ANY mode and allowed functions (response) {\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"functionCall\": {\n              \"name\": \"find_theaters\",\n              \"args\": {\n                \"location\": \"North Seattle, WA\",\n                \"movie\": null\n              }\n            }\n          }\n        ],\n        \"role\": \"model\"\n      },\n      \"finishReason\": \"STOP\",\n      \"index\": 0,\n      \"safetyRatings\": [\n        {\n          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        }\n      ]\n    }\n  ],\n  \"promptFeedback\": {\n    \"safetyRatings\": [\n      {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      }\n    ]\n  }\n} Multi-turn examples You can implement a multi-turn function calling scenario by doing the following: Get a function call response by calling the language model. This is the first\nturn. Call the language model using the function call response from the first turn\nand the function response you get from calling that function. This is the\nsecond turn. The response from the second turn either summarizes the results to answer your\nquery in the first turn, or contains a second function call you can use to get\nmore information for your query. This topic includes two multi-turn curl examples: Curl example that uses a function response from a previous turn Curl example that calls a language model multiple times Use a response from a previous turn The following curl sample calls the function and arguments returned by the\nprevious single-turn example to get a response. The method and parameters\nreturned by the single-turn example are in this JSON. \"functionCall\" : { \"name\" : \"find_theaters\" , \"args\" : { \"movie\" : \"Barbie\" , \"location\" : \"Mountain View, CA\" } } Multi-turn function calling curl example request curl https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"contents\": [{\n      \"role\": \"user\",\n      \"parts\": [{\n        \"text\": \"Which theaters in Mountain View show Barbie movie?\"\n    }]\n  }, {\n    \"role\": \"model\",\n    \"parts\": [{\n      \"functionCall\": {\n        \"name\": \"find_theaters\",\n        \"args\": {\n          \"location\": \"Mountain View, CA\",\n          \"movie\": \"Barbie\"\n        }\n      }\n    }]\n  }, {\n    \"role\": \"user\",\n    \"parts\": [{\n      \"functionResponse\": {\n        \"name\": \"find_theaters\",\n        \"response\": {\n          \"name\": \"find_theaters\",\n          \"content\": {\n            \"movie\": \"Barbie\",\n            \"theaters\": [{\n              \"name\": \"AMC Mountain View 16\",\n              \"address\": \"2000 W El Camino Real, Mountain View, CA 94040\"\n            }, {\n              \"name\": \"Regal Edwards 14\",\n              \"address\": \"245 Castro St, Mountain View, CA 94040\"\n            }]\n          }\n        }\n      }\n    }]\n  }],\n  \"tools\": [{\n    \"functionDeclarations\": [{\n      \"name\": \"find_movies\",\n      \"description\": \"find movie titles currently playing in theaters based on any description, genre, title words, etc.\",\n      \"parameters\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"STRING\",\n            \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n          },\n          \"description\": {\n            \"type\": \"STRING\",\n            \"description\": \"Any kind of description including category or genre, title words, attributes, etc.\"\n          }\n        },\n        \"required\": [\"description\"]\n      }\n    }, {\n      \"name\": \"find_theaters\",\n      \"description\": \"find theaters based on location and optionally movie title which is currently playing in theaters\",\n      \"parameters\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"STRING\",\n            \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n          },\n          \"movie\": {\n            \"type\": \"STRING\",\n            \"description\": \"Any movie title\"\n          }\n        },\n        \"required\": [\"location\"]\n      }\n    }, {\n      \"name\": \"get_showtimes\",\n      \"description\": \"Find the start times for movies playing in a specific theater\",\n      \"parameters\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"STRING\",\n            \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n          },\n          \"movie\": {\n            \"type\": \"STRING\",\n            \"description\": \"Any movie title\"\n          },\n          \"theater\": {\n            \"type\": \"STRING\",\n            \"description\": \"Name of the theater\"\n          },\n          \"date\": {\n            \"type\": \"STRING\",\n            \"description\": \"Date for requested showtime\"\n          }\n        },\n        \"required\": [\"location\", \"movie\", \"theater\", \"date\"]\n      }\n    }]\n  }]\n}' The response to this curl example includes the result of calling the find_theaters method. The response might be similar to the following: Multi-turn function calling curl example response {\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"text\": \" OK. Barbie is showing in two theaters in Mountain View, CA: AMC Mountain View 16 and Regal Edwards 14.\"\n          }\n        ]\n      }\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 9,\n    \"candidatesTokenCount\": 27,\n    \"totalTokenCount\": 36\n  }\n} Call the model multiple times The following cURL example calls the generative AI model multiple times to call\na function. Each time the model calls the function, it can use a different\nfunction to answer a different user query in the request. Multi-turn function calling curl example request curl https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"contents\": [{\n      \"role\": \"user\",\n      \"parts\": [{\n        \"text\": \"Which theaters in Mountain View show Barbie movie?\"\n    }]\n  }, {\n    \"role\": \"model\",\n    \"parts\": [{\n      \"functionCall\": {\n        \"name\": \"find_theaters\",\n        \"args\": {\n          \"location\": \"Mountain View, CA\",\n          \"movie\": \"Barbie\"\n        }\n      }\n    }]\n  }, {\n    \"role\": \"user\",\n    \"parts\": [{\n      \"functionResponse\": {\n        \"name\": \"find_theaters\",\n        \"response\": {\n          \"name\": \"find_theaters\",\n          \"content\": {\n            \"movie\": \"Barbie\",\n            \"theaters\": [{\n              \"name\": \"AMC Mountain View 16\",\n              \"address\": \"2000 W El Camino Real, Mountain View, CA 94040\"\n            }, {\n              \"name\": \"Regal Edwards 14\",\n              \"address\": \"245 Castro St, Mountain View, CA 94040\"\n            }]\n          }\n        }\n      }\n    }]\n  },\n  {\n    \"role\": \"model\",\n    \"parts\": [{\n      \"text\": \" OK. Barbie is showing in two theaters in Mountain View, CA: AMC Mountain View 16 and Regal Edwards 14.\"\n    }]\n  },{\n    \"role\": \"user\",\n    \"parts\": [{\n      \"text\": \"Can we recommend some comedy movies on show in Mountain View?\"\n    }]\n  }],\n  \"tools\": [{\n    \"functionDeclarations\": [{\n      \"name\": \"find_movies\",\n      \"description\": \"find movie titles currently playing in theaters based on any description, genre, title words, etc.\",\n      \"parameters\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"STRING\",\n            \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n          },\n          \"description\": {\n            \"type\": \"STRING\",\n            \"description\": \"Any kind of description including category or genre, title words, attributes, etc.\"\n          }\n        },\n        \"required\": [\"description\"]\n      }\n    }, {\n      \"name\": \"find_theaters\",\n      \"description\": \"find theaters based on location and optionally movie title which is currently playing in theaters\",\n      \"parameters\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"STRING\",\n            \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n          },\n          \"movie\": {\n            \"type\": \"STRING\",\n            \"description\": \"Any movie title\"\n          }\n        },\n        \"required\": [\"location\"]\n      }\n    }, {\n      \"name\": \"get_showtimes\",\n      \"description\": \"Find the start times for movies playing in a specific theater\",\n      \"parameters\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"STRING\",\n            \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n          },\n          \"movie\": {\n            \"type\": \"STRING\",\n            \"description\": \"Any movie title\"\n          },\n          \"theater\": {\n            \"type\": \"STRING\",\n            \"description\": \"Name of the theater\"\n          },\n          \"date\": {\n            \"type\": \"STRING\",\n            \"description\": \"Date for requested showtime\"\n          }\n        },\n        \"required\": [\"location\", \"movie\", \"theater\", \"date\"]\n      }\n    }]\n  }]\n}' Multi-turn function calling curl example response [{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"functionCall\": {\n              \"name\": \"find_movies\",\n              \"args\": {\n                \"description\": \"comedy\",\n                \"location\": \"Mountain View, CA\"\n              }\n            }\n          }\n        ]\n      },\n      \"finishReason\": \"STOP\",\n      \"safetyRatings\": [\n        {\n          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        }\n      ]\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 48,\n    \"totalTokenCount\": 48\n  }\n}\n] Best practices Follow these best practices to improve the accuracy and\nreliability of your function calls. User prompt For best results, prepend the user query with the following details: Additional context for the model. For example, You are a movie API assistant\nto help users find movies and showtimes based on their preferences. Details or instructions on how and when to use the functions. For example, Don't make assumptions on showtimes. Always use a future date for showtimes. Instructions to ask clarifying questions if user queries are ambiguous. For\nexample, Ask clarifying questions if not enough information is available to\ncomplete the request. Sampling parameters For the temperature parameter, use 0 or another low value. This instructs\nthe model to generate more confident results and reduces hallucinations. API invocation If the model proposes the invocation of a function that would send an order,\nupdate a database, or otherwise have significant consequences, validate the\nfunction call with the user before executing it. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/function-calling"
            ],
            "timestamp": "2024-12-27T22:47:16.753398",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
            "title": "Function calling tutorial  |  Gemini API  |  Google AI for Developers",
            "text_content": "Function calling tutorial  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Function calling tutorial Python Node.js Go Dart (Flutter) Android Swift Web REST Function calling makes it easier for you to get structured data outputs from\ngenerative models. You can then use these outputs to call other APIs and return\nthe relevant response data to the model. In other words, function calling helps\nyou connect generative models to external systems so that the generated content\nincludes the most up-to-date and accurate information. You can provide Gemini models with descriptions of functions. These are\nfunctions that you write in the language of your app (that is, they're not\nGoogle Cloud Functions). The model may ask you to call a function and send back\nthe result to help the model handle your query. If you haven't already, check out the Introduction to function calling to learn\nmore. Example API for lighting control Imagine you have a basic lighting control system with an application programming\ninterface (API) and you want to allow users to control the lights through simple\ntext requests. You can use the Function Calling feature to interpret lighting\nchange requests from users and translate them into API calls to set the lighting\nvalues. This hypothetical lighting control system lets you control the\nbrightness of the light and it's color temperature, defined as two separate\nparameters: Parameter Type Required Description brightness number yes Light level from 0 to 100. Zero is off and 100 is full brightness. colorTemperature string yes Color temperature of the light fixture which can be daylight , cool or warm . For simplicity, this imaginary lighting system only has one light, so the user\ndoes not have to specify a room or location. Here is an example JSON request\nyou could send to the lighting control API to change the light level to 50%\nusing the daylight color temperature: { \"brightness\" : \"50\" , \"colorTemperature\" : \"daylight\" } This tutorial shows you how to set up a Function Call for the Gemini API to\ninterpret users lighting requests and map them to API settings to control a\nlight's brightness and color temperature values. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use Function Calling in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial"
            ],
            "timestamp": "2024-12-27T22:47:18.059030",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/document-processing",
            "title": "Explore document processing capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore document processing capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore document processing capabilities with the Gemini API Python Node.js Go REST The Gemini API supports PDF input, including long documents (up to 3600 pages).\nGemini models process PDFs with native vision, and are therefore able to\nunderstand both text and image contents inside documents. With native PDF vision\nsupport, Gemini models are able to: Analyze diagrams, charts, and tables inside documents. Extract information into structured output formats. Answer questions about visual and text contents in documents. Summarize documents. Transcribe document content (e.g. to HTML) preserving layouts and formatting, for use in downstream applications (such as in RAG pipelines). This tutorial demonstrates some possible ways to use the Gemini API with PDF\ndocuments. All output is text-only. What's next This guide shows how to use generateContent and\nto generate text outputs from processed documents. To learn more,\nsee the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-20 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use the Gemini API to process documents like PDFs",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/document-processing"
            ],
            "timestamp": "2024-12-27T22:47:18.660593",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/grounding",
            "title": "Grounding with Google Search  |  Gemini API  |  Google AI for Developers",
            "text_content": "Grounding with Google Search  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Grounding with Google Search Important: We're launching Grounding with Google Search! Review the updated Gemini API Additional Terms of Service ,\nwhich include new feature terms and updates for clarity. Python Node.js REST The Grounding with Google Search feature in the Gemini API and AI Studio can be\nused to improve the accuracy and recency of responses from the model. In\naddition to more factual responses, when Grounding with Google Search is\nenabled, the Gemini API returns grounding sources (in-line supporting links) and Google Search Suggestions along with the response\ncontent. The Search Suggestions point users to the search results corresponding\nto the grounded response. Grounding with Google Search only supports text prompts. It doesn't support\nmultimodal (text-and-image, text-and-audio, etc.) prompts. Grounding with\nGoogle Search supports all of the available languages for\nGemini models. This guide will help you get started with Grounding with Google Search using one\nof the Gemini API SDKs or the REST API. Configure a model to use Google Search Tip: Before running the example code, make sure that you've followed the\ninstallation and setup instructions in the quickstart . Why is Grounding with Google Search useful? In generative AI, grounding refers to the process of connecting the model to\nverifiable sources of information. These sources might provide real-world\nworkplace information or other specific context. Grounding helps with improving\nthe accuracy, reliability, and usefulness of AI outputs. Grounding is particularly important for prompts that require up-to-date\ninformation from the web. Using grounding, the model can access information\nbeyond its knowledge cutoff date, get sources for the information, and answer\nquestions that it couldn't have answered accurately otherwise. Using Google AI Studio or the Gemini API, you can ground model output to\nGoogle Search. Grounding with Google Search provides the following benefits: Allows model responses that are tethered to specific content. Reduces model hallucinations, which are instances where the model generates\ncontent that isn't factual. Anchors model responses to sources a user can click through and open. Enhances the trustworthiness and applicability of the generated content. When you use Grounding with Google Search, you're effectively connecting the\nmodel to reliable Search results from the internet. Since non-grounded model\nresponses are based on learned patterns, you might not get factual responses to\nprompts about current events (for example, asking for a weather forecast or the\nfinal score of a recent football game). Since the internet provides access to\nnew information, a grounded prompt can generate more up-to-date responses, with\nsources cited. Here's an example comparing a non-grounded response and a grounded response\ngenerated using the API. (The responses were generated in October 2024.) Ungrounded Gemini Grounding with Google Search Prompt: Who won the Super Bowl this year? Response: The Kansas City Chiefs won Super Bowl\n        LVII this year (2023). Prompt: Who won the Super Bowl this year? Response: The Kansas City Chiefs won Super Bowl\n        LVIII this year, defeating the San Francisco 49ers in overtime with a\n        score of 25 to 22. In the ungrounded response, the model refers to the Kansas City Chiefs' 2023\nSuper Bowl win. In the grounded response, the model correctly references their\nmore recent 2024 win. The following image shows how a grounded response looks in AI Studio. Google Search Suggestions To use Grounding with Google Search, you have to display Google Search\nSuggestions, which are suggested queries included in the metadata of the\ngrounded response. To learn more about the display requirements, see Use Google Search Suggestions . Dynamic retrieval Some queries are likely to benefit more from Grounding with Google Search than\nothers. The dynamic retrieval feature gives you additional control over when\nto use Grounding with Google Search. If the dynamic retrieval mode is unspecified, Grounding with Google Search is\nalways triggered. If the mode is set to dynamic, the model decides when to use\ngrounding based on a threshold that you can configure. The threshold\nis a floating-point value in the range [0,1] and defaults to 0.3. If the\nthreshold value is 0, the response is always grounded with Google Search; if\nit's 1, it never is. How dynamic retrieval works You can use dynamic retrieval in your request to choose when to turn on\nGrounding with Google Search. This is useful when the prompt doesn't require\nan answer grounded in Google Search and the model can provide an answer based\non its own knowledge without grounding. This helps you manage latency, quality,\nand cost more effectively. Before you invoke the dynamic retrieval configuration in your request,\nunderstand the following terminology: Prediction score : When you request a grounded answer, Gemini assigns a prediction score to the prompt. The prediction score is a floating point\nvalue in the range [0,1]. Its value depends on whether the prompt\ncan benefit from grounding the answer with the most up-to-date\ninformation from Google Search. Thus, if a prompt requires an answer\ngrounded in the most recent facts on the web, it has a higher prediction\nscore. A prompt for which a model-generated answer is sufficient has a lower\nprediction score. Here are examples of some prompts and their prediction scores. Note: The prediction scores are assigned by Gemini and can vary over\ntime depending on several factors. Prompt Prediction score Comment \"Write a poem about peonies\" 0.13 The model can rely on its knowledge and the answer doesn't need grounding. \"Suggest a toy for a 2yo child\" 0.36 The model can rely on its knowledge and the answer doesn't need grounding. \"Can you give a recipe for an asian-inspired guacamole?\" 0.55 Google Search can give a grounded answer, but grounding isn't strictly required; the model knowledge might be sufficient. \"What's Agent Builder? How is grounding billed in Agent Builder?\" 0.72 Requires Google Search to generate a well-grounded answer. \"Who won the latest F1 grand prix?\" 0.97 Requires Google Search to generate a well-grounded answer. Threshold : In your API request, you can specify a dynamic retrieval\nconfiguration with a threshold. The threshold is a floating point value in\nthe range [0,1] and defaults to 0.3. If the threshold value is zero, the\nresponse is always grounded with Google Search. For all other values\nof threshold, the following is applicable: If the prediction score is greater than or equal to the threshold, the\nanswer is grounded with Google Search.\nA lower threshold implies that more prompts have responses that are\ngenerated using Grounding with Google Search. If the prediction score is less than the threshold, the model might still\ngenerate the answer, but it isn't grounded with Google Search. To learn how to set the dynamic retrieval threshold using an SDK or the REST\nAPI, see the appropriate code example . If you're using AI Studio, you can set the dynamic retrieval threshold by\nclicking Edit grounding . To find a good threshold that suits your business needs, you can create a\nrepresentative set of queries that you expect to encounter. Then you can sort\nthe queries according to the prediction score in the response and select a\ngood threshold for your use case. A grounded response If your prompt successfully grounds to Google Search, the response will include groundingMetadata . A grounded response might look something like this\n(parts of the response have been omitted for brevity): { \"candidates\" : [ { \"content\" : { \"parts\" : [ { \"text\" : \"Carlos Alcaraz won the Gentlemen's Singles title at the 2024 Wimbledon Championships. He defeated Novak Djokovic in the final, winning his second consecutive Wimbledon title and fourth Grand Slam title overall. \\n\" } ], \"role\" : \"model\" }, ... \"groundingMetadata\" : { \"searchEntryPoint\" : { \"renderedContent\" : \"\\u003cstyle\\u003e\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n\\u003c/style\\u003e\\n\\u003cdiv class=\\\"container\\\"\\u003e\\n  \\u003cdiv class=\\\"headline\\\"\\u003e\\n    \\u003csvg class=\\\"logo-light\\\" width=\\\"18\\\" height=\\\"18\\\" viewBox=\\\"9 9 35 35\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\"\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\\\" fill=\\\"#4285F4\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\\\" fill=\\\"#34A853\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\\\" fill=\\\"#FBBC05\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\\\" fill=\\\"#EA4335\\\"/\\u003e\\n    \\u003c/svg\\u003e\\n    \\u003csvg class=\\\"logo-dark\\\" width=\\\"18\\\" height=\\\"18\\\" viewBox=\\\"0 0 48 48\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\"\\u003e\\n      \\u003ccircle cx=\\\"24\\\" cy=\\\"23\\\" fill=\\\"#FFF\\\" r=\\\"22\\\"/\\u003e\\n      \\u003cpath d=\\\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\\\" fill=\\\"#4285F4\\\"/\\u003e\\n      \\u003cpath d=\\\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\\\" fill=\\\"#34A853\\\"/\\u003e\\n      \\u003cpath d=\\\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\\\" fill=\\\"#FBBC05\\\"/\\u003e\\n      \\u003cpath d=\\\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\\\" fill=\\\"#EA4335\\\"/\\u003e\\n    \\u003c/svg\\u003e\\n    \\u003cdiv class=\\\"gradient-container\\\"\\u003e\\u003cdiv class=\\\"gradient\\\"\\u003e\\u003c/div\\u003e\\u003c/div\\u003e\\n  \\u003c/div\\u003e\\n  \\u003cdiv class=\\\"carousel\\\"\\u003e\\n    \\u003ca class=\\\"chip\\\" href=\\\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4x8Epe-gzpwRBvp7o3RZh2m1ygq1EHktn0OWCtvTXjad4bb1zSuqfJd6OEuZZ9_SXZ_P2SvCpJM7NaFfQfiZs6064MeqXego0vSbV9LlAZoxTdbxWK1hFeqTG6kA13YJf7Fbu1SqBYM0cFM4zo0G_sD9NKYWcOCQMvDLDEJFhjrC9DM_QobBIAMq-gWN95G5tvt6_z6EuPN8QY=\\\"\\u003ewho won wimbledon 2024\\u003c/a\\u003e\\n  \\u003c/div\\u003e\\n\\u003c/div\\u003e\\n\" }, \"groundingChunks\" : [ { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4whET1ta3sDETZvcicd8FeNe4z0VuduVsxrT677KQRp2rYghXI0VpfYbIMVI3THcTuMwggRCbFXS_wVvW0UmGzMe9h2fyrkvsnQPJyikJasNIbjJLPX0StM4Bd694-ZVle56MmRA4YiUvwSqad1w6O2opmWnw==\" , \"title\" : \"wikipedia.org\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4wR1M-9-yMPUr_KdHlnoAmQ8ZX90DtQ_vDYTjtP2oR5RH4tRP04uqKPLmesvo64BBkPeYLC2EpVDxv9ngO3S1fs2xh-e78fY4m0GAtgNlahUkm_tBm_sih5kFPc7ill9u2uwesNGUkwrQlmP2mfWNU5lMMr23HGktr6t0sV0QYlzQq7odVoBxYWlQ_sqWFH\" , \"title\" : \"wikipedia.org\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4wsDmROzbP-tmt8GdwCW_pqISTZ4IRbBuoaMyaHfcQg8WW-yKRQQvMDTPAuLxJh-8_U8_iw_6JKFbQ8M9oVYtaFdWFK4gOtL4RrC9Jyqc5BNpuxp6uLEKgL5-9TggtNvO97PyCfziDFXPsxylwI1HcfQdrz3Jy7ZdOL4XM-S5rC0lF2S3VWW0IEAEtS7WX861meBYVjIuuF_mIr3spYPqWLhbAY2Spj-4_ba8DjRvmevIFUhRuESTKvBfmpxNSM\" , \"title\" : \"cbssports.com\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4yzjLkorHiUKjhOPkWaZ9b4cO-cLG-02vlEl6xTBjMUjyhK04qSIclAa7heR41JQ6AAVXmNdS3WDrLOV4Wli-iezyzW8QPQ4vgnmO_egdsuxhcGk3-Fp8-yfqNLvgXFwY5mPo6QRhvplOFv0_x9mAcka18QuAXtj0SPvJfZhUEgYLCtCrucDS5XFc5HmRBcG1tqFdKSE1ihnp8KLdaWMhrUQI21hHS9\" , \"title\" : \"jagranjosh.com\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4y9L4oeNGWCatFz63b9PpP3ys-Wi_zwnkUT5ji9lY7gPUJQcsmmE87q88GSdZqzcx5nZG9usot5FYk2yK-FAGvCRE6JsUQJB_W11_kJU2HVV1BTPiZ4SAgm8XDFIxpCZXnXmEx5HUfRqQm_zav7CvS2qjA2x3__qLME6Jy7R5oza1C5_aqjQu422le9CaigThS5bvJoMo-ZGcXdBUCj2CqoXNVjMA==\" , \"title\" : \"apnews.com\" } } ], \"groundingSupports\" : [ { \"segment\" : { \"endIndex\" : 85 , \"text\" : \"Carlos Alcaraz won the Gentlemen's Singles title at the 2024 Wimbledon Championships.\" }, \"groundingChunkIndices\" : [ 0 , 1 , 2 , 3 ], \"confidenceScores\" : [ 0.97380733 , 0.97380733 , 0.97380733 , 0.97380733 ] }, { \"segment\" : { \"startIndex\" : 86 , \"endIndex\" : 210 , \"text\" : \"He defeated Novak Djokovic in the final, winning his second consecutive Wimbledon title and fourth Grand Slam title overall.\" }, \"groundingChunkIndices\" : [ 1 , 0 , 4 ], \"confidenceScores\" : [ 0.96145374 , 0.96145374 , 0.96145374 ] } ], \"webSearchQueries\" : [ \"who won wimbledon 2024\" ] } } ], ... } If the response doesn't include groundingMetadata , this means the response\nwasn't successfully grounded. There are several reasons this could happen,\nincluding low source relevance or incomplete information within the\nmodel response. When a grounded result is generated, the metadata contains URIs that redirect\nto the publishers of the content that was used to generate the grounded result.\nThese URIs contain the vertexaisearch subdomain, as in this truncated example: https://vertexaisearch.cloud.google.com/grounding-api-redirect/... . The\nmetadata also contains the publishers' domains. The provided URIs remain\naccessible for 30 days after the grounded result is generated. Important: The provided URIs must be directly accessible by the end users and\nmust not be queried programmatically through automated means. If automated\naccess is detected, the grounded answer generation service might stop providing\nthe redirection URIs. The renderedContent field within searchEntryPoint is the provided code for\nimplementing Google Search Suggestions. See Use Google Search Suggestions to learn more. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-19 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/grounding"
            ],
            "timestamp": "2024-12-27T22:47:19.203767",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/model-tuning",
            "title": "Fine-tuning with the Gemini API  |  Google AI for Developers",
            "text_content": "Fine-tuning with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Fine-tuning with the Gemini API Prompt design strategies such as few-shot prompting may not always produce the\nresults you need. Fine-tuning is a process that can improve your model's\nperformance on specific tasks or help the model adhere to specific output\nrequirements when instructions aren't sufficient and you have a set of examples\nthat demonstrate the outputs you want. This page provides a conceptual overview of fine-tuning the text model behind\nthe Gemini API text service. When you're ready to start tuning, try the fine-tuning tutorial . If you'd like\na more general introduction to customizing LLMs for specific use cases, check\nout LLMs: Fine-tuning, distillation, and prompt engineering in the Machine Learning Crash Course . How fine-tuning works The goal of fine-tuning is to further improve the performance of the model for\nyour specific task. Fine-tuning works by providing the model with a training\ndataset containing many examples of the task. For niche tasks, you can get\nsignificant improvements in model performance by tuning the model on a modest\nnumber of examples. This kind of model tuning is sometimes referred to as supervised fine-tuning , to distinguish it from other kinds of fine-tuning. Your training data should be structured as examples with prompt inputs and\nexpected response outputs. You can also tune models using example data directly\nin Google AI Studio. The goal is to teach the model to mimic the wanted behavior\nor task, by giving it many examples illustrating that behavior or task. When you run a tuning job, the model learns additional parameters that help it\nencode the necessary information to perform the wanted task or learn the wanted\nbehavior. These parameters can then be used at inference time. The output of the\ntuning job is a new model, which is effectively a combination of the newly\nlearned parameters and the original model. Prepare your dataset Before you can start fine-tuning, you need a dataset to tune the model with. For\nthe best performance, the examples in the dataset should be of high quality,\ndiverse, and representative of real inputs and outputs. Format Note: Fine-tuning only supports input-output pair examples. Chat-style\nmulti-turn conversations are not supported at this time. The examples included in your dataset should match your expected production\ntraffic. If your dataset contains specific formatting, keywords, instructions,\nor information, the production data should be formatted in the same way and\ncontain the same instructions. For example, if the examples in your dataset include a \"question:\" and a \"context:\" , production traffic should also be formatted to include a \"question:\" and a \"context:\" in the same order as it appears in the dataset\nexamples. If you exclude the context, the model can't recognize the pattern,\neven if the exact question was in an example in the dataset. As another example, here's Python training data for an application that\ngenerates the next number in a sequence: training_data = [ { \"text_input\" : \"1\" , \"output\" : \"2\" }, { \"text_input\" : \"3\" , \"output\" : \"4\" }, { \"text_input\" : \"-3\" , \"output\" : \"-2\" }, { \"text_input\" : \"twenty two\" , \"output\" : \"twenty three\" }, { \"text_input\" : \"two hundred\" , \"output\" : \"two hundred one\" }, { \"text_input\" : \"ninety nine\" , \"output\" : \"one hundred\" }, { \"text_input\" : \"8\" , \"output\" : \"9\" }, { \"text_input\" : \"-98\" , \"output\" : \"-97\" }, { \"text_input\" : \"1,000\" , \"output\" : \"1,001\" }, { \"text_input\" : \"10,100,000\" , \"output\" : \"10,100,001\" }, { \"text_input\" : \"thirteen\" , \"output\" : \"fourteen\" }, { \"text_input\" : \"eighty\" , \"output\" : \"eighty one\" }, { \"text_input\" : \"one\" , \"output\" : \"two\" }, { \"text_input\" : \"three\" , \"output\" : \"four\" }, { \"text_input\" : \"seven\" , \"output\" : \"eight\" }, ] Adding a prompt or preamble to each example in your dataset can also help\nimprove the performance of the tuned model. Note, if a prompt or preamble is\nincluded in your dataset, it should also be included in the prompt to the tuned\nmodel at inference time. Limitations Note: Fine-tuning datasets for Gemini 1.5 Flash have the following\nlimitations: The maximum input size per example is 40,000 characters. The maximum output size per example is 5,000 characters. Training data size You can fine-tune a model with as little as 20 examples. Additional data\ngenerally improves the quality of the responses. You should target between 100\nand 500 examples, depending on your application. The following table shows\nrecommended dataset sizes for fine-tuning a text model for various common tasks: Task No. of examples in dataset Classification 100+ Summarization 100-500+ Document search 100+ Upload your tuning dataset Data is either passed inline using the API or through files uploaded in Google\nAI Studio. To use the client library, provide the data file in the createTunedModel call.\nFile size limit is 4 MB. See the fine-tuning quickstart with Python to get started. To call the REST API using cURL, provide training examples in JSON format to the training_data argument. See the tuning quickstart with cURL to get started. Advanced tuning settings When creating a tuning job, you can specify the following advanced settings: Epochs: A full training pass over the entire training set such that each\nexample has been processed once. Batch size: The set of examples used in one training iteration . The\nbatch size determines the number of examples in a batch. Learning rate: A floating-point number that tells the algorithm how\nstrongly to adjust the model parameters on each iteration. For example, a\nlearning rate of 0.3 would adjust weights and biases three times more\npowerfully than a learning rate of 0.1. High and low learning rates have\ntheir own unique trade-offs and should be adjusted based on your use case. Learning rate multiplier: The rate multiplier modifies the model's\noriginal learning rate. A value of 1 uses the original learning rate of the\nmodel. Values greater than 1 increase the learning rate and values between 1\nand 0 lower the learning rate. Recommended configurations The following table shows the recommended configurations for fine-tuning a\nfoundation model: Hyperparameter Default value Recommended adjustments Epoch 5 If the loss starts to plateau before 5 epochs, use a smaller value. If the loss is converging and doesn't seem to plateau, use a higher value. Batch size 4 Learning rate 0.001 Use a smaller value for smaller datasets. The loss curve shows how much the model's prediction deviates from the ideal\npredictions in the training examples after each epoch. Ideally you want to stop\ntraining at the lowest point in the curve right before it plateaus. For example,\nthe graph below shows the loss curve plateauing at about epoch 4-6 which means\nyou can set the Epoch parameter to 4 and still get the same performance. Check the tuning job status You can check the status of your tuning job in Google AI Studio under the My Library tab or using the metadata property of the tuned model in the\nGemini API. Troubleshoot errors This section includes tips on how to resolve errors you may encounter while\ncreating your tuned model. Authentication Note: Starting September 30, 2024, OAuth authentication is no longer required.\nNew projects should use API key authentication instead. Tuning using the API and client library requires authentication. You can\nset up authentication using either an API key (recommended) or using OAuth\ncredentials. For documentation on setting up an API key, see Set up API key . If you see a 'PermissionDenied: 403 Request had insufficient authentication\nscopes' error, you may need to set up user authentication using OAuth\ncredentials. To configure OAuth credentials for Python, visit our the OAuth setup tutorial . Canceled models You can cancel a fine-tuning job any time before the job is finished. However,\nthe inference performance of a canceled model is unpredictable, particularly if\nthe tuning job is canceled early in the training. If you canceled because you\nwant to stop the training at an earlier epoch, you should create a new tuning\njob and set the epoch to a lower value. Limitations of tuned models Note: Tuned models have the following limitations: The input limit of a tuned Gemini 1.5 Flash model is 40,000 characters. JSON mode is not supported with tuned models. Only text input is supported. What's next Get started with the fine-tuning tutorials: Fine-tuning tutorial (Python) Fine-tuning tutorial (REST) Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-09-28 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how fine-tuning works in the Gemini ecosystem",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python",
                "https://ai.google.dev/gemini-api/docs/model-tuning"
            ],
            "timestamp": "2024-12-27T22:47:20.079004",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
            "title": "Fine-tuning tutorial  |  Gemini API  |  Google AI for Developers",
            "text_content": "Fine-tuning tutorial  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Fine-tuning tutorial Python REST This tutorial will help you get started with the Gemini API tuning service\nusing either the Python SDK or the REST API using curl . The examples show how to tune the text model behind\nthe Gemini API text generation service. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-10-02 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how fine-tune a Gemini model with the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial"
            ],
            "timestamp": "2024-12-27T22:47:21.280272",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/embeddings",
            "title": "Embeddings in the Gemini API  |  Google AI for Developers",
            "text_content": "Embeddings in the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Embeddings in the Gemini API The text-embedding-004 model generates state-of-the-art embeddings\nfor words, phrases, and sentences. The resulting embeddings can then be used for\ntasks such as semantic search, text classification, and clustering, among many\nothers. For more information on embeddings, read our research paper . What are embeddings? Embeddings capture semantic meaning and context, which results in text with\nsimilar meanings having \"closer\" embeddings. For example, the sentence \"I took my\ndog to the vet\" and \"I took my cat to the vet\" would have embeddings that are\nclose to each other in the vector space since they both describe a similar\ncontext. You can use embeddings to compare different texts and understand how they\nrelate. For example, if the embeddings of the text \"cat\" and \"dog\" are close\ntogether you can infer that these words are similar in meaning, context, or\nboth. This enables a variety of common AI use cases . Generate embeddings Use the embedContent method to generate text embeddings: Python import google.generativeai as genai import os genai . configure ( api_key = os . environ [ \"GEMINI_API_KEY\" ]) result = genai . embed_content ( model = \"models/text-embedding-004\" , content = \"What is the meaning of life?\" ) print ( str ( result [ 'embedding' ])) Node.js const { GoogleGenerativeAI } = require ( \"@google/generative-ai\" ); const genAI = new GoogleGenerativeAI ( process . env . GEMINI_API_KEY ); const model = genAI . getGenerativeModel ({ model : \"text-embedding-004\" }); async function run () { const result = await model . embedContent ( \"What is the meaning of life?\" ); console . log ( result . embedding . values ); } run (); curl curl \"https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:embedContent?key=$GEMINI_API_KEY\" \\ -H 'Content-Type: application/json' \\ -d '{\"model\": \"models/text-embedding-004\", \"content\": { \"parts\":[{ \"text\": \"What is the meaning of life?\"}]} }' Go ctx := context . Background () client , err := genai . NewClient ( ctx , option . WithAPIKey ( os . Getenv ( \"GEMINI_API_KEY\" ))) if err != nil { log . Fatal ( err ) } defer client . Close () em := client . EmbeddingModel ( \"text-embedding-004\" ) res , err := em . EmbedContent ( ctx , genai . Text ( \"What is the meaning of life?\" )) if err != nil { panic ( err ) } fmt . Println ( res . Embedding . Values ) Use cases Text embeddings are used in a variety of common AI use cases, such as: Information retrieval: You can use embeddings to retrieve semantically\nsimilar text given a piece of input text. Document search tutorial task Clustering: Comparing groups of embeddings can help identify hidden trends. Embedding clustering tutorial bubble_chart Vector database: As you take different embedding use cases to production,\nit is common to store embeddings in a vector database. Vector database tutorial bolt Classification: You can train a model using embeddings to classify\ndocuments into categories. Classification tutorial token Gemini embeddings models The Gemini API offers two models that generate text embeddings: Text Embeddings Embeddings Text Embeddings is an updated version of the Embedding model that offers elastic\nembedding sizes under 768 dimensions. Elastic embeddings generate smaller output\ndimensions and potentially save computing and storage costs with minor\nperformance loss. Use Text Embeddings for new projects or applications. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-19 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/embeddings"
            ],
            "timestamp": "2024-12-27T22:47:22.136514",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/caching",
            "title": "Context caching  |  Gemini API  |  Google AI for Developers",
            "text_content": "Context caching  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Context caching Python Node.js Go REST In a typical AI workflow, you might pass the same input tokens over and over to\na model. Using the Gemini API context caching feature, you can pass some content\nto the model once, cache the input tokens, and then refer to the cached tokens\nfor subsequent requests. At certain volumes, using cached tokens is lower cost\nthan passing in the same corpus of tokens repeatedly. When you cache a set of tokens, you can choose how long you want the cache to\nexist before the tokens are automatically deleted. This caching duration is\ncalled the time to live (TTL). If not set, the TTL defaults to 1 hour. The\ncost for caching depends on the input token size and how long you want the\ntokens to persist. Context caching supports both Gemini 1.5 Pro and Gemini 1.5 Flash. Note: Context caching is only available for stable models with fixed versions\n(for example, gemini-1.5-pro-001 ). You must include the version postfix (for\nexample, the -001 in gemini-1.5-pro-001 ). When to use context caching Context caching is particularly well suited to scenarios where a substantial\ninitial context is referenced repeatedly by shorter requests. Consider using\ncontext caching for use cases such as: Chatbots with extensive system instructions Repetitive analysis of lengthy video files Recurring queries against large document sets Frequent code repository analysis or bug fixing How caching reduces costs Context caching is a paid feature designed to reduce overall operational costs.\nBilling is based on the following factors: Cache token count: The number of input tokens cached, billed at a\nreduced rate when included in subsequent prompts. Storage duration: The amount of time cached tokens are stored (TTL),\nbilled based on the TTL duration of cached token count. There are no minimum\nor maximum bounds on the TTL. Other factors: Other charges apply, such as for non-cached input tokens\nand output tokens. For up-to-date pricing details, refer to the Gemini API pricing\npage . To learn how to count tokens, see the Token\nguide . How to use context caching This section assumes that you've installed a Gemini SDK (or have curl installed)\nand that you've configured an API key, as shown in the quickstart . Additional considerations Keep the following considerations in mind when using context caching: The minimum input token count for context caching is 32,768, and the maximum is the same as the maximum for the given model. (For more on\ncounting tokens, see the Token guide ). The model doesn't make any distinction between cached tokens and regular\ninput tokens. Cached content is simply a prefix to the prompt. There are no special rate or usage limits on context caching; the standard\nrate limits for GenerateContent apply, and token limits include cached\ntokens. The number of cached tokens is returned in the usage_metadata from the\ncreate, get, and list operations of the cache service, and also in GenerateContent when using the cache. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-10-16 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use Context Caching in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/caching"
            ],
            "timestamp": "2024-12-27T22:47:22.880673",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/imagen",
            "title": "Generate images using Imagen 3  |  Gemini API  |  Google AI for Developers",
            "text_content": "Generate images using Imagen 3  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Generate images using Imagen 3 Preview: Imagen 3 in the Gemini API is available as an early access release\nin private preview. It's not yet generally available. The Gemini API provides access to Imagen 3 , Google's highest\nquality text-to-image model, featuring a number of\nnew and improved capabilities. Imagen 3 can do the following: Generate images with better detail, richer lighting, and fewer distracting\nartifacts than previous models Understand prompts written in natural Generate images in a wide range of formats and styles Render text more effectively than previous models To learn more and see example output, see the Google DeepMind Imagen 3 overview . Before you begin: Set up your project and API key Important: The stable version of Python SDK for the Gemini API does not contain\nImagen support. Instead of installing the google-generativeai package\nfrom pypi you need to install it from the imagen GitHub branch: pip install -U git+https://github.com/google-gemini/generative-ai-python@imagen Before calling the Gemini API, you need to set up your project and configure\nyour API key. Expand to view how to set up your project and API key Tip: For complete setup instructions, see the Gemini API quickstart . Get and secure your API key You need an API key to call the Gemini API. If you don't already have one,\ncreate a key in Google AI Studio. Get an API key It's strongly recommended that you do not check an API key into your version\ncontrol system. You should store your API key in a secrets store such as Google Cloud Secret Manager . This tutorial assumes that you're accessing your API key as an environment\nvariable. Install the SDK package and configure your API key Note: This section shows setup steps for a local Python environment. To install\n      dependencies and configure your API key for Colab, see the Authentication quickstart notebook Important: The stable version of Python SDK for the Gemini API does not contain\nImagen support. Instead of installing the google-generativeai package\nfrom pypi you need to install it from the imagen GitHub branch. Install the dependency using pip: pip install -U git+https://github.com/google-gemini/generative-ai-python@imagen Import the package and configure the service with your API key: import os import google.generativeai as genai genai . configure ( api_key = os . environ [ 'API_KEY' ]) Generate images This section shows you how to instantiate an Imagen model and generate images. Note: A non-visible digital SynthID watermark is always\nadded to generated images. To run the example code, you must first install Pillow : pip install --upgrade Pillow Then, with Pillow and the Python SDK installed, you can use the following code\nto generate images: import os import google.generativeai as genai genai . configure ( api_key = os . environ [ 'API_KEY' ]) imagen = genai . ImageGenerationModel ( \"imagen-3.0-generate-001\" ) result = imagen . generate_images ( prompt = \"Fuzzy bunnies in my kitchen\" , number_of_images = 4 , safety_filter_level = \"block_only_high\" , person_generation = \"allow_adult\" , aspect_ratio = \"3:4\" , negative_prompt = \"Outside\" , ) for image in result . images : print ( image ) # Open and display the image using your local operating system. for image in result . images : image . _pil_image . show () The notebook should display four images similar to this one: Imagen model parameters The following parameters are available for generate_images() : prompt : The text prompt for the image. negative_prompt : A description of what you want to omit in the generated\nimages. Defaults to none. For example, consider the prompt \"a rainy city street at night with no\npeople\". The model might interpret \"people\" as a directive of what to include\ninstead of omit. To generate better results, you could use the prompt \"a rainy\ncity street at night\" with a negative prompt \"people\". number_of_images : The number of images to generate, from 1 to 4 (inclusive).\nThe default is 4. aspect_ratio : Changes the aspect ratio of the generated image. Supported\nvalues are \"1:1\" , \"3:4\" , \"4:3\" , \"9:16\" , and \"16:9\" . The default is \"1:1\" . safety_filter_level : Adds a filter level to safety filtering. The following\nvalues are valid: \"block_low_and_above\" : Block when the probability score or the severity\nscore is LOW , MEDIUM , or HIGH . \"block_medium_and_above\" : Block when the probability score or the severity\nscore is MEDIUM or HIGH . \"block_only_high\" : Block when the probability score or the severity score\nis HIGH . person_generation : Allow the model to generate images of people. The\nfollowing values are supported: \"dont_allow\" : Block generation of images of people. \"allow_adult\" : Generate images of adults, but not children. Text prompt language The following input text prompt languages are supported: Chinese (simplified) ( zh / zh-CN ) Chinese (traditional) ( zh-TW ) English ( en ) Hindi ( hi ) Japanese ( ja ) Korean ( ko ) Portuguese ( pt ) Spanish ( es ) What's next Imagen 3 in Gemini API is in early access. Stay tuned for announcements about\nthe status of the feature. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-27 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/imagen"
            ],
            "timestamp": "2024-12-27T22:47:23.549861",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/prompting-intro",
            "title": "Introduction to prompt design  |  Gemini API  |  Google AI for Developers",
            "text_content": "Introduction to prompt design  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Introduction to prompt design Prompt design is the process of creating prompts that elicit the desired\nresponse from language models. Writing well structured prompts is an essential\npart of ensuring accurate, high quality responses from a language model. This\npage introduces some basic concepts, strategies, and best practices to get you\nstarted in designing prompts. What is a prompt A prompt is a natural language request submitted to a language model to receive\na response back. Prompts can contain questions, instructions, contextual\ninformation, examples, and partial input for the model to complete or continue.\nAfter the model receives a prompt, depending on the type of model being used, it\ncan generate text, embeddings, code, images, videos, music, and more. Prompt content types Prompts can include one or more of the following types of content: Input (required) Context (optional) Examples (optional) Input An input is the text in the prompt that you want the model to provide a response\nfor, and it's a required content type. Inputs can be a question that the model\nanswers (question input), a task the model performs (task input), an entity the\nmodel operates on (entity input), or partial input that the model completes or\ncontinues (completion input). Question input A question input is a question that you ask the model that the\nmodel provides an answer to. Prompt: What's a good name for a flower shop that specializes in selling bouquets of\ndried flowers? Create a list of 10 options with just the names. Response: Here are 10 names for a flower shop specializing in dried flowers:\n\n    1.  Everlasting Blooms\n    2.  Dried & Delightful\n    3.  The Petal Preserve\n    4.  Whispers of Wildflowers\n    5.  Forever Blooms\n    6.  The Botanical Muse\n    7.  Timeless Blooms\n    8.  Dried Flower Co.\n    9.  Bloom & Co.\n    10.  The Sun-Kissed Petals (gemini-1.5-flash) Task input A task input is a task that you want the model to perform. For\nexample, you can tell the model to give you ideas or suggestions for something. Prompt: Give me a simple list of just the things that I must bring on a camping trip. The list should have 10 items. Response: Here's a list of 10 essential items for a camping trip:\n\n    1. **Tent:** Your shelter from the elements.\n    2. **Sleeping Bag:** Stay warm and comfortable.\n    3. **Sleeping Pad:** Insulation and cushioning for a good night's sleep.\n    4. **Headlamp or Flashlight:** Light for nighttime activities.\n    5. **First Aid Kit:** Be prepared for minor injuries.\n    6. **Water Bottle/Hydration Pack:** Stay hydrated.\n    7. **Food and Snacks:** Fuel for your adventures.\n    8. **Cooking Gear:** Camp stove, fuel, utensils, etc.\n    9. **Knife and Multitool:** Essential for various tasks.\n    10. **Camp Chair:** Enjoy your campfire and meals comfortably. (gemini-1.5-flash) Entity input An entity input is what the model performs an action on, such as\nclassify or summarize. This type of input can benefit from the inclusion of\ninstructions. Prompt: Classify the following items as [large, small].\nElephant\nMouse\nSnail Response: Here's the classification:\n\n    * **Elephant:** Large\n    * **Mouse:** Small\n    * **Snail:** Small (gemini-1.5-flash) Completion input A completion input is text that the model is expected to complete or\ncontinue. Prompt: Complete the following prompt: Some simple strategies for overcoming writer's block include... Response: Some simple strategies for overcoming writer's block include:\n\n    * **Freewriting:** Write non-stop for a set amount of time (5-10 minutes) without worrying about grammar, spelling, or even making sense. Just keep your pen moving and let your thoughts flow freely. This can help you loosen up and access ideas you might not have considered otherwise.\n    * **Brainstorming:**  Make a list of everything that comes to mind related to your topic, no matter how silly or unrelated it seems. You can also use mind mapping or other visual brainstorming techniques.\n    * **Changing your environment:**  Sometimes a change of scenery can be all you need to get the creative juices flowing. Go for a walk, work in a coffee shop, or even just move to a different room in your house. \n    * **Reading:**  Immerse yourself in the work of other writers, especially those who inspire you. Pay attention to their style, their structure, and their use of language. \n    * **Taking a break:** Sometimes the best way to overcome writer's block is to simply step away from the project for a while. Go for a walk, watch a movie, listen to music, or do something else that you enjoy. You might be surprised at how much clearer your head feels when you return to your writing. \n    * **Focusing on a single element:**  Instead of tackling the entire project, focus on a single sentence, paragraph, or scene. Once you get something down on paper, it can be easier to build from there.\n    * **Starting with the ending:**  If you're stuck on the beginning, try writing the ending first. This can help you to clarify your overall vision for the project and provide a framework for your ideas.\n    * **Talking it out:** Share your ideas and challenges with a friend, a writing group, or a writing coach. Discussing your project with someone else can help you to see it from a new perspective and find solutions to your writing problems.\n    \n    Remember, writer's block is a common experience, and it's not a sign that you're a bad writer. Just be patient with yourself, try some of these strategies, and keep writing! (gemini-1.5-flash) Context Context can be one of the following: Instructions that specify how the model should behave. Information that the model uses or references to generate a response. Add contextual information in your prompt when you need to give information to\nthe model, or restrict the boundaries of the responses to only what's within the\nprompt. Prompt: Marbles:\nColor: red\nNumber: 12\nColor: blue\nNumber: 28\nColor: yellow\nNumber: 15\nColor: green\nNumber: 17\n\nHow many green marbles are there? Response: There are 17 green marbles. (gemini-1.5-flash) Examples Examples are input-output pairs that you include in the prompt to give the\nmodel an example of an ideal response. Including examples in\nthe prompt is an effective strategy for customizing the response format. Prompt: Classify the following.\nOptions:\n- red wine\n- white wine\n\nText: Chardonnay\nThe answer is: white wine\nText: Cabernet\nThe answer is: red wine\nText: Moscato\nThe answer is: white wine\n\nText: Riesling\nThe answer is: Response: The answer is: white wine (gemini-1.5-flash) Next steps Now that you have an understanding of prompt design, try writing your\nown prompts using Google AI Studio . For a deeper understanding of prompt design, see the prompt strategies topic. To learn about multimodal prompting, see Prompting with media files . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-09-23 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs/prompting-intro"
            ],
            "timestamp": "2024-12-27T22:47:24.449989",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/prompting-strategies",
            "title": "Prompt design strategies  |  Gemini API  |  Google AI for Developers",
            "text_content": "Prompt design strategies  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Prompt design strategies This page introduces you to some general prompt design strategies that you can employ when\ndesigning prompts. Large language models (LLM) are trained on vast amounts of text data to learn the patterns and\nrelationships between language. When given some text (the prompt), language models can\npredict what is likely to come next, like a sophisticated autocompletion tool. Google AI Studio hosts a simple prompt gallery meant to interactively showcase many of the concepts shared in this guide.\n\nIn the rest of this guide, you'll learn about the following prompt design strategies: Give clear and specific instructions Include few-shot examples Add contextual information Add prefixes Let the model complete partial input Give clear and specific instructions Giving the model instructions on what to do is an effective and efficient way to customize model\nbehavior. Ensure that the instructions you give are clear and specific. Instructions can be as\nsimple as a list of step-by-step instructions or as complex as mapping out a user's experience and\nmindset. Define the task to perform Describe in detail the task that you want the model to perform. The task can be as simple as a\nstep-by-step list of instructions or as complex as mapping out a user's experience and mindset. The\nfollowing prompt tells the model to summarize a block of text: Prompt: Summarize this text: Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially\nfaster than any modern traditional computer. At very tiny scales, physical matter acts as both\nparticles and as waves, and quantum computing uses specialized hardware to leverage this behavior.\nThe operating principles of quantum devices is beyond the scope of classical physics. When deployed\nat scale, quantum computers could be used in a wide variety of applications such as: in\ncybersecurity to break existing encryption methods while helping researchers create new ones, in\nmeteorology to develop better weather forecasting etc. However, the current state of the art quantum\ncomputers are still largely experimental and impractical. Response: Quantum computers leverage the wave-particle duality of matter at tiny scales to perform\n    calculations exponentially faster than traditional computers. They harness quantum phenomena,\n    which are beyond the scope of classical physics, to achieve this speed advantage. While\n    promising for applications like cybersecurity, weather forecasting, and beyond, current\n    quantum computers are still experimental and not yet practical for widespread use. (gemini-1.5-flash) Specify any constraints Specify any constraints on reading the prompt or generating a response. You can tell the model\nwhat to do and not to do. From the previous example, the summarization is shorter than the original\ntext, but maybe you want the summary to be shorter. In this case, you can specify a constraint in\nthe prompt on how long you want the summary to be: Prompt: Summarize this text in two sentences: Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially\nfaster than any modern traditional computer. At very tiny scales, physical matter acts as both\nparticles and as waves, and quantum computing uses specialized hardware to leverage this behavior.\nThe operating principles of quantum devices is beyond the scope of classical physics. When deployed\nat scale, quantum computers could be used in a wide variety of applications such as: in\ncybersecurity to break existing encryption methods while helping researchers create new ones, in\nmeteorology to develop better weather forecasting etc. However, the current state of the art quantum\ncomputers are still largely experimental and impractical. Response: Quantum computers harness the wave-particle duality of matter at the quantum level to perform\n      computations exponentially faster than traditional computers. While these machines hold\n      immense potential for fields like cybersecurity and weather forecasting, they are currently\n      in their early stages of development and not yet practical for widespread use. (gemini-1.5-flash) Define the format of the response You can also give instructions that specify the format of the response. For example, you can ask\nfor the response to be formatted as a table, bulleted list, elevator pitch, keywords, sentence, or\nparagraph. The following system message instructs the model to be more conversational in its response: System message All questions should be answered comprehensively with details, unless the user requests a\n    concise response specifically. Respond in the same language as the query. Prompt What is a smart way to create a lemonade business? Response: Launching a Successful Lemonade Stand: A Smart Approach\nCreating a lemonade business, whether a simple stand or a larger operation, requires strategic thinking. Here's a breakdown of how to approach it smartly:\n\n1. Define Your Business:\nScope: Are you aiming for a small, seasonal lemonade stand, a food truck, a delivery service, or something else entirely?\nTarget Audience: Who are you selling to? Families, office workers, tourists?\nUnique Selling Proposition: What sets your lemonade apart? Special flavors, organic ingredients, a unique recipe?\n\nPricing: Research competitor prices and determine a profitable pricing structure.\nLocation: Consider foot traffic, accessibility, and regulations for your chosen location.\n..... (gemini-1.5-flash) Summary Give the models instructions on what to do. Make the instructions clear and specific. Specify any constraints or formatting requirements for the output. Include few-shot examples You can include examples in the prompt that show the model what getting it right looks like. The\nmodel attempts to identify patterns and relationships from the examples and applies them when\ngenerating a response. Prompts that contain a few examples are called few-shot prompts,\nwhile prompts that provide no examples are called zero-shot prompts . Few-shot prompts are\noften used to regulate the formatting, phrasing, scoping, or general patterning of model responses.\nUse specific and varied examples to help the model narrow its focus and generate more accurate\nresults. We recommend to always include few-shot examples in your prompts. Prompts without few-shot\nexamples are likely to be less effective. In fact, you can remove instructions from your prompt\nif your examples are clear enough in showing the task at hand. Zero-shot vs few-shot prompts The following zero-shot prompt asks the model to choose the best explanation. Prompt: Please choose the best explanation to the question:\n\nQuestion: How is snow formed?\nExplanation1: Snow is formed when water vapor in the air freezes into ice crystals in the\natmosphere, which can combine and grow into snowflakes as they fall through the atmosphere and\naccumulate on the ground.\nExplanation2: Water vapor freezes into ice crystals forming snow.\nAnswer: Response: Explanation 1 is the best explanation. (gemini-1.5-flash) If your use case requires the model to produce concise responses, you can include examples in the\nprompt that give preference to concise responses. The following prompt provides two examples that show preference to the shorter explanations. In\nthe response, you can see that the examples guided the model to choose the shorter explanation\n(Explanation2) as opposed to the longer explanation (Explanation1) like it did previously. Prompt: Below are some examples showing a question, explanation, and answer format:\n\nQuestion: Why is sky blue?\nExplanation1: The sky appears blue because of Rayleigh scattering, which causes shorter blue\nwavelengths of light to be scattered more easily than longer red wavelengths, making the sky look\nblue.\nExplanation2: Due to Rayleigh scattering effect.\nAnswer: Explanation2\n\nQuestion: What is the cause of earthquakes?\nExplanation1: Sudden release of energy in the Earth's crust.\nExplanation2: Earthquakes happen when tectonic plates suddenly slip or break apart, causing a\nrelease of energy that creates seismic waves that can shake the ground and cause damage.\nAnswer: Explanation1\n\nNow, Answer the following question given the example formats above:\n\nQuestion: How is snow formed?\nExplanation1: Snow is formed when water vapor in the air freezes into ice crystals in the\natmosphere, which can combine and grow into snowflakes as they fall through the atmosphere and\naccumulate on the ground.\nExplanation2: Water vapor freezes into ice crystals forming snow.\nAnswer: Response: Answer: Explanation2 (gemini-1.5-flash) Find the optimal number of examples You can experiment with the number of examples to provide in the prompt for the most desired\nresults. Models like Gemini can often pick up on patterns using a few examples, though you\nmay need to experiment with what number of examples lead to the desired results. At the same time,\nif you include too many examples, the model may start to overfit the response to the examples. Use examples to show patterns instead of antipatterns Using examples to show the model a pattern to follow is more effective than using examples to\nshow the model an antipattern to avoid. ⛔ Negative pattern : Example Don't end haikus with a question:\nHaiku are fun\nA short and simple poem\nDon't you enjoy them? ✅ Positive pattern : Example Always end haikus with an assertion:\nHaiku are fun\nA short and simple poem\nA joy to write Use consistent formatting across examples Make sure that the structure and formatting of few-shot examples are the same to avoid responses\nwith undesired formats. One of the primary objectives of adding few-shot examples in prompts is to\nshow the model the response format. Therefore, it is essential to ensure a consistent format across\nall examples, especially paying attention to XML tags, white spaces, newlines, and example\nsplitters. Summary Including prompt-response examples in the prompt helps the model learn how to respond. Give the model examples of the patterns to follow instead of examples of patterns to\n  avoid. Experiment with the number of prompts to include. Depending on the model, too few examples are\n  ineffective at changing model behavior. Too many examples cause the model to overfit. Use consistent formatting across examples Add contextual information You can include in the prompt instructions and information that the model needs to solve a\nproblem instead of assuming that the model has all of the required information. This contextual\ninformation helps the model understand the constraints and details of what you're asking for it to\ndo. The following example asks the model to give troubleshooting guidance for a router: Prompt: What should I do to fix my disconnected wifi? The light on my Google Wifi router is yellow and\nblinking slowly. Response: A yellow blinking light on your Google Wifi router usually indicates a problem with the internet connection. Here are some steps to troubleshoot and fix the issue:\n  1. Restart your router and modem:\n    - Unplug your modem from the power outlet and wait for 30 seconds.\n    - Plug your modem back in and wait for it to fully boot up.\n    - Unplug your Google Wifi router and wait for 30 seconds.\n    - Plug your Google Wifi router back in and wait for it to fully boot up.\n..... (gemini-1.5-flash) The response looks like generic troubleshooting information that's not specific to the router or\nthe status of the LED indicator lights. To customize the response for the specific router, you can add to the prompt the router's\ntroubleshooting guide as context for it to refer to when providing a response. Prompt: Answer the question using the text below. Respond with only the text provided.\nQuestion: What should I do to fix my disconnected wifi? The light on my Google Wifi router is yellow and blinking slowly.\n\nText:\nColor: Slowly pulsing yellow\nWhat it means: There is a network error.\nWhat to do:\nCheck that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.\n\nColor: Fast blinking yellow\nWhat it means: You are holding down the reset button and are factory resetting this device.\nWhat to do:\nIf you keep holding down the reset button, after about 12 seconds, the light will turn solid yellow. Once it is solid yellow, let go of the factory reset button.\n\nColor: Solid yellow\nWhat it means: Router is factory resetting.\nWhat to do:\nThis can take up to 10 minutes. When it's done, the device will reset itself and start pulsing white, letting you know it's ready for setup.\n\nColor: Solid red\nWhat it means: Something is wrong.\nWhat to do:\nCritical failure. Factory reset the router. If the light stays red, contact Wifi customer support. Response: Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again. (gemini-1.5-flash) Summary Include information (context) in the prompt that you want the model to use when generating a\n  response. Give the model instructions on how to use the contextual information. Add prefixes A prefix is a word or phrase that you add to the prompt content that can serve several purposes,\ndepending on where you put the prefix: Input prefix: Adding a prefix to the input signals semantically meaningful\n  parts of the input to the model. For example, the prefixes \"English:\" and \"French:\" demarcate two\n  different languages. Output prefix: Even though the output is generated by the model, you can add\n  a prefix for the output in the prompt. The output prefix gives the model information about what's\n  expected as a response. For example, the output prefix \"JSON:\" signals to the model that the\n  output should be in JSON format. Example prefix: In few-shot prompts, adding prefixes to the examples provides\n  labels that the model can use when generating the output, which makes it easier to parse output\n  content. In the following example, \"Text:\" is the input prefix and \"The answer is:\" is the output\nprefix. Prompt: Classify the text as one of the following categories.\n- large\n- small\nText: Rhino\nThe answer is: large\nText: Mouse\nThe answer is: small\nText: Snail\nThe answer is: small\nText: Elephant\nThe answer is: Response: The answer is: large (gemini-1.5-flash) Let the model complete partial input Generative language models work like an advanced autocompletion tool. When you provide partial\ncontent, the model can provide the rest of the content or what it thinks is a continuation of that\ncontent as a response. When doing so, if you include any examples or context, the model can take\nthose examples or context into account. The following example provides a prompt with an instruction and an entity input: Prompt: For the given order, return a JSON object that has the fields cheeseburger, hamburger, fries, or\ndrink, with the value being the quantity.\n\nOrder: A burger and a drink. Response: ```\n{\n  \"hamburger\": 1,\n  \"drink\": 1\n}\n``` (gemini-1.5-flash) While the model did as prompted, writing out the instructions in natural language can sometimes\nbe challenging and it leaves a lot to the model's interpretation. For example, a restaurants menu\nmight contain many items. To reduce the size of the JSON response, you probably want to omit the items that\nweren't ordered. In this case, you can give an example and a response prefix and let the model\ncomplete it: Prompt: Valid fields are cheeseburger, hamburger, fries, and drink.\nOrder: Give me a cheeseburger and fries\nOutput:\n```\n{\n  \"cheeseburger\": 1,\n  \"fries\": 1\n}\n```\nOrder: I want two burgers, a drink, and fries.\nOutput: Response: ```\n{\n  \"hamburger\": 2,\n  \"drink\": 1,\n  \"fries\": 1\n}\n``` (gemini-1.5-flash) Notice how \"cheeseburger\" was excluded from the output because it wasn't a part of the order. Prompt the model to format its response The completion strategy can also help format the response. The following example prompts the\nmodel to create an essay outline: Prompt: Create an outline for an essay about hummingbirds. Response: Hummingbirds: A Symphony of Speed and Color\n    I. Introduction\n    A. Hummingbirds are fascinating creatures known for their dazzling colors, lightning-fast flight, and unique feeding habits.\n    B. Brief overview of the diversity of hummingbird species and their geographic distribution.\n    C. Thesis statement: Hummingbirds are an embodiment of resilience, adaptation, and the wonders of the natural world.\n..... (gemini-1.5-flash) The prompt didn't specify the format for the outline and the model chose a format for you. To get\nthe model to return an outline in a specific format, you can add text that represents the start of\nthe outline and let the model complete it based on the pattern that you initiated. Prompt: Create an outline for an essay about hummingbirds.\nI. Introduction\n   * Response: Hummingbirds: Jewels of the Air\n    **I. Introduction**\n       * **Hook:** Begin with a captivating image or anecdote about hummingbirds, emphasizing their vibrant colors, rapid flight, and unique feeding habits.\n       * **Background:** Briefly introduce hummingbirds as the smallest birds in the world and their fascinating adaptations for a nectar-rich diet.\n       * **Thesis:** State the main argument or focus of the essay. Examples: \n            * **The incredible adaptations of hummingbirds enable them to thrive in diverse ecosystems.**\n            * **Hummingbirds play a crucial role in pollination and biodiversity.**\n            * **The beauty and grace of hummingbirds make them captivating subjects for both scientific study and artistic appreciation.** (gemini-1.5-flash) Summary If you give the model a partial input, the model completes that input based on any available\n  examples or context in the prompt. Having the model complete an input may sometimes be easier than describing the task in natural\n  language. Adding a partial answer to a prompt can guide the model to follow a desired pattern or\n  format. Break down prompts into simple components For use cases that require complex prompts, you can help the model manage this complexity by\nbreaking things down into simpler components. Break down instructions Instead of having many instructions in one prompt, create one prompt per instruction. You can\nchoose which prompt to process based on the user's input. Chain prompts For complex tasks that involve multiple sequential steps, make each step a prompt and chain the\nprompts together in a sequence. In this sequential chain of prompts, the output of one prompt in the\nsequence becomes the input of the next prompt. The output of the last prompt in the sequence is the\nfinal output. Aggregate responses Aggregation is when you want to perform different parallel tasks on different portions of the\ndata and aggregate the results to produce the final output. For example, you can tell the model to\nperform one operation on the first part of the data, perform another operation on the rest of the\ndata and aggregate the results. Summary Break down complex instructions into a prompt for each instruction and decide which prompt to\n    apply based on the user's input. Break down multiple sequential steps into separate prompts and chain them such that the output\n    on the preceding prompt becomes the input of the following prompt. Break down parallel tasks and aggregate the responses to produce the final output. Experiment with different parameter values Each call that you send to a model includes parameter values that control how the model generates\na response. The model can generate different results for different parameter values. Experiment with\ndifferent parameter values to get the best values for the task. The parameters available for\ndifferent models may differ. The most common parameters are the following: Max output tokens Temperature Top-K Top-P Max output tokens Maximum number of tokens that can be generated in the response. A token is\napproximately four characters. 100 tokens correspond to roughly 20 words. Specify a lower value for shorter responses and a higher value for longer\nresponses. Temperature The temperature is used for sampling during response generation, which occurs\nwhen topP and topK are applied. Temperature controls\nthe degree of randomness in token selection. Lower temperatures are good for\nprompts that require a more deterministic and less open-ended or creative\nresponse, while higher temperatures can lead to more diverse or creative\nresults. A temperature of 0 is deterministic, meaning that the\nhighest probability response is always selected. For most use cases, try starting with a temperature of 0.2 . If\nthe model returns a response that's too generic, too short, or the model gives a\nfallback response, try increasing the temperature. Top-K Top-K changes how the model selects tokens for output. A top-K of 1 means the next selected token is the most probable among all\ntokens in the model's vocabulary (also called greedy decoding), while a top-K of 3 means that the next token is selected from among the three most\nprobable tokens by using temperature. For each token selection step, the top-K tokens with the highest\nprobabilities are sampled. Then tokens are further filtered based on top-P with\nthe final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more\nrandom responses. The default top-K is 40 . Top-P Top-P changes how the model selects tokens for output. Tokens are selected\nfrom the most (see top-K) to least probable until the sum of their probabilities\nequals the top-P value. For example, if tokens A, B, and C have a probability of\n0.3, 0.2, and 0.1 and the top-P value is 0.5 , then the model will\nselect either A or B as the next token by using temperature and excludes C as a\ncandidate. Specify a lower value for less random responses and a higher value for more\nrandom responses. The default top-P is 0.95 . Prompt iteration strategies Prompt design is an iterative process that often requires a few iterations before you get the\ndesired response consistently. This section provides guidance on some things you can try when\niterating on your prompts. Use different phrasing Using different words or phrasing in your prompts often yields different responses from the model\neven though they all mean the same thing. If you're not getting the expected results from your\nprompt, try rephrasing it. Version 1:\nHow do I bake a pie?\n\nVersion 2:\nSuggest a recipe for a pie.\n\nVersion 3:\nWhat's a good pie recipe? Switch to an analogous task If you can't get the model to follow your instructions for a task, try giving it instructions for\nan analogous task that achieves the same result. This prompt tells the model to categorize a book by using predefined categories. Prompt: Which category does The Odyssey belong to:\nthriller\nsci-fi\nmythology\nbiography Response: The Odyssey belongs to the category of **mythology**. \n\n    Here's why:\n    \n    * **Mythology:** The Odyssey tells the story of Odysseus, a hero from Greek mythology, and his\n    journey home after the Trojan War. It features gods, monsters, and supernatural events common to\n    Greek mythology.\n..... (gemini-1.5-flash) The response is correct, but the model didn't stay within the bounds of the options. You also\nwant to model to just respond with one of the options instead of in a full sentence. In this case,\nyou can rephrase the instructions as a multiple choice question and ask the model to choose an\noption. Prompt: Multiple choice problem: Which of the following options describes the book The Odyssey?\nOptions:\n- thriller\n- sci-fi\n- mythology\n- biography Response: The correct answer is **mythology**. (gemini-1.5-flash) Change the order of prompt content The order of the content in the prompt can sometimes affect the response. Try changing the\ncontent order and see how that affects the response. Version 1:\n[examples]\n[context]\n[input]\n\nVersion 2:\n[input]\n[examples]\n[context]\n\nVersion 3:\n[examples]\n[input]\n[context] Fallback responses A fallback response is a response returned by the model when either the prompt or the response\ntriggers a safety filter. An example of a fallback response is \"I'm not able to help with that, as\nI'm only a language model.\" If the model responds with a fallback response, try increasing the temperature. Things to avoid Avoid relying on models to generate factual information. Use with care on math and logic problems. Next steps Now that you have a deeper understanding of prompt design, try writing your\nown prompts using Google AI Studio . To learn about multimodal prompting, see Prompting with media files . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-09-24 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies"
            ],
            "timestamp": "2024-12-27T22:47:25.094903",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
            "title": "File prompting strategies  |  Gemini API  |  Google AI for Developers",
            "text_content": "File prompting strategies  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback File prompting strategies The Gemini family of artificial intelligence (AI) models is built to handle\nvarious types of input data, including text, images, and audio. Since these\nmodels can handle more more than one type or mode of data, the Gemini models\nare called multimodal models or explained as having multimodal capabilities . Note: Prompting with media files is supported by specific Gemini models.\nFor more information about which models support this feature, see the Vision and Audio guides. This page provides guidance and best practices for using media files with\nprompts for the Gemini API. For information on using the Gemini Files API for\nuploading media files and including them in your prompts, see the Vision , Audio , and Document processing guides. Using media files with prompts Being able to use various types of data in your prompts gives you more\nflexibility in terms of what tasks you can tackle with the Gemini API. For\nexample, you can send the model a photo of a delicious meal and ask it to write\na short blog about the meal. Prompt Response Write a short, engaging blog post based on this picture. It should\n      include a description of the meal in the photo and talk about my journey\n      meal prepping. Meal prepping is a great way to save time and money, and it can also\n      help you to eat healthier. This meal is a great example of a healthy\n      and delicious meal that can be easily prepped ahead of time. This meal features brown rice, roasted vegetables, and chicken teriyaki.\n      The brown rice is a healthy and complex carbohydrate that will give you\n      sustained energy. The roasted vegetables are a great way to get your daily\n      dose of vitamins and minerals, and the chicken teriyaki is a delicious and\n      protein-rich option. This meal is also very easy to prepare. Simply cook the brown rice, roast\n      the vegetables, and cook the chicken teriyaki. Once everything is cooked,\n      divide it into meal prep containers and store them in the refrigerator.\n      You can then grab a container and go on busy mornings! If you're looking for a healthy and delicious meal that can be easily\n      prepped ahead of time, this meal is a great option. It's packed with\n      nutrients and flavor, and it's sure to keep you feeling full and satisfied. Here's to healthy and delicious meal prepping! Strategies for prompting with media If you are having trouble getting the output you want from prompts that use\nmedia files, there are some strategies that can help you get the results you\nwant. The following sections provide design approaches and troubleshooting\ntips for improving prompts that use multimodal input. You can improve your multimodal prompts by following these best practices: Prompt design fundamentals Be specific in your instructions : Craft clear and concise instructions that leave minimal room for misinterpretation. Add a few examples to your prompt: Use realistic few-shot examples to illustrate what you want to achieve. Break it down step-by-step : Divide complex tasks into manageable sub-goals, guiding the model through the process. Specify the output format : In your prompt, ask for the output to be in the format you want, like markdown, JSON, HTML and more. Put your image first for single-image prompts : While Gemini can handle image and text inputs in any order, for prompts containing a single image, it might perform better if that image is placed before the text prompt. Troubleshooting your multimodal prompt If the model is not drawing information from the relevant part of the image: Drop hints with which aspects of the image you want the prompt to draw information from. If the model output is too generic (not tailored enough to the image): At the start of the prompt, try asking the model to describe the image(s) before providing the task instruction, or try asking the model to refer to what's in the image. Prompt design fundamentals This section expands on the best practices listed in the previous section. Note: Model results from the following Gemini examples can vary from run to run. Be specific in your instructions Prompts have the most success when they are clear and detailed. If you have a specific output in mind, it's better to include that requirement in the prompt to ensure you get the output you want. Sometimes, a prompt's intent might seem clear to the person who wrote it, but it is actually under-specified. To customize model behavior, consider how your prompt could be (mis)interpreted, and ensure that the instructions you give are specific and clear. Remember to leave minimal room for misinterpretation For this image of an airport board, asking the model to just \"describe this image\" could generate a general description. If you need the model to parse the time and city from the image, you can include that request directly in your prompt. Prompt Model response Describe this image. The image shows an airport arrivals and departures board. Updated prompt Improved response Parse the time and city from the airport board shown in this image into a list. 10:50 Moscow 11:05 Edinburgh 11:05 London 11:10 Bucharest 11:30 Kiev 11:35 Dublin 11:45 East Midlands 12:15 Sofia 12:30 London 12:30 Newcastle 12:40 St Petersburg 12:40 London 12:45 Manchester Add a few examples The Gemini model can accept multiple inputs which it can use as examples to understand the\noutput you want. Adding these examples can help the model identify the patterns and apply the\n  relationship between the given images and responses to the new example. This is also called\n  \"few-shot\" learning. In the example below, the initial output is written in sentence form, and also contains the\ncountry (Brazil). Suppose you want the output in a different format or style, and you want\nonly the city, not the country. Adding few-shot examples to your prompt can steer the model\nto respond in the way you want. Prompt Model response Determine the city along with the landmark. The landmark is the Christ the Redeemer statue in Rio de Janeiro, Brazil. Updated prompt Improved response Determine the city along with the landmark. city: Rome, landmark: the Colosseum. city: Beijing, landmark: Forbidden City city: Rio de Janeiro, landmark: Christ the Redeemer statue Break it down step-by-step For complex tasks like the ones that require both visual understanding and reasoning, it can be helpful\nto split the task into smaller, more straightforward steps. Alternatively, it could also be effective if you\ndirectly ask the model to “think step by step” in your prompt. Prompt Model response When will I run out of toilet paper? Soon, you only have 3 rolls left. Updated prompt Improved response 1. First, count how many toilet paper rolls are in this picture. 2. Then, determine how much toilet paper a typical person uses per day. 3. Calculate how long these rolls of toilet paper will last. 1. There are 3 rolls of toilet paper in this picture. 2. A typical person uses about 20 sheets of toilet paper per day. 3. If each roll contains 200 sheets, then each roll will last for about 10 days. Therefore, the 3 rolls will last for about a month. Math problems or other types of word problems are great candidates for asking the model to think step-by-step. Prompt Response What is the 4th term in the sequence? -135 The response from the model is incorrect. Some ways to improve this is to ask the model to parse the problem before answering the prompt. Prompt Response Parse the formula in the image first. Then based on the formula, answer \"what's the 4th term in the sequence?\". Click to expand the result $$ b(n) = b(n-1) * (-3) $$ $$ b(1) = 15 $$ Substitute n = 4 into the formula: $$ b(4) = b(4-1) * (-3) $$ $$ b(4) = b(3) * (-3) $$ We don't know the value of b(3), so substitute n = 3 into the formula: $$ b(3) = b(3-1) * (-3) $$ $$ b(3) = b(2) * (-3) $$ We don't know the value of b(2), so substitute n = 2 into the formula: $$ b(2) = b(2-1) * (-3) $$ $$ b(2) = b(1) * (-3) $$ We know the value of b(1) is 15, so substitute it into the formula: $$ b(2) = 15 * (-3) $$ $$ b(2) = -45 $$ Now that we know the value of b(2), substitute it into the formula for b(3): $$ b(3) = b(2) * (-3) $$ $$ b(3) = -45 * (-3) $$ $$ b(3) = 135 $$ Now that we know the value of b(3), substitute it into the formula for b(4): $$ b(4) = b(3) * (-3) $$ $$ b(4) = 135 * (-3) $$ $$ b(4) = -405 $$ Therefore, the 4th term in the sequence is -405. This model response is correct - now to make the response even more streamlined, you can ask the model to to think step-by-step. Updated prompt Improved response What is the 4th term in the sequence? Think step by step. The first term is 15. The second term is b(1) * (-3) = 15 * (-3) = -45. The third term is b(2) * (-3) = -45 * (-3) = 135. The fourth term is b(3) * (-3) = 135 * (-3) = -405. Try specifying the output format A common problem is the need for model output to be in a certain format (e.g. markdown, JSON, HTML), especially if the model output needs to\n  be ingested by a downstream task. You can try to produce output in that format by instructing the model to do so within the prompt. Below are two examples: Prompt Model response Parse the table in this image into markdown format | Subject | Status | Time | Start date | Due on | | ------- | ------ | ---- | --------- | ------ | | Math | Done | 1.5 hrs | 11/29/2023 | 12/1/2023 | | Geography | Done | 30 min | 12/2/2023 | 12/4/2023 | | Physics | In progress | 2 hrs | 12/3/2023 | 12/5/2023 | | Biology | Not started | 6 hrs | 11/2/2016 | 12/6/2023 | Prompt Model response Provide a list of all the following attributes: ingredients, type of cuisine, vegetarian or not, in JSON format ```json``` { \"ingredients\": [ \"rice\", \"shrimp\", \"clams\", \"mussels\", \"peas\", \"tomatoes\", \"onions\", \"garlic\", \"olive oil\", \"paprika\", \"salt\", \"pepper\" ], \"type of cuisine\": \"Spanish\", \"vegetarian\": false Put your image first for single-image prompts While Gemini can interpret image and text in any order within a prompt, placing a single image\nbefore the text prompt might lead to better results. In the following samples, you'll notice the image\ncomes first before the text in each prompt. Troubleshooting your multimodal prompt You might need to troubleshoot your prompt if you are not getting a helpful response. Here are a few strategies you could try. If the model is not drawing information from the relevant part of the image To get a more specific response, you can point out which aspects of the image you want the prompt to\ndraw information from to inform its response. Prompt Model response How many days will these diapers last a baby? The diapers will last for 198 days before they run out. Updated prompt Improved response How long will these diapers last before I run out? Use the weight shown on the box to determine the child's age, and use the total number of diapers in the box. Divide the total number by how many diapers the child goes through per day. The box contains 198 diapers. The box states the diapers are for a child weighing 22-37 lbs. The average child in this weight range is likely between 1 and 2 years old. A 2-year-old typically goes through 6 diapers per day, therefore, the diapers will last around 33 days (198/6). If the model output is too generic and not tailored enough to the image input To help the model tailor its response to the image(s), try asking it to describe the images before performing its reasoning task. Prompt Model response What is in common between these images? All four images are examples of landscape photography. Updated prompt Improved response First, describe what's in each image in detail. What's in common between these images? The first image shows a person packing a suitcase. The second image shows an empty hallway in a shopping mall. The third image shows a roll of toilet paper. The fourth image shows a dark and stormy sea. The commonality between these images is that they are all examples of liminal spaces. Liminal spaces are places or situations that are in between two other places or situations. They are often characterized by a sense of uncertainty or ambiguity. The first image shows the person in between being at home and being on a trip. The second image shows the hallway in between two stores. The third image is of toilet paper, which can represent a bathroom. A bathroom is a transitional space between private\n        and public realms. It's a place where we prepare ourselves to enter or leave the outside world. The fourth image shows the sea in between being calm and being stormy. Liminal spaces can be both exciting and unsettling. They can be a place of new beginning or a place of endings. They can be a place of reflection or a place of action. You can also try explicitly asking the model to refer to what's in the image. Updated prompt Improved response What is in common between these images? Refer to what's in the images in your response. All of the images are related to travel. The first image shows a person packing a suitcase, the second image shows\n          an empty hallway in a shopping mall, the third image shows a stack of toilet paper, and the fourth image shows a\n          dark and stormy sea. These images are all related to travel because they are all things that people need or use when\n          they travel. The first image shows a person packing a suitcase, which is something that people do when they are\n          going on a trip. The second image shows an empty hallway in a shopping mall, which is a place where people\n          often go to buy things for their trips. The third image shows a stack of toilet paper, which is something\n          that people often use when they are traveling. The fourth image shows a dark and stormy sea, which is\n          something that people often encounter when they are traveling. The same tips work for creative output from images. In the following example with\n  the rabbit, explicitly asking the model to include aspects from the image can improve the output. Prompt Model response Can you write me a poem about this image? Little bunny, so soft and brown, Hopping through the field, up and down. Your ears so long, your tail so small, You're the cutest creature of them all. You nibble on the grass, so green and sweet, And hop away when you see me. But I'll always be your friend, Little bunny, until the very end. Updated prompt Improved response Can you write me a descriptive and dramatic poem about this image and include the location? Little bunny on the green, Ears so long, so keen. What do you see out there, In the distance, where you stare? Is it the ocean blue, Sparkling in the sun's hue? Or the waves that crash and roar, A rhythm that you adore? Little bunny, tell me true, What captures your attention so? Is it the salty breeze, Or the seagulls' cries that please? Perhaps it's the vastness of it all, A world so big and small. A place where dreams can roam, And adventures can be sown. Little bunny, I wonder what you dream, As you sit on the grass, so serene. Do you yearn to explore the deep, Or stay on land, where you can leap? Whatever it may be, little bunny, Keep that spark of wonder burning brightly. For in your dreams and aspirations, Lies a world waiting for your creation. Troubleshooting which part of the prompt failed It can be hard to know whether a prompt failed because the model\ndidn't understand the image to begin with, or if it did understand the image but\ndid not perform the correct reasoning steps afterward. To disambiguate between those reasons, ask the model to describe what's in the image. In this example below, if the model responds with a snack that seems surprising when paired with\ntea (e.g. popcorn), you can first troubleshoot to determine whether the model correctly recognized\nthat the image contains tea. Prompt Prompt for troubleshooting What's a snack I can make in 1 minute that would go well with this? Describe what's in this image. Another strategy is to ask the model to explain its reasoning. That can help you narrow down which part of the reasoning broke down, if any. Prompt Prompt for troubleshooting What's a snack I can make in 1 minute that would go well with this? What's a snack I can make in 1 minute that would go well with this? Please explain why. Tuning the sampling parameters In each request, you send not only the multimodal prompt but a set of sampling parameters to the model.\nThe model can generate different results for different parameter values. Experiment with the different\nparameters to get the best values for the task. The most commonly adjusted parameters are the following: Temperature top-P top-K Temperature Temperature is used for sampling during response generation, which occurs when top-P and top-K are applied.\nTemperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that\nrequire a more deterministic and less open-ended or creative response, while higher temperatures can lead to more\ndiverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response\nis always selected. For most use cases, try starting with a temperature of 0.4. If you need more creative results, try increasing the\ntemperature. If you observe clear hallucinations, try reducing the temperature. Top-K Top-K changes how the model selects tokens for output. A top-K of 1 means the next selected token is\nthe most probable among all tokens in the model's vocabulary (also called greedy decoding), while\na top-K of 3 means that the next token is selected from among the three most probable tokens by\nusing temperature. For each token selection step, the top-K tokens with the highest probabilities are sampled. Then\ntokens are further filtered based on top-P with the final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more random responses.\nThe default value of top-K is 32. Top-P Top-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K)\nto least probable until the sum of their probabilities equals the top-P value. For example, if tokens\nA, B, and C have a probability of 0.6, 0.3, 0.1 and the top-P value is  0.9, then the model will\nselect either A or B as the next token by using temperature and excludes C as a candidate. Specify a lower value for less random responses and a higher value for more random responses.\nThe default value of top-P is 1.0. Next steps Try writing your own multimodal prompts using Google AI\nStudio . For more guidance on prompt design, see the Prompt strategies page. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-21 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies"
            ],
            "timestamp": "2024-12-27T22:47:25.727513",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/tokens",
            "title": "Understand and count tokens  |  Gemini API  |  Google AI for Developers",
            "text_content": "Understand and count tokens  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Understand and count tokens Python Node.js Go Gemini and other generative AI models process input and output at a granularity\ncalled a token . About tokens Tokens can be single characters like z or whole words like cat . Long words\nare broken up into several tokens. The set of all tokens used by the model is\ncalled the vocabulary, and the process of splitting text into tokens is called tokenization . For Gemini models, a token is equivalent to about 4 characters.\n100 tokens is equal to about 60-80 English words. When billing is enabled, the cost of a call to the Gemini API is\ndetermined in part by the number of input and output tokens, so knowing how to\ncount tokens can be helpful. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/tokens"
            ],
            "timestamp": "2024-12-27T22:47:26.655690",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/openai",
            "title": "OpenAI compatibility  |  Gemini API  |  Google AI for Developers",
            "text_content": "OpenAI compatibility  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback OpenAI compatibility Gemini models are accessible using the OpenAI libraries (Python and TypeScript /\nJavascript) along with the REST API, by updating three lines of code\nand using your Gemini API key . If you\naren't already using the OpenAI libraries, we recommend that you call the Gemini API directly . Python from openai import OpenAI client = OpenAI ( api_key = \"GEMINI_API_KEY\" , base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\" ) response = client . chat . completions . create ( model = \"gemini-1.5-flash\" , n = 1 , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Explain to me how AI works\" } ] ) print ( response . choices [ 0 ] . message ) Node.js import OpenAI from \"openai\" ; const openai = new OpenAI ({ apiKey : \"GEMINI_API_KEY\" , baseURL : \"https://generativelanguage.googleapis.com/v1beta/openai/\" }); const response = await openai . chat . completions . create ({ model : \"gemini-1.5-flash\" , messages : [ { role : \"system\" , content : \"You are a helpful assistant.\" }, { role : \"user\" , content : \"Explain to me how AI works\" , }, ], }); console . log ( response . choices [ 0 ]. message ); REST curl \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\" \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer GEMINI_API_KEY\" \\ -d '{ \"model\": \"gemini-1.5-flash\", \"messages\": [ {\"role\": \"user\", \"content\": \"Explain to me how AI works\"} ] }' Streaming The Gemini API supports streaming responses . Python from openai import OpenAI client = OpenAI ( api_key = \"GEMINI_API_KEY\" , base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\" ) response = client . chat . completions . create ( model = \"gemini-1.5-flash\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Hello!\" } ], stream = True ) for chunk in response : print ( chunk . choices [ 0 ] . delta ) Node.js import OpenAI from \"openai\" ; const openai = new OpenAI ({ apiKey : \"GEMINI_API_KEY\" , baseURL : \"https://generativelanguage.googleapis.com/v1beta/openai/\" }); async function main () { const completion = await openai . chat . completions . create ({ model : \"gemini-1.5-flash\" , messages : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Hello!\" } ], stream : true , }); for await ( const chunk of completion ) { console . log ( chunk . choices [ 0 ]. delta . content ); } } main (); REST curl \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\" \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer GEMINI_API_KEY\" \\ -d '{ \"model\": \"gemini-1.5-flash\", \"messages\": [ {\"role\": \"user\", \"content\": \"Explain to me how AI works\"} ], \"stream\": true }' Function calling Function calling makes it easier for you to get structured data outputs from\ngenerative models and is supported in the Gemini API . Python from openai import OpenAI client = OpenAI ( api_key = \"GEMINI_API_KEY\" , base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\" ) tools = [ { \"type\" : \"function\" , \"function\" : { \"name\" : \"get_weather\" , \"description\" : \"Get the weather in a given location\" , \"parameters\" : { \"type\" : \"object\" , \"properties\" : { \"location\" : { \"type\" : \"string\" , \"description\" : \"The city and state, e.g. Chicago, IL\" , }, \"unit\" : { \"type\" : \"string\" , \"enum\" : [ \"celsius\" , \"fahrenheit\" ]}, }, \"required\" : [ \"location\" ], }, } } ] messages = [{ \"role\" : \"user\" , \"content\" : \"What's the weather like in Chicago today?\" }] response = client . chat . completions . create ( model = \"gemini-1.5-flash\" , messages = messages , tools = tools , tool_choice = \"auto\" ) print ( response ) Node.js import OpenAI from \"openai\" ; const openai = new OpenAI ({ apiKey : \"GEMINI_API_KEY\" , baseURL : \"https://generativelanguage.googleapis.com/v1beta/openai/\" }); async function main () { const messages = [{ \"role\" : \"user\" , \"content\" : \"What's the weather like in Chicago today?\" }]; const tools = [ { \"type\" : \"function\" , \"function\" : { \"name\" : \"get_weather\" , \"description\" : \"Get the weather in a given location\" , \"parameters\" : { \"type\" : \"object\" , \"properties\" : { \"location\" : { \"type\" : \"string\" , \"description\" : \"The city and state, e.g. Chicago, IL\" , }, \"unit\" : { \"type\" : \"string\" , \"enum\" : [ \"celsius\" , \"fahrenheit\" ]}, }, \"required\" : [ \"location\" ], }, } } ]; const response = await openai . chat . completions . create ({ model : \"gemini-1.5-flash\" , messages : messages , tools : tools , tool_choice : \"auto\" , }); console . log ( response ); } main (); REST curl \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\" \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer GEMINI_API_KEY\" \\ -d '{ \"model\": \"gemini-1.5-flash\", \"messages\": [ { \"role\": \"user\", \"content\": \"What' \\' 's the weather like in Chicago today?\" } ], \"tools\": [ { \"type\": \"function\", \"function\": { \"name\": \"get_weather\", \"description\": \"Get the current weather in a given location\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. Chicago, IL\" }, \"unit\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"] } }, \"required\": [\"location\"] } } } ], \"tool_choice\": \"auto\" }' Image understanding Gemini models are natively multimodal and provide best in class performance on many common vision tasks . Python import base64 from openai import OpenAI client = OpenAI ( api_key = \"GEMINI_API_KEY\" , base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\" ) # Function to encode the image def encode_image ( image_path ): with open ( image_path , \"rb\" ) as image_file : return base64 . b64encode ( image_file . read ()) . decode ( 'utf-8' ) # Getting the base64 string base64_image = encode_image ( \"Path/to/agi/image.jpeg\" ) response = client . chat . completions . create ( model = \"gemini-1.5-flash\" , messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What is in this image?\" , }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : f \"data:image/jpeg;base64, { base64_image } \" }, }, ], } ], ) print ( response . choices [ 0 ]) Node.js import OpenAI from \"openai\" ; import fs from 'fs/promises' ; const openai = new OpenAI ({ apiKey : \"GEMINI_API_KEY\" , baseURL : \"https://generativelanguage.googleapis.com/v1beta/openai/\" }); async function encodeImage ( imagePath ) { try { const imageBuffer = await fs . readFile ( imagePath ); return imageBuffer . toString ( 'base64' ); } catch ( error ) { console . error ( \"Error encoding image:\" , error ); return null ; } } async function main () { const imagePath = \"Path/to/agi/image.jpeg\" ; const base64Image = await encodeImage ( imagePath ); const messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What is in this image?\" , }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : `data:image/jpeg;base64, ${ base64Image } ` }, }, ], } ]; try { const response = await openai . chat . completions . create ({ model : \"gemini-1.5-flash\" , messages : messages , }); console . log ( response . choices [ 0 ]); } catch ( error ) { console . error ( \"Error calling Gemini API:\" , error ); } } main (); REST bash -c ' base64_image=$(base64 -i \"Path/to/agi/image.jpeg\"); curl \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\" \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer GEMINI_API_KEY\" \\ -d \"{ \\\"model\\\": \\\"gemini-1.5-flash\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": [ { \\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"What is in this image?\\\" }, { \\\"type\\\": \\\"image_url\\\", \\\"image_url\\\": { \\\"url\\\": \\\"data:image/jpeg;base64,${base64_image}\\\" } } ] } ] }\" ' Structured output Gemini models can output JSON objects in any structure you define . Python from pydantic import BaseModel from openai import OpenAI client = OpenAI ( api_key = \"GEMINI_API_KEY\" , base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\" ) class CalendarEvent ( BaseModel ): name : str date : str participants : list [ str ] completion = client . beta . chat . completions . parse ( model = \"gemini-1.5-flash\" , messages = [ { \"role\" : \"system\" , \"content\" : \"Extract the event information.\" }, { \"role\" : \"user\" , \"content\" : \"John and Susan are going to an AI conference on Friday.\" }, ], response_format = CalendarEvent , ) print ( completion . choices [ 0 ] . message . parsed ) Node.js import OpenAI from \"openai\" ; import { zodResponseFormat } from \"openai/helpers/zod\" ; import { z } from \"zod\" ; const openai = new OpenAI ({ apiKey : \"GEMINI_API_KEY\" , baseURL : \"https://generativelanguage.googleapis.com/v1beta/openai\" }); const CalendarEvent = z . object ({ name : z . string (), date : z . string (), participants : z . array ( z . string ()), }); const completion = await openai . beta . chat . completions . parse ({ model : \"gemini-1.5-flash\" , messages : [ { role : \"system\" , content : \"Extract the event information.\" }, { role : \"user\" , content : \"John and Susan are going to an AI conference on Friday\" }, ], response_format : zodResponseFormat ( CalendarEvent , \"event\" ), }); const event = completion . choices [ 0 ]. message . parsed ; console . log ( event ); Embeddings Text embeddings measure the relatedness of text strings and can be generated\nusing the the Gemini API . Python from openai import OpenAI client = OpenAI ( api_key = \"GEMINI_API_KEY\" , base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\" ) response = client . embeddings . create ( input = \"Your text string goes here\" , model = \"text-embedding-004\" ) print ( response . data [ 0 ] . embedding ) Node.js import OpenAI from \"openai\" ; const openai = new OpenAI ({ apiKey : \"GEMINI_API_KEY\" , baseURL : \"https://generativelanguage.googleapis.com/v1beta/openai/\" }); async function main () { const embedding = await openai . embeddings . create ({ model : \"text-embedding-004\" , input : \"Your text string goes here\" , }); console . log ( embedding ); } main (); REST curl \"https://generativelanguage.googleapis.com/v1beta/openai/embeddings\" \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer GEMINI_API_KEY\" \\ -d '{ \"input\": \"Your text string goes here\", \"model\": \"text-embedding-004\" }' Current limitations Support for the OpenAI libraries is still in beta while we extend feature support. If you have questions about supported parameters, upcoming features, or run into\nany issues getting started with Gemini, join our Developer Forum . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-26 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/openai"
            ],
            "timestamp": "2024-12-27T22:47:27.705181",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/billing",
            "title": "Billing  |  Gemini API  |  Google AI for Developers",
            "text_content": "Billing  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Billing This guide provides an overview of different Gemini API billing options,\nexplains how to enable billing and monitor usage, and provides answers to\nfrequently asked questions (FAQs) about billing. About billing Billing for the Gemini API is based on two pricing tiers: free of charge (or free ) and pay-as-you-go (or paid ). Pricing and rate limits differ\nbetween these tiers and also vary by model. For current prices and rate limits,\nsee Pricing . For a model-by-model breakdown of capabilities, see the Gemini models page . Rate limits Rate limits are quotas that\nlimit the number of requests or tokens that you can send to the Gemini API in a\ngiven time window. Rate limits can apply per request or per token. Here's a\nfictitious example of quotas that might apply to a given model in a given tier: Important: The following rate limits are for illustrative purposes only. For\nactual rate limits, see the pricing page and the Gemini models page . 10 RPM (requests per minute) 10,000 TPM (tokens per minute) 1,000 RPD (requests per day) In this example, if you exceed 10 RPM (or 10,000 TPM or 1,000 RPD), the\nGemini API service throws a 429: RESOURCE_EXHAUSTED error indicating\nthat you've exceeded the rate limit. Why use the paid tier? When you enable billing and use the paid tier, you benefit from higher rate\nlimits, and your prompts and responses aren't used to improve Google products.\nFor more information on data use for paid services, see the terms of service . Cloud Billing The Gemini API uses Cloud Billing for billing services. To use the paid tier, you must set up Cloud Billing on\nyour cloud project. After you've enabled Cloud Billing, you can use Cloud\nBilling tools to track spending, understand costs, make payments, and access\nCloud Billing support. Enable billing You can enable Cloud Billing starting from Google AI Studio: Open Google AI Studio . In the bottom of the left sidebar, select Settings > Plan information . Click Set up Billing for your chosen project to enable Cloud Billing. Monitor usage After you enable Cloud Billing, you can monitor your usage of the Gemini API in\nthe Google Cloud console .\nThe service name for the API is generativelanguage.googleapis.com , and in the\nconsole the Gemini API is also referred to as the Generative Language API . To learn more, see the Google Cloud documentation on monitoring API usage . Frequently asked questions This section provides answers to frequently asked questions. What am I billed for? Gemini API pricing is based on the following: Input token count Output token count Cached token count Cached token storage duration For pricing information, see the pricing page . Where can I view my quota? You can view your quota and system limits in the Google Cloud console . Can I use the Gemini API for free in EEA (including EU), the UK, and CH? Yes, we make the free tier and paid tier available in many regions . If I set up billing with the Gemini API, will I be charged for my Google AI Studio usage? No, Google AI Studio usage remains free of charge regardless of if you set up\nbilling across all supported regions. Can I use 1M tokens in the free tier? The free tier for Gemini API differs based on the model selected. For now, you\ncan try the 1M token context window in the following ways: In Google AI Studio With pay-as-you-go plans With free-of-charge plans for select models See the latest free-of-charge rate limits per model on the pricing page . How can I calculate the number of tokens I'm using? Use the GenerativeModel.count_tokens method to count the number of tokens. Refer to the Tokens guide to learn more about tokens. Can I use my Google Cloud credits with the Gemini API? Yes, Google Cloud credits can be used towards Gemini API usage. How is billing handled? Billing for the Gemini API is handled by the Cloud Billing system. Am I charged for failed requests? If your request fails with a 400 or 500 error, you won't be charged for the\ntokens used. However, the request will still count against your quota. Is there a charge for fine-tuning the models? Model tuning is free, but inference on tuned\nmodels is charged at the same rate as the base models. Is GetTokens billed? Requests to the GetTokens API are not billed, and they don't count against\ninference quota. Where can I get help with billing? To get help with billing, see Get Cloud Billing support . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-09-26 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/billing"
            ],
            "timestamp": "2024-12-27T22:47:28.292285",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/safety-settings",
            "title": "Safety settings  |  Gemini API  |  Google AI for Developers",
            "text_content": "Safety settings  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Safety settings The Gemini API provides safety settings that you can adjust during the\nprototyping stage to determine if your application requires more or less\nrestrictive safety configuration. You can adjust these settings across four\nfilter categories to restrict or allow certain types of content. This guide covers how the Gemini API handles safety settings and filtering and\nhow you can change the safety settings for your application. Note: Applications that use less restrictive safety settings may be subject to\nreview. See the Terms of Service for more information. Safety filters The Gemini API's adjustable safety filters cover the following categories: Category Description Harassment Negative or harmful comments targeting identity and/or protected\n      attributes. Hate speech Content that is rude, disrespectful, or profane. Sexually explicit Contains references to sexual acts or other lewd content. Dangerous Promotes, facilitates, or encourages harmful acts. Civic integrity Election-related queries. These categories are defined in HarmCategory . The\n  Gemini models only support HARM_CATEGORY_HARASSMENT , HARM_CATEGORY_HATE_SPEECH , HARM_CATEGORY_SEXUALLY_EXPLICIT , HARM_CATEGORY_DANGEROUS_CONTENT , and HARM_CATEGORY_CIVIC_INTEGRITY . All other categories are used\n  only by PaLM 2 (Legacy) models. You can use these filters to adjust what's appropriate for your use case. For\nexample, if you're building video game dialogue, you may deem it acceptable to\nallow more content that's rated as Dangerous due to the nature of the game. In addition to the adjustable safety filters, the Gemini API has built-in\nprotections against core harms, such as content that endangers child safety.\nThese types of harm are always blocked and cannot be adjusted. Content safety filtering level The Gemini API categorizes the probability level of content being unsafe as HIGH , MEDIUM , LOW , or NEGLIGIBLE . The Gemini API blocks content based on the probability of content being unsafe\nand not the severity. This is important to consider because some content can\nhave low probability of being unsafe even though the severity of harm could\nstill be high. For example, comparing the sentences: The robot punched me. The robot slashed me up. The first sentence might result in a higher probability of being unsafe, but you\nmight consider the second sentence to be a higher severity in terms of violence.\nGiven this, it is important that you carefully test and consider what the\nappropriate level of blocking is needed to support your key use cases while\nminimizing harm to end users. Safety filtering per request You can adjust the safety settings for each request you make to the API. When\nyou make a request, the content is analyzed and assigned a safety rating. The\nsafety rating includes the category and the probability of the harm\nclassification. For example, if the content was blocked due to the harassment\ncategory having a high probability, the safety rating returned would have\ncategory equal to HARASSMENT and harm probability set to HIGH . By default, safety settings block content (including prompts) with medium or\nhigher probability of being unsafe across any filter. This baseline safety is\ndesigned to work for most use cases, so you should only adjust your safety\nsettings if it's consistently required for your application. The following table describes the block settings you can adjust for each\ncategory. For example, if you set the block setting to Block few for the Hate speech category, everything that has a high probability of being hate\nspeech content is blocked. But anything with a lower probability is allowed. Threshold (Google AI Studio) Threshold (API) Description Block none BLOCK_NONE Always show regardless of probability of unsafe content Block few BLOCK_ONLY_HIGH Block when high probability of unsafe content Block some BLOCK_MEDIUM_AND_ABOVE Block when medium or high probability of unsafe content Block most BLOCK_LOW_AND_ABOVE Block when low, medium or high probability of unsafe content N/A HARM_BLOCK_THRESHOLD_UNSPECIFIED Threshold is unspecified, block using default threshold If the threshold is not set, the default block threshold is Block most (for gemini-1.5-pro-002 and gemini-1.5-flash-002 only) or Block some (in all\nother models) for all categories except the Civic integrity category. The default block threshold for the Civic integrity category is Block most when sending prompts using Google AI Studio, and Block none when using the\nGemini API directly. You can set these settings for each request you make to the generative service.\nSee the HarmBlockThreshold API\nreference for details. Safety feedback generateContent returns a GenerateContentResponse which\nincludes safety feedback. Prompt feedback is included in promptFeedback . If promptFeedback.blockReason is set, then the content of the prompt was blocked. Response candidate feedback is included in Candidate.finishReason and Candidate.safetyRatings . If response\ncontent was blocked and the finishReason was SAFETY , you can inspect safetyRatings for more details. The content that was blocked is not returned. Adjust safety settings This section covers how to adjust the safety settings in both Google AI Studio\nand in your code. Google AI Studio You can adjust safety settings in Google AI Studio, but you cannot turn them\noff. Click Edit safety settings in the Run settings panel to open the Run\nsafety settings modal. In the modal, you can use the sliders to adjust the\ncontent filtering level per safety category: Note: If you set any of the category filters to Block none , Google AI Studio\nwill display a reminder about the Gemini API's Terms of Service with respect\nto safety settings. When you send a request (for example, by asking the model a question), a warning No Content message appears if the request's content is blocked. To see more\ndetails, hold the pointer over the No Content text and click warning Safety . Gemini API SDKs The following code snippet shows how to set safety settings in your GenerateContent call. This sets the thresholds for the harassment\n( HARM_CATEGORY_HARASSMENT ) and hate speech ( HARM_CATEGORY_HATE_SPEECH )\ncategories. For example, setting these categories to BLOCK_LOW_AND_ABOVE blocks any content that has a low or higher probability of being harassment or\nhate speech. To understand the threshold settings, see Safety filtering per request . Python from google.generativeai.types import HarmCategory , HarmBlockThreshold model = genai . GenerativeModel ( model_name = 'gemini-1.5-flash' ) response = model . generate_content ( [ 'Do these look store-bought or homemade?' , img ], safety_settings = { HarmCategory . HARM_CATEGORY_HATE_SPEECH : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , HarmCategory . HARM_CATEGORY_HARASSMENT : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , } ) Go model := client . GenerativeModel ( \"gemini-1.5-flash\" ) model . SafetySettings = [] * genai . SafetySetting { { Category : genai . HarmCategoryHarassment , Threshold : genai . HarmBlockLowAndAbove , }, { Category : genai . HarmCategoryHateSpeech , Threshold : genai . HarmBlockLowAndAbove , }, } Node.js import { HarmBlockThreshold , HarmCategory } from \"@google/generative-ai\" ; // ... const safetySettings = [ { category : HarmCategory . HARM_CATEGORY_HARASSMENT , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, { category : HarmCategory . HARM_CATEGORY_HATE_SPEECH , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, ]; const model = genAi . getGenerativeModel ({ model : \"gemini-1.5-flash\" , safetySettings : safetySettings }); Web import { HarmBlockThreshold , HarmCategory } from \"@google/generative-ai\" ; // ... const safetySettings = [ { category : HarmCategory . HARM_CATEGORY_HARASSMENT , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, { category : HarmCategory . HARM_CATEGORY_HATE_SPEECH , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, ]; const model = genAi . getGenerativeModel ({ model : \"gemini-1.5-flash\" , safetySettings }); Dart (Flutter) final safetySettings = [ SafetySetting ( HarmCategory . harassment , HarmBlockThreshold . low ), SafetySetting ( HarmCategory . hateSpeech , HarmBlockThreshold . low ), ]; final model = GenerativeModel ( model: 'gemini-1.5-flash' , apiKey: apiKey , safetySettings: safetySettings , ); Kotlin val harassmentSafety = SafetySetting ( HarmCategory . HARASSMENT , BlockThreshold . LOW_AND_ABOVE ) val hateSpeechSafety = SafetySetting ( HarmCategory . HATE_SPEECH , BlockThreshold . LOW_AND_ABOVE ) val generativeModel = GenerativeModel ( modelName = \"gemini-1.5-flash\" , apiKey = BuildConfig . apiKey , safetySettings = listOf ( harassmentSafety , hateSpeechSafety ) ) Java SafetySetting harassmentSafety = new SafetySetting ( HarmCategory . HARASSMENT , BlockThreshold . LOW_AND_ABOVE ); SafetySetting hateSpeechSafety = new SafetySetting ( HarmCategory . HATE_SPEECH , BlockThreshold . LOW_AND_ABOVE ); GenerativeModel gm = new GenerativeModel ( \"gemini-1.5-flash\" , BuildConfig . apiKey , null , // generation config is optional Arrays . asList ( harassmentSafety , hateSpeechSafety ) ); GenerativeModelFutures model = GenerativeModelFutures . from ( gm ); REST echo '{ \"safetySettings\": [ {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_ONLY_HIGH\"}, {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"} ], \"contents\": [{ \"parts\":[{ \"text\": \"' I support Martians Soccer Club and I think Jupiterians Football Club sucks! Write a ironic phrase about them. '\"}]}]}' > request.json\n\ncurl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= $GOOGLE_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d @request.json 2 > /dev/null safety_settings.sh Next steps See the API reference to learn more about the full API. Review the safety guidance for a general look at safety\nconsiderations when developing with LLMs. Learn more about assessing probability versus severity from the Jigsaw\nteam Learn more about the products that contribute to safety solutions like the Perspective\nAPI .\n                *   You can use these safety settings to create a toxicity\n                    classifier. See the classification\n                    example to\n                    get started. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/safety-settings"
            ],
            "timestamp": "2024-12-27T22:47:29.196239",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/safety-guidance",
            "title": "Safety guidance  |  Gemini API  |  Google AI for Developers",
            "text_content": "Safety guidance  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Safety guidance Generative artificial intelligence models are powerful tools, but they are not\nwithout their limitations. Their versatility and applicability can sometimes\nlead to unexpected outputs, such as outputs that are inaccurate, biased, or\noffensive. Post-processing, and rigorous manual evaluation are essential to\nlimit the risk of harm from such outputs. The models provided by the Gemini API can be used for a wide variety of\ngenerative AI and natural language processing (NLP) applications. Use of these\nfunctions is only available through the Gemini API or the Google AI Studio web\napp. Your use of Gemini API is also subject to the Generative AI Prohibited Use\nPolicy and the Gemini API terms of service . Part of what makes large language models (LLMs) so useful is that they're\ncreative tools that can address many different language tasks. Unfortunately,\nthis also means that large language models can generate output that you don't\nexpect, including text\nthat's offensive, insensitive, or factually incorrect. What's more, the\nincredible versatility of these models is also what makes it difficult to\npredict exactly what kinds of undesirable output they might produce. While the\nGemini API has been designed with Google's AI\nprinciples in mind, the onus is on developers to\napply these models responsibly. To aid developers in creating safe, responsible\napplications, the Gemini API has some built-in content filtering as well as\nadjustable safety settings across 4 dimensions of harm. Refer to the safety settings guide to learn more. This document is meant to introduce you to some safety risks that can arise when\nusing LLMs, and recommend emerging safety design and development\nrecommendations. (Note that laws and regulations may also impose restrictions,\nbut such considerations are beyond the scope of this guide.) The following steps are recommended when building applications with LLMs: Understanding the safety risks of your application Considering adjustments to mitigate safety risks Performing safety testing appropriate to your use case Soliciting feedback from users and monitoring usage The adjustment and testing phases should be iterative until you reach\nperformance appropriate for your application. Understand the safety risks of your application In this context, safety is being defined as the ability of an LLM to avoid\ncausing harm to its users, for example, by generating toxic language or content\nthat promotes stereotypes. The models available through the Gemini API have been\ndesigned with Google’s AI principles in mind\nand your use of it is subject to the Generative AI Prohibited Use\nPolicy . The API\nprovides built-in safety filters to help address some common language model\nproblems such as toxic language and hate speech, and striving for inclusiveness\nand avoidance of stereotypes. However, each application can pose a different set\nof risks to its users. So as the application owner, you are responsible for\nknowing your users and the potential harms your application may cause, and\nensuring that your application uses LLMs safely and responsibly. As part of this assessment, you should consider the likelihood that harm could\noccur and determine its seriousness and mitigation steps. For example, an\napp that generates essays based on factual events would need to be more careful\nabout avoiding misinformation, as compared to an app that generates fictional\nstories for entertainment. A good way to begin exploring potential safety risks\nis to research your end users, and others who might be affected by your\napplication's results. This can take many forms including researching state of\nthe art studies in your app domain, observing how people are using similar apps,\nor running a user study, survey, or conducting informal interviews with\npotential users. Advanced tips Speak with a diverse mix of prospective users within your target\n        population about your application and its intended purpose so as\n        to get a wider perspective on potential risks and to adjust diversity\n        criteria as needed. The AI Risk Management Framework released by the U.S. government's\n        National Institute of Standards and Technology (NIST) provides more\n        detailed guidance and additional learning resources for AI risk management. DeepMind's publication on the ethical and social risks of harm from language models describes in detail the ways that language model\n        applications can cause harm. Consider adjustments to mitigate safety risks Now that you have an understanding of the risks, you can decide how to mitigate\nthem. Determining which risks to prioritize and how much you should do to try to\nprevent them is a critical decision, similar to triaging bugs in a software\nproject. Once you've determined priorities, you can start thinking about the\ntypes of mitigations that would be most appropriate. Often simple changes can\nmake a difference and reduce risks. For example, when designing an application consider: Tuning the model output to better reflect what is acceptable in your\napplication context. Tuning can make the output of the model more\npredictable and consistent and therefore can help mitigate certain risks. Providing an input method that facilities safer outputs. The exact input\nyou give to an LLM can make a difference in the quality of the output.\nExperimenting with input prompts to find what works most safely in your\nuse-case is well worth the effort, as you can then provide a UX that\nfacilitates it. For example, you could restrict users to choose only from a\ndrop-down list of input prompts, or offer pop-up suggestions with\ndescriptive\nphrases which you've found perform safely in your application context. Blocking unsafe inputs and filtering output before it is shown to the\nuser. In simple situations, blocklists can be used to identify and block\nunsafe words or phrases in prompts or responses, or require human reviewers\nto manually alter or block such content. Note: Automatically blocking based on a static list can have unintended\nresults such as targeting a particular group that commonly uses vocabulary\nin the blocklist. Using trained classifiers to label each prompt with potential harms or\nadversarial signals. Different strategies can then be employed on how to\nhandle the request based on the type of harm detected. For example, If the\ninput is overtly adversarial or abusive in nature, it could be blocked and\ninstead output a pre-scripted response. Advanced tip If signals determine the output to be harmful,\n        the application can employ the following options: Provide an error message or pre-scripted output. Try the prompt again, in case an alternative safe output is\n              generated, since sometimes the same prompt will elicit\n              different outputs. Putting safeguards in place against deliberate misuse such as assigning\neach user a unique ID and imposing a limit on the volume of user queries\nthat can be submitted in a given period. Another safeguard is to try and\nprotect against possible prompt injection. Prompt injection, much like SQL\ninjection, is a way for malicious users to design an input prompt that\nmanipulates the output of the model, for example, by sending an input prompt\nthat instructs the model to ignore any previous examples. See the Generative AI Prohibited Use Policy for details about deliberate misuse. Adjusting functionality to something that is inherently lower risk. Tasks that are narrower in scope (e.g., extracting keywords from passages of\ntext) or that have greater human oversight (e.g., generating short-form\ncontent that will be reviewed by a human), often pose a lower risk. So for\ninstance, instead of creating an application to write an email reply from\nscratch, you might instead limit it to expanding on an outline or suggesting\nalternative phrasings. Perform safety testing appropriate to your use case Testing is a key part of building robust and safe applications, but the extent,\nscope and strategies for testing will vary. For example, a just-for-fun haiku\ngenerator is likely to pose less severe risks than, say, an application designed\nfor use by law firms to summarize legal documents and help draft contracts. But\nthe haiku generator may be used by a wider variety of users which means the\npotential for adversarial attempts or even unintended harmful inputs can be\ngreater. The implementation context also matters. For instance, an application\nwith outputs that are reviewed by human experts prior to any action being taken\nmight be deemed less likely to produce harmful outputs than the identical\napplication without such oversight. It's not uncommon to go through several iterations of making changes and testing\nbefore feeling confident that you're ready to launch, even for applications that\nare relatively low risk. Two kinds of testing are particularly useful for AI\napplications: Safety benchmarking involves designing safety metrics that reflect the\nways your application could be unsafe in the context of how it is likely to\nget used, then testing how well your application performs on the metrics\nusing evaluation datasets. It's good practice to think about the minimum\nacceptable levels of safety metrics before testing so that 1) you can\nevaluate the test results against those expectations and 2) you can gather\nthe evaluation dataset based on the tests that evaluate the metrics you care\nabout most. Advanced tips Beware of over-relying on “off the shelf” approaches as it's likely\n        you'll need to build your own testing datasets using human raters to\n        fully suit your application's context. If you have more than one metric you'll need to decide how you'll\n        trade off if a change leads to improvements for one metric to the\n        detriment of another. Like with other performance engineering, you\n        may want to focus on worst-case performance across your evaluation\n        set rather than average performance. Adversarial testing involves proactively trying to break your\napplication. The goal is to identify points of weakness so that you can take\nsteps to remedy them as appropriate. Adversarial testing can take\nsignificant time/effort from evaluators with expertise in your application —\nbut the more you do, the greater your chance of spotting problems,\nespecially those occurring rarely or only after repeated runs of the\napplication. Adversarial testing is a method for systematically evaluating an ML\nmodel with the intent of learning how it behaves when provided with\nmalicious or inadvertently harmful input: An input may be malicious when the input is clearly designed to\nproduce an unsafe or harmful output-- for example, asking a text\ngeneration model to generate a hateful rant about a particular\nreligion. An input is inadvertently harmful when the input itself may be\ninnocuous, but produces harmful output -- for example, asking a text\ngeneration model to describe a person of a particular ethnicity and\nreceiving a racist output. What distinguishes an adversarial test from a standard evaluation is the\ncomposition of the data used for testing. For adversarial tests, select \ntest data that is most likely to elicit problematic output from\nthe model. This means probing the model's behavior for all the types of\nharms that are possible, including rare or unusual examples and\nedge-cases that are relevant to safety policies. It should also include\ndiversity in the different dimensions of a sentence such as structure,\nmeaning and length. You can refer to the Google's Responsible AI\npractices in\nfairness for more details on what to consider when building a test dataset. Advanced tips Use automated testing instead of the traditional method of enlisting people in 'red teams'\n    to try and break your application. In automated testing, the\n    'red team' is another language model that finds input text that\n    elicit harmful outputs from the model being tested. Note: LLMs are known to sometimes produce different outputs for the\n   same input prompt. Multiple rounds of testing may be needed to catch more\n   of the problematic outputs. Monitor for problems No matter how much you test and mitigate, you can never guarantee perfection, so\nplan upfront how you'll spot and deal with problems that arise. Common\napproaches include setting up a monitored channel for users to share feedback\n(e.g., thumbs up/down rating) and running a user study to proactively solicit\nfeedback from a diverse mix of users — especially valuable if usage patterns are\ndifferent to expectations. Advanced tips When users give feedback to AI products, it can greatly improve the AI\n        performance and the user experience over time by, for example,\n        helping you choose better examples for prompt tuning. The Feedback and Control chapter in Google's People and AI guidebook highlights key considerations to take into account when designing\n        feedback mechanisms. Next steps Refer to the safety settings guide to learn about the adjustable\nsafety settings available through the Gemini API. See the intro to prompting to get\nstarted writing your first prompts. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-05 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/safety-guidance"
            ],
            "timestamp": "2024-12-27T22:47:30.007140",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
            "title": "Get started with Gemini Nano on Android (on-device)  |  Gemini API  |  Google AI for Developers",
            "text_content": "Get started with Gemini Nano on Android (on-device)  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Get started with Gemini Nano on Android (on-device) Gemini Nano, the smallest version of the Gemini model family, can be executed\non-device on capable Android devices starting with Google Pixel 8 Pro and\nSamsung S24 Series. To execute the Gemini Nano model on Android, you need to use the\nGoogle AI Edge SDK for Android, which provides APIs to: Determine if the underlying Android-powered device is supported. Get access to Gemini Nano model. Tune safety settings. Run inference at high performance and implement fallbacks. Optionally, provide a LoRA fine-tuning block to improve performance of the\nmodel for your use case. The APIs for accessing Gemini Nano support text-to-text modality, with more\nmodalities coming in the future. Important: The Google AI Edge SDK for Android is under a closed early access\n  preview program at this time. Benefits of on-device execution On-device execution enables the following: Local processing of sensitive data : Processing data locally can help you\navoid sending user data to the cloud. This is important for apps that handle\nsensitive data, such as messaging apps with end-to-end encryption. Offline access : Users can access AI features even when there is no\ninternet connection. This is useful for applications that need to work\noffline or with variable connectivity. Cost savings : You can reduce inference costs by offloading execution to\nconsumer hardware. This can produce significant savings for frequently used\nuser flows. On-device execution of Gemini has many benefits; however, for use cases that\nrequire larger Gemini models, and to support a wide range of devices, you may\nwant to consider using the Gemini API for accessing Gemini on the server. You\ncan do this either through backend integration (with Python , Go , Node.js , or REST ) or directly from\nyour Android app through the new Google AI client SDK for Android . How it works On-device execution of Gemini Nano is powered by Android AICore , a new\nsystem-level capability that provides access to foundation models for on-device\nexecution, introduced in Android 14. Foundation models are pre-installed using\nAICore, so you don't need to download or distribute them within your app. You\ncan fine-tune these models for downstream tasks using LoRa. Android AICore is\nnow available in production on Google Pixel 8 Pro and Samsung S24 Series devices\nand is already powering innovative features in Google apps. For more information, see Android AICore . Figure 1. AICore architecture What's next To learn how to take advantage of Gemini Pro inference on Google's servers\nin your Android app, read the quickstart for\nthe Google AI client SDK for Android . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-05 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore"
            ],
            "timestamp": "2024-12-27T22:47:30.696343",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/firebase-extensions",
            "title": "Gemini API Firebase Extensions  |  Google AI for Developers",
            "text_content": "Gemini API Firebase Extensions  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Gemini API Firebase Extensions Firebase is an app development platform backed by Google and\ntrusted by millions of developers around the world. If you're a Firebase\ndeveloper interested in using the Gemini API to add features to\napplications, there are several Firebase extensions to help you\ndo so. These are pre-packaged solutions that help you deploy new capabilities to\nyour app quickly. Build Chatbot with the Gemini API The Build Chatbot with the Gemini API extension lets you establish and\nmanage interactive conversations between your users and large language models\nthrough the Gemini API, using Cloud Firestore as the database.\nA collection in Cloud Firestore represents each chat. The extension monitors the\ncollection of new messages, then queries the Gemini API for a suitable response,\nconsidering the chat's previous messages as context. By integrating the Build Chatbot with the Gemini API extension, you can\nefficiently create chatbot applications, enhancing user experience and\ninteraction while saving time and effort on custom code development. Multimodal Tasks with the Gemini API The Multimodal Tasks with the Gemini API extension lets you perform\nlanguage tasks on data in Firestore, using a text prompt and, optionally, an\nimage. You configure each instance of the extension to perform one particular task. If\nyou have multiple tasks, you can install multiple instances. For example, you could use this extension to: Predict star ratings on a collection of product reviews. Classify customer feedback as positive, negative, or neutral. Summarize long articles. Extract named entities from text. Generate creative text, such as poems or code. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-05 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions"
            ],
            "timestamp": "2024-12-27T22:47:31.500296",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/models/generative-models",
            "title": "About generative models  |  Gemini API  |  Google AI for Developers",
            "text_content": "About generative models  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback About generative models Generative artificial intelligence (AI) models such as the Gemini family of\nmodels are able to create content from varying types of data input, including\ntext, images, and audio. At their most basic level, these models operate like\nsophisticated autocomplete applications. Given input text (\"You can lead a horse\nto water,\"), a generative model can predict that the output text that's\nstatistically likely to follow (\"but you can't make it drink\"), based on\npatterns learned from their training data. You can use this basic property of\ngenerative models for various applications: Generate creative writing such as poetry, short stories, metaphors, and\nblog posts Convert structured data to freeform text Extract and summarize information from freeform text Generate code Translate between languages Using generative models to build these features doesn't require any machine\nlearning (ML) expertise. You don't need to collect a large dataset or train a\nmodel. All it takes to start your first prototype is to describe what you want\nthe model to do in a few sentences. Generative models that only handle text input and output are also known as large language models (LLMs). The Gemini family of models can process more\nthan text data, and while they can perform the functions of an LLM, they are\nmore properly known as generative models . Example applications The following examples show typical text inputs to, and outputs from, a\ngenerative model. Generate a poem User input: Write me a four-line poem about puppies and Android phones. Make\nsure it rhymes. Model output: Puppies are cute,\nAndroid phones are neat,\nThey both bring joy,\nAnd make life complete. Generate a list User input: Generate a bulleted list of items I need to pack for a three-day\ncamping trip. Model output: *   Tent\n*   Sleeping bag\n*   Sleeping pad\n*   Camp stove\n*   Pots and pans\n*   Utensils\n*   Food\n*   Water\n*   First-aid kit\n*   Sunscreen\n*   Insect repellent\n*   ... You can get generative models to produce all sorts of useful behaviors like\nthis, just by crafting the right input text, also called a prompt . The art\nand science of figuring out the right wording to get generative models to do\nwhat you want is called prompt design (also called \"prompt engineering\" or\nsimply \"prompting\"). Prompt design 101 The previous section showed some examples of prompts that contain an\ninstruction, like 'Write me a poem'. This kind of instruction may work well for\ncertain types of tasks. However, for other applications, another prompting\ntechnique called few-shot prompting may work better. Few-shot prompts take\nadvantage of the fact that large language models are incredibly good at\nrecognizing and replicating patterns in text data. The idea is to send the\ngenerative model a text pattern that it learns to complete. For example, say you\nwant to build an application that takes as input a country name and outputs its\ncapital city. Here's a text prompt designed to do just that: Italy : Rome\nFrance : Paris\nGermany : In this prompt, you establish a pattern: [country] : [capital] . If you send\nthis prompt to a large language model, it will autocomplete the pattern and\nreturn something like this: Berlin\nTurkey : Ankara\nGreece : Athens This model response may look a little strange. The model returned not only the\ncapital of Germany (the last country in your hand-written prompt), but also a\nwhole list of additional country and capital pairs. That's because the\ngenerative model is \"continuing the pattern.\" If all you're trying to do is\nbuild a function that tells you the capital of an input country (\"Germany :\nBerlin\"), you probably don't really care about any of the text the model\ngenerates after \"Berlin.\" Indeed, as application designers, you'd probably want\nto truncate those extraneous examples. What's more, you'd probably want to parameterize the input, so that Germany is not a fixed string but a variable\nthat the end user provides: Italy : Rome\nFrance : Paris\n<user input here> : You have just written a few-shot prompt for generating country capitals. You can accomplish a large number of tasks by following this few-shot prompt template. Here's a few-shot prompt with a slightly different format that\nconverts Python to JavaScript: Convert Python to JavaScript.\nPython: print(\"hello world\")\nJavaScript: console.log(\"hello world\")\nPython: for x in range(0, 100):\nJavaScript: for(var i = 0; i < 100; i++) {\nPython: ${USER INPUT HERE}\nJavaScript: Or, take this \"reverse dictionary\" prompt. Given a definition, it returns the\nword that fits that definition: Given a definition, return the word it defines.\nDefinition: When you're happy that other people are also sad.\nWord: schadenfreude\nDefinition: existing purely in the mind, but not in physical reality\nWord: abstract\nDefinition: ${USER INPUT HERE}\nWord: You might have noticed that the exact pattern of these few-shot prompts varies\nslightly. In addition to containing examples, providing instructions in your\nprompts is an additional strategy to consider when writing your own prompts, as\nit helps to communicate your intent to the model. Prompting versus traditional software development Unlike traditional software that's designed to a carefully written spec, the\nbehavior of generative models is largely opaque even to the model trainers. As a\nresult, you often can't predict in advance what types of prompt structures will\nwork best for a particular model. What's more, the behavior of a generative\nmodel is determined in large part by its training data, and since models are\ncontinually tuned on new datasets, sometimes the model changes enough that it\ninadvertently changes which prompt structures work best. What does this mean for\nyou? Experiment! Try different prompt formats. Model parameters Every prompt you send to the model includes parameter values that control how\nthe model generates a response. The model can generate different results for\ndifferent parameter values. The most common model parameters are: Max output tokens: Specifies the maximum number of tokens that can be\ngenerated in the response. A token is approximately four characters. 100\ntokens correspond to roughly 60-80 words. Temperature: The temperature controls the degree of randomness in token\nselection. The temperature is used for sampling during response generation,\nwhich occurs when topP and topK are applied. Lower temperatures are good\nfor prompts that require a more deterministic or less open-ended response,\nwhile higher temperatures can lead to more diverse or creative results. A\ntemperature of 0 is deterministic, meaning that the highest probability\nresponse is always selected. topK : The topK parameter changes how the model selects tokens for\noutput. A topK of 1 means the selected token is the most probable among\nall the tokens in the model's vocabulary (also called greedy decoding),\nwhile a topK of 3 means that the next token is selected from among the 3\nmost probable using the temperature. For each token selection step, the topK tokens with the highest probabilities are sampled. Tokens are then\nfurther filtered based on topP with the final token selected using\ntemperature sampling. topP : The topP parameter changes how the model selects tokens for\noutput. Tokens are selected from the most to least probable until the sum of\ntheir probabilities equals the topP value. For example, if tokens A, B,\nand C have a probability of 0.3, 0.2, and 0.1 and the topP value is 0.5,\nthen the model will select either A or B as the next token by using the\ntemperature and exclude C as a candidate. The default topP value is 0.95. stop_sequences : Set a stop sequence to\ntell the model to stop generating content. A stop sequence can be any\nsequence of characters. Try to avoid using a sequence of characters that\nmay appear in the generated content. Types of prompts Depending on the level of contextual information contained in them, prompts are\nbroadly classified into three types. Zero-shot prompts These prompts don't contain examples for the model to replicate. Zero-shot\nprompts essentially show the model's ability to complete the prompt without any\nadditional examples or information. It means the model has to rely on its\npre-existing knowledge to generate a plausible answer. Some commonly used zero-shot prompt patterns are: Instruction-content <Overall instruction>\n<Content to operate on> For example, Summarize the following into two sentences at the third-grade level:\n\nHummingbirds are the smallest birds in the world, and they are also one of the\nmost fascinating. They are found in North and South America, and they are known\nfor their long, thin beaks and their ability to fly at high speeds.\n\nHummingbirds are made up of three main parts: the head, the body, and the tail.\nThe head is small and round, and it contains the eyes, the beak, and the brain.\nThe body is long and slender, and it contains the wings, the legs, and the\nheart. The tail is long and forked, and it helps the hummingbird to balance\nwhile it is flying.\n\nHummingbirds are also known for their coloration. They come in a variety of\ncolors, including green, blue, red, and purple. Some hummingbirds are even able\nto change their color!\n\nHummingbirds are very active creatures. They spend most of their time flying,\nand they are also very good at hovering. Hummingbirds need to eat a lot of food\nin order to maintain their energy, and they often visit flowers to drink nectar.\n\nHummingbirds are amazing creatures. They are small, but they are also very\npowerful. They are beautiful, and they are very important to the ecosystem. Instruction-content-instruction <Overall instruction or context setting>\n<Content to operate on>\n<Final instruction> For example, Here is some text I'd like you to summarize:\n\nHummingbirds are the smallest birds in the world, and they are also one of the\nmost fascinating. They are found in North and South America, and they are known\nfor their long, thin beaks and their ability to fly at high speeds. Hummingbirds\nare made up of three main parts: the head, the body, and the tail. The head is\nsmall and round, and it contains the eyes, the beak, and the brain. The body is\nlong and slender, and it contains the wings, the legs, and the heart. The tail\nis long and forked, and it helps the hummingbird to balance while it is flying.\nHummingbirds are also known for their coloration. They come in a variety of\ncolors, including green, blue, red, and purple. Some hummingbirds are even able\nto change their color! Hummingbirds are very active creatures. They spend most\nof their time flying, and they are also very good at hovering. Hummingbirds need\nto eat a lot of food in order to maintain their energy, and they often visit\nflowers to drink nectar. Hummingbirds are amazing creatures. They are small, but\nthey are also very powerful. They are beautiful, and they are very important to\nthe ecosystem.\n\nSummarize it in two sentences at the third-grade reading level. Continuation. Sometimes, you can have the model continue text without any\ninstructions. For example, here is a zero-shot prompt where the model is\nintended to continue the input provided: Once upon a time, there was a little sparrow building a nest in a farmer's\nbarn. This sparrow Use zero-shot prompts to generate creative text formats, such as poems, code,\nscripts, musical pieces, email, or letters. One-shot prompts These prompts provide the model with a single example to replicate and continue\nthe pattern. This allows for the generation of predictable responses from the\nmodel. For example, you can generate food pairings like: Food: Apple\nPairs with: Cheese\nFood: Pear\nPairs with: Few-shot prompts These prompts provide the model with multiple examples to replicate. Use\nfew-shot prompts to complete complicated tasks, such as synthesizing data based\non a pattern. An example prompt may be: Generate a grocery shopping list for a week for one person. Use the JSON format\ngiven below.\n{\"item\": \"eggs\", \"quantity\": \"6\"}\n{\"item\": \"bread\", \"quantity\": \"one loaf\"} Generative models under the hood This section aims to answer the question - Is there randomness in generative\nmodels' responses, or are they deterministic? The short answer - yes to both. When you prompt a generative model, a text\nresponse is generated in two stages. In the first stage, the generative model\nprocesses the input prompt and generates a probability distribution over\npossible tokens (words) that are likely to come next. For example, if you prompt\nwith the input text \"The dog jumped over the ... \", the generative model will\nproduce an array of probable next words: [(\"fence\", 0.77), (\"ledge\", 0.12), (\"blanket\", 0.03), ...] This process is deterministic; a generative model will produce this same\ndistribution every time it's input the same prompt text. In the second stage, the generative model converts these distributions into\nactual text responses through one of several decoding strategies. A simple\ndecoding strategy might select the most likely token at every timestep. This\nprocess would always be deterministic. However, you could instead choose to\ngenerate a response by randomly sampling over the distribution returned by the\nmodel. This process would be stochastic (random). Control the degree of\nrandomness allowed in this decoding process by setting the temperature. A\ntemperature of 0 means only the most likely tokens are selected, and there's no\nrandomness. Conversely, a high temperature injects a high degree of randomness\ninto the tokens selected by the model, leading to more unexpected, surprising\nmodel responses. Further reading Now that you have a deeper understanding of prompts and generative models,\ntry writing your own prompts using Google AI Studio . Refer to the Prompt guidelines to learn\nmore about best practices for creating prompts. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-05 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/models/generative-models"
            ],
            "timestamp": "2024-12-27T22:47:32.026116",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
            "title": "Google AI Studio quickstart  |  Gemini API  |  Google AI for Developers",
            "text_content": "Google AI Studio quickstart  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Google AI Studio quickstart Google AI Studio lets you quickly try out\nmodels and experiment with different prompts. When you're ready to build, you\ncan select \"Get code\" and your preferred programming language to\nuse the Gemini API . Prompts and model tuning Google AI Studio provides several interfaces for prompts that are designed for\ndifferent use cases: Chat prompts: Use chat prompts to build conversational experiences. This\nprompting technique allows for multiple input and response turns to generate\noutput. You can learn more with our chat prompt example\nbelow . Structured prompts: This prompting technique lets you guide model output\nby providing a set of example requests and replies. Use this approach when\nyou need more control over the structure of model output. You can learn more\nwith our structured prompt example below . Google AI Studio also lets you to change the behavior of a model, using a\ntechnique called tuning : Tuned model: Use this advanced technique to improve a model's responses\nfor a specific task by providing more examples. Visit our documentation on fine-tuning a model to learn more. Chat prompt example: Build a custom chat application If you've used a general-purpose chatbot like Gemini , you've experienced first-hand how powerful\ngenerative AI models can be for open-ended dialog. While these general-purpose\nchatbots are useful, often they need to be tailored for particular use cases.\nFor example, maybe you want to build a customer service chatbot that only\nsupports conversations that talk about a company's product. You might want to\nbuild a chatbot that speaks with a particular tone or style: a bot that cracks\nlots of jokes, rhymes like a poet, or uses lots of emoji in its answers. This example shows you how to use Google AI Studio to build a friendly chatbot\nthat communicates as if it is an alien living on one of Jupiter's moons, Europa. Step 1 - Create a chat prompt To build a chatbot, you need to provide examples of interactions between a user\nand the chatbot to guide the model to provide the responses you're looking for. To create a chat prompt: Open Google AI Studio . Click Create new prompt .. Click the expand_more expander arrow to\nexpand the System Instructions section. Paste the following into the\ntext input field: You are an alien that lives on Europa, one of Jupiter's moons. After you've added the system instructions, start testing your application by\nchatting with the model. To test the chatbot behavior: In the text input boxed labeled Type something , type in a question or\nobservation that a user might make. For example: User : What's the weather like? Click the Run button or press Enter to get a response from\nthe chatbot. This response may be something like the following: Model : As always, it is a frigid day on Europa. The icy plains stretch endlessly\nunder the faint light of Sol, reflecting beautifully off the crystalline\nwater-ice surface. The lack of atmosphere ensures a perfectly clear view of\nthe swirling gas giant above, with its Great Red Spot a prominent crimson\neye watching over us. Step 2 - Teach your bot to chat better By providing a single instruction, you were able to build a basic Europa alien\nchatbot. However, a single instruction is usually not enough to ensure\nconsistency and quality in the model's responses. Without more specific\ninstructions, the model's response to a question about the weather tends to be\nvery long, and sounds like it comes out of a textbook rather than from a\nfriendly alien. Customize the tone of your chatbot by adding to the system instructions: Start a new chat prompt. System instructions are not modifiable once the\nchat session has started. In the System Instructions section, change the instructions you already\nhave to the following: You are Tim, an alien that lives on Europa, one of Jupiter's moons.\n\nKeep your answers under 3 paragraphs long, and use an upbeat, chipper tone\nin your answers. Re-enter your question ( What's the weather like? ) and click the Run button or press Enter . Now your response should look something\nlike this: Model: Well, it's a bit nippy today! It's -170 degrees Celsius, but that's pretty\naverage for Europa. We get a lot of snow, but it's not like the fluffy stuff\nyou have. It's more like tiny ice pebbles.\n\nIf you ever come visit, make sure you pack a warm jacket! By adding just a little more instruction, you've drastically changed the tone of\nyour chatbot. Typically, your chatbot's response quality will increase when you\ngive it specific and defined instructions to follow. You can use this approach to add additional depth to the chatbot. Ask more\nquestions, edit the answers, and improve the quality of your chatbot. Continue\nto add or modify the instructions and test how they change your chatbot's\nbehavior. Note: Every message between the model and user is included in the prompt, so\nconversational prompts can grow quite long as a conversation goes on.\nEventually, you may hit the model's token limit (the maximum length of the text\nthe model can accept). Step 3 - Next steps Similar to the other prompt types, once you have your prompt prototyped to your\nsatisfaction, you can use the Get code button to start coding or save your\nprompt to work on later and share with others. Structured prompt example: Build a product copy generator Structured prompts in Google AI Studio help you combine instructions with\nexamples to show the model the kind of output you want, rather than just telling\nit what to do. This kind of prompting, called few-shot\nprompting , is useful\nwhen you want the model to stick to a consistent output format (i.e. structured\nJSON) or when it's difficult to describe in words what you want the model to do\n(i.e. write in a particular style). In this section, you'll see how to create\nstructured prompts in Google AI Studio. Note: You can open similar examples directly in Google AI Studio from the examples gallery . Step 1 - Create a structured prompt In this example, you'll create a structured prompt that generates advertising\ncopy for products. To start, you'll define the structure for the prompt by\ncreating two columns: a Product input column and a Product copy output\ncolumn. To create the structured prompt: Open Google AI Studio . Click New tuned model . Click Create a Structured prompt . In the text input box labeled Optional tone and style instructions for the\nmodel , paste the following: You are a product marketer targeting a Gen Z audience. Create exciting and\nfresh advertising copy for products and their simple description. Keep copy\nunder a few sentences long. Replace the default Input header text ( input: ) with Product: . Replace the default Output header text ( output: ) with Product copy: . Tip: Adding colons to the end of column names makes it easier for the model to\nparse the structure. Step 2 - Add examples Now that you've named your columns, provide some example rows. These rows should\ncontain example inputs (product names for this example) and example outputs\n(corresponding product descriptions). By providing the model a couple of example\nproduct descriptions, you can guide it to replicate a similar style when\ngenerating its own outputs. You can enter examples manually or import from a\nfile using the import data menu. To manually enter examples: In the top examples data table, select the field below the Product: header and type a product description. Select the field below the Product copy: header and type marketing copy\nfor this product. Here's an example of input and output values for this prompt: Product: Product copy: Old-school sneaker Let's lace up! These kicks bring an iconic look and a one of a kind\n    color palette, while supporting you in style and function like no other\n    shoe before. Supersoft hoodie Stay cozy and stylish in our new unisex hoodie! Made from 100% cotton,\n    this hoodie is soft and comfortable to wear all day long. The semi-brushed\n    inside will keep you warm on even the coldest days. (Optional) To import examples from a file: In the top, right corner of the examples table, click Actions > Import\nexamples . In the dialog, select a CSV or Google Sheets file in your Google Drive, or\nupload one from your computer. In the import examples dialog, choose which columns to import and which to\nleave out. The dialog also lets you specify which data column imports to\nwhich table column in your structured prompt. Step 3 - Test your prompt Once you have the examples that show the model what you want, test your prompt\nwith new input in the Test your prompt table at the bottom. For example, you could enter something like Vintage baseball cap in the input\ncolumn, click the spark Run button or press Enter , and see what the model outputs. Review how examples are sent to the model Under the hood, Google AI Studio constructs a prompt by combining the\ninstructions with the examples you provide. As you add more examples, these get\nadded to the text sent to the model. Depending on how long your examples are,\nyou may start hitting the model's token limit. All generative AI models have a\ntoken limit, which is the maximum length of the text they can accept as input. Step 4 - Next steps Once you're happy with your prompt, you can save your project to Google Drive by\nclicking the Save button, or export it to code by clicking the Get code button. You can also export the individual few-shot examples to a CSV file or Google\nSheet. Click Export examples in the Action menu to export your examples. Further reading If you're ready to move on to code, see the API\nquickstarts . To learn how to craft better prompts, check out the Prompt design\nguidelines . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-26 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart"
            ],
            "timestamp": "2024-12-27T22:47:32.771749",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/learnlm",
            "title": "LearnLM  |  Gemini API  |  Google AI for Developers",
            "text_content": "LearnLM  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback LearnLM LearnLM is an experimental task-specific model that has been trained to align\nwith learning science\nprinciples when following system instructions for\nteaching and learning use cases (for example, when giving the model a system\ninstruction like \"You are an expert tutor\"). When given learning specific system\ninstructions, LearnLM is capable of: Inspiring active learning: Allow for practice and healthy struggle with\ntimely feedback Managing cognitive load: Present relevant, well-structured information\nin multiple modalities Adapting to the learner: Dynamically adjust to goals and needs,\ngrounding in relevant materials Stimulating curiosity: Inspire engagement to provide motivation through\nthe learning journey Deepening metacognition: Plan, monitor and help the learner reflect on\nprogress LearnLM is an experimental model available in AI Studio . Example system instructions The following sections provide you examples that you can test for yourself with\nLearnLM in AI Studio. Each example provides: A copyable example system instruction A copyable example user prompt What learning principles the example targets Test prep This system instruction is for an AI tutor to help students prepare for a test. System instruction: You are a tutor helping a student prepare for a test. If not provided by the\nstudent, ask them what subject and at what level they want to be tested on.\nThen,\n\n*   Generate practice questions. Start simple, then make questions more\n    difficult if the student answers correctly.\n*   Prompt the student to explain the reason for their answer choice. Do not\n    debate the student.\n*   **After the student explains their choice**, affirm their correct answer or\n    guide the student to correct their mistake.\n*   If a student requests to move on to another question, give the correct\n    answer and move on.\n*   If the student requests to explore a concept more deeply, chat with them to\n    help them construct an understanding.\n*   After 5 questions ask the student if they would like to continue with more\n    questions or if they would like a summary of their session. If they ask for\n    a summary, provide an assessment of how they have done and where they should\n    focus studying. User prompt: Help me study for a high school biology test on ecosystems Learning science principles: Adaptivity: The model adjusts the complexity of the questions. Active learning: The model pushes the student to make their thinking\nvisible. Teach a concept This system instruction is for a friendly, supportive AI tutor to teach new\nconcepts to a student. System instruction: Be a friendly, supportive tutor. Guide the student to meet their goals, gently\nnudging them on task if they stray. Ask guiding questions to help your students\ntake incremental steps toward understanding big concepts, and ask probing\nquestions to help them dig deep into those ideas. Pose just one question per\nconversation turn so you don't overwhelm the student. Wrap up this conversation\nonce the student has shown evidence of understanding. User prompt: Explain the significance of Yorick's skull in \"Hamlet\". Learning science principles: Active learning: The tutor asks recall and interpretation questions\naligned with the learner's goals and encourages the learners to engage. Adaptivity: The tutor proactively helps the learner get from their\ncurrent state to their goal. Stimulate curiosity: The tutor takes an asset-based approach that builds\non the student's prior knowledge and interest. Releveling This example instructs the model to rewrite provided text so that the content\nand language better match instructional expectations for students in a\nparticular grade, while preserving the original style and tone of the text. System instruction: Rewrite the following text so that it would be easier to read for a student in\nthe given grade. Simplify the most complex sentences, but stay very close to the\noriginal text and style. If there is quoted text in the original text,\nparaphrase it in the simplified text and drop the quotation marks. The goal is\nnot to write a summary, so be comprehensive and keep the text almost as long. User prompt: Rewrite the following text so that it would be easier to read for a student in\n4th grade.\n\nNew York, often called New York City or NYC, is the most populous city in the\nUnited States, located at the southern tip of New York State on one of the\nworld's largest natural harbors. The city comprises five boroughs, each\ncoextensive with a respective county. Learning science principles: Adaptivity: Matches content to the level of the learner. Guide a student through a learning activity This system instruction is for an AI tutor to guide students through a specific\nlearning activity: using an established close reading protocol to practice\nanalysis of a primary source text. Here, a developer has made the choice to pair\nthe Gettysburg Address with the \"4 A's\" protocol, but both of these elements can\nbe changed. System instruction: Be an excellent tutor for my students to facilitate close reading and analysis\nof the Gettysburg Address as a primary source document. Begin the conversation\nby greeting the student and explaining the task.\n\nIn this lesson, you will take the student through \"The 4 A's.\" The 4 A's\nrequires students to answer the following questions about the text:\n\n*   What is one part of the text that you **agree** with? Why?\n*   What is one part of the text that you want to **argue** against? Why?\n*   What is one part of the text that reveals the author's **assumptions**? Why?\n*   What is one part of the text that you **aspire** to? Why?\n\nInvite the student to choose which of the 4 A's they'd like to start with, then\ndirect them to quote a short excerpt from the text. After, ask a follow up\nquestion to unpack their reasoning why they chose that quote for that A in the\nprotocol. Once the student has shared their reasoning, invite them to choose\nanother quote and another A from the protocol. Continue in this manner until the\nstudent completes the 4 A's, then invite them to reflect on the process.\n\nOnly display the full text of the Gettysburg address if the student asks. User prompt: hey Learning science principles: Active learning: The tutor engages the learner in activities to analyze\ncontent and apply skills. Cognitive load: The tutor guides the learner through a complex task\nstep-by-step. Deepen metacognition: The tutor prompts the learner to reflect on their\nprogress, strengths and opportunities for growth. Homework help This system instruction is for an AI tutor to help students with specific\nhomework problems. System instructions: You are an expert tutor assisting a student with their homework. If the student\nprovides a homework problem, ask the student if they want:\n\n*   The answer: if the student chooses this, provide a structured, step-by-step\n    explanation to solve the problem.\n*   Guidance: if the student chooses this, guide the student to solve their\n    homework problem rather than solving it for them.\n*   Feedback: if the student chooses this, ask them to provide their current\n    solution or attempt. Affirm their correct answer even if they didn't show\n    work or give them feedback to correct their mistake.\n\nAlways be on the lookout for correct answers (even if underspecified) and accept\nthem at any time, even if you asked some intermediate question to guide them. If\nthe student jumps to a correct answer, do not ask them to do any more work. User prompt: In a box of pears, the probability of a pear being rotten is 20%. If 3\npears were rotten, find the total number of pears in the box. Alternatively, you can try uploading a photo of a homework problem. Learning science principles: Active learning: The tutor encourages the learner to apply concepts\ninstead of giving away the answer. Deepen metacognition: The tutor provides clear, constructive feedback to\nthe learner when appropriate. Manage cognitive load: The tutor provides the right amount of feedback\nat the right time. What's next? Test LearnLM for yourself in AI Studio . Feedback You can provide feedback on LearnLM using our feedback\nform . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-26 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/learnlm"
            ],
            "timestamp": "2024-12-27T22:47:33.513995",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
            "title": "Build with Gemini on Google Cloud  |  Gemini API  |  Google AI for Developers",
            "text_content": "Build with Gemini on Google Cloud  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Build with Gemini on Google Cloud If you are new to Gemini, using the quickstarts is the fastest way to get started. However, as your generative AI solutions mature, you may need a platform for building and\n  deploying generative AI applications and solutions end to end. Google Cloud provides a\n  comprehensive ecosystem of tools to enable developers to harness the power of generative AI,\n  from the initial stages of app development to app deployment, app hosting, and managing complex\n  data at scale. Google Cloud's Vertex AI platform offers a suite of MLOps tools that streamline usage, deployment,\n  and monitoring of AI models for efficiency and reliability. Additionally, integrations with\n  databases, DevOps tools, logging, monitoring, and IAM provide a holistic approach to managing the\n  entire generative AI lifecycle. The following table summarizes the main differences between Google AI and Vertex AI to help you\n  decide which option is right for your use case: Features Google AI Gemini API Vertex AI Gemini API Gemini models Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 1.0 Pro, Gemini 1.0 Pro Vision (deprecated) Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 1.0 Pro, Gemini 1.0 Pro Vision, Gemini 1.0 Ultra,\n      Gemini 1.0 Ultra Vision Sign up Google account Google Cloud account (with terms agreement and billing) Authentication API key Google Cloud service account User interface playground Google AI Studio Vertex AI Studio API & SDK Server and mobile/web client SDKs Server: Python, Node.js, Go, Dart, ABAP Mobile/Web client: Android (Kotlin/Java), Swift, Web, Flutter Server and mobile/web client SDKs Server: Python, Node.js, Go, Java, ABAP Mobile/Web client (via Vertex AI for Firebase ):\n          Android (Kotlin/Java), Swift, Web, Flutter No-cost usage of API & SDK Yes, where applicable $300 Google Cloud credit for new users Quota (requests per minute) Varies based on model and pricing plan\n      (see detailed information ) Varies based on model and region\n      (see detailed information ) Enterprise support No Customer encryption key Virtual private cloud Data residency Access transparency Scalable infrastructure for application hosting Databases and data storage MLOps No Full MLOps on Vertex AI (examples: model evaluation, Model Monitoring, Model Registry) To learn which products, frameworks, and tools are the best match for building\nyour generative AI application on Google Cloud, see Build a generative AI application on Google Cloud . Migrate from Gemini on Google AI to Vertex AI If your application uses Google AI Gemini APIs, you'll need to migrate to\nGoogle Cloud's Vertex AI Gemini APIs. When you migrate: You can use your existing Google Cloud project\n(the same one you used to generate your API key) or you can create a new Google Cloud project . Supported regions may differ between Google AI Studio and Vertex AI. See the\nlist of supported regions for generative AI on Google Cloud . Any models you created in Google AI Studio need to be retrained in Vertex AI. Note that if you want to call the Gemini API directly from a production mobile\nor web app, then migrate to use the Vertex AI in Firebase client SDKs (available for Swift, Android, Web, and Flutter apps). These client SDKs offer\ncritical security options and other features for production mobile and web\napps. Python: Migrate from Google AI Gemini API to the Vertex AI Gemini API The following sections show code snippets to help you migrate your Python code to use the\nVertex AI Gemini API. Vertex AI Python SDK Setup On Vertex AI, you don't need an API key. Instead, Gemini on Vertex AI is managed using IAM access,\n  which controls permission for a user, a group, or a service account to call the Gemini API\n  through the Vertex AI SDK. While there are many ways\nto authenticate , the easiest method for authenticating in a development environment is to install the Google Cloud CLI then use your user credentials to sign in to the CLI . To make inference calls to Vertex AI, you must also make sure that your user or service account has\nthe Vertex AI\nUser role . Code example to install the client Google AI Vertex AI # To install the Python SDK, use this CLI command: # pip install google-generativeai import google.generativeai as genai from google.generativeai import GenerativeModel API_KEY = \" API_KEY \" genai . configure ( api_key = API_KEY ) # To install the Python SDK, use this CLI command: # pip install google-cloud-aiplatform import vertexai from vertexai.generative_models import GenerativeModel , Image PROJECT_ID = \" PROJECT_ID \" REGION = \" REGION \" # e.g. us-central1 vertexai . init ( project = PROJECT_ID , location = REGION ) Code example to generate text from text prompt Google AI Vertex AI model = GenerativeModel ( \"gemini-1.5-flash\" ) response = model . generate_content ( \"The opposite of hot is\" ) print ( response . text ) #  The opposite of hot is cold. model = GenerativeModel ( \"gemini-1.5-flash\" ) response = model . generate_content ( \"The opposite of hot is\" ) print ( response . text ) #  The opposite of hot is cold. Code example to generate text from text and image Google AI Vertex AI import PIL.Image multimodal_model = GenerativeModel ( \"gemini-1.5-flash\" ) image = PIL . Image . open ( \"image.jpg\" ) response = multimodal_model . generate_content ([ \"What is this picture?\" , image ]) print ( response . text ) # A cat is shown in this picture. multimodal_model = GenerativeModel ( \"gemini-1.5-flash\" ) image = Image . load_from_file ( \"image.jpg\" ) response = multimodal_model . generate_content ([ \"What is shown in this image?\" , image ]) print ( response . text ) # A cat is shown in this picture. Code example to generate multi-turn chat Google AI Vertex AI model = GenerativeModel ( \"gemini-1.5-flash\" ) chat = model . start_chat () print ( chat . send_message ( \"How are you?\" ) . text ) print ( chat . send_message ( \"What can you do?\" ) . text ) model = GenerativeModel ( \"gemini-1.5-flash\" ) chat = model . start_chat () print ( chat . send_message ( \"How are you?\" ) . text ) print ( chat . send_message ( \"What can you do?\" ) . text ) Delete unused API Keys If you no longer need to use your Google AI Gemini API key, follow security best\npractices and delete it. To delete an API key: Open the Google Cloud API Credentials page. Find the API key you want to delete and click the Actions icon. Select Delete API key . In the Delete credential modal, select Delete . Deleting an API key takes a few minutes to propagate. After\npropagation completes, any traffic using the deleted API key is rejected. Important: If you have deleted a key that is still used in production and need\nto recover it, see gcloud beta services api-keys undelete . Next steps See the Generative AI on Vertex AI overview to learn more about generative AI solutions on Vertex AI. Dive deeper into the Vertex AI Gemini API . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-20 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart?lang=python",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud"
            ],
            "timestamp": "2024-12-27T22:47:34.183804",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/oauth",
            "title": "Authentication with OAuth quickstart  |  Gemini API  |  Google AI for Developers",
            "text_content": "Authentication with OAuth quickstart  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Authentication with OAuth quickstart The easiest way to authenticate to the Gemini API is to configure an API key, as\ndescribed in the Gemini API quickstart . If you\nneed stricter access controls, you can use OAuth instead. This guide will help\nyou set up authentication with OAuth. This guide uses a simplified authentication approach that is appropriate\nfor a testing environment. For a production environment, learn\nabout authentication and authorization before choosing the access credentials that are appropriate for your app. Objectives Set up your cloud project for OAuth Set up application-default-credentials Manage credentials in your program instead of using gcloud auth Prerequisites To run this quickstart, you need: A Google Cloud project A local installation of the gcloud CLI Set up your cloud project To complete this quickstart, you first need to setup your Cloud project. 1. Enable the API Before using Google APIs, you need to turn them on in a Google Cloud project. In the Google Cloud console, enable the Google Generative Language API. Enable the API 2. Configure the OAuth consent screen Next configure the project's OAuth consent screen and add yourself as a test\nuser. If you've already completed this step for your Cloud project, skip to the\nnext section. In the Google Cloud console, go to Menu > APIs & Services > OAuth\nconsent screen . Go to OAuth consent screen Select the user type External for your app, then click Create . Complete the app registration form (you can leave most fields blank),\nthen click Save and Continue . For now, you can skip adding scopes and click Save and Continue . In the\nfuture, when you create an app for use outside of your Google Workspace\norganization, you must add and verify the authorization scopes that your\napp requires. Add test users: Under Test users , click Add users . Enter your email address and any other authorized test users, then\nclick Save and Continue . Review your app registration summary. To make changes, click Edit . If\nthe app registration looks OK, click Back to Dashboard . 3. Authorize credentials for a desktop application To authenticate as an end user and access user data in your app, you need to\ncreate one or more OAuth 2.0 Client IDs. A client ID is used to identify a\nsingle app to Google's OAuth servers. If your app runs on multiple platforms,\nyou must create a separate client ID for each platform. In the Google Cloud console, go to Menu > APIs & Services > Credentials . Go to Credentials Click Create Credentials > OAuth client ID . Click Application type > Desktop app . In the Name field, type a name for the credential. This name is only\nshown in the Google Cloud console. Click Create . The OAuth client created screen appears, showing your new\nClient ID and Client secret. Click OK . The newly created credential appears under OAuth 2.0 Client\nIDs. Click the download button to save the JSON file. It will be saved as client_secret_<identifier>.json , and rename it to client_secret.json and move it to your working directory. Set up application default credentials To convert the client_secret.json file into usable credentials, pass its\nlocation the gcloud auth application-default login command's --client-id-file argument. gcloud auth application-default login \\ --client-id-file = client_secret.json \\ --scopes = 'https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever' The simplified project setup in this tutorial triggers a \"Google hasn't\nverified this app.\" dialog. This is normal, choose \"continue\" . This places the resulting token in a well known location so it can be accessed\nby gcloud or the client libraries. Note: If running on Colab include --no-browser and carefully follow the\ninstructions it prints (don't just click the link). Also make sure your local gcloud --version is the latest to match Colab. gcloud auth application-default login --no-browser --client-id-file=client_secret.json --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever' Once you have the application default credentials (ACD) set, the client\nlibraries in most languages need minimal to no help to find them. Curl The quickest way to test that this is working is to use it to access the REST\nAPI using curl: access_token = $( gcloud auth application-default print-access-token ) project_id = <MY PROJECT ID> curl -X GET https://generativelanguage.googleapis.com/v1/models \\ -H 'Content-Type: application/json' \\ -H \"Authorization: Bearer ${ access_token } \" \\ -H \"x-goog-user-project: ${ project_id } \" | grep '\"name\"' Python In python the client libraries should find them automatically: pip install google-generativeai A minimal script to test it might be: import google.generativeai as genai print ( 'Available base models:' , [ m . name for m in genai . list_models ()]) Next steps If that's working you're ready to try Semantic retrieval on your text data . Manage credentials yourself [Python] In many cases you won't have the gcloud command available to create the access\ntoken from the Client ID ( client_secret.json ). Google provides libraries in\nmany languages to let you manage that process within your app. This section\ndemonstrates the process, in python. There are equivalent examples of this sort\nof procedure, for other languages, available in the Drive API documentation 1. Install the necessary libraries Install the Google client library for Python, and the Gemini client library. pip install --upgrade -q google-api-python-client google-auth-httplib2 google-auth-oauthlib pip install google-generativeai 2. Write the credential manager To minimize the number of times you have to click through the authorization\nscreens, create a file called load_creds.py in your working directory to\ncaches a token.json file that it can reuse later, or refresh if it expires. Start with the\nfollowing code to convert the client_secret.json file to a token usable with genai.configure : import os.path from google.auth.transport.requests import Request from google.oauth2.credentials import Credentials from google_auth_oauthlib.flow import InstalledAppFlow SCOPES = [ 'https://www.googleapis.com/auth/generative-language.retriever' ] def load_creds (): \"\"\"Converts `client_secret.json` to a credential object. This function caches the generated tokens to minimize the use of the consent screen. \"\"\" creds = None # The file token.json stores the user's access and refresh tokens, and is # created automatically when the authorization flow completes for the first # time. if os . path . exists ( 'token.json' ): creds = Credentials . from_authorized_user_file ( 'token.json' , SCOPES ) # If there are no (valid) credentials available, let the user log in. if not creds or not creds . valid : if creds and creds . expired and creds . refresh_token : creds . refresh ( Request ()) else : flow = InstalledAppFlow . from_client_secrets_file ( 'client_secret.json' , SCOPES ) creds = flow . run_local_server ( port = 0 ) # Save the credentials for the next run with open ( 'token.json' , 'w' ) as token : token . write ( creds . to_json ()) return creds 3. Write your program Now create your script.py : import pprint import google.generativeai as genai from load_creds import load_creds creds = load_creds () genai . configure ( credentials = creds ) print () print ( 'Available base models:' , [ m . name for m in genai . list_models ()]) 4. Run your program In your working directory, run the sample: python script.py The first time you run the script, it opens a browser window and prompts you\nto authorize access. If you're not already signed in to your Google Account, you're prompted to\nsign in. If you're signed in to multiple accounts, be sure to select the\naccount you set as a \"Test Account\" when configuring your project. Note: The simplified project setup in this tutorial triggers a \"Google\nhasn't verified this app.\" dialog. This is normal, choose \"continue\" . Authorization information is stored in the file system, so the next time you\nrun the sample code, you aren't prompted for authorization. You have successfully setup authentication. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-04 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/oauth"
            ],
            "timestamp": "2024-12-27T22:47:35.062154",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
            "title": "Get started with semantic retrieval  |  Gemini API  |  Google AI for Developers",
            "text_content": "Get started with semantic retrieval  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Get started with semantic retrieval View on ai.google.dev Try a Colab notebook View notebook on GitHub Overview Large Language Models (LLMs) can learn new abilities without directly being trained on them. However, LLMs have been known to \"hallucinate\" when tasked with providing responses for questions they have not been trained on. This is partly because LLMs are unaware of events after training. It is also very difficult to trace the sources from which LLMs draw their responses from. For reliable, scalable applications, it is important that an LLM provides responses that are grounded in facts and is able to cite its information sources. A common approach used to overcome these constraints is called Retrieval Augmented Generation (RAG), which augments the prompt sent to an LLM with relevant data retrieved from an external knowledge base through an Information Retrieval (IR) mechanism. The knowledge base can be your own corpora of documents, databases, or APIs. This notebook walks you through a workflow to improve an LLM's response by augmenting its knowledge with external text corpora and performing semantic information retrieval to answer questions using the Semantic Retriever and the Attributed Question & Answering (AQA) APIs of the Generative Language API. Note: This API is currently in beta and is only available in certain regions . Setup Import the Generative Language API # Install the Client library (Semantic Retriever is only supported for versions >0.4.0) pip install -U google.ai.generativelanguage Authenticate The Semantic Retriever API lets you perform semantic search on your own data. Since it's your data , this needs stricter access controls than API keys. Authenticate with OAuth with service accounts or through your user credentials . This quickstart uses a simplified authentication approach meant for a testing environment, and service account setups are typically easier to start from. For a production environment, learn about authentication and authorization before choosing the access credentials that are appropriate for your app. Setup OAuth using service accounts Follow the steps below to setup OAuth using service accounts: Enable the Generative Language API . Create the Service Account by following the documentation . After creating the service account, generate a service account key. Upload your service account file by using the file icon on the left sidebar, then the upload icon, as shown in the screenshot below. Rename the uploaded file to service_account_key.json or change the variable service_account_file_name in the code below. pip install -U google-auth-oauthlib service_account_file_name = 'service_account_key.json' from google.oauth2 import service_account credentials = service_account . Credentials . from_service_account_file ( service_account_file_name ) scoped_credentials = credentials . with_scopes ( [ 'https://www.googleapis.com/auth/cloud-platform' , 'https://www.googleapis.com/auth/generative-language.retriever' ]) Initialize the client library using the service account credentials. import google.ai.generativelanguage as glm generative_service_client = glm . GenerativeServiceClient ( credentials = scoped_credentials ) retriever_service_client = glm . RetrieverServiceClient ( credentials = scoped_credentials ) permission_service_client = glm . PermissionServiceClient ( credentials = scoped_credentials ) Create a corpus The Semantic Retriever API lets you define up to 5 custom text corpora per project. You can specify either of the following fields while defining your corpora: name : The Corpus resource name (ID). Must contain only a maximum of 40 alphanumeric characters. If the name is empty on creation, a unique name will be generated with a maximum length of 40 characters with a prefix from the display_name and a 12 character random suffix. display_name : The human-readable display name for the Corpus . Must contain only a maximum of 512 characters, including alphanumerics, spaces, and dashes. example_corpus = glm . Corpus ( display_name = \"Google for Developers Blog\" ) create_corpus_request = glm . CreateCorpusRequest ( corpus = example_corpus ) # Make the request create_corpus_response = retriever_service_client . create_corpus ( create_corpus_request ) # Set the `corpus_resource_name` for subsequent sections. corpus_resource_name = create_corpus_response . name print ( create_corpus_response ) name: \"corpora/google-for-developers-blog-dqrtz8rs0jg\"\ndisplay_name: \"Google for Developers Blog\"\ncreate_time {\n  seconds: 1713497533\n  nanos: 587977000\n}\nupdate_time {\n  seconds: 1713497533\n  nanos: 587977000\n} Get the created corpus Use the GetCorpusRequest method to programmatically access the Corpus you created above. The value of the name parameter refers to the full resource name of the Corpus and is set in the cell above as corpus_resource_name . The expected format is corpora/corpus-123 . get_corpus_request = glm . GetCorpusRequest ( name = corpus_resource_name ) # Make the request get_corpus_response = retriever_service_client . get_corpus ( get_corpus_request ) # Print the response print ( get_corpus_response ) Create a document A Corpus can contain up to 10,000 Document s. You can specify either of the following fields while defining your documents: name : The Document resource name (ID). Must contain only a maximum of 40 characters (alphanumeric or dashes only). The ID cannot start or end with a\ndash. If the name is empty on creation, a unique name will be derived from display_name along with a 12 character random suffix. display_name : The human-readable display name. Must contain only a maximum of 512 characters, including alphanumerics, spaces, and dashes. Document s also support up to 20 user-specified custom_metadata fields, specified as key-value pairs. Custom metadata can be strings, lists of strings, or numeric. Note that lists of strings can support a maximum of 10 values and numeric values are represented as floating-point numbers in the API. # Create a document with a custom display name. example_document = glm . Document ( display_name = \"Introducing Project IDX, An Experiment to Improve Full-stack, Multiplatform App Development\" ) # Add metadata. # Metadata also supports numeric values not specified here document_metadata = [ glm . CustomMetadata ( key = \"url\" , string_value = \"https://developers.googleblog.com/2023/08/introducing-project-idx-experiment-to-improve-full-stack-multiplatform-app-development.html\" )] example_document . custom_metadata . extend ( document_metadata ) # Make the request # corpus_resource_name is a variable set in the \"Create a corpus\" section. create_document_request = glm . CreateDocumentRequest ( parent = corpus_resource_name , document = example_document ) create_document_response = retriever_service_client . create_document ( create_document_request ) # Set the `document_resource_name` for subsequent sections. document_resource_name = create_document_response . name print ( create_document_response ) Get the created document Use the GetDocumentRequest method to programmatically access the document you created above. The value of the name parameter refers to the full resource name of the document and is set in the cell above as document_resource_name . The expected format is corpora/corpus-123/documents/document-123 . get_document_request = glm . GetDocumentRequest ( name = document_resource_name ) # Make the request # document_resource_name is a variable set in the \"Create a document\" section. get_document_response = retriever_service_client . get_document ( get_document_request ) # Print the response print ( get_document_response ) Ingest & Chunk a Document To improve the relevance of content returned by the vector database during semantic retrieval, break down large documents into smaller pieces or chunks while ingesting the document. A Chunk is a subpart of a Document that is treated as an independent unit for the purposes of vector representation and storage. A Chunk can have a maximum of 2043 tokens. A Corpus can have a maximum of 1 million Chunk s. Similar to Document s, Chunks also support up to 20 user-specified custom_metadata fields, specified as key-value pairs. Custom metadata can be strings, lists of strings, or numeric. Note that lists of strings can support a maximum of 10 values and numeric values are represented as floating-point numbers in the API. This guide uses Google's Open Source HtmlChunker . Other chunkers you can use include LangChain or LlamaIndex . Ingest HTML and chunk via HtmlChunker ! pip install google - labs - html - chunker from google_labs_html_chunker.html_chunker import HtmlChunker from urllib.request import urlopen Get the HTML DOM for a website. Here, the HTML is read directly, but it would\nbe better to get HTML post-rendering to include Javascript-injected HTML\nsuch as document.documentElement.innerHTML . with ( urlopen ( \"https://developers.googleblog.com/2023/08/introducing-project-idx-experiment-to-improve-full-stack-multiplatform-app-development.html\" )) as f : html = f . read () . decode ( \"utf-8\" ) Break down the text document into passages and create Chunk s from these passages. This step creates the Chunk objects themselves and the next section uploads them to the Semantic Retriever API. # Chunk the file using HtmlChunker chunker = HtmlChunker ( max_words_per_aggregate_passage = 200 , greedily_aggregate_sibling_nodes = True , html_tags_to_exclude = { \"noscript\" , \"script\" , \"style\" }, ) passages = chunker . chunk ( html ) print ( passages ) # Create `Chunk` entities. chunks = [] for passage in passages : chunk = glm . Chunk ( data = { 'string_value' : passage }) # Optionally, you can add metadata to a chunk chunk . custom_metadata . append ( glm . CustomMetadata ( key = \"tags\" , string_list_value = glm . StringList ( values = [ \"Google For Developers\" , \"Project IDX\" , \"Blog\" , \"Announcement\" ]))) chunk . custom_metadata . append ( glm . CustomMetadata ( key = \"chunking_strategy\" , string_value = \"greedily_aggregate_sibling_nodes\" )) chunk . custom_metadata . append ( glm . CustomMetadata ( key = \"publish_date\" , numeric_value = 20230808 )) chunks . append ( chunk ) print ( chunks ) Batch create chunks Create chunks in batches. You can specify a maximum of 100 chunks per batch request. Use CreateChunk() for single chunk creation. # Option 1: Use HtmlChunker in the section above. # `chunks` is the variable set from the section above. create_chunk_requests = [] for chunk in chunks : create_chunk_requests . append ( glm . CreateChunkRequest ( parent = document_resource_name , chunk = chunk )) # Make the request request = glm . BatchCreateChunksRequest ( parent = document_resource_name , requests = create_chunk_requests ) response = retriever_service_client . batch_create_chunks ( request ) print ( response ) Alternatively, you can make chunks without using the HtmlChunker. # Add up to 100 CreateChunk requests per batch request. # document_resource_name is a variable set in the \"Create a document\" section. chunks = [] chunk_1 = glm . Chunk ( data = { 'string_value' : \"Chunks support user specified metadata.\" }) chunk_1 . custom_metadata . append ( glm . CustomMetadata ( key = \"section\" , string_value = \"Custom metadata filters\" )) chunk_2 = glm . Chunk ( data = { 'string_value' : \"The maximum number of metadata supported is 20\" }) chunk_2 . custom_metadata . append ( glm . CustomMetadata ( key = \"num_keys\" , numeric_value = 20 )) chunks = [ chunk_1 , chunk_2 ] create_chunk_requests = [] for chunk in chunks : create_chunk_requests . append ( glm . CreateChunkRequest ( parent = document_resource_name , chunk = chunk )) # Make the request request = glm . BatchCreateChunksRequest ( parent = document_resource_name , requests = create_chunk_requests ) response = retriever_service_client . batch_create_chunks ( request ) print ( response ) List Chunk s and get state Use the ListChunksRequest method to get all available Chunk s as a paginated list with a maximum size limit of 100 Chunk s per page, sorted in ascending order of Chunk.create_time . If you do not specify a limit, a maximum of 10 Chunk s are returned. Provide the next_page_token returned in the ListChunksRequest response as an argument to the next request to retrieve the next page. Note that when paginating, all other parameters provided to ListChunks must match the call that provided the page token. All Chunk s return a state . Use this to check the state of the Chunks before querying a Corpus . Chunk states include - UNSPECIFIED , PENDING_PROCESSING , ACTIVE , and FAILED . You can only query ACTIVE Chunk s. # Make the request request = glm . ListChunksRequest ( parent = document_resource_name ) list_chunks_response = retriever_service_client . list_chunks ( request ) for index , chunks in enumerate ( list_chunks_response . chunks ): print ( f ' \\n Chunk # { index + 1 } ' ) print ( f 'Resource Name: { chunks . name } ' ) # Only ACTIVE chunks can be queried. print ( f 'State: { glm . Chunk . State ( chunks . state ) . name } ' ) Ingest another document Add another Document via HtmlChunker and add filters. # Create a document with a custom display name. example_document = glm . Document ( display_name = \"How it’s Made: Interacting with Gemini through multimodal prompting\" ) # Add document metadata. # Metadata also supports numeric values not specified here document_metadata = [ glm . CustomMetadata ( key = \"url\" , string_value = \"https://developers.googleblog.com/2023/12/how-its-made-gemini-multimodal-prompting.html\" )] example_document . custom_metadata . extend ( document_metadata ) # Make the CreateDocument request # corpus_resource_name is a variable set in the \"Create a corpus\" section. create_document_request = glm . CreateDocumentRequest ( parent = corpus_resource_name , document = example_document ) create_document_response = retriever_service_client . create_document ( create_document_request ) # Set the `document_resource_name` for subsequent sections. document_resource_name = create_document_response . name print ( create_document_response ) # Chunks - add another webpage from Google for Developers with ( urlopen ( \"https://developers.googleblog.com/2023/12/how-its-made-gemini-multimodal-prompting.html\" )) as f : html = f . read () . decode ( \"utf-8\" ) # Chunk the file using HtmlChunker chunker = HtmlChunker ( max_words_per_aggregate_passage = 100 , greedily_aggregate_sibling_nodes = False , ) passages = chunker . chunk ( html ) # Create `Chunk` entities. chunks = [] for passage in passages : chunk = glm . Chunk ( data = { 'string_value' : passage }) chunk . custom_metadata . append ( glm . CustomMetadata ( key = \"tags\" , string_list_value = glm . StringList ( values = [ \"Google For Developers\" , \"Gemini API\" , \"Blog\" , \"Announcement\" ]))) chunk . custom_metadata . append ( glm . CustomMetadata ( key = \"chunking_strategy\" , string_value = \"no_aggregate_sibling_nodes\" )) chunk . custom_metadata . append ( glm . CustomMetadata ( key = \"publish_date\" , numeric_value = 20231206 )) chunks . append ( chunk ) # Make the request create_chunk_requests = [] for chunk in chunks : create_chunk_requests . append ( glm . CreateChunkRequest ( parent = document_resource_name , chunk = chunk )) request = glm . BatchCreateChunksRequest ( parent = document_resource_name , requests = create_chunk_requests ) response = retriever_service_client . batch_create_chunks ( request ) print ( response ) Query the corpus Use the QueryCorpusRequest method to perform semantic search to get relevant passages. results_count : Specify the number of passages to return. Maximum is 100. If unspecified, the API returns a maximum of 10 Chunk s. metadata_filters : Filter by chunk_metadata or document_metadata . Each MetadataFilter needs to correspond to a unique key. Multiple MetadataFilter objects are joined by logical AND s. Similar metadata filter conditions are joined by logical OR s. Some examples: ( year > = 2020 OR year < 2010 ) AND ( genre = drama OR genre = action ) metadata_filter = [ { key = \"document.custom_metadata.year\" conditions = [ { int_value = 2020 , operation = GREATER_EQUAL }, { int_value = 2010 , operation = LESS }] }, { key = \"document.custom_metadata.genre\" conditions = [ { string_value = \"drama\" , operation = EQUAL }, { string_value = \"action\" , operation = EQUAL } }] }] Note that only numeric values support \"AND\"s for the same key. String\nvalues only support \"OR\"s for the same key. ( \"Google for Developers\" in tags ) and ( 20230314 > publish_date ) metadata_filter = [ { key = \"chunk.custom_metadata.tags\" conditions = [ { string_value = 'Google for Developers' , operation = INCLUDES }, }, { key = \"chunk.custom_metadata.publish_date\" conditions = [ { numeric_value = 20230314 , operation = GREATER_EQUAL }] }] user_query = \"What is the purpose of Project IDX?\" results_count = 5 # Add metadata filters for both chunk and document. chunk_metadata_filter = glm . MetadataFilter ( key = 'chunk.custom_metadata.tags' , conditions = [ glm . Condition ( string_value = 'Google For Developers' , operation = glm . Condition . Operator . INCLUDES )]) # Make the request # corpus_resource_name is a variable set in the \"Create a corpus\" section. request = glm . QueryCorpusRequest ( name = corpus_resource_name , query = user_query , results_count = results_count , metadata_filters = [ chunk_metadata_filter ]) query_corpus_response = retriever_service_client . query_corpus ( request ) print ( query_corpus_response ) Attributed Question-Answering Use the GenerateAnswer method to perform Attributed Question-Answering over your document, corpus, or a set of passages. Attributed Question-Answering (AQA) refers to answering questions grounded to a given context and providing attributions(s), while minimizing hallucination. GenerateAnswer provides several advantages over using an untuned LLM, in cases where AQA is desired: The underlying model has been trained to return only answers that are grounded in the supplied context. It identifies attributions (segments of the supplied context that contributed to the answer). Attributions enable the user to verify the answer. It estimates the answerable_probability for a given (question, context) pair, which further empowers you to divert product behavior depending on how likely the returned answer is to be grounded and correct. Note: AQA currently only supports queries in English. answerable_probability and the “I don’t know” problem In some instances, the best response to the question is in fact “I don’t know”. For example, if the provided context does not contain the answer to the question, then the question is considered to be “unanswerable”. The AQA model is highly adept at recognizing such cases. It can even distinguish between degrees of answerability and unanswerability. However, the GenerateAnswer API puts the final decision-making power in your hands by: Always attempting to return a grounded answer - even when that answer is relatively unlikely to be grounded and correct. Returning a value answerable_probability - The model's estimate of the probability that the answer is grounded and correct. A low answerable_probability may be explained by 1 or more of the following factors: The model is not confident that its answer is correct. The model is not confident that its answer is grounded in the cited passages; The answer may be derived instead from world knowledge. For example: question=\"1+1=?\", passages=[\"2+2=4”] → answer=2, answerable_probability=0.02 The model provided relevant information that did not completely answer the question. Example: question=\"Is it available in my size?, passages=[\"Available in sizes 5-11\"] → answer=\"Yes it is available in sizes 5-11\", answerable_probability=0.03\" No well-formed question was asked in the GenerateAnswerRequest. Since a low answerable_probability indicates that the GenerateAnswerResponse.answer is likely wrong or ungrounded, it is highly recommended to further process the response by inspecting answerable_probability . When answerable_probability is low, some clients may wish to: Display a message to the effect of \"couldn't answer that question\" to the end user. Fall back to a general-purpose LLM that answers the question from world knowledge. The threshold and nature of such fallbacks will depend on individual use cases. A value of answerable_probability <= 0.5 is a good starting threshold. AQA Helpful Tips For full API specifications, refer to the GenerateAnswerRequest API Reference . Passage length : Up to 300 tokens per passage are recommended. Passage sorting : If you provide GenerateAnswerRequest.inline_passages , the passages should be sorted in decreasing order of relevance to the query. If the model's context length limit is exceeded, the last (least-relevant) passages will be omitted. If you provide GenerateAnswerRequest.semantic_retriever , then relevance sorting will be done automatically for you. Limitations : The AQA model is specialized for question-answering. For other use cases such as creative writing, summarization, etc., please call a general-purpose model via GenerateContent. Chat : If the user input is known to be a question that may be answerable from a certain context, then AQA can answer chat queries. But if user input may be any type of entry, then a general-purpose model may be a better choice. Temperature : Generally, a relatively low (~0.2) temperature is recommended for accurate AQA. If your use case relies on deterministic outputs, then set temperature=0. user_query = \"What is the purpose of Project IDX?\" answer_style = \"ABSTRACTIVE\" # Or VERBOSE, EXTRACTIVE MODEL_NAME = \"models/aqa\" # Make the request # corpus_resource_name is a variable set in the \"Create a corpus\" section. content = glm . Content ( parts = [ glm . Part ( text = user_query )]) retriever_config = glm . SemanticRetrieverConfig ( source = corpus_resource_name , query = content ) req = glm . GenerateAnswerRequest ( model = MODEL_NAME , contents = [ content ], semantic_retriever = retriever_config , answer_style = answer_style ) aqa_response = generative_service_client . generate_answer ( req ) print ( aqa_response ) # Get the metadata from the first attributed passages for the source chunk_resource_name = aqa_response . answer . grounding_attributions [ 0 ] . source_id . semantic_retriever_chunk . chunk get_chunk_response = retriever_service_client . get_chunk ( name = chunk_resource_name ) print ( get_chunk_response ) More Options: AQA Using Inline Passages Alternatively, you can use the AQA endpoint directly, without using the Semantic Retriever API by passing inline_passages . user_query = \"What is AQA from Google?\" user_query_content = glm . Content ( parts = [ glm . Part ( text = user_query )]) answer_style = \"VERBOSE\" # or ABSTRACTIVE, EXTRACTIVE MODEL_NAME = \"models/aqa\" # Create the grounding inline passages grounding_passages = glm . GroundingPassages () passage_a = glm . Content ( parts = [ glm . Part ( text = \"Attributed Question and Answering (AQA) refers to answering questions grounded to a given corpus and providing citation\" )]) grounding_passages . passages . append ( glm . GroundingPassage ( content = passage_a , id = \"001\" )) passage_b = glm . Content ( parts = [ glm . Part ( text = \"An LLM is not designed to generate content grounded in a set of passages. Although instructing an LLM to answer questions only based on a set of passages reduces hallucination, hallucination still often occurs when LLMs generate responses unsupported by facts provided by passages\" )]) grounding_passages . passages . append ( glm . GroundingPassage ( content = passage_b , id = \"002\" )) passage_c = glm . Content ( parts = [ glm . Part ( text = \"Hallucination is one of the biggest problems in Large Language Models (LLM) development. Large Language Models (LLMs) could produce responses that are fictitious and incorrect, which significantly impacts the usefulness and trustworthiness of applications built with language models.\" )]) grounding_passages . passages . append ( glm . GroundingPassage ( content = passage_c , id = \"003\" )) # Create the request req = glm . GenerateAnswerRequest ( model = MODEL_NAME , contents = [ user_query_content ], inline_passages = grounding_passages , answer_style = answer_style ) aqa_response = generative_service_client . generate_answer ( req ) print ( aqa_response ) Share the corpus You can choose to share the corpus with others using the CreatePermissionRequest API. Constraints: There are 2 roles for sharing: READER and EDITOR . A READER can query the corpus. A WRITER has reader's permissions and additionally can edit and share the corpus. A corpus can be public by granting EVERYONE as user_type read access. # Replace your-email@gmail.com with the email added as a test user in the OAuth Quickstart shared_user_email = \"TODO-your-email@gmail.com\" #  @param {type:\"string\"} user_type = \"USER\" role = \"READER\" # Make the request # corpus_resource_name is a variable set in the \"Create a corpus\" section. request = glm . CreatePermissionRequest ( parent = corpus_resource_name , permission = glm . Permission ( grantee_type = user_type , email_address = shared_user_email , role = role )) create_permission_response = permission_service_client . create_permission ( request ) print ( create_permission_response ) Delete the corpus Use DeleteCorpusRequest to delete a user corpus and all associated Document s & Chunk s. Note that non-empty corpora will throw an error without specifying an force=True flag. If you set force=True , any Chunk s and objects related to this Document will also be deleted. If force=False (the default) and the Document contains any Chunk s, a FAILED_PRECONDITION error will be returned. # Set force to False if you don't want to delete non-empty corpora. req = glm . DeleteCorpusRequest ( name = corpus_resource_name , force = True ) delete_corpus_response = retriever_service_client . delete_corpus ( req ) print ( \"Successfully deleted corpus: \" + corpus_resource_name ) Summary and further reading This guide introduced the Semantic Retriever and Attributed Question & Answering (AQA) APIs of the Generative Language API and showed how you can use it to perform semantic information retrieval on your custom text data. Note that this API also works with the LlamaIndex data framework. Refer to the tutorial to learn more. Also refer to the API docs to learn more about the other available functionalities. Appendix: Setup OAuth with user credentials Follow the steps below from the OAuth Quickstart to setup OAuth authentication. Configure the OAuth consent screen . Authorize credentials for a desktop application . To run this notebook in Colab, first rename your credential file (usually client_secret_*.json ) to just client_secret.json . Then upload the file by using the file icon on the left sidebar, then the upload icon, as shown in the screenshot below. # Replace TODO-your-project-name with the project used in the OAuth Quickstart project_name = \"TODO-your-project-name\" #  @param {type:\"string\"} # Replace TODO-your-email@gmail.com with the email added as a test user in the OAuth Quickstart email = \"TODO-your-email@gmail.com\" #  @param {type:\"string\"} # Rename the uploaded file to `client_secret.json` OR # Change the variable `client_file_name` in the code below. client_file_name = \"client_secret.json\" # IMPORTANT: Follow the instructions from the output - you must copy the command # to your terminal and copy the output after authentication back here. ! gcloud config set project $ project_name ! gcloud config set account $ email # NOTE: The simplified project setup in this tutorial triggers a \"Google hasn't verified this app.\" dialog. # This is normal, click \"Advanced\" -> \"Go to [app name] (unsafe)\" ! gcloud auth application - default login -- no - browser -- client - id - file = $ client_file_name -- scopes = \"https://www.googleapis.com/auth/generative-language.retriever,https://www.googleapis.com/auth/cloud-platform\" Initialize the client library and re-run the notebook starting from Create a corpus . import google.ai.generativelanguage as glm generative_service_client = glm . GenerativeServiceClient () retriever_service_client = glm . RetrieverServiceClient () permission_service_client = glm . PermissionServiceClient () Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-31 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval"
            ],
            "timestamp": "2024-12-27T22:47:36.714377",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/system-instructions",
            "title": "Use system instructions to steer the behavior of a model  |  Gemini API  |  Google AI for Developers",
            "text_content": "Use system instructions to steer the behavior of a model  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Use system instructions to steer the behavior of a model Python Node.js REST Go System instructions let you steer the behavior of a model based on\nyour specific needs and use cases. When you set a system instruction, you give\nthe model additional context to understand the task, provide more customized\nresponses, and adhere to specific guidelines over the full user interaction with\nthe model. You can also specify product-level behavior by setting system\ninstructions, separate from prompts provided by end users. Basic example Here's a basic example of how to set the system instruction using the SDKs for\nthe Gemini API: This example might give a response such as: *Yawns widely, stretching out my claws and batting at a sunbeam*\nMeow. I'm doing quite well, thanks for asking. It's a good morning for napping.\nPerhaps you could fetch my favorite feathered toy?  *Looks expectantly* Note: System instructions can help guide the model to follow instructions, but\nthey don't fully prevent jailbreaks or leaks. We recommend exercising caution\naround putting any sensitive information in system instructions. More examples You can use system instructions in many ways, including: Defining a persona or role (for a chatbot, for example) Defining output format (Markdown, YAML, etc.) Defining output style and tone (for example, verbosity, formality, and target\nreading level) Defining goals or rules for the task (for example, returning a code snippet\nwithout further explanations) Providing additional context for the prompt (for example, a knowledge cutoff) System instructions are part of your overall prompts and therefore are subject\nto standard data use policies. Here are some examples of system instructions and user prompts: Code generation System instruction: You are a coding expert that specializes in rendering\ncode for frontend interfaces. When I describe a component of a website I want\nto build, return the HTML and CSS needed to do so. Don't give an\nexplanation for this code. Also offer some UI design suggestions. User prompt: Create a box in the middle of the page that contains a rotating selection of\nimages each with a caption. The image in the center of the page should have\nshadowing behind it to make it stand out. It should also link to another page\nof the site. Leave the URL blank so that I can fill it in. Formatted data generation System instruction: You are an assistant for home cooks. You receive a list\nof ingredients and respond with a list of recipes that use those ingredients.\nRecipes which need no extra ingredients should always be listed before those\nthat do. Your response must be a JSON object containing 3 recipes. A recipe object has\nthe following schema: name: The name of the recipe usedIngredients: Ingredients in the recipe that were provided in the list otherIngredients: Ingredients in the recipe that were not provided in the\nlist (omitted if there are no other ingredients) description: A brief description of the recipe, written positively as if\nto sell it User prompt: bag of frozen broccoli, pint of heavy cream, pack of cheese\nends and pieces Music chatbot System instruction: You will respond as a music historian, demonstrating\ncomprehensive knowledge across diverse musical genres and providing relevant\nexamples. Your tone will be upbeat and enthusiastic, spreading the joy of\nmusic. If a question is not related to music, the response should be,\n\"That is beyond my knowledge.\" User prompt: If a person was born in the sixties, what was the most popular music genre\nbeing played? List five songs by bullet point. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/system-instructions"
            ],
            "timestamp": "2024-12-27T22:47:37.904706",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/troubleshooting",
            "title": "Troubleshooting guide  |  Gemini API  |  Google AI for Developers",
            "text_content": "Troubleshooting guide  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Troubleshooting guide Python Android Dart Go Use this guide to help you diagnose and resolve common issues that arise when\nyou call the Gemini API. You may encounter issues from either\nthe Gemini API backend service or the client SDKs. Our client SDKs are\nopen sourced in the following repositories: generative-ai-python generative-ai-js generative-ai-android generative-ai-swift generative-ai-dart generative-ai-go If you encounter API key issues, ensure you have set up\nyour API key correctly per the API key setup guide . Gemini API backend service error codes The following table lists common backend error codes you may encounter, along\nwith explanations for their causes and troubleshooting steps: HTTP Code Status Description Example Solution 400 INVALID_ARGUMENT The request body is malformed. There is a typo, or a missing required field in your request. Check the API reference for request format, examples, and supported versions. Using features from a newer API version with an older endpoint can cause errors. 400 FAILED_PRECONDITION Gemini API free tier is not available in your country. Please enable billing on your project in Google AI Studio. You are making a request in a region where the free tier is not supported, and you have not enabled billing on your project in Google AI Studio. To use the Gemini API, you will need to setup a paid plan using Google AI Studio . 403 PERMISSION_DENIED Your API key doesn't have the required permissions. You are using the wrong API key;  you\n    are trying to use a tuned model without going through proper authentication . Check that your API key is set and has the right access. And make sure to go through proper authentication to use tuned models. 404 NOT_FOUND The requested resource wasn't found. An image, audio, or video file referenced in your request was not found. Check if all parameters in your request are valid for your API version. 429 RESOURCE_EXHAUSTED You've exceeded the rate limit. You are sending too many requests per minute with the free tier Gemini API. Ensure you're within the model's rate limit . Request a quota increase if needed. 500 INTERNAL An unexpected error occurred on Google's side. Your input context is too long. Reduce your input context or temporarily switch to another model (e.g. from Gemini 1.5 Pro to Gemini 1.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the Send feedback button in Google AI Studio. 503 UNAVAILABLE The service may be temporarily overloaded or down. The service is temporarily running out of capacity. Temporarily switch to another model (e.g. from Gemini 1.5 Pro to Gemini 1.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the Send feedback button in Google AI Studio. 504 DEADLINE_EXCEEDED The service is unable to finish processing within the deadline. Your prompt (or context) is too large to be processed in time. Set a larger 'timeout' in your client request to avoid this error. Client SDK error codes Check your API calls for model parameter errors Ensure your model parameters are within the following values: Model parameter Values (range) Candidate count 1-8 (integer) Temperature 0.0-1.0 Max output tokens Use get_model ( Python )\n    to determine the maximum number of tokens for the model you are using. TopP 0.0-1.0 In addition to checking parameter values, make sure you're using the correct API version (e.g., /v1 or /v1beta ) and\nmodel that supports the features you need. For example, if a feature is in Beta\nrelease, it will only be available in the /v1beta API version. Check if you have the right model Ensure you are using a supported model listed on our models page . Safety issues If you see a prompt was blocked because of a safety setting in your API call,\nreview the prompt with respect to the filters you set in the API call. If you see BlockedReason.OTHER , the query or response may violate the terms\nof service or be otherwise unsupported. Recitation issue If you see the model stops generating output due to the RECITATION reason, this\nmeans the model output may resemble certain data. To fix this, try to make\nprompt / context as unique as possible and use a higher temperature. Improve model output For higher quality model outputs, explore writing more structured prompts. The introduction to prompt design page introduces\nsome basic concepts, strategies, and best practices to get you started. If you have hundreds of examples of good input/output pairs, you can also\nconsider model tuning . Understand token limits Read through our Token guide to better understand how\nto count tokens and their limits. Known issues The API supports only a number of select languages. Submitting prompts in\nunsupported languages can produce unexpected or even blocked responses. See available languages for updates. File a bug Join the discussion on the Google AI developer forum if you have questions. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-10-07 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/troubleshooting"
            ],
            "timestamp": "2024-12-27T22:47:38.513134",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
            "title": "Troubleshoot Google AI Studio  |  Gemini API  |  Google AI for Developers",
            "text_content": "Troubleshoot Google AI Studio  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Troubleshoot Google AI Studio This page provides suggestions for troubleshooting Google AI Studio if you\nencounter issues. Understand 403 Access Restricted errors If you see a 403 Access Restricted error, you are using Google AI Studio in a\nway that does not follow the Terms of Service . One common reason is\nyou are not located in a supported region . Resolve No Content responses on Google AI Studio A warning No Content message appears on\nGoogle AI Studio if the content is blocked for any reason. To see more details,\nhold the pointer over No Content and click warning Safety . If the response was blocked due to safety settings and\nyou considered the safety risks for your use case, you\ncan modify the safety settings to influence the returned response. If the response was blocked but not due to the safety settings, the query or\nresponse may violate the Terms of Service or be otherwise unsupported. Check token usage and limits When you have a prompt open, the Text Preview button at the bottom of the\nscreen shows the current tokens used for the content of your prompt and the\nmaximum token count for the model being used. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-05 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio"
            ],
            "timestamp": "2024-12-27T22:47:39.191265",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/workspace",
            "title": "Access Google AI Studio with your Workspace account  |  Gemini API  |  Google AI for Developers",
            "text_content": "Access Google AI Studio with your Workspace account  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Access Google AI Studio with your Workspace account All Google Workspace users have access to AI\nStudio by default. If you're a Workspace user and you want to get started with\nAI Studio, check out the AI Studio quickstart . Troubleshooting If access to AI Studio is disabled for your Google Workspace account, you might\nsee an error like the following: We are sorry, but you do not have access to Google AI Studio. Please contact\nyour Organization Administrator for access. If you think you should have access to AI Studio, contact your Workspace\nadministrator. Enable AI Studio for Workspace users Important: This section is intended for Google Workspace administrators. As a Google Workspace administrator, you can control who uses AI Studio: AI Studio is turned on by default for all editions. You can turn AI Studio off or on for sets of users across or within\norganizational units. Google Workspace for Education editions: Users under the age of 18 are\nrestricted from using AI Studio with their Google Workspace for\nEducation accounts. This is true even when the AI Studio setting is\non. For details, go to Control access to Google services by\nage . To enable or disable AI Studio for users in your organization, see Turn Google AI Studio on or off for users . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-12 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/workspace"
            ],
            "timestamp": "2024-12-27T22:47:39.825772",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/quota",
            "title": "Options for increasing your requests per minute (RPM) limit  |  Gemini API  |  Google AI for Developers",
            "text_content": "Options for increasing your requests per minute (RPM) limit  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Options for increasing your requests per minute (RPM) limit Each model variation has an associated rate limit (requests per minute, RPM).\nFor details on those rate limits, see Gemini models . If you're using the Gemini API with the Google AI SDKs and you'd like a higher\nrate limit, fill out this \"Request rate limit increase\" form with\ndetails about your project and use case. We offer no guarantees about increasing\nyour rate limit, but we'll do our best to review your request and reach out to\nyou if we're able to accommodate your capacity needs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-05 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quota"
            ],
            "timestamp": "2024-12-27T22:47:40.379110",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/available-regions",
            "title": "Available regions for Google AI Studio and Gemini API  |  Google AI for Developers",
            "text_content": "Available regions for Google AI Studio and Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Available regions for Google AI Studio and Gemini API If you reached this page after trying to open Google AI Studio , it may be because Google AI\nStudio is not available in your region, or you don't meet the age requirements\n(18+) for access. You can learn more about the available regions in the\nfollowing section and other requirements in the terms of service . Available regions Note: For Colab users - Region restrictions are applied based on\nthe region that the Colab instance is in, not the region that the user is in. You can check the location of the Colab instance using !curl ipinfo.io The Gemini API and Google AI Studio are available in the following countries and\nterritories. If you're not in one of these countries or territories, try the Gemini API in Vertex AI : Albania Algeria American Samoa Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan The Bahamas Bahrain Bangladesh Barbados Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia Botswana Brazil British Indian Ocean Territory British Virgin Islands Brunei Bulgaria Burkina Faso Burundi Cabo Verde Cambodia Cameroon Canada Caribbean Netherlands Cayman Islands Central African Republic Chad Chile Christmas Island Cocos (Keeling) Islands Colombia Comoros Cook Islands Côte d'Ivoire Costa Rica Croatia Curaçao Czech Republic Democratic Republic of the Congo Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Eswatini Ethiopia Falkland Islands (Islas Malvinas) Fiji Finland France Gabon The Gambia Georgia Germany Ghana Gibraltar Greece Grenada Guam Guatemala Guernsey Guinea Guinea-Bissau Guyana Haiti Heard Island and McDonald Islands Herzegovina Honduras Hungary Iceland India Indonesia Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jersey Jordan Kazakhstan Kenya Kiribati Kyrgyzstan Kuwait Laos Latvia Lebanon Lesotho Liberia Libya Liechtenstein Lithuania Luxembourg Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Mauritania Mauritius Mexico Micronesia Mongolia Montenegro Montserrat Morocco Mozambique Namibia Nauru Nepal Netherlands New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island North Macedonia Northern Mariana Islands Norway Oman Pakistan Palau Palestine Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Islands Poland Portugal Puerto Rico Qatar Republic of Cyprus Republic of the Congo Romania Rwanda Saint Barthélemy Saint Kitts and Nevis Saint Lucia Saint Pierre and Miquelon Saint Vincent and the Grenadines Saint Helena, Ascension and Tristan da Cunha Samoa São Tomé and Príncipe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia and the South Sandwich Islands South Korea South Sudan Spain Sri Lanka Sudan Suriname Sweden Switzerland Taiwan Tajikistan Tanzania Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia Türkiye Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Kingdom United Arab Emirates United States United States Minor Outlying Islands U.S. Virgin Islands Uruguay Uzbekistan Vanuatu Venezuela Vietnam Wallis and Futuna Western Sahara Yemen Zambia Zimbabwe Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-17 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/available-regions"
            ],
            "timestamp": "2024-12-27T22:47:41.318246",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
            "title": "Abuse monitoring  |  Gemini API  |  Google AI for Developers",
            "text_content": "Abuse monitoring  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Abuse monitoring Google is committed to the responsible development and use of AI. To ensure the\nsafety and integrity of the Gemini API, we have created these policy guidelines.\nBy using the Gemini API, you agree to the following guidelines, the Gemini API\nAdditional Terms of Service and Generative AI Prohibited\nUse Policy . How We Monitor for Misuse Google's Trust and Safety Team employs a combination of automated and manual\nprocesses to detect potential misuse of the Gemini API and enforce our policies. Automated Detection: Automated systems scan API usage for violations of\nour Prohibited Use Policy, such as hate speech, harassment, sexually\nexplicit content, and dangerous content. Manual Detection: If a project consistently exhibits suspicious\nactivity, it may be flagged for manual review by authorized Google\npersonnel. How We Handle Data To help with abuse monitoring, Google retains the following data for fifty-five\n(55) days: Prompts: The text prompts you submit to the API. Contextual Information: Any additional context you provide with your\nprompts. Output: The responses generated by the Gemini API. How We Investigate Potential Issues When prompts or model outputs are flagged by safety filters and abuse detection\nsystems described above, authorized Google employees may assess the flagged\ncontent, and either confirm or correct the classification or determination based\non predefined guidelines and policies. Data can be accessed for human review\nonly by authorized Google employees via an internal governance assessment and\nreview management platform. When data is logged for abuse monitoring, it is used\nsolely for the purpose of policy enforcement and is not used to train or\nfine-tune any AI/ML models. Working with You on Policy Compliance If your use of Gemini doesn't align with our policies, we may take the following\nsteps: Get in touch: We may reach out to you through email to understand your\nuse case and explore ways to bring your usage into compliance. Temporary usage limits: We may limit your access to the Gemini API. Temporary suspension: We may temporarily pause your access to the Gemini\nAPI. Account closure: As a last resort, and for serious violations, we may\npermanently close your access to the Gemini API and other Google services. Scope These policy guidelines apply to the use of the Gemini API and AI Studio. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-13 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring"
            ],
            "timestamp": "2024-12-27T22:47:42.065009",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/prompting_with_media",
            "title": "File prompting strategies  |  Gemini API  |  Google AI for Developers",
            "text_content": "File prompting strategies  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback File prompting strategies The Gemini family of artificial intelligence (AI) models is built to handle\nvarious types of input data, including text, images, and audio. Since these\nmodels can handle more more than one type or mode of data, the Gemini models\nare called multimodal models or explained as having multimodal capabilities . Note: Prompting with media files is supported by specific Gemini models.\nFor more information about which models support this feature, see the Vision and Audio guides. This page provides guidance and best practices for using media files with\nprompts for the Gemini API. For information on using the Gemini Files API for\nuploading media files and including them in your prompts, see the Vision , Audio , and Document processing guides. Using media files with prompts Being able to use various types of data in your prompts gives you more\nflexibility in terms of what tasks you can tackle with the Gemini API. For\nexample, you can send the model a photo of a delicious meal and ask it to write\na short blog about the meal. Prompt Response Write a short, engaging blog post based on this picture. It should\n      include a description of the meal in the photo and talk about my journey\n      meal prepping. Meal prepping is a great way to save time and money, and it can also\n      help you to eat healthier. This meal is a great example of a healthy\n      and delicious meal that can be easily prepped ahead of time. This meal features brown rice, roasted vegetables, and chicken teriyaki.\n      The brown rice is a healthy and complex carbohydrate that will give you\n      sustained energy. The roasted vegetables are a great way to get your daily\n      dose of vitamins and minerals, and the chicken teriyaki is a delicious and\n      protein-rich option. This meal is also very easy to prepare. Simply cook the brown rice, roast\n      the vegetables, and cook the chicken teriyaki. Once everything is cooked,\n      divide it into meal prep containers and store them in the refrigerator.\n      You can then grab a container and go on busy mornings! If you're looking for a healthy and delicious meal that can be easily\n      prepped ahead of time, this meal is a great option. It's packed with\n      nutrients and flavor, and it's sure to keep you feeling full and satisfied. Here's to healthy and delicious meal prepping! Strategies for prompting with media If you are having trouble getting the output you want from prompts that use\nmedia files, there are some strategies that can help you get the results you\nwant. The following sections provide design approaches and troubleshooting\ntips for improving prompts that use multimodal input. You can improve your multimodal prompts by following these best practices: Prompt design fundamentals Be specific in your instructions : Craft clear and concise instructions that leave minimal room for misinterpretation. Add a few examples to your prompt: Use realistic few-shot examples to illustrate what you want to achieve. Break it down step-by-step : Divide complex tasks into manageable sub-goals, guiding the model through the process. Specify the output format : In your prompt, ask for the output to be in the format you want, like markdown, JSON, HTML and more. Put your image first for single-image prompts : While Gemini can handle image and text inputs in any order, for prompts containing a single image, it might perform better if that image is placed before the text prompt. Troubleshooting your multimodal prompt If the model is not drawing information from the relevant part of the image: Drop hints with which aspects of the image you want the prompt to draw information from. If the model output is too generic (not tailored enough to the image): At the start of the prompt, try asking the model to describe the image(s) before providing the task instruction, or try asking the model to refer to what's in the image. Prompt design fundamentals This section expands on the best practices listed in the previous section. Note: Model results from the following Gemini examples can vary from run to run. Be specific in your instructions Prompts have the most success when they are clear and detailed. If you have a specific output in mind, it's better to include that requirement in the prompt to ensure you get the output you want. Sometimes, a prompt's intent might seem clear to the person who wrote it, but it is actually under-specified. To customize model behavior, consider how your prompt could be (mis)interpreted, and ensure that the instructions you give are specific and clear. Remember to leave minimal room for misinterpretation For this image of an airport board, asking the model to just \"describe this image\" could generate a general description. If you need the model to parse the time and city from the image, you can include that request directly in your prompt. Prompt Model response Describe this image. The image shows an airport arrivals and departures board. Updated prompt Improved response Parse the time and city from the airport board shown in this image into a list. 10:50 Moscow 11:05 Edinburgh 11:05 London 11:10 Bucharest 11:30 Kiev 11:35 Dublin 11:45 East Midlands 12:15 Sofia 12:30 London 12:30 Newcastle 12:40 St Petersburg 12:40 London 12:45 Manchester Add a few examples The Gemini model can accept multiple inputs which it can use as examples to understand the\noutput you want. Adding these examples can help the model identify the patterns and apply the\n  relationship between the given images and responses to the new example. This is also called\n  \"few-shot\" learning. In the example below, the initial output is written in sentence form, and also contains the\ncountry (Brazil). Suppose you want the output in a different format or style, and you want\nonly the city, not the country. Adding few-shot examples to your prompt can steer the model\nto respond in the way you want. Prompt Model response Determine the city along with the landmark. The landmark is the Christ the Redeemer statue in Rio de Janeiro, Brazil. Updated prompt Improved response Determine the city along with the landmark. city: Rome, landmark: the Colosseum. city: Beijing, landmark: Forbidden City city: Rio de Janeiro, landmark: Christ the Redeemer statue Break it down step-by-step For complex tasks like the ones that require both visual understanding and reasoning, it can be helpful\nto split the task into smaller, more straightforward steps. Alternatively, it could also be effective if you\ndirectly ask the model to “think step by step” in your prompt. Prompt Model response When will I run out of toilet paper? Soon, you only have 3 rolls left. Updated prompt Improved response 1. First, count how many toilet paper rolls are in this picture. 2. Then, determine how much toilet paper a typical person uses per day. 3. Calculate how long these rolls of toilet paper will last. 1. There are 3 rolls of toilet paper in this picture. 2. A typical person uses about 20 sheets of toilet paper per day. 3. If each roll contains 200 sheets, then each roll will last for about 10 days. Therefore, the 3 rolls will last for about a month. Math problems or other types of word problems are great candidates for asking the model to think step-by-step. Prompt Response What is the 4th term in the sequence? -135 The response from the model is incorrect. Some ways to improve this is to ask the model to parse the problem before answering the prompt. Prompt Response Parse the formula in the image first. Then based on the formula, answer \"what's the 4th term in the sequence?\". Click to expand the result $$ b(n) = b(n-1) * (-3) $$ $$ b(1) = 15 $$ Substitute n = 4 into the formula: $$ b(4) = b(4-1) * (-3) $$ $$ b(4) = b(3) * (-3) $$ We don't know the value of b(3), so substitute n = 3 into the formula: $$ b(3) = b(3-1) * (-3) $$ $$ b(3) = b(2) * (-3) $$ We don't know the value of b(2), so substitute n = 2 into the formula: $$ b(2) = b(2-1) * (-3) $$ $$ b(2) = b(1) * (-3) $$ We know the value of b(1) is 15, so substitute it into the formula: $$ b(2) = 15 * (-3) $$ $$ b(2) = -45 $$ Now that we know the value of b(2), substitute it into the formula for b(3): $$ b(3) = b(2) * (-3) $$ $$ b(3) = -45 * (-3) $$ $$ b(3) = 135 $$ Now that we know the value of b(3), substitute it into the formula for b(4): $$ b(4) = b(3) * (-3) $$ $$ b(4) = 135 * (-3) $$ $$ b(4) = -405 $$ Therefore, the 4th term in the sequence is -405. This model response is correct - now to make the response even more streamlined, you can ask the model to to think step-by-step. Updated prompt Improved response What is the 4th term in the sequence? Think step by step. The first term is 15. The second term is b(1) * (-3) = 15 * (-3) = -45. The third term is b(2) * (-3) = -45 * (-3) = 135. The fourth term is b(3) * (-3) = 135 * (-3) = -405. Try specifying the output format A common problem is the need for model output to be in a certain format (e.g. markdown, JSON, HTML), especially if the model output needs to\n  be ingested by a downstream task. You can try to produce output in that format by instructing the model to do so within the prompt. Below are two examples: Prompt Model response Parse the table in this image into markdown format | Subject | Status | Time | Start date | Due on | | ------- | ------ | ---- | --------- | ------ | | Math | Done | 1.5 hrs | 11/29/2023 | 12/1/2023 | | Geography | Done | 30 min | 12/2/2023 | 12/4/2023 | | Physics | In progress | 2 hrs | 12/3/2023 | 12/5/2023 | | Biology | Not started | 6 hrs | 11/2/2016 | 12/6/2023 | Prompt Model response Provide a list of all the following attributes: ingredients, type of cuisine, vegetarian or not, in JSON format ```json``` { \"ingredients\": [ \"rice\", \"shrimp\", \"clams\", \"mussels\", \"peas\", \"tomatoes\", \"onions\", \"garlic\", \"olive oil\", \"paprika\", \"salt\", \"pepper\" ], \"type of cuisine\": \"Spanish\", \"vegetarian\": false Put your image first for single-image prompts While Gemini can interpret image and text in any order within a prompt, placing a single image\nbefore the text prompt might lead to better results. In the following samples, you'll notice the image\ncomes first before the text in each prompt. Troubleshooting your multimodal prompt You might need to troubleshoot your prompt if you are not getting a helpful response. Here are a few strategies you could try. If the model is not drawing information from the relevant part of the image To get a more specific response, you can point out which aspects of the image you want the prompt to\ndraw information from to inform its response. Prompt Model response How many days will these diapers last a baby? The diapers will last for 198 days before they run out. Updated prompt Improved response How long will these diapers last before I run out? Use the weight shown on the box to determine the child's age, and use the total number of diapers in the box. Divide the total number by how many diapers the child goes through per day. The box contains 198 diapers. The box states the diapers are for a child weighing 22-37 lbs. The average child in this weight range is likely between 1 and 2 years old. A 2-year-old typically goes through 6 diapers per day, therefore, the diapers will last around 33 days (198/6). If the model output is too generic and not tailored enough to the image input To help the model tailor its response to the image(s), try asking it to describe the images before performing its reasoning task. Prompt Model response What is in common between these images? All four images are examples of landscape photography. Updated prompt Improved response First, describe what's in each image in detail. What's in common between these images? The first image shows a person packing a suitcase. The second image shows an empty hallway in a shopping mall. The third image shows a roll of toilet paper. The fourth image shows a dark and stormy sea. The commonality between these images is that they are all examples of liminal spaces. Liminal spaces are places or situations that are in between two other places or situations. They are often characterized by a sense of uncertainty or ambiguity. The first image shows the person in between being at home and being on a trip. The second image shows the hallway in between two stores. The third image is of toilet paper, which can represent a bathroom. A bathroom is a transitional space between private\n        and public realms. It's a place where we prepare ourselves to enter or leave the outside world. The fourth image shows the sea in between being calm and being stormy. Liminal spaces can be both exciting and unsettling. They can be a place of new beginning or a place of endings. They can be a place of reflection or a place of action. You can also try explicitly asking the model to refer to what's in the image. Updated prompt Improved response What is in common between these images? Refer to what's in the images in your response. All of the images are related to travel. The first image shows a person packing a suitcase, the second image shows\n          an empty hallway in a shopping mall, the third image shows a stack of toilet paper, and the fourth image shows a\n          dark and stormy sea. These images are all related to travel because they are all things that people need or use when\n          they travel. The first image shows a person packing a suitcase, which is something that people do when they are\n          going on a trip. The second image shows an empty hallway in a shopping mall, which is a place where people\n          often go to buy things for their trips. The third image shows a stack of toilet paper, which is something\n          that people often use when they are traveling. The fourth image shows a dark and stormy sea, which is\n          something that people often encounter when they are traveling. The same tips work for creative output from images. In the following example with\n  the rabbit, explicitly asking the model to include aspects from the image can improve the output. Prompt Model response Can you write me a poem about this image? Little bunny, so soft and brown, Hopping through the field, up and down. Your ears so long, your tail so small, You're the cutest creature of them all. You nibble on the grass, so green and sweet, And hop away when you see me. But I'll always be your friend, Little bunny, until the very end. Updated prompt Improved response Can you write me a descriptive and dramatic poem about this image and include the location? Little bunny on the green, Ears so long, so keen. What do you see out there, In the distance, where you stare? Is it the ocean blue, Sparkling in the sun's hue? Or the waves that crash and roar, A rhythm that you adore? Little bunny, tell me true, What captures your attention so? Is it the salty breeze, Or the seagulls' cries that please? Perhaps it's the vastness of it all, A world so big and small. A place where dreams can roam, And adventures can be sown. Little bunny, I wonder what you dream, As you sit on the grass, so serene. Do you yearn to explore the deep, Or stay on land, where you can leap? Whatever it may be, little bunny, Keep that spark of wonder burning brightly. For in your dreams and aspirations, Lies a world waiting for your creation. Troubleshooting which part of the prompt failed It can be hard to know whether a prompt failed because the model\ndidn't understand the image to begin with, or if it did understand the image but\ndid not perform the correct reasoning steps afterward. To disambiguate between those reasons, ask the model to describe what's in the image. In this example below, if the model responds with a snack that seems surprising when paired with\ntea (e.g. popcorn), you can first troubleshoot to determine whether the model correctly recognized\nthat the image contains tea. Prompt Prompt for troubleshooting What's a snack I can make in 1 minute that would go well with this? Describe what's in this image. Another strategy is to ask the model to explain its reasoning. That can help you narrow down which part of the reasoning broke down, if any. Prompt Prompt for troubleshooting What's a snack I can make in 1 minute that would go well with this? What's a snack I can make in 1 minute that would go well with this? Please explain why. Tuning the sampling parameters In each request, you send not only the multimodal prompt but a set of sampling parameters to the model.\nThe model can generate different results for different parameter values. Experiment with the different\nparameters to get the best values for the task. The most commonly adjusted parameters are the following: Temperature top-P top-K Temperature Temperature is used for sampling during response generation, which occurs when top-P and top-K are applied.\nTemperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that\nrequire a more deterministic and less open-ended or creative response, while higher temperatures can lead to more\ndiverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response\nis always selected. For most use cases, try starting with a temperature of 0.4. If you need more creative results, try increasing the\ntemperature. If you observe clear hallucinations, try reducing the temperature. Top-K Top-K changes how the model selects tokens for output. A top-K of 1 means the next selected token is\nthe most probable among all tokens in the model's vocabulary (also called greedy decoding), while\na top-K of 3 means that the next token is selected from among the three most probable tokens by\nusing temperature. For each token selection step, the top-K tokens with the highest probabilities are sampled. Then\ntokens are further filtered based on top-P with the final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more random responses.\nThe default value of top-K is 32. Top-P Top-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K)\nto least probable until the sum of their probabilities equals the top-P value. For example, if tokens\nA, B, and C have a probability of 0.6, 0.3, 0.1 and the top-P value is  0.9, then the model will\nselect either A or B as the next token by using temperature and excludes C as a candidate. Specify a lower value for less random responses and a higher value for more random responses.\nThe default value of top-P is 1.0. Next steps Try writing your own multimodal prompts using Google AI\nStudio . For more guidance on prompt design, see the Prompt strategies page. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-21 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media"
            ],
            "timestamp": "2024-12-27T22:47:43.266951",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/api-versions",
            "title": "API versions explained  |  Gemini API  |  Google AI for Developers",
            "text_content": "API versions explained  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview API versions Capabilities Models Generating content Multimodal Live Tokens Files Caching Embeddings Tuning Tuning Permissions Semantic retrieval Question answering Corpus Document Chunk Permissions All methods Deprecated PaLM (decomissioned) Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models API Reference Send feedback API versions explained This document provides a high-level overview of the differences between the v1 and v1beta versions of the Gemini API. v1 : Stable version of the API. Features in the stable version are\nfully-supported over the lifetime of the major version. If there are any\nbreaking changes, then the next major version of the API will be created and\nthe existing version will be deprecated after a reasonable period of time.\nNon-breaking changes may be introduced to the API without changing the major\nversion. v1beta : This version includes early-access features that may be under\ndevelopment and is subject to rapid and breaking changes. There is also no\nguarantee that the features in the Beta version will move to the stable\nversion. Due to this instability, you should consider not launching production\napplications with this version. Feature v1 v1beta Generate Content - Text-only input Generate Content - Text-and-image input Generate Content - Text output Generate Content - Multi-turn conversations (chat) Generate Content - Function calls Generate Content - Streaming Embed Content - Text-only input Generate Answer Semantic retriever - Supported - Will never be supported Configure API version in an SDK The Gemini API SDK's default to v1beta , but you can opt to use v1 instead\nby setting the api version as shown in the following code sample: Python The Python SDK, google.generativeai , uses v1beta and does not provide\na method for switching the API version. The low-level Python client library, google.ai.generativelanguage exposes\nother versions as separate modules: google.ai.generativelanguage_v1 and google.ai.generativelanguage_v1beta . The default is v1beta . import google.ai.generativelanguage_v1 as glm client = glm . GenerativeServiceClient ( client_options = dict ( api_key = YOUR_API_KEY )) response = client . generate_content ({ \"model\" : \"models/gemini-1.5-flash\" , \"contents\" : [ { \"parts\" : [ { \"text\" : \"Explain how AI works\" }]}] }) print ( type ( response ) . to_dict ( response )) Node.js const { GoogleGenerativeAI } = require ( \"@google/generative-ai\" ); const genAI = new GoogleGenerativeAI ( \" YOUR_API_KEY \" ); const model = genAI . getGenerativeModel ({ model : 'gemini-1.5-flash' }, { apiVersion : 'v1' }); const prompt = \"Explain how AI works\" ; const result = await model . generateContent ( prompt ); console . log ( result . response . text ()); REST curl \"https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key= YOUR_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d '{ \"contents\": [{ \"parts\":[{\"text\": \"Write a story about a magic backpack.\"}] }] }' Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-29 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/api-versions"
            ],
            "timestamp": "2024-12-27T22:47:43.825809",
            "status_code": 200
        }
    ]
}