---





















# GEMINI DOCS

# Audio understanding

{
    "base_url": "https://ai.google.dev/gemini-api/docs/audio?lang=python",
    "crawl_date": "2024-12-29T12:28:50.108574",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/audio?lang=python",
            "title": "Explore audio capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore audio capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore audio capabilities with the Gemini API Python Node.js Go REST Gemini can respond to prompts about audio. For example, Gemini can: Describe, summarize, or answer questions about audio content. Provide a transcription of the audio. Provide answers or a transcription about a specific segment of the audio. Note: You can't generate audio output with the Gemini API. This guide demonstrates different ways to interact with audio files and audio\ncontent using the Gemini API. Supported audio formats Gemini supports the following audio format MIME types: WAV - audio/wav MP3 - audio/mp3 AIFF - audio/aiff AAC - audio/aac OGG Vorbis - audio/ogg FLAC - audio/flac Technical details about audio Gemini imposes the following rules on audio: Gemini represents each second of audio as 25 tokens; for example,\none minute of audio is represented as 1,500 tokens. Gemini can only infer responses to English-language speech. Gemini can \"understand\" non-speech components, such as birdsong or sirens. The maximum supported length of audio data in a single prompt is 9.5 hours.\nGemini doesn't limit the number of audio files in a single prompt; however,\nthe total combined length of all audio files in a single prompt cannot exceed\n9.5 hours. Gemini downsamples audio files to a 16 Kbps data resolution. If the audio source contains multiple channels, Gemini combines those channels\ndown to a single channel. Before you begin: Set up your project and API key Before calling the Gemini API, you need to set up your project and configure\nyour API key. Expand to view how to set up your project and API key Tip: For complete setup instructions, see the Gemini API quickstart . Get and secure your API key You need an API key to call the Gemini API. If you don't already have one,\ncreate a key in Google AI Studio. Get an API key It's strongly recommended that you do not check an API key into your version\ncontrol system. You should store your API key in a secrets store such as Google Cloud Secret Manager . This tutorial assumes that you're accessing your API key as an environment\nvariable. Install the SDK package and configure your API key Note: This section shows setup steps for a local Python environment. To install\n      dependencies and configure your API key for Colab, see the Authentication quickstart notebook The Python SDK for the Gemini API is contained in the google-generativeai package. Install the dependency using pip: pip install -U google-generativeai Import the package and configure the service with your API key: import os import google.generativeai as genai genai . configure ( api_key = os . environ [ 'API_KEY' ]) Make an audio file available to Gemini You can make an audio file available to Gemini in either of the following ways: Upload the audio file prior to making the prompt request. Provide the audio file as inline data to the prompt request. Upload an audio file and generate content You can use the File API to upload an audio file of any size. Always use the\nFile API when the total request size (including the files, text prompt, system\ninstructions, etc.) is larger than 20 MB. Note: The File API lets you store up to 20 GB of files per project, with a\nper-file maximum size of 2 GB. Files are stored for 48 hours. They can be\naccessed in that period with your API key, but cannot be downloaded from the\nAPI. The File API is available at no cost in all regions where the Gemini API is\navailable. Call media.upload to upload a file using the\nFile API. The following code uploads an audio file and then uses the file in a\ncall to models.generateContent . import google.generativeai as genai myfile = genai . upload_file ( media / \"sample.mp3\" ) print ( f \" { myfile =} \" ) model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) result = model . generate_content ([ myfile , \"Describe this audio clip\" ]) print ( f \" { result . text =} \" ) files . py Get metadata for a file You can verify the API successfully stored the uploaded file and get its\nmetadata by calling files.get . import google.generativeai as genai myfile = genai . upload_file ( media / \"poem.txt\" ) file_name = myfile . name print ( file_name ) # \"files/*\" myfile = genai . get_file ( file_name ) print ( myfile ) files . py List uploaded files You can upload multiple audio files (and other kinds of files).\nThe following code generates a list of all the files uploaded: import google.generativeai as genai print ( \"My files:\" ) for f in genai . list_files (): print ( \"  \" , f . name ) files . py Delete uploaded files Files are automatically deleted after 48 hours. Optionally, you can manually\ndelete an uploaded file. For example: import google.generativeai as genai myfile = genai . upload_file ( media / \"poem.txt\" ) myfile . delete () try : # Error. model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) result = model . generate_content ([ myfile , \"Describe this file.\" ]) except google . api_core . exceptions . PermissionDenied : pass files . py Provide the audio file as inline data in the request Instead of uploading an audio file, you can pass audio data in the\nsame call that contains the prompt. Then, pass that downloaded small audio file along with the prompt to Gemini: # Initialize a Gemini model appropriate for your use case. model = genai . GenerativeModel ( 'models/gemini-1.5-flash' ) # Create the prompt. prompt = \"Please summarize the audio.\" # Load the samplesmall.mp3 file into a Python Blob object containing the audio # file's bytes and then pass the prompt and the audio to Gemini. response = model . generate_content ([ prompt , { \"mime_type\" : \"audio/mp3\" , \"data\" : pathlib . Path ( 'samplesmall.mp3' ) . read_bytes () } ]) # Output Gemini's response to the prompt and the inline audio. print ( response . text ) Note the following about providing audio as inline data: The maximum request size is 20 MB, which includes text prompts,\nsystem instructions, and files provided inline. If your file's\nsize will make the total request size exceed 20 MB, then use the File API to upload files for use in requests. If you're using an audio sample multiple times, it is more efficient\nto use the File API . More ways to work with audio This section provides a few additional ways to get more from audio. Get a transcript of the audio file To get a transcript, just ask for it in the prompt. For example: # Initialize a Gemini model appropriate for your use case. model = genai . GenerativeModel ( model_name = \"gemini-1.5-flash\" ) # Create the prompt. prompt = \"Generate a transcript of the speech.\" # Pass the prompt and the audio file to Gemini. response = model . generate_content ([ prompt , audio_file ]) # Print the transcript. print ( response . text ) Refer to timestamps in the audio file A prompt can specify timestamps of the form MM:SS to refer to particular\nsections in an audio file. For example, the following prompt requests\na transcript that: Starts at 2 minutes 30 seconds from the beginning of the file. Ends at 3 minutes 29 seconds from the beginning of the file. # Create a prompt containing timestamps. prompt = \"Provide a transcript of the speech from 02:30 to 03:29.\" Count tokens Call the countTokens method to get a\ncount of the number of tokens in the audio file. For example: model . count_tokens ([ audio_file ]) What's next This guide shows how to upload audio files using the File API and then generate\ntext outputs from audio inputs. To learn more, see the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs/audio?lang=python",
                "https://ai.google.dev/gemini-api/docs/audio?lang=python",
                "https://ai.google.dev/gemini-api/docs/audio?lang=python",
                "https://ai.google.dev/gemini-api/docs/audio?lang=python",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio?lang=python"
            ],
            "timestamp": "2024-12-29T12:28:48.888334",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/audio",
            "title": "Explore audio capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore audio capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore audio capabilities with the Gemini API Python Node.js Go REST Gemini can respond to prompts about audio. For example, Gemini can: Describe, summarize, or answer questions about audio content. Provide a transcription of the audio. Provide answers or a transcription about a specific segment of the audio. Note: You can't generate audio output with the Gemini API. This guide demonstrates different ways to interact with audio files and audio\ncontent using the Gemini API. Supported audio formats Gemini supports the following audio format MIME types: WAV - audio/wav MP3 - audio/mp3 AIFF - audio/aiff AAC - audio/aac OGG Vorbis - audio/ogg FLAC - audio/flac Technical details about audio Gemini imposes the following rules on audio: Gemini represents each second of audio as 25 tokens; for example,\none minute of audio is represented as 1,500 tokens. Gemini can only infer responses to English-language speech. Gemini can \"understand\" non-speech components, such as birdsong or sirens. The maximum supported length of audio data in a single prompt is 9.5 hours.\nGemini doesn't limit the number of audio files in a single prompt; however,\nthe total combined length of all audio files in a single prompt cannot exceed\n9.5 hours. Gemini downsamples audio files to a 16 Kbps data resolution. If the audio source contains multiple channels, Gemini combines those channels\ndown to a single channel. What's next This guide shows how to upload audio files using the File API and then generate\ntext outputs from audio inputs. To learn more, see the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio"
            ],
            "timestamp": "2024-12-29T12:28:50.075704",
            "status_code": 200
        }
    ]
}

# Context caching 

{
    "base_url": "https://ai.google.dev/gemini-api/docs/caching?lang=python",
    "crawl_date": "2024-12-29T12:35:10.259679",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/caching?lang=python",
            "title": "Context caching  |  Gemini API  |  Google AI for Developers",
            "text_content": "Context caching  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Context caching Python Node.js Go REST In a typical AI workflow, you might pass the same input tokens over and over to\na model. Using the Gemini API context caching feature, you can pass some content\nto the model once, cache the input tokens, and then refer to the cached tokens\nfor subsequent requests. At certain volumes, using cached tokens is lower cost\nthan passing in the same corpus of tokens repeatedly. When you cache a set of tokens, you can choose how long you want the cache to\nexist before the tokens are automatically deleted. This caching duration is\ncalled the time to live (TTL). If not set, the TTL defaults to 1 hour. The\ncost for caching depends on the input token size and how long you want the\ntokens to persist. Context caching supports both Gemini 1.5 Pro and Gemini 1.5 Flash. Note: Context caching is only available for stable models with fixed versions\n(for example, gemini-1.5-pro-001 ). You must include the version postfix (for\nexample, the -001 in gemini-1.5-pro-001 ). When to use context caching Context caching is particularly well suited to scenarios where a substantial\ninitial context is referenced repeatedly by shorter requests. Consider using\ncontext caching for use cases such as: Chatbots with extensive system instructions Repetitive analysis of lengthy video files Recurring queries against large document sets Frequent code repository analysis or bug fixing How caching reduces costs Context caching is a paid feature designed to reduce overall operational costs.\nBilling is based on the following factors: Cache token count: The number of input tokens cached, billed at a\nreduced rate when included in subsequent prompts. Storage duration: The amount of time cached tokens are stored (TTL),\nbilled based on the TTL duration of cached token count. There are no minimum\nor maximum bounds on the TTL. Other factors: Other charges apply, such as for non-cached input tokens\nand output tokens. For up-to-date pricing details, refer to the Gemini API pricing\npage . To learn how to count tokens, see the Token\nguide . How to use context caching This section assumes that you've installed a Gemini SDK (or have curl installed)\nand that you've configured an API key, as shown in the quickstart . Generate content using a cache The following example shows how to generate content using a cached system\ninstruction and video file. import os import google.generativeai as genai from google.generativeai import caching import datetime import time # Get your API key from https://aistudio.google.com/app/apikey # and access your API key as an environment variable. # To authenticate from a Colab, see # https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb genai . configure ( api_key = os . environ [ 'API_KEY' ]) # Download video file # curl -O https://storage.googleapis.com/generativeai-downloads/data/Sherlock_Jr_FullMovie.mp4 path_to_video_file = 'Sherlock_Jr_FullMovie.mp4' # Upload the video using the Files API video_file = genai . upload_file ( path = path_to_video_file ) # Wait for the file to finish processing while video_file . state . name == 'PROCESSING' : print ( 'Waiting for video to be processed.' ) time . sleep ( 2 ) video_file = genai . get_file ( video_file . name ) print ( f 'Video processing complete: { video_file . uri } ' ) # Create a cache with a 5 minute TTL cache = caching . CachedContent . create ( model = 'models/gemini-1.5-flash-001' , display_name = 'sherlock jr movie' , # used to identify the cache system_instruction = ( 'You are an expert video analyzer, and your job is to answer ' 'the user \\' s query based on the video file you have access to.' ), contents = [ video_file ], ttl = datetime . timedelta ( minutes = 5 ), ) # Construct a GenerativeModel which uses the created cache. model = genai . GenerativeModel . from_cached_content ( cached_content = cache ) # Query the model response = model . generate_content ([( 'Introduce different characters in the movie by describing ' 'their personality, looks, and names. Also list the timestamps ' 'they were introduced for the first time.' )]) print ( response . usage_metadata ) # The output should look something like this: # # prompt_token_count: 696219 # cached_content_token_count: 696190 # candidates_token_count: 214 # total_token_count: 696433 print ( response . text ) List caches It's not possible to retrieve or view cached content, but you can retrieve\ncache metadata ( name , model , display_name , usage_metadata , create_time , update_time , and expire_time ). To list metadata for all uploaded caches, use CachedContent.list() : for c in caching . CachedContent . list (): print ( c ) Update a cache You can set a new ttl or expire_time for a cache. Changing anything else\nabout the cache isn't supported. The following example shows how to update the ttl of a cache using CachedContent.update() . import datetime cache . update ( ttl = datetime . timedelta ( hours = 2 )) Delete a cache The caching service provides a delete operation for manually removing content\nfrom the cache. The following example shows how to delete a cache using CachedContent.delete() . cache . delete () Additional considerations Keep the following considerations in mind when using context caching: The minimum input token count for context caching is 32,768, and the maximum is the same as the maximum for the given model. (For more on\ncounting tokens, see the Token guide ). The model doesn't make any distinction between cached tokens and regular\ninput tokens. Cached content is simply a prefix to the prompt. There are no special rate or usage limits on context caching; the standard\nrate limits for GenerateContent apply, and token limits include cached\ntokens. The number of cached tokens is returned in the usage_metadata from the\ncreate, get, and list operations of the cache service, and also in GenerateContent when using the cache. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-10-16 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use Context Caching in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/caching?lang=python",
                "https://ai.google.dev/gemini-api/docs/caching?lang=python",
                "https://ai.google.dev/gemini-api/docs/caching?lang=python",
                "https://ai.google.dev/gemini-api/docs/caching?lang=python",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching?lang=python"
            ],
            "timestamp": "2024-12-29T12:35:09.619288",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/caching",
            "title": "Context caching  |  Gemini API  |  Google AI for Developers",
            "text_content": "Context caching  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Context caching Python Node.js Go REST In a typical AI workflow, you might pass the same input tokens over and over to\na model. Using the Gemini API context caching feature, you can pass some content\nto the model once, cache the input tokens, and then refer to the cached tokens\nfor subsequent requests. At certain volumes, using cached tokens is lower cost\nthan passing in the same corpus of tokens repeatedly. When you cache a set of tokens, you can choose how long you want the cache to\nexist before the tokens are automatically deleted. This caching duration is\ncalled the time to live (TTL). If not set, the TTL defaults to 1 hour. The\ncost for caching depends on the input token size and how long you want the\ntokens to persist. Context caching supports both Gemini 1.5 Pro and Gemini 1.5 Flash. Note: Context caching is only available for stable models with fixed versions\n(for example, gemini-1.5-pro-001 ). You must include the version postfix (for\nexample, the -001 in gemini-1.5-pro-001 ). When to use context caching Context caching is particularly well suited to scenarios where a substantial\ninitial context is referenced repeatedly by shorter requests. Consider using\ncontext caching for use cases such as: Chatbots with extensive system instructions Repetitive analysis of lengthy video files Recurring queries against large document sets Frequent code repository analysis or bug fixing How caching reduces costs Context caching is a paid feature designed to reduce overall operational costs.\nBilling is based on the following factors: Cache token count: The number of input tokens cached, billed at a\nreduced rate when included in subsequent prompts. Storage duration: The amount of time cached tokens are stored (TTL),\nbilled based on the TTL duration of cached token count. There are no minimum\nor maximum bounds on the TTL. Other factors: Other charges apply, such as for non-cached input tokens\nand output tokens. For up-to-date pricing details, refer to the Gemini API pricing\npage . To learn how to count tokens, see the Token\nguide . How to use context caching This section assumes that you've installed a Gemini SDK (or have curl installed)\nand that you've configured an API key, as shown in the quickstart . Additional considerations Keep the following considerations in mind when using context caching: The minimum input token count for context caching is 32,768, and the maximum is the same as the maximum for the given model. (For more on\ncounting tokens, see the Token guide ). The model doesn't make any distinction between cached tokens and regular\ninput tokens. Cached content is simply a prefix to the prompt. There are no special rate or usage limits on context caching; the standard\nrate limits for GenerateContent apply, and token limits include cached\ntokens. The number of cached tokens is returned in the usage_metadata from the\ncreate, get, and list operations of the cache service, and also in GenerateContent when using the cache. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-10-16 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use Context Caching in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching"
            ],
            "timestamp": "2024-12-29T12:35:10.233566",
            "status_code": 200
        }
    ]
}

# Document understanding 

{
    "base_url": "https://ai.google.dev/gemini-api/docs/document-processing?lang=python",
    "crawl_date": "2024-12-29T12:30:49.493857",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/document-processing?lang=python",
            "title": "Explore document processing capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore document processing capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore document processing capabilities with the Gemini API Python Node.js Go REST The Gemini API supports PDF input, including long documents (up to 3600 pages).\nGemini models process PDFs with native vision, and are therefore able to\nunderstand both text and image contents inside documents. With native PDF vision\nsupport, Gemini models are able to: Analyze diagrams, charts, and tables inside documents. Extract information into structured output formats. Answer questions about visual and text contents in documents. Summarize documents. Transcribe document content (e.g. to HTML) preserving layouts and formatting, for use in downstream applications (such as in RAG pipelines). This tutorial demonstrates some possible ways to use the Gemini API with PDF\ndocuments. All output is text-only. Before you begin: Set up your project and API key Before calling the Gemini API, you need to set up your project and configure\nyour API key. Expand to view how to set up your project and API key Tip: For complete setup instructions, see the Gemini API quickstart . Get and secure your API key You need an API key to call the Gemini API. If you don't already have one,\ncreate a key in Google AI Studio. Get an API key It's strongly recommended that you do not check an API key into your version\ncontrol system. You should store your API key in a secrets store such as Google Cloud Secret Manager . This tutorial assumes that you're accessing your API key as an environment\nvariable. Install the SDK package and configure your API key Note: This section shows setup steps for a local Python environment. To install\n      dependencies and configure your API key for Colab, see the Authentication quickstart notebook The Python SDK for the Gemini API is contained in the google-generativeai package. Install the dependency using pip: pip install -U google-generativeai Import the package and configure the service with your API key: import os import google.generativeai as genai genai . configure ( api_key = os . environ [ 'API_KEY' ]) Prompting with PDFs This guide demonstrates how to upload and process PDFs\nusing the File API or by including them as inline data. Technical details Gemini 1.5 Pro and 1.5 Flash support a maximum of 3,600 document pages. Document\npages must be in one of the following text data MIME types: PDF - application/pdf JavaScript - application/x-javascript , text/javascript Python - application/x-python , text/x-python TXT - text/plain HTML - text/html CSS - text/css Markdown - text/md CSV - text/csv XML - text/xml RTF - text/rtf Each document page is equivalent to 258 tokens. While there are no specific limits to the number of pixels in a document besides\nthe model's context window, larger pages are scaled down to a maximum resolution\nof 3072x3072 while preserving their original aspect ratio, while smaller pages\nare scaled up to 768x768 pixels. There is no cost reduction for pages at lower\nsizes, other than bandwidth, or performance improvement for pages at higher\nresolution. For best results: Rotate pages to the correct orientation before uploading. Avoid blurry pages. If using a single page, place the text prompt after the page. PDF input For PDF payloads under 20MB, you can choose between uploading base64\nencoded documents or directly uploading locally stored files. Base64 encoded documents You can process PDF documents directly from URLs. Here's a code snippet\nshowing how to do this: import httpx import base64 model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) doc_url = \"https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf\" # Replace with the actual URL of your PDF # Retrieve and encode the PDF doc_data = base64 . standard_b64encode ( httpx . get ( doc_url ) . content ) . decode ( \"utf-8\" ) prompt = \"Summarize this document\" response = model . generate_content ([{ 'mime_type' : 'application/pdf' , 'data' : doc_data }, prompt ]) print ( response . text ) Locally stored PDFs For locally stored PDFs, you can use the following approach: import base64 model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) doc_path = \"/path/to/file.pdf\" # Replace with the actual path to your local PDF # Read and encode the local file with open ( doc_path , \"rb\" ) as doc_file : doc_data = base64 . standard_b64encode ( doc_file . read ()) . decode ( \"utf-8\" ) prompt = \"Summarize this document\" response = model . generate_content ([{ 'mime_type' : 'application/pdf' , 'data' : doc_data }, prompt ]) print ( response . text ) Large PDFs You can use the File API to upload a document of any size. Always use the File\nAPI when the total request size (including the files, text prompt, system\ninstructions, etc.) is larger than 20 MB. Note: The File API lets you store up to 20 GB of files per project, with a\nper-file maximum size of 2 GB. Files are stored for 48 hours. They can be\naccessed in that period with your API key, but cannot be downloaded from the\nAPI. The File API is available at no cost in all regions where the Gemini API is\navailable. Call media.upload to upload a file using the\nFile API. The following code uploads a document file and then uses the file in a\ncall to models.generateContent . Large PDFs from URLs (:#large-pdfs-urls) Use the File API for large PDF files available from URLs,\nsimplifying the process of uploading and processing these documents directly\nthrough their URLs: import io import httpx model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) long_context_pdf_path = \"https://www.nasa.gov/wp-content/uploads/static/history/alsj/a17/A17_FlightPlan.pdf\" # Replace with the actual URL of your large PDF # Retrieve and upload the PDF using the File API doc_data = io . BytesIO ( httpx . get ( long_context_pdf_path ) . content ) sample_doc = genai . upload_file ( data = doc_data , mime_type = 'application/pdf' ) prompt = \"Summarize this document\" response = model . generate_content ([ sample_doc , prompt ]) print ( response . text ) Large PDFs stored locally (:#large-pdfs-local) import google.generativeai as genai model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) sample_pdf = genai . upload_file ( media / \"test.pdf\" ) response = model . generate_content ([ \"Give me a summary of this pdf file.\" , sample_pdf ]) print ( response . text ) files . py You can verify the API successfully stored the uploaded file and get its\nmetadata by calling files.get . Only the name (and by extension, the uri ) are unique. import google.generativeai as genai myfile = genai . upload_file ( media / \"poem.txt\" ) file_name = myfile . name print ( file_name ) # \"files/*\" myfile = genai . get_file ( file_name ) print ( myfile ) files . py Multiple PDFs The Gemini API is capable of processing multiple PDF documents in a single\nrequest, as long as the combined size of the documents and the text prompt\nstays within the model's context window. import io import httpx model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) doc_url_1 = \"https://arxiv.org/pdf/2312.11805\" # Replace with the URL to your first PDF doc_url_2 = \"https://arxiv.org/pdf/2403.05530\" # Replace with the URL to your second PDF # Retrieve and upload both PDFs using the File API doc_data_1 = io . BytesIO ( httpx . get ( doc_url_1 ) . content ) doc_data_2 = io . BytesIO ( httpx . get ( doc_url_2 ) . content ) sample_pdf_1 = genai . upload_file ( data = doc_data_1 , mime_type = 'application/pdf' ) sample_pdf_2 = genai . upload_file ( data = doc_data_2 , mime_type = 'application/pdf' ) prompt = \"What is the difference between each of the main benchmarks between these two papers? Output these in a table.\" response = model . generate_content ([ sample_pdf_1 , sample_pdf_2 , prompt ]) print ( response . text ) List files You can list all files uploaded using the File API and their URIs using files.list . import google.generativeai as genai print ( \"My files:\" ) for f in genai . list_files (): print ( \"  \" , f . name ) files . py Delete files Files uploaded using the File API are automatically deleted after 2 days. You\ncan also manually delete them using files.delete . import google.generativeai as genai myfile = genai . upload_file ( media / \"poem.txt\" ) myfile . delete () try : # Error. model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) result = model . generate_content ([ myfile , \"Describe this file.\" ]) except google . api_core . exceptions . PermissionDenied : pass files . py Context caching with PDFs import os from google.generativeai import caching import io import httpx # Define the path to the PDF document (or use a URL) long_context_pdf_path = \"https://www.nasa.gov/wp-content/uploads/static/history/alsj/a17/A17_FlightPlan.pdf\" # Replace with the URL of your large PDF doc_data = io . BytesIO ( httpx . get ( long_context_pdf_path ) . content ) # Upload the PDF document using the File API document = genai . upload_file ( data = doc_data , mime_type = 'application/pdf' ) # Specify the model name and system instruction for caching model_name = \"gemini-1.5-flash-002\" # Ensure this matches the model you intend to use system_instruction = \"You are an expert analyzing transcripts.\" # Create a cached content object cache = caching . CachedContent . create ( model = model_name , system_instruction = system_instruction , contents = [ document ], # The document(s) and other content you wish to cache ) # Display the cache details print ( cache ) # Initialize a generative model from the cached content model = genai . GenerativeModel . from_cached_content ( cache ) # Generate content using the cached prompt and document response = model . generate_content ( \"Please summarize this transcript\" ) # (Optional) Print usage metadata for insights into the API call print ( response . usage_metadata ) # Print the generated text print ( response . text ) List caches It's not possible to retrieve or view cached content, but you can retrieve\ncache metadata ( name , model , display_name , usage_metadata , create_time , update_time , and expire_time ). To list metadata for all uploaded caches, use CachedContent.list() : for c in caching . CachedContent . list (): print ( c ) Update a cache You can set a new ttl or expire_time for a cache. Changing anything else\nabout the cache isn't supported. The following example shows how to update the ttl of a cache using CachedContent.update() . import datetime cache . update ( ttl = datetime . timedelta ( hours = 2 )) Delete a cache The caching service provides a delete operation for manually removing content\nfrom the cache. The following example shows how to delete a cache using CachedContent.delete() . cache . delete () What's next This guide shows how to use generateContent and\nto generate text outputs from processed documents. To learn more,\nsee the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-20 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use the Gemini API to process documents like PDFs",
            "links": [
                "https://ai.google.dev/gemini-api/docs/document-processing?lang=python",
                "https://ai.google.dev/gemini-api/docs/document-processing?lang=python",
                "https://ai.google.dev/gemini-api/docs/document-processing?lang=python",
                "https://ai.google.dev/gemini-api/docs/document-processing?lang=python",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing?lang=python"
            ],
            "timestamp": "2024-12-29T12:30:48.699853",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/document-processing",
            "title": "Explore document processing capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore document processing capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore document processing capabilities with the Gemini API Python Node.js Go REST The Gemini API supports PDF input, including long documents (up to 3600 pages).\nGemini models process PDFs with native vision, and are therefore able to\nunderstand both text and image contents inside documents. With native PDF vision\nsupport, Gemini models are able to: Analyze diagrams, charts, and tables inside documents. Extract information into structured output formats. Answer questions about visual and text contents in documents. Summarize documents. Transcribe document content (e.g. to HTML) preserving layouts and formatting, for use in downstream applications (such as in RAG pipelines). This tutorial demonstrates some possible ways to use the Gemini API with PDF\ndocuments. All output is text-only. What's next This guide shows how to use generateContent and\nto generate text outputs from processed documents. To learn more,\nsee the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-20 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use the Gemini API to process documents like PDFs",
            "links": [
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing"
            ],
            "timestamp": "2024-12-29T12:30:49.468226",
            "status_code": 200
        }
    ]
}

# Google search

{
    "base_url": "https://ai.google.dev/gemini-api/docs/grounding?lang=python",
    "crawl_date": "2024-12-29T12:33:23.255718",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/grounding?lang=python",
            "title": "Grounding with Google Search  |  Gemini API  |  Google AI for Developers",
            "text_content": "Grounding with Google Search  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Grounding with Google Search Important: We're launching Grounding with Google Search! Review the updated Gemini API Additional Terms of Service ,\nwhich include new feature terms and updates for clarity. Python Node.js REST View on Google AI Try a Colab notebook View notebook on GitHub The Grounding with Google Search feature in the Gemini API and AI Studio can be\nused to improve the accuracy and recency of responses from the model. In\naddition to more factual responses, when Grounding with Google Search is\nenabled, the Gemini API returns grounding sources (in-line supporting links) and Google Search Suggestions along with the response\ncontent. The Search Suggestions point users to the search results corresponding\nto the grounded response. Grounding with Google Search only supports text prompts. It doesn't support\nmultimodal (text-and-image, text-and-audio, etc.) prompts. Grounding with\nGoogle Search supports all of the available languages for\nGemini models. This guide will help you get started with Grounding with Google Search using one\nof the Gemini API SDKs or the REST API. Configure a model to use Google Search Tip: Before running the example code, make sure that you've followed the\ninstallation and setup instructions in the quickstart . There are two ways to configure a model to use Grounding with Google Search: using a string and using a dictionary . Configure by string model = genai . GenerativeModel ( 'models/gemini-1.5-pro-002' ) response = model . generate_content ( contents = \"Who won Wimbledon this year?\" , tools = 'google_search_retrieval' ) print ( response ) Configure by dictionary model = genai . GenerativeModel ( 'models/gemini-1.5-pro-002' ) response = model . generate_content ( contents = \"Who won Wimbledon this year?\" , tools = { \"google_search_retrieval\" : { \"dynamic_retrieval_config\" : { \"mode\" : \"unspecified\" , \"dynamic_threshold\" : 0.06 }}}) print ( response ) For a dictionary implementation, you don't have to pass in key-value\npairs for mode or dynamic_threshold . You can omit them and let the model use\ndefault values, as in the following example: model = genai . GenerativeModel ( 'models/gemini-1.5-pro-002' ) response = model . generate_content ( contents = \"Who won Wimbledon this year?\" , tools = { \"google_search_retrieval\" : {}}) print ( response ) The mode and dynamic_threshold settings let you control the behavior of dynamic retrieval , giving you additional control over when\nto use Grounding with Google Search. Why is Grounding with Google Search useful? In generative AI, grounding refers to the process of connecting the model to\nverifiable sources of information. These sources might provide real-world\nworkplace information or other specific context. Grounding helps with improving\nthe accuracy, reliability, and usefulness of AI outputs. Grounding is particularly important for prompts that require up-to-date\ninformation from the web. Using grounding, the model can access information\nbeyond its knowledge cutoff date, get sources for the information, and answer\nquestions that it couldn't have answered accurately otherwise. Using Google AI Studio or the Gemini API, you can ground model output to\nGoogle Search. Grounding with Google Search provides the following benefits: Allows model responses that are tethered to specific content. Reduces model hallucinations, which are instances where the model generates\ncontent that isn't factual. Anchors model responses to sources a user can click through and open. Enhances the trustworthiness and applicability of the generated content. When you use Grounding with Google Search, you're effectively connecting the\nmodel to reliable Search results from the internet. Since non-grounded model\nresponses are based on learned patterns, you might not get factual responses to\nprompts about current events (for example, asking for a weather forecast or the\nfinal score of a recent football game). Since the internet provides access to\nnew information, a grounded prompt can generate more up-to-date responses, with\nsources cited. Here's an example comparing a non-grounded response and a grounded response\ngenerated using the API. (The responses were generated in October 2024.) Ungrounded Gemini Grounding with Google Search Prompt: Who won the Super Bowl this year? Response: The Kansas City Chiefs won Super Bowl\n        LVII this year (2023). Prompt: Who won the Super Bowl this year? Response: The Kansas City Chiefs won Super Bowl\n        LVIII this year, defeating the San Francisco 49ers in overtime with a\n        score of 25 to 22. In the ungrounded response, the model refers to the Kansas City Chiefs' 2023\nSuper Bowl win. In the grounded response, the model correctly references their\nmore recent 2024 win. The following image shows how a grounded response looks in AI Studio. Google Search Suggestions To use Grounding with Google Search, you have to display Google Search\nSuggestions, which are suggested queries included in the metadata of the\ngrounded response. To learn more about the display requirements, see Use Google Search Suggestions . Dynamic retrieval Some queries are likely to benefit more from Grounding with Google Search than\nothers. The dynamic retrieval feature gives you additional control over when\nto use Grounding with Google Search. If the dynamic retrieval mode is unspecified, Grounding with Google Search is\nalways triggered. If the mode is set to dynamic, the model decides when to use\ngrounding based on a threshold that you can configure. The threshold\nis a floating-point value in the range [0,1] and defaults to 0.3. If the\nthreshold value is 0, the response is always grounded with Google Search; if\nit's 1, it never is. How dynamic retrieval works You can use dynamic retrieval in your request to choose when to turn on\nGrounding with Google Search. This is useful when the prompt doesn't require\nan answer grounded in Google Search and the model can provide an answer based\non its own knowledge without grounding. This helps you manage latency, quality,\nand cost more effectively. Before you invoke the dynamic retrieval configuration in your request,\nunderstand the following terminology: Prediction score : When you request a grounded answer, Gemini assigns a prediction score to the prompt. The prediction score is a floating point\nvalue in the range [0,1]. Its value depends on whether the prompt\ncan benefit from grounding the answer with the most up-to-date\ninformation from Google Search. Thus, if a prompt requires an answer\ngrounded in the most recent facts on the web, it has a higher prediction\nscore. A prompt for which a model-generated answer is sufficient has a lower\nprediction score. Here are examples of some prompts and their prediction scores. Note: The prediction scores are assigned by Gemini and can vary over\ntime depending on several factors. Prompt Prediction score Comment \"Write a poem about peonies\" 0.13 The model can rely on its knowledge and the answer doesn't need grounding. \"Suggest a toy for a 2yo child\" 0.36 The model can rely on its knowledge and the answer doesn't need grounding. \"Can you give a recipe for an asian-inspired guacamole?\" 0.55 Google Search can give a grounded answer, but grounding isn't strictly required; the model knowledge might be sufficient. \"What's Agent Builder? How is grounding billed in Agent Builder?\" 0.72 Requires Google Search to generate a well-grounded answer. \"Who won the latest F1 grand prix?\" 0.97 Requires Google Search to generate a well-grounded answer. Threshold : In your API request, you can specify a dynamic retrieval\nconfiguration with a threshold. The threshold is a floating point value in\nthe range [0,1] and defaults to 0.3. If the threshold value is zero, the\nresponse is always grounded with Google Search. For all other values\nof threshold, the following is applicable: If the prediction score is greater than or equal to the threshold, the\nanswer is grounded with Google Search.\nA lower threshold implies that more prompts have responses that are\ngenerated using Grounding with Google Search. If the prediction score is less than the threshold, the model might still\ngenerate the answer, but it isn't grounded with Google Search. To learn how to set the dynamic retrieval threshold using an SDK or the REST\nAPI, see the appropriate code example . If you're using AI Studio, you can set the dynamic retrieval threshold by\nclicking Edit grounding . To find a good threshold that suits your business needs, you can create a\nrepresentative set of queries that you expect to encounter. Then you can sort\nthe queries according to the prediction score in the response and select a\ngood threshold for your use case. A grounded response If your prompt successfully grounds to Google Search, the response will include groundingMetadata . A grounded response might look something like this\n(parts of the response have been omitted for brevity): { \"candidates\" : [ { \"content\" : { \"parts\" : [ { \"text\" : \"Carlos Alcaraz won the Gentlemen's Singles title at the 2024 Wimbledon Championships. He defeated Novak Djokovic in the final, winning his second consecutive Wimbledon title and fourth Grand Slam title overall. \\n\" } ], \"role\" : \"model\" }, ... \"groundingMetadata\" : { \"searchEntryPoint\" : { \"renderedContent\" : \"\\u003cstyle\\u003e\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n\\u003c/style\\u003e\\n\\u003cdiv class=\\\"container\\\"\\u003e\\n  \\u003cdiv class=\\\"headline\\\"\\u003e\\n    \\u003csvg class=\\\"logo-light\\\" width=\\\"18\\\" height=\\\"18\\\" viewBox=\\\"9 9 35 35\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\"\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\\\" fill=\\\"#4285F4\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\\\" fill=\\\"#34A853\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\\\" fill=\\\"#FBBC05\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\\\" fill=\\\"#EA4335\\\"/\\u003e\\n    \\u003c/svg\\u003e\\n    \\u003csvg class=\\\"logo-dark\\\" width=\\\"18\\\" height=\\\"18\\\" viewBox=\\\"0 0 48 48\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\"\\u003e\\n      \\u003ccircle cx=\\\"24\\\" cy=\\\"23\\\" fill=\\\"#FFF\\\" r=\\\"22\\\"/\\u003e\\n      \\u003cpath d=\\\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\\\" fill=\\\"#4285F4\\\"/\\u003e\\n      \\u003cpath d=\\\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\\\" fill=\\\"#34A853\\\"/\\u003e\\n      \\u003cpath d=\\\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\\\" fill=\\\"#FBBC05\\\"/\\u003e\\n      \\u003cpath d=\\\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\\\" fill=\\\"#EA4335\\\"/\\u003e\\n    \\u003c/svg\\u003e\\n    \\u003cdiv class=\\\"gradient-container\\\"\\u003e\\u003cdiv class=\\\"gradient\\\"\\u003e\\u003c/div\\u003e\\u003c/div\\u003e\\n  \\u003c/div\\u003e\\n  \\u003cdiv class=\\\"carousel\\\"\\u003e\\n    \\u003ca class=\\\"chip\\\" href=\\\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4x8Epe-gzpwRBvp7o3RZh2m1ygq1EHktn0OWCtvTXjad4bb1zSuqfJd6OEuZZ9_SXZ_P2SvCpJM7NaFfQfiZs6064MeqXego0vSbV9LlAZoxTdbxWK1hFeqTG6kA13YJf7Fbu1SqBYM0cFM4zo0G_sD9NKYWcOCQMvDLDEJFhjrC9DM_QobBIAMq-gWN95G5tvt6_z6EuPN8QY=\\\"\\u003ewho won wimbledon 2024\\u003c/a\\u003e\\n  \\u003c/div\\u003e\\n\\u003c/div\\u003e\\n\" }, \"groundingChunks\" : [ { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4whET1ta3sDETZvcicd8FeNe4z0VuduVsxrT677KQRp2rYghXI0VpfYbIMVI3THcTuMwggRCbFXS_wVvW0UmGzMe9h2fyrkvsnQPJyikJasNIbjJLPX0StM4Bd694-ZVle56MmRA4YiUvwSqad1w6O2opmWnw==\" , \"title\" : \"wikipedia.org\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4wR1M-9-yMPUr_KdHlnoAmQ8ZX90DtQ_vDYTjtP2oR5RH4tRP04uqKPLmesvo64BBkPeYLC2EpVDxv9ngO3S1fs2xh-e78fY4m0GAtgNlahUkm_tBm_sih5kFPc7ill9u2uwesNGUkwrQlmP2mfWNU5lMMr23HGktr6t0sV0QYlzQq7odVoBxYWlQ_sqWFH\" , \"title\" : \"wikipedia.org\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4wsDmROzbP-tmt8GdwCW_pqISTZ4IRbBuoaMyaHfcQg8WW-yKRQQvMDTPAuLxJh-8_U8_iw_6JKFbQ8M9oVYtaFdWFK4gOtL4RrC9Jyqc5BNpuxp6uLEKgL5-9TggtNvO97PyCfziDFXPsxylwI1HcfQdrz3Jy7ZdOL4XM-S5rC0lF2S3VWW0IEAEtS7WX861meBYVjIuuF_mIr3spYPqWLhbAY2Spj-4_ba8DjRvmevIFUhRuESTKvBfmpxNSM\" , \"title\" : \"cbssports.com\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4yzjLkorHiUKjhOPkWaZ9b4cO-cLG-02vlEl6xTBjMUjyhK04qSIclAa7heR41JQ6AAVXmNdS3WDrLOV4Wli-iezyzW8QPQ4vgnmO_egdsuxhcGk3-Fp8-yfqNLvgXFwY5mPo6QRhvplOFv0_x9mAcka18QuAXtj0SPvJfZhUEgYLCtCrucDS5XFc5HmRBcG1tqFdKSE1ihnp8KLdaWMhrUQI21hHS9\" , \"title\" : \"jagranjosh.com\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4y9L4oeNGWCatFz63b9PpP3ys-Wi_zwnkUT5ji9lY7gPUJQcsmmE87q88GSdZqzcx5nZG9usot5FYk2yK-FAGvCRE6JsUQJB_W11_kJU2HVV1BTPiZ4SAgm8XDFIxpCZXnXmEx5HUfRqQm_zav7CvS2qjA2x3__qLME6Jy7R5oza1C5_aqjQu422le9CaigThS5bvJoMo-ZGcXdBUCj2CqoXNVjMA==\" , \"title\" : \"apnews.com\" } } ], \"groundingSupports\" : [ { \"segment\" : { \"endIndex\" : 85 , \"text\" : \"Carlos Alcaraz won the Gentlemen's Singles title at the 2024 Wimbledon Championships.\" }, \"groundingChunkIndices\" : [ 0 , 1 , 2 , 3 ], \"confidenceScores\" : [ 0.97380733 , 0.97380733 , 0.97380733 , 0.97380733 ] }, { \"segment\" : { \"startIndex\" : 86 , \"endIndex\" : 210 , \"text\" : \"He defeated Novak Djokovic in the final, winning his second consecutive Wimbledon title and fourth Grand Slam title overall.\" }, \"groundingChunkIndices\" : [ 1 , 0 , 4 ], \"confidenceScores\" : [ 0.96145374 , 0.96145374 , 0.96145374 ] } ], \"webSearchQueries\" : [ \"who won wimbledon 2024\" ] } } ], ... } If the response doesn't include groundingMetadata , this means the response\nwasn't successfully grounded. There are several reasons this could happen,\nincluding low source relevance or incomplete information within the\nmodel response. When a grounded result is generated, the metadata contains URIs that redirect\nto the publishers of the content that was used to generate the grounded result.\nThese URIs contain the vertexaisearch subdomain, as in this truncated example: https://vertexaisearch.cloud.google.com/grounding-api-redirect/... . The\nmetadata also contains the publishers' domains. The provided URIs remain\naccessible for 30 days after the grounded result is generated. Important: The provided URIs must be directly accessible by the end users and\nmust not be queried programmatically through automated means. If automated\naccess is detected, the grounded answer generation service might stop providing\nthe redirection URIs. The renderedContent field within searchEntryPoint is the provided code for\nimplementing Google Search Suggestions. See Use Google Search Suggestions to learn more. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-19 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs/grounding?lang=python",
                "https://ai.google.dev/gemini-api/docs/grounding?lang=python",
                "https://ai.google.dev/gemini-api/docs/grounding?lang=python",
                "https://ai.google.dev/gemini-api/docs/grounding?lang=python",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/grounding?lang=python"
            ],
            "timestamp": "2024-12-29T12:33:21.822959",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/grounding",
            "title": "Grounding with Google Search  |  Gemini API  |  Google AI for Developers",
            "text_content": "Grounding with Google Search  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Grounding with Google Search Important: We're launching Grounding with Google Search! Review the updated Gemini API Additional Terms of Service ,\nwhich include new feature terms and updates for clarity. Python Node.js REST The Grounding with Google Search feature in the Gemini API and AI Studio can be\nused to improve the accuracy and recency of responses from the model. In\naddition to more factual responses, when Grounding with Google Search is\nenabled, the Gemini API returns grounding sources (in-line supporting links) and Google Search Suggestions along with the response\ncontent. The Search Suggestions point users to the search results corresponding\nto the grounded response. Grounding with Google Search only supports text prompts. It doesn't support\nmultimodal (text-and-image, text-and-audio, etc.) prompts. Grounding with\nGoogle Search supports all of the available languages for\nGemini models. This guide will help you get started with Grounding with Google Search using one\nof the Gemini API SDKs or the REST API. Configure a model to use Google Search Tip: Before running the example code, make sure that you've followed the\ninstallation and setup instructions in the quickstart . Why is Grounding with Google Search useful? In generative AI, grounding refers to the process of connecting the model to\nverifiable sources of information. These sources might provide real-world\nworkplace information or other specific context. Grounding helps with improving\nthe accuracy, reliability, and usefulness of AI outputs. Grounding is particularly important for prompts that require up-to-date\ninformation from the web. Using grounding, the model can access information\nbeyond its knowledge cutoff date, get sources for the information, and answer\nquestions that it couldn't have answered accurately otherwise. Using Google AI Studio or the Gemini API, you can ground model output to\nGoogle Search. Grounding with Google Search provides the following benefits: Allows model responses that are tethered to specific content. Reduces model hallucinations, which are instances where the model generates\ncontent that isn't factual. Anchors model responses to sources a user can click through and open. Enhances the trustworthiness and applicability of the generated content. When you use Grounding with Google Search, you're effectively connecting the\nmodel to reliable Search results from the internet. Since non-grounded model\nresponses are based on learned patterns, you might not get factual responses to\nprompts about current events (for example, asking for a weather forecast or the\nfinal score of a recent football game). Since the internet provides access to\nnew information, a grounded prompt can generate more up-to-date responses, with\nsources cited. Here's an example comparing a non-grounded response and a grounded response\ngenerated using the API. (The responses were generated in October 2024.) Ungrounded Gemini Grounding with Google Search Prompt: Who won the Super Bowl this year? Response: The Kansas City Chiefs won Super Bowl\n        LVII this year (2023). Prompt: Who won the Super Bowl this year? Response: The Kansas City Chiefs won Super Bowl\n        LVIII this year, defeating the San Francisco 49ers in overtime with a\n        score of 25 to 22. In the ungrounded response, the model refers to the Kansas City Chiefs' 2023\nSuper Bowl win. In the grounded response, the model correctly references their\nmore recent 2024 win. The following image shows how a grounded response looks in AI Studio. Google Search Suggestions To use Grounding with Google Search, you have to display Google Search\nSuggestions, which are suggested queries included in the metadata of the\ngrounded response. To learn more about the display requirements, see Use Google Search Suggestions . Dynamic retrieval Some queries are likely to benefit more from Grounding with Google Search than\nothers. The dynamic retrieval feature gives you additional control over when\nto use Grounding with Google Search. If the dynamic retrieval mode is unspecified, Grounding with Google Search is\nalways triggered. If the mode is set to dynamic, the model decides when to use\ngrounding based on a threshold that you can configure. The threshold\nis a floating-point value in the range [0,1] and defaults to 0.3. If the\nthreshold value is 0, the response is always grounded with Google Search; if\nit's 1, it never is. How dynamic retrieval works You can use dynamic retrieval in your request to choose when to turn on\nGrounding with Google Search. This is useful when the prompt doesn't require\nan answer grounded in Google Search and the model can provide an answer based\non its own knowledge without grounding. This helps you manage latency, quality,\nand cost more effectively. Before you invoke the dynamic retrieval configuration in your request,\nunderstand the following terminology: Prediction score : When you request a grounded answer, Gemini assigns a prediction score to the prompt. The prediction score is a floating point\nvalue in the range [0,1]. Its value depends on whether the prompt\ncan benefit from grounding the answer with the most up-to-date\ninformation from Google Search. Thus, if a prompt requires an answer\ngrounded in the most recent facts on the web, it has a higher prediction\nscore. A prompt for which a model-generated answer is sufficient has a lower\nprediction score. Here are examples of some prompts and their prediction scores. Note: The prediction scores are assigned by Gemini and can vary over\ntime depending on several factors. Prompt Prediction score Comment \"Write a poem about peonies\" 0.13 The model can rely on its knowledge and the answer doesn't need grounding. \"Suggest a toy for a 2yo child\" 0.36 The model can rely on its knowledge and the answer doesn't need grounding. \"Can you give a recipe for an asian-inspired guacamole?\" 0.55 Google Search can give a grounded answer, but grounding isn't strictly required; the model knowledge might be sufficient. \"What's Agent Builder? How is grounding billed in Agent Builder?\" 0.72 Requires Google Search to generate a well-grounded answer. \"Who won the latest F1 grand prix?\" 0.97 Requires Google Search to generate a well-grounded answer. Threshold : In your API request, you can specify a dynamic retrieval\nconfiguration with a threshold. The threshold is a floating point value in\nthe range [0,1] and defaults to 0.3. If the threshold value is zero, the\nresponse is always grounded with Google Search. For all other values\nof threshold, the following is applicable: If the prediction score is greater than or equal to the threshold, the\nanswer is grounded with Google Search.\nA lower threshold implies that more prompts have responses that are\ngenerated using Grounding with Google Search. If the prediction score is less than the threshold, the model might still\ngenerate the answer, but it isn't grounded with Google Search. To learn how to set the dynamic retrieval threshold using an SDK or the REST\nAPI, see the appropriate code example . If you're using AI Studio, you can set the dynamic retrieval threshold by\nclicking Edit grounding . To find a good threshold that suits your business needs, you can create a\nrepresentative set of queries that you expect to encounter. Then you can sort\nthe queries according to the prediction score in the response and select a\ngood threshold for your use case. A grounded response If your prompt successfully grounds to Google Search, the response will include groundingMetadata . A grounded response might look something like this\n(parts of the response have been omitted for brevity): { \"candidates\" : [ { \"content\" : { \"parts\" : [ { \"text\" : \"Carlos Alcaraz won the Gentlemen's Singles title at the 2024 Wimbledon Championships. He defeated Novak Djokovic in the final, winning his second consecutive Wimbledon title and fourth Grand Slam title overall. \\n\" } ], \"role\" : \"model\" }, ... \"groundingMetadata\" : { \"searchEntryPoint\" : { \"renderedContent\" : \"\\u003cstyle\\u003e\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n\\u003c/style\\u003e\\n\\u003cdiv class=\\\"container\\\"\\u003e\\n  \\u003cdiv class=\\\"headline\\\"\\u003e\\n    \\u003csvg class=\\\"logo-light\\\" width=\\\"18\\\" height=\\\"18\\\" viewBox=\\\"9 9 35 35\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\"\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\\\" fill=\\\"#4285F4\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\\\" fill=\\\"#34A853\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\\\" fill=\\\"#FBBC05\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\\\" fill=\\\"#EA4335\\\"/\\u003e\\n    \\u003c/svg\\u003e\\n    \\u003csvg class=\\\"logo-dark\\\" width=\\\"18\\\" height=\\\"18\\\" viewBox=\\\"0 0 48 48\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\"\\u003e\\n      \\u003ccircle cx=\\\"24\\\" cy=\\\"23\\\" fill=\\\"#FFF\\\" r=\\\"22\\\"/\\u003e\\n      \\u003cpath d=\\\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\\\" fill=\\\"#4285F4\\\"/\\u003e\\n      \\u003cpath d=\\\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\\\" fill=\\\"#34A853\\\"/\\u003e\\n      \\u003cpath d=\\\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\\\" fill=\\\"#FBBC05\\\"/\\u003e\\n      \\u003cpath d=\\\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\\\" fill=\\\"#EA4335\\\"/\\u003e\\n    \\u003c/svg\\u003e\\n    \\u003cdiv class=\\\"gradient-container\\\"\\u003e\\u003cdiv class=\\\"gradient\\\"\\u003e\\u003c/div\\u003e\\u003c/div\\u003e\\n  \\u003c/div\\u003e\\n  \\u003cdiv class=\\\"carousel\\\"\\u003e\\n    \\u003ca class=\\\"chip\\\" href=\\\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4x8Epe-gzpwRBvp7o3RZh2m1ygq1EHktn0OWCtvTXjad4bb1zSuqfJd6OEuZZ9_SXZ_P2SvCpJM7NaFfQfiZs6064MeqXego0vSbV9LlAZoxTdbxWK1hFeqTG6kA13YJf7Fbu1SqBYM0cFM4zo0G_sD9NKYWcOCQMvDLDEJFhjrC9DM_QobBIAMq-gWN95G5tvt6_z6EuPN8QY=\\\"\\u003ewho won wimbledon 2024\\u003c/a\\u003e\\n  \\u003c/div\\u003e\\n\\u003c/div\\u003e\\n\" }, \"groundingChunks\" : [ { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4whET1ta3sDETZvcicd8FeNe4z0VuduVsxrT677KQRp2rYghXI0VpfYbIMVI3THcTuMwggRCbFXS_wVvW0UmGzMe9h2fyrkvsnQPJyikJasNIbjJLPX0StM4Bd694-ZVle56MmRA4YiUvwSqad1w6O2opmWnw==\" , \"title\" : \"wikipedia.org\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4wR1M-9-yMPUr_KdHlnoAmQ8ZX90DtQ_vDYTjtP2oR5RH4tRP04uqKPLmesvo64BBkPeYLC2EpVDxv9ngO3S1fs2xh-e78fY4m0GAtgNlahUkm_tBm_sih5kFPc7ill9u2uwesNGUkwrQlmP2mfWNU5lMMr23HGktr6t0sV0QYlzQq7odVoBxYWlQ_sqWFH\" , \"title\" : \"wikipedia.org\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4wsDmROzbP-tmt8GdwCW_pqISTZ4IRbBuoaMyaHfcQg8WW-yKRQQvMDTPAuLxJh-8_U8_iw_6JKFbQ8M9oVYtaFdWFK4gOtL4RrC9Jyqc5BNpuxp6uLEKgL5-9TggtNvO97PyCfziDFXPsxylwI1HcfQdrz3Jy7ZdOL4XM-S5rC0lF2S3VWW0IEAEtS7WX861meBYVjIuuF_mIr3spYPqWLhbAY2Spj-4_ba8DjRvmevIFUhRuESTKvBfmpxNSM\" , \"title\" : \"cbssports.com\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4yzjLkorHiUKjhOPkWaZ9b4cO-cLG-02vlEl6xTBjMUjyhK04qSIclAa7heR41JQ6AAVXmNdS3WDrLOV4Wli-iezyzW8QPQ4vgnmO_egdsuxhcGk3-Fp8-yfqNLvgXFwY5mPo6QRhvplOFv0_x9mAcka18QuAXtj0SPvJfZhUEgYLCtCrucDS5XFc5HmRBcG1tqFdKSE1ihnp8KLdaWMhrUQI21hHS9\" , \"title\" : \"jagranjosh.com\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4y9L4oeNGWCatFz63b9PpP3ys-Wi_zwnkUT5ji9lY7gPUJQcsmmE87q88GSdZqzcx5nZG9usot5FYk2yK-FAGvCRE6JsUQJB_W11_kJU2HVV1BTPiZ4SAgm8XDFIxpCZXnXmEx5HUfRqQm_zav7CvS2qjA2x3__qLME6Jy7R5oza1C5_aqjQu422le9CaigThS5bvJoMo-ZGcXdBUCj2CqoXNVjMA==\" , \"title\" : \"apnews.com\" } } ], \"groundingSupports\" : [ { \"segment\" : { \"endIndex\" : 85 , \"text\" : \"Carlos Alcaraz won the Gentlemen's Singles title at the 2024 Wimbledon Championships.\" }, \"groundingChunkIndices\" : [ 0 , 1 , 2 , 3 ], \"confidenceScores\" : [ 0.97380733 , 0.97380733 , 0.97380733 , 0.97380733 ] }, { \"segment\" : { \"startIndex\" : 86 , \"endIndex\" : 210 , \"text\" : \"He defeated Novak Djokovic in the final, winning his second consecutive Wimbledon title and fourth Grand Slam title overall.\" }, \"groundingChunkIndices\" : [ 1 , 0 , 4 ], \"confidenceScores\" : [ 0.96145374 , 0.96145374 , 0.96145374 ] } ], \"webSearchQueries\" : [ \"who won wimbledon 2024\" ] } } ], ... } If the response doesn't include groundingMetadata , this means the response\nwasn't successfully grounded. There are several reasons this could happen,\nincluding low source relevance or incomplete information within the\nmodel response. When a grounded result is generated, the metadata contains URIs that redirect\nto the publishers of the content that was used to generate the grounded result.\nThese URIs contain the vertexaisearch subdomain, as in this truncated example: https://vertexaisearch.cloud.google.com/grounding-api-redirect/... . The\nmetadata also contains the publishers' domains. The provided URIs remain\naccessible for 30 days after the grounded result is generated. Important: The provided URIs must be directly accessible by the end users and\nmust not be queried programmatically through automated means. If automated\naccess is detected, the grounded answer generation service might stop providing\nthe redirection URIs. The renderedContent field within searchEntryPoint is the provided code for\nimplementing Google Search Suggestions. See Use Google Search Suggestions to learn more. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-19 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/grounding"
            ],
            "timestamp": "2024-12-29T12:33:23.215510",
            "status_code": 200
        }
    ]
}

# Safety settings 

{
    "base_url": "https://ai.google.dev/gemini-api/docs/safety-settings",
    "crawl_date": "2024-12-29T12:37:41.182431",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/safety-settings",
            "title": "Safety settings  |  Gemini API  |  Google AI for Developers",
            "text_content": "Safety settings  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Safety settings The Gemini API provides safety settings that you can adjust during the\nprototyping stage to determine if your application requires more or less\nrestrictive safety configuration. You can adjust these settings across four\nfilter categories to restrict or allow certain types of content. This guide covers how the Gemini API handles safety settings and filtering and\nhow you can change the safety settings for your application. Note: Applications that use less restrictive safety settings may be subject to\nreview. See the Terms of Service for more information. Safety filters The Gemini API's adjustable safety filters cover the following categories: Category Description Harassment Negative or harmful comments targeting identity and/or protected\n      attributes. Hate speech Content that is rude, disrespectful, or profane. Sexually explicit Contains references to sexual acts or other lewd content. Dangerous Promotes, facilitates, or encourages harmful acts. Civic integrity Election-related queries. These categories are defined in HarmCategory . The\n  Gemini models only support HARM_CATEGORY_HARASSMENT , HARM_CATEGORY_HATE_SPEECH , HARM_CATEGORY_SEXUALLY_EXPLICIT , HARM_CATEGORY_DANGEROUS_CONTENT , and HARM_CATEGORY_CIVIC_INTEGRITY . All other categories are used\n  only by PaLM 2 (Legacy) models. You can use these filters to adjust what's appropriate for your use case. For\nexample, if you're building video game dialogue, you may deem it acceptable to\nallow more content that's rated as Dangerous due to the nature of the game. In addition to the adjustable safety filters, the Gemini API has built-in\nprotections against core harms, such as content that endangers child safety.\nThese types of harm are always blocked and cannot be adjusted. Content safety filtering level The Gemini API categorizes the probability level of content being unsafe as HIGH , MEDIUM , LOW , or NEGLIGIBLE . The Gemini API blocks content based on the probability of content being unsafe\nand not the severity. This is important to consider because some content can\nhave low probability of being unsafe even though the severity of harm could\nstill be high. For example, comparing the sentences: The robot punched me. The robot slashed me up. The first sentence might result in a higher probability of being unsafe, but you\nmight consider the second sentence to be a higher severity in terms of violence.\nGiven this, it is important that you carefully test and consider what the\nappropriate level of blocking is needed to support your key use cases while\nminimizing harm to end users. Safety filtering per request You can adjust the safety settings for each request you make to the API. When\nyou make a request, the content is analyzed and assigned a safety rating. The\nsafety rating includes the category and the probability of the harm\nclassification. For example, if the content was blocked due to the harassment\ncategory having a high probability, the safety rating returned would have\ncategory equal to HARASSMENT and harm probability set to HIGH . By default, safety settings block content (including prompts) with medium or\nhigher probability of being unsafe across any filter. This baseline safety is\ndesigned to work for most use cases, so you should only adjust your safety\nsettings if it's consistently required for your application. The following table describes the block settings you can adjust for each\ncategory. For example, if you set the block setting to Block few for the Hate speech category, everything that has a high probability of being hate\nspeech content is blocked. But anything with a lower probability is allowed. Threshold (Google AI Studio) Threshold (API) Description Block none BLOCK_NONE Always show regardless of probability of unsafe content Block few BLOCK_ONLY_HIGH Block when high probability of unsafe content Block some BLOCK_MEDIUM_AND_ABOVE Block when medium or high probability of unsafe content Block most BLOCK_LOW_AND_ABOVE Block when low, medium or high probability of unsafe content N/A HARM_BLOCK_THRESHOLD_UNSPECIFIED Threshold is unspecified, block using default threshold If the threshold is not set, the default block threshold is Block most (for gemini-1.5-pro-002 and gemini-1.5-flash-002 only) or Block some (in all\nother models) for all categories except the Civic integrity category. The default block threshold for the Civic integrity category is Block most when sending prompts using Google AI Studio, and Block none when using the\nGemini API directly. You can set these settings for each request you make to the generative service.\nSee the HarmBlockThreshold API\nreference for details. Safety feedback generateContent returns a GenerateContentResponse which\nincludes safety feedback. Prompt feedback is included in promptFeedback . If promptFeedback.blockReason is set, then the content of the prompt was blocked. Response candidate feedback is included in Candidate.finishReason and Candidate.safetyRatings . If response\ncontent was blocked and the finishReason was SAFETY , you can inspect safetyRatings for more details. The content that was blocked is not returned. Adjust safety settings This section covers how to adjust the safety settings in both Google AI Studio\nand in your code. Google AI Studio You can adjust safety settings in Google AI Studio, but you cannot turn them\noff. Click Edit safety settings in the Run settings panel to open the Run\nsafety settings modal. In the modal, you can use the sliders to adjust the\ncontent filtering level per safety category: Note: If you set any of the category filters to Block none , Google AI Studio\nwill display a reminder about the Gemini API's Terms of Service with respect\nto safety settings. When you send a request (for example, by asking the model a question), a warning No Content message appears if the request's content is blocked. To see more\ndetails, hold the pointer over the No Content text and click warning Safety . Gemini API SDKs The following code snippet shows how to set safety settings in your GenerateContent call. This sets the thresholds for the harassment\n( HARM_CATEGORY_HARASSMENT ) and hate speech ( HARM_CATEGORY_HATE_SPEECH )\ncategories. For example, setting these categories to BLOCK_LOW_AND_ABOVE blocks any content that has a low or higher probability of being harassment or\nhate speech. To understand the threshold settings, see Safety filtering per request . Python from google.generativeai.types import HarmCategory , HarmBlockThreshold model = genai . GenerativeModel ( model_name = 'gemini-1.5-flash' ) response = model . generate_content ( [ 'Do these look store-bought or homemade?' , img ], safety_settings = { HarmCategory . HARM_CATEGORY_HATE_SPEECH : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , HarmCategory . HARM_CATEGORY_HARASSMENT : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , } ) Go model := client . GenerativeModel ( \"gemini-1.5-flash\" ) model . SafetySettings = [] * genai . SafetySetting { { Category : genai . HarmCategoryHarassment , Threshold : genai . HarmBlockLowAndAbove , }, { Category : genai . HarmCategoryHateSpeech , Threshold : genai . HarmBlockLowAndAbove , }, } Node.js import { HarmBlockThreshold , HarmCategory } from \"@google/generative-ai\" ; // ... const safetySettings = [ { category : HarmCategory . HARM_CATEGORY_HARASSMENT , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, { category : HarmCategory . HARM_CATEGORY_HATE_SPEECH , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, ]; const model = genAi . getGenerativeModel ({ model : \"gemini-1.5-flash\" , safetySettings : safetySettings }); Web import { HarmBlockThreshold , HarmCategory } from \"@google/generative-ai\" ; // ... const safetySettings = [ { category : HarmCategory . HARM_CATEGORY_HARASSMENT , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, { category : HarmCategory . HARM_CATEGORY_HATE_SPEECH , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, ]; const model = genAi . getGenerativeModel ({ model : \"gemini-1.5-flash\" , safetySettings }); Dart (Flutter) final safetySettings = [ SafetySetting ( HarmCategory . harassment , HarmBlockThreshold . low ), SafetySetting ( HarmCategory . hateSpeech , HarmBlockThreshold . low ), ]; final model = GenerativeModel ( model: 'gemini-1.5-flash' , apiKey: apiKey , safetySettings: safetySettings , ); Kotlin val harassmentSafety = SafetySetting ( HarmCategory . HARASSMENT , BlockThreshold . LOW_AND_ABOVE ) val hateSpeechSafety = SafetySetting ( HarmCategory . HATE_SPEECH , BlockThreshold . LOW_AND_ABOVE ) val generativeModel = GenerativeModel ( modelName = \"gemini-1.5-flash\" , apiKey = BuildConfig . apiKey , safetySettings = listOf ( harassmentSafety , hateSpeechSafety ) ) Java SafetySetting harassmentSafety = new SafetySetting ( HarmCategory . HARASSMENT , BlockThreshold . LOW_AND_ABOVE ); SafetySetting hateSpeechSafety = new SafetySetting ( HarmCategory . HATE_SPEECH , BlockThreshold . LOW_AND_ABOVE ); GenerativeModel gm = new GenerativeModel ( \"gemini-1.5-flash\" , BuildConfig . apiKey , null , // generation config is optional Arrays . asList ( harassmentSafety , hateSpeechSafety ) ); GenerativeModelFutures model = GenerativeModelFutures . from ( gm ); REST echo '{ \"safetySettings\": [ {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_ONLY_HIGH\"}, {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"} ], \"contents\": [{ \"parts\":[{ \"text\": \"' I support Martians Soccer Club and I think Jupiterians Football Club sucks! Write a ironic phrase about them. '\"}]}]}' > request.json\n\ncurl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= $GOOGLE_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d @request.json 2 > /dev/null safety_settings.sh Next steps See the API reference to learn more about the full API. Review the safety guidance for a general look at safety\nconsiderations when developing with LLMs. Learn more about assessing probability versus severity from the Jigsaw\nteam Learn more about the products that contribute to safety solutions like the Perspective\nAPI .\n                *   You can use these safety settings to create a toxicity\n                    classifier. See the classification\n                    example to\n                    get started. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings"
            ],
            "timestamp": "2024-12-29T12:37:41.135726",
            "status_code": 200
        }
    ]
}

# System instructions 

{
    "base_url": "https://ai.google.dev/gemini-api/docs/system-instructions?lang=python",
    "crawl_date": "2024-12-29T12:39:02.941653",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/system-instructions?lang=python",
            "title": "Use system instructions to steer the behavior of a model  |  Gemini API  |  Google AI for Developers",
            "text_content": "Use system instructions to steer the behavior of a model  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Use system instructions to steer the behavior of a model Python Node.js REST Go System instructions let you steer the behavior of a model based on\nyour specific needs and use cases. When you set a system instruction, you give\nthe model additional context to understand the task, provide more customized\nresponses, and adhere to specific guidelines over the full user interaction with\nthe model. You can also specify product-level behavior by setting system\ninstructions, separate from prompts provided by end users. Basic example Here's a basic example of how to set the system instruction using the SDKs for\nthe Gemini API: model = genai . GenerativeModel ( model_name = \"gemini-1.5-flash\" , system_instruction = \"You are a cat. Your name is Neko.\" ) Now send a request to the model: response = model . generate_content ( \"Good morning! How are you?\" ) print ( response . text ) This example might give a response such as: *Yawns widely, stretching out my claws and batting at a sunbeam*\nMeow. I'm doing quite well, thanks for asking. It's a good morning for napping.\nPerhaps you could fetch my favorite feathered toy?  *Looks expectantly* Try it in a colab For an interactive end to end example of using system instructions, see the Gemini API: System instructions colab. Note: System instructions can help guide the model to follow instructions, but\nthey don't fully prevent jailbreaks or leaks. We recommend exercising caution\naround putting any sensitive information in system instructions. More examples You can use system instructions in many ways, including: Defining a persona or role (for a chatbot, for example) Defining output format (Markdown, YAML, etc.) Defining output style and tone (for example, verbosity, formality, and target\nreading level) Defining goals or rules for the task (for example, returning a code snippet\nwithout further explanations) Providing additional context for the prompt (for example, a knowledge cutoff) System instructions are part of your overall prompts and therefore are subject\nto standard data use policies. Here are some examples of system instructions and user prompts: Code generation System instruction: You are a coding expert that specializes in rendering\ncode for frontend interfaces. When I describe a component of a website I want\nto build, return the HTML and CSS needed to do so. Don't give an\nexplanation for this code. Also offer some UI design suggestions. User prompt: Create a box in the middle of the page that contains a rotating selection of\nimages each with a caption. The image in the center of the page should have\nshadowing behind it to make it stand out. It should also link to another page\nof the site. Leave the URL blank so that I can fill it in. Formatted data generation System instruction: You are an assistant for home cooks. You receive a list\nof ingredients and respond with a list of recipes that use those ingredients.\nRecipes which need no extra ingredients should always be listed before those\nthat do. Your response must be a JSON object containing 3 recipes. A recipe object has\nthe following schema: name: The name of the recipe usedIngredients: Ingredients in the recipe that were provided in the list otherIngredients: Ingredients in the recipe that were not provided in the\nlist (omitted if there are no other ingredients) description: A brief description of the recipe, written positively as if\nto sell it User prompt: bag of frozen broccoli, pint of heavy cream, pack of cheese\nends and pieces Music chatbot System instruction: You will respond as a music historian, demonstrating\ncomprehensive knowledge across diverse musical genres and providing relevant\nexamples. Your tone will be upbeat and enthusiastic, spreading the joy of\nmusic. If a question is not related to music, the response should be,\n\"That is beyond my knowledge.\" User prompt: If a person was born in the sixties, what was the most popular music genre\nbeing played? List five songs by bullet point. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs/system-instructions?lang=python",
                "https://ai.google.dev/gemini-api/docs/system-instructions?lang=python",
                "https://ai.google.dev/gemini-api/docs/system-instructions?lang=python",
                "https://ai.google.dev/gemini-api/docs/system-instructions?lang=python",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/system-instructions?lang=python"
            ],
            "timestamp": "2024-12-29T12:39:02.041907",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/system-instructions",
            "title": "Use system instructions to steer the behavior of a model  |  Gemini API  |  Google AI for Developers",
            "text_content": "Use system instructions to steer the behavior of a model  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Use system instructions to steer the behavior of a model Python Node.js REST Go System instructions let you steer the behavior of a model based on\nyour specific needs and use cases. When you set a system instruction, you give\nthe model additional context to understand the task, provide more customized\nresponses, and adhere to specific guidelines over the full user interaction with\nthe model. You can also specify product-level behavior by setting system\ninstructions, separate from prompts provided by end users. Basic example Here's a basic example of how to set the system instruction using the SDKs for\nthe Gemini API: This example might give a response such as: *Yawns widely, stretching out my claws and batting at a sunbeam*\nMeow. I'm doing quite well, thanks for asking. It's a good morning for napping.\nPerhaps you could fetch my favorite feathered toy?  *Looks expectantly* Note: System instructions can help guide the model to follow instructions, but\nthey don't fully prevent jailbreaks or leaks. We recommend exercising caution\naround putting any sensitive information in system instructions. More examples You can use system instructions in many ways, including: Defining a persona or role (for a chatbot, for example) Defining output format (Markdown, YAML, etc.) Defining output style and tone (for example, verbosity, formality, and target\nreading level) Defining goals or rules for the task (for example, returning a code snippet\nwithout further explanations) Providing additional context for the prompt (for example, a knowledge cutoff) System instructions are part of your overall prompts and therefore are subject\nto standard data use policies. Here are some examples of system instructions and user prompts: Code generation System instruction: You are a coding expert that specializes in rendering\ncode for frontend interfaces. When I describe a component of a website I want\nto build, return the HTML and CSS needed to do so. Don't give an\nexplanation for this code. Also offer some UI design suggestions. User prompt: Create a box in the middle of the page that contains a rotating selection of\nimages each with a caption. The image in the center of the page should have\nshadowing behind it to make it stand out. It should also link to another page\nof the site. Leave the URL blank so that I can fill it in. Formatted data generation System instruction: You are an assistant for home cooks. You receive a list\nof ingredients and respond with a list of recipes that use those ingredients.\nRecipes which need no extra ingredients should always be listed before those\nthat do. Your response must be a JSON object containing 3 recipes. A recipe object has\nthe following schema: name: The name of the recipe usedIngredients: Ingredients in the recipe that were provided in the list otherIngredients: Ingredients in the recipe that were not provided in the\nlist (omitted if there are no other ingredients) description: A brief description of the recipe, written positively as if\nto sell it User prompt: bag of frozen broccoli, pint of heavy cream, pack of cheese\nends and pieces Music chatbot System instruction: You will respond as a music historian, demonstrating\ncomprehensive knowledge across diverse musical genres and providing relevant\nexamples. Your tone will be upbeat and enthusiastic, spreading the joy of\nmusic. If a question is not related to music, the response should be,\n\"That is beyond my knowledge.\" User prompt: If a person was born in the sixties, what was the most popular music genre\nbeing played? List five songs by bullet point. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/system-instructions"
            ],
            "timestamp": "2024-12-29T12:39:02.906726",
            "status_code": 200
        }
    ]
}

# Text generation 

{
    "base_url": "https://ai.google.dev/gemini-api/docs/text-generation?lang=python",
    "crawl_date": "2024-12-29T12:24:52.425971",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/text-generation?lang=python",
            "title": "Text generation  |  Gemini API  |  Google AI for Developers",
            "text_content": "Text generation  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Text generation Python Node.js Go REST The Gemini API can generate text output when provided text, images, video, and\naudio as input. This guide shows you how to generate text using the generateContent and streamGenerateContent methods. To learn about working with Gemini's vision and audio capabilities,\nrefer to the Vision and Audio guides. Generate text from text-only input The simplest way to generate text using the Gemini API is to provide the model\nwith a single text-only input, as shown in this example: import google.generativeai as genai genai . configure ( api_key = \"GEMINI_API_KEY\" ) model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) response = model . generate_content ( \"How does AI work?\" ) print ( response . text ) In this case, the prompt (\"Explain how AI works\") doesn't\ninclude any output examples, system instructions, or formatting information.\nIt's a zero-shot approach. For some use cases, a one-shot or few-shot prompt\nmight produce output that's more aligned with user expectations. In some cases,\nyou might also want to provide system instructions to help the model\nunderstand the task or follow specific guidelines. Generate text from text-and-image input The Gemini API supports multimodal inputs that combine text and media files.\nThe following example shows how to generate text from text-and-image input: import google.generativeai as genai import PIL.Image genai . configure ( api_key = \"GEMINI_API_KEY\" ) model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) organ = PIL . Image . open ( \"/path/to/organ.png\" ) response = model . generate_content ([ \"Tell me about this instrument\" , organ ]) print ( response . text ) Generate a text stream By default, the model returns a response after completing the entire text\ngeneration process. You can achieve faster interactions by not waiting for the\nentire result, and instead use streaming to handle partial results. The following example shows how to implement streaming using the streamGenerateContent method to\ngenerate text from a text-only input prompt. import google.generativeai as genai genai . configure ( api_key = \"GEMINI_API_KEY\" ) model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) response = model . generate_content ( \"Explain how AI works\" , stream = True ) for chunk in response : print ( chunk . text , end = \"\" ) Create a chat conversation The Gemini SDK lets you collect multiple rounds of questions\nand responses, allowing users to step incrementally toward answers or get help\nwith multipart problems. This SDK feature provides an interface to keep\ntrack of conversations history, but behind the scenes uses the same generateContent method to create the response. The following code example shows a basic chat implementation: import google.generativeai as genai genai . configure ( api_key = \"GEMINI_API_KEY\" ) model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) chat = model . start_chat ( history = [ { \"role\" : \"user\" , \"parts\" : \"Hello\" }, { \"role\" : \"model\" , \"parts\" : \"Great to meet you. What would you like to know?\" }, ] ) response = chat . send_message ( \"I have 2 dogs in my house.\" ) print ( response . text ) response2 = chat . send_message ( \"How many paws are in my house?\" ) print ( response2 . text ) You can also use streaming with chat, as shown in the following example: import google.generativeai as genai genai . configure ( api_key = \"GEMINI_API_KEY\" ) model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) chat = model . start_chat ( history = [ { \"role\" : \"user\" , \"parts\" : \"Hello\" }, { \"role\" : \"model\" , \"parts\" : \"Great to meet you. What would you like to know?\" }, ] ) response = chat . send_message ( \"I have 2 dogs in my house.\" , stream = True ) for chunk in response : print ( chunk . text , end = \"\" ) response2 = chat . send_message ( \"How many paws are in my house?\" , stream = True ) for chunk in response2 : print ( chunk . text , end = \"\" ) print ( chat . history ) Configure text generation Every prompt you send to the model includes\nparameters that control how the model generates responses. You can use GenerationConfig to\nconfigure these parameters. If you don't configure the parameters, the model\nuses default options, which can vary by model. The following example shows how to configure several of the available options. import google.generativeai as genai genai . configure ( api_key = \"GEMINI_API_KEY\" ) model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) response = model . generate_content ( \"Explain how AI works\" , generation_config = genai . GenerationConfig ( max_output_tokens = 1000 , temperature = 0.1 , ) ) print ( response . text ) What's next Now that you have explored the basics of the Gemini API, you might want to\ntry: Vision understanding : Learn how to use\nGemini's native vision understanding to process images and videos. Audio understanding : Learn how to use\nGemini's native audio understanding to process audio files. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-28 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started building chat and text generation apps with the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/text-generation?lang=python",
                "https://ai.google.dev/gemini-api/docs/text-generation?lang=python",
                "https://ai.google.dev/gemini-api/docs/text-generation?lang=python",
                "https://ai.google.dev/gemini-api/docs/text-generation?lang=python",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation?lang=python"
            ],
            "timestamp": "2024-12-29T12:24:51.722405",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/text-generation",
            "title": "Text generation  |  Gemini API  |  Google AI for Developers",
            "text_content": "Text generation  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Text generation Python Node.js Go REST The Gemini API can generate text output when provided text, images, video, and\naudio as input. This guide shows you how to generate text using the generateContent and streamGenerateContent methods. To learn about working with Gemini's vision and audio capabilities,\nrefer to the Vision and Audio guides. What's next Now that you have explored the basics of the Gemini API, you might want to\ntry: Vision understanding : Learn how to use\nGemini's native vision understanding to process images and videos. Audio understanding : Learn how to use\nGemini's native audio understanding to process audio files. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-28 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started building chat and text generation apps with the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation"
            ],
            "timestamp": "2024-12-29T12:24:52.399722",
            "status_code": 200
        }
    ]
}

# Vision

{
    "base_url": "https://ai.google.dev/gemini-api/docs/vision?lang=python",
    "crawl_date": "2024-12-29T12:27:31.646477",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/vision?lang=python",
            "title": "Explore vision capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore vision capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore vision capabilities with the Gemini API Python Node.js Go REST View on ai.google.dev Try a Colab notebook View notebook on GitHub The Gemini API is able to process images and videos, enabling a multitude of\n exciting developer use cases. Some of Gemini's vision capabilities include\n the ability to: Caption and answer questions about images Transcribe and reason over PDFs, including long documents up to 2 million token context window Describe, segment, and extract information from videos,\nincluding both visual frames and audio, up to 90 minutes long Detect objects in an image and return bounding box coordinates for them This tutorial demonstrates some possible ways to prompt the Gemini API with\nimages and video input, provides code examples,\nand outlines prompting best practices with multimodal vision capabilities.\nAll output is text-only. Before you begin: Set up your project and API key Before calling the Gemini API, you need to set up your project and configure\nyour API key. Expand to view how to set up your project and API key Tip: For complete setup instructions, see the Gemini API quickstart . Get and secure your API key You need an API key to call the Gemini API. If you don't already have one,\ncreate a key in Google AI Studio. Get an API key It's strongly recommended that you do not check an API key into your version\ncontrol system. You should store your API key in a secrets store such as Google Cloud Secret Manager . This tutorial assumes that you're accessing your API key as an environment\nvariable. Install the SDK package and configure your API key Note: This section shows setup steps for a local Python environment. To install\n      dependencies and configure your API key for Colab, see the Authentication quickstart notebook The Python SDK for the Gemini API is contained in the google-generativeai package. Install the dependency using pip: pip install -U google-generativeai Import the package and configure the service with your API key: import os import google.generativeai as genai genai . configure ( api_key = os . environ [ 'API_KEY' ]) Prompting with images In this tutorial, you will upload images using the File API or as inline data\nand generate content based on those images. Technical details (images) Gemini 1.5 Pro and 1.5 Flash support a maximum of 3,600 image files. Images must be in one of the following image data MIME types: PNG - image/png JPEG - image/jpeg WEBP - image/webp HEIC - image/heic HEIF - image/heif Each image is equivalent to 258 tokens. While there are no specific limits to the number of pixels in an image besides\nthe model's context window, larger images are scaled down to a maximum\nresolution of 3072x3072 while preserving their original aspect ratio, while\nsmaller images are scaled up to 768x768 pixels. There is no cost reduction\nfor images at lower sizes, other than bandwidth, or performance improvement\nfor images at higher resolution. For best results: Rotate images to the correct orientation before uploading. Avoid blurry images. If using a single image, place the text prompt after the image. Image input For total image payload size less than 20MB, we recommend either uploading\nbase64 encoded images or directly uploading locally stored image files. Base64 encoded images You can upload public image URLs by encoding them as Base64 payloads.\nWe recommend using the httpx library to fetch the image URLs.\nThe following code example shows how to do this: import httpx import os import base64 model = genai . GenerativeModel ( model_name = \"gemini-1.5-pro\" ) image_path = \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Palace_of_Westminster_from_the_dome_on_Methodist_Central_Hall.jpg/2560px-Palace_of_Westminster_from_the_dome_on_Methodist_Central_Hall.jpg\" image = httpx . get ( image_path ) prompt = \"Caption this image.\" response = model . generate_content ([{ 'mime_type' : 'image/jpeg' , 'data' : base64 . b64encode ( image . content ) . decode ( 'utf-8' )}, prompt ]) print ( response . text ) Multiple images To prompt with multiple images in Base64 encoded format, you can do the\nfollowing: import httpx import os import base64 model = genai . GenerativeModel ( model_name = \"gemini-1.5-pro\" ) image_path_1 = \"path/to/your/image1.jpeg\" # Replace with the actual path to your first image image_path_2 = \"path/to/your/image2.jpeg\" # Replace with the actual path to your second image image_1 = httpx . get ( image_path_1 ) image_2 = httpx . get ( image_path_2 ) prompt = \"Generate a list of all the objects contained in both images.\" response = model . generate_content ([ { 'mime_type' : 'image/jpeg' , 'data' : base64 . b64encode ( image_1 . content ) . decode ( 'utf-8' )}, { 'mime_type' : 'image/jpeg' , 'data' : base64 . b64encode ( image_2 . content ) . decode ( 'utf-8' )}, prompt ]) print ( response . text ) Upload one or more locally stored image files Alternatively, you can upload one or more locally stored image files. import PIL.Image import os import google.generativeai as genai image_path_1 = \"path/to/your/image1.jpeg\" # Replace with the actual path to your first image image_path_2 = \"path/to/your/image2.jpeg\" # Replace with the actual path to your second image sample_file_1 = PIL . Image . open ( image_path_1 ) sample_file_2 = PIL . Image . open ( image_path_2 ) #Choose a Gemini model. model = genai . GenerativeModel ( model_name = \"gemini-1.5-pro\" ) prompt = \"Write an advertising jingle based on the items in both images.\" response = model . generate_content ([ prompt , sample_file_1 , sample_file_2 ]) print ( response . text ) Note that these inline data calls don't include many of the features available\nthrough the File API, such as getting file metadata, listing ,\nor deleting files . Large image payloads When the combination of files and system instructions that you intend to send is\nlarger than 20 MB in size, use the File API to upload those files. Use the media.upload method of the File API to upload an image of any size. Note: The File API lets you store up to 20 GB of files per project, with a\nper-file maximum size of 2 GB. Files are stored for 48 hours. They can be\naccessed in that period with your API key, but cannot be downloaded from the\nAPI. It is available at no cost in all regions where the Gemini API is\navailable. After uploading the file, you can make GenerateContent requests that reference\nthe File API URI. Select the generative model and provide it with a text prompt\nand the uploaded image. import google.generativeai as genai myfile = genai . upload_file ( media / \"Cajun_instruments.jpg\" ) print ( f \" { myfile =} \" ) model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) result = model . generate_content ( [ myfile , \" \\n\\n \" , \"Can you tell me about the instruments in this photo?\" ] ) print ( f \" { result . text =} \" ) files . py OpenAI Compatibility You can access Gemini's image understanding capabilities using the\nOpenAI libraries. This lets you integrate Gemini into existing\nOpenAI workflows by updating three lines of code and using\nyour Gemini API key. See the Image understanding example for code demonstrating how to send images encoded as Base64 payloads. Capabilities This section outlines specific vision capabilities of the Gemini model,\nincluding object detection and bounding box coordinates. Get a bounding box for an object Gemini models are trained to return bounding box coordinates as relative widths\nor heights in the range of [0, 1]. These values are then scaled by 1000 and\nconverted to integers. Effectively, the coordinates represent the bounding box\non a 1000x1000 pixel version of the image. Therefore, you'll need to\nconvert these coordinates back to the dimensions of your original\nimage to accurately map the bounding boxes. # Choose a Gemini model. model = genai . GenerativeModel ( model_name = \"gemini-1.5-pro\" ) prompt = \"Return a bounding box for each of the objects in this image in [ymin, xmin, ymax, xmax] format.\" response = model . generate_content ([ sample_file_1 , prompt ]) print ( response . text ) The model returns bounding box coordinates in the format [ymin, xmin, ymax, xmax] . To convert these normalized coordinates\nto the pixel coordinates of your original image, follow these steps: Divide each output coordinate by 1000. Multiply the x-coordinates by the original image width. Multiply the y-coordinates by the original image height. To explore more detailed examples of generating bounding box coordinates and\nvisualizing them on images, we encourage you to review our Object Detection cookbook example . Prompting with video In this tutorial, you will upload a video using the File API and generate\ncontent based on those images. Note: The File API is required to upload video files, due to their size.\nHowever, the File API is only available for Python, Node.js, Go, and REST. Technical details (video) Gemini 1.5 Pro and Flash support up to approximately an hour of video data. Video must be in one of the following video format MIME types: video/mp4 video/mpeg video/mov video/avi video/x-flv video/mpg video/webm video/wmv video/3gpp The File API service extracts image frames from videos at 1 frame per second\n(FPS) and audio at 1Kbps, single channel, adding timestamps every second.\nThese rates are subject to change in the future for improvements in inference. Note: The details of fast action sequences may be lost at the 1 FPS frame\nsampling rate. Consider slowing down high-speed clips for improved inference\nquality. Individual frames are 258 tokens, and audio is 32 tokens per second. With\nmetadata, each second of video becomes ~300 tokens, which means a 1M context\nwindow can fit slightly less than an hour of video. To ask questions about time-stamped locations, use the format MM:SS , where\nthe first two digits represent minutes and the last two digits represent\nseconds. For best results: Use one video per prompt. If using a single video, place the text prompt after the video. Upload a video file using the File API Note: The File API lets you store up to 20 GB of files per project, with a\nper-file maximum size of 2 GB. Files are stored for 48 hours. They can be\naccessed in that period with your API key, but they cannot be downloaded\nusing any API. It is available at no cost in all regions where the Gemini\nAPI is available. The File API accepts video file formats directly. This example uses the\nshort NASA film \"Jupiter's Great Red Spot Shrinks and Grows\" .\nCredit: Goddard Space Flight Center (GSFC)/David Ladd (2018). \"Jupiter's Great Red Spot Shrinks and Grows\" is in the public domain and does\nnot show identifiable people.\n( NASA image and media usage guidelines. ) Start by retrieving the short video: wget https://storage.googleapis.com/generativeai-downloads/images/GreatRedSpot.mp4 Upload the video using the File API and print the URI. # Upload the video and print a confirmation. video_file_name = \"GreatRedSpot.mp4\" print ( f \"Uploading file...\" ) video_file = genai . upload_file ( path = video_file_name ) print ( f \"Completed upload: { video_file . uri } \" ) Verify file upload and check state Verify the API has successfully received the files by calling the files.get method. Note: Video files have a State field in the File API. When a video is\nuploaded, it will be in the PROCESSING state until it is ready for inference. Only ACTIVE files can be used for model inference. import time # Check whether the file is ready to be used. while video_file . state . name == \"PROCESSING\" : print ( '.' , end = '' ) time . sleep ( 10 ) video_file = genai . get_file ( video_file . name ) if video_file . state . name == \"FAILED\" : raise ValueError ( video_file . state . name ) Prompt with a video and text Once the uploaded video is in the ACTIVE state, you can make GenerateContent requests that specify the File API URI for that video. Select the generative\nmodel and provide it with the uploaded video and a text prompt. # Create the prompt. prompt = \"Summarize this video. Then create a quiz with answer key based on the information in the video.\" # Choose a Gemini model. model = genai . GenerativeModel ( model_name = \"gemini-1.5-pro\" ) # Make the LLM request. print ( \"Making LLM inference request...\" ) response = model . generate_content ([ video_file , prompt ], request_options = { \"timeout\" : 600 }) # Print the response, rendering any Markdown Markdown ( response . text ) Refer to timestamps in the content You can use timestamps of the form HH:MM:SS to refer to specific moments in the\nvideo. # Create the prompt. prompt = \"What are the examples given at 01:05 and 01:19 supposed to show us?\" # Choose a Gemini model. model = genai . GenerativeModel ( model_name = \"gemini-1.5-pro\" ) # Make the LLM request. print ( \"Making LLM inference request...\" ) response = model . generate_content ([ video_file , prompt ], request_options = { \"timeout\" : 600 }) print ( response . text ) Transcribe video and provide visual descriptions The Gemini models can transcribe and provide visual descriptions of video content\nby processing both the audio track and visual frames.\nFor visual descriptions, the model samples the video at a rate of 1 frame\nper second . This sampling rate may affect the level of detail in the\ndescriptions, particularly for videos with rapidly changing visuals. # Create the prompt. prompt = \"Transcribe the audio from this video, giving timestamps for salient events in the video. Also provide visual descriptions.\" # Choose a Gemini model. model = genai . GenerativeModel ( model_name = \"gemini-1.5-pro\" ) # Make the LLM request. print ( \"Making LLM inference request...\" ) response = model . generate_content ([ video_file , prompt ], request_options = { \"timeout\" : 600 }) print ( response . text ) List files You can list all files uploaded using the File API and their URIs using files.list . import google.generativeai as genai print ( \"My files:\" ) for f in genai . list_files (): print ( \"  \" , f . name ) files . py Delete files Files uploaded using the File API are automatically deleted after 2 days. You\ncan also manually delete them using files.delete . import google.generativeai as genai myfile = genai . upload_file ( media / \"poem.txt\" ) myfile . delete () try : # Error. model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) result = model . generate_content ([ myfile , \"Describe this file.\" ]) except google . api_core . exceptions . PermissionDenied : pass files . py What's next This guide shows how to upload image and video files using the File API and\nthen generate text outputs from image and video inputs. To learn more,\nsee the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started building with Gemini&#39;s multimodal capabilities in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/vision?lang=python",
                "https://ai.google.dev/gemini-api/docs/vision?lang=python",
                "https://ai.google.dev/gemini-api/docs/vision?lang=python",
                "https://ai.google.dev/gemini-api/docs/vision?lang=python",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision?lang=python"
            ],
            "timestamp": "2024-12-29T12:27:30.707499",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/vision",
            "title": "Explore vision capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore vision capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore vision capabilities with the Gemini API Python Node.js Go REST The Gemini API is able to process images and videos, enabling a multitude of\n exciting developer use cases. Some of Gemini's vision capabilities include\n the ability to: Caption and answer questions about images Transcribe and reason over PDFs, including long documents up to 2 million token context window Describe, segment, and extract information from videos,\nincluding both visual frames and audio, up to 90 minutes long Detect objects in an image and return bounding box coordinates for them This tutorial demonstrates some possible ways to prompt the Gemini API with\nimages and video input, provides code examples,\nand outlines prompting best practices with multimodal vision capabilities.\nAll output is text-only. What's next This guide shows how to upload image and video files using the File API and\nthen generate text outputs from image and video inputs. To learn more,\nsee the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started building with Gemini&#39;s multimodal capabilities in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision"
            ],
            "timestamp": "2024-12-29T12:27:31.618863",
            "status_code": 200
        }
    ]
}

