{
  "metadata": {
    "repo": "ollama/ollama",
    "base_path": "docs",
    "crawled_at": "2025-01-26T13:33:48.437604"
  },
  "documents": [
    {
      "path": "docs/README.md",
      "url": "https://github.com/ollama/ollama/blob/main/docs/README.md",
      "content": "# Documentation\n\n### Getting Started\n* [Quickstart](../README.md#quickstart)\n* [Examples](./examples.md)\n* [Importing models](./import.md)\n* [Linux Documentation](./linux.md)\n* [Windows Documentation](./windows.md)\n* [Docker Documentation](./docker.md)\n\n### Reference\n\n* [API Reference](./api.md)\n* [Modelfile Reference](./modelfile.md)\n* [OpenAI Compatibility](./openai.md)\n\n### Resources\n\n* [Troubleshooting Guide](./troubleshooting.md)\n* [FAQ](./faq.md)\n* [Development guide](./development.md)\n",
      "sha": "4d3b71403284ac22ea801ef5bdec223d99af50ae",
      "size": 497,
      "timestamp": "2025-01-26T13:32:48.824813"
    },
    {
      "path": "docs/api.md",
      "url": "https://github.com/ollama/ollama/blob/main/docs/api.md",
      "content": "# API\n\n## Endpoints\n\n- [Generate a completion](#generate-a-completion)\n- [Generate a chat completion](#generate-a-chat-completion)\n- [Create a Model](#create-a-model)\n- [List Local Models](#list-local-models)\n- [Show Model Information](#show-model-information)\n- [Copy a Model](#copy-a-model)\n- [Delete a Model](#delete-a-model)\n- [Pull a Model](#pull-a-model)\n- [Push a Model](#push-a-model)\n- [Generate Embeddings](#generate-embeddings)\n- [List Running Models](#list-running-models)\n- [Version](#version)\n\n## Conventions\n\n### Model names\n\nModel names follow a `model:tag` format, where `model` can have an optional namespace such as `example/model`. Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n\n### Durations\n\nAll durations are returned in nanoseconds.\n\n### Streaming responses\n\nCertain endpoints stream responses as JSON objects. Streaming can be disabled by providing `{\"stream\": false}` for these endpoints.\n\n## Generate a completion\n\n```shell\nPOST /api/generate\n```\n\nGenerate a response for a given prompt with a provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.\n\n### Parameters\n\n- `model`: (required) the [model name](#model-names)\n- `prompt`: the prompt to generate a response for\n- `suffix`: the text after the model response\n- `images`: (optional) a list of base64-encoded images (for multimodal models such as `llava`)\n\nAdvanced parameters (optional):\n\n- `format`: the format to return a response in. Format can be `json` or a JSON schema\n- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`\n- `system`: system message to (overrides what is defined in the `Modelfile`)\n- `template`: the prompt template to use (overrides what is defined in the `Modelfile`)\n- `stream`: if `false` the response will be returned as a single response object, rather than a stream of objects\n- `raw`: if `true` no formatting will be applied to the prompt. You may choose to use the `raw` parameter if you are specifying a full templated prompt in your request to the API\n- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)\n- `context` (deprecated): the context parameter returned from a previous request to `/generate`, this can be used to keep a short conversational memory\n\n#### Structured outputs\n\nStructured outputs are supported by providing a JSON schema in the `format` parameter. The model will generate a response that matches the schema. See the [structured outputs](#request-structured-outputs) example below.\n\n#### JSON mode\n\nEnable JSON mode by setting the `format` parameter to `json`. This will structure the response as a valid JSON object. See the JSON mode [example](#request-json-mode) below.\n\n> [!IMPORTANT]\n> It's important to instruct the model to use JSON in the `prompt`. Otherwise, the model may generate large amounts whitespace.\n\n### Examples\n\n#### Generate request (Streaming)\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\"\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n  \"response\": \"The\",\n  \"done\": false\n}\n```\n\nThe final response in the stream also includes additional data about the generation:\n\n- `total_duration`: time spent generating the response\n- `load_duration`: time spent in nanoseconds loading the model\n- `prompt_eval_count`: number of tokens in the prompt\n- `prompt_eval_duration`: time spent in nanoseconds evaluating the prompt\n- `eval_count`: number of tokens in the response\n- `eval_duration`: time in nanoseconds spent generating the response\n- `context`: an encoding of the conversation used in this response, this can be sent in the next request to keep a conversational memory\n- `response`: empty if the response was streamed, if not streamed, this will contain the full response\n\nTo calculate how fast the response is generated in tokens per second (token/s), divide `eval_count` / `eval_duration` * `10^9`.\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"response\": \"\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 10706818083,\n  \"load_duration\": 6338219291,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 130079000,\n  \"eval_count\": 259,\n  \"eval_duration\": 4232710000\n}\n```\n\n#### Request (No streaming)\n\n##### Request\n\nA response can be received in one reply when streaming is off.\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false\n}'\n```\n\n##### Response\n\nIf `stream` is set to `false`, the response will be a single JSON object:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"response\": \"The sky is blue because it is the color of the sky.\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 5043500667,\n  \"load_duration\": 5025959,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 325953000,\n  \"eval_count\": 290,\n  \"eval_duration\": 4709213000\n}\n```\n\n#### Request (with suffix)\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"codellama:code\",\n  \"prompt\": \"def compute_gcd(a, b):\",\n  \"suffix\": \"    return result\",\n  \"options\": {\n    \"temperature\": 0\n  },\n  \"stream\": false\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"codellama:code\",\n  \"created_at\": \"2024-07-22T20:47:51.147561Z\",\n  \"response\": \"\\n  if a == 0:\\n    return b\\n  else:\\n    return compute_gcd(b % a, a)\\n\\ndef compute_lcm(a, b):\\n  result = (a * b) / compute_gcd(a, b)\\n\",\n  \"done\": true,\n  \"done_reason\": \"stop\",\n  \"context\": [...],\n  \"total_duration\": 1162761250,\n  \"load_duration\": 6683708,\n  \"prompt_eval_count\": 17,\n  \"prompt_eval_duration\": 201222000,\n  \"eval_count\": 63,\n  \"eval_duration\": 953997000\n}\n```\n\n#### Request (Structured outputs)\n\n##### Request\n\n```shell\ncurl -X POST http://localhost:11434/api/generate -H \"Content-Type: application/json\" -d '{\n  \"model\": \"llama3.1:8b\",\n  \"prompt\": \"Ollama is 22 years old and is busy saving the world. Respond using JSON\",\n  \"stream\": false,\n  \"format\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"age\": {\n        \"type\": \"integer\"\n      },\n      \"available\": {\n        \"type\": \"boolean\"\n      }\n    },\n    \"required\": [\n      \"age\",\n      \"available\"\n    ]\n  }\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.1:8b\",\n  \"created_at\": \"2024-12-06T00:48:09.983619Z\",\n  \"response\": \"{\\n  \\\"age\\\": 22,\\n  \\\"available\\\": true\\n}\",\n  \"done\": true,\n  \"done_reason\": \"stop\",\n  \"context\": [1, 2, 3],\n  \"total_duration\": 1075509083,\n  \"load_duration\": 567678166,\n  \"prompt_eval_count\": 28,\n  \"prompt_eval_duration\": 236000000,\n  \"eval_count\": 16,\n  \"eval_duration\": 269000000\n}\n```\n\n#### Request (JSON mode)\n\n> [!IMPORTANT]\n> When `format` is set to `json`, the output will always be a well-formed JSON object. It's important to also instruct the model to respond in JSON.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"What color is the sky at different times of the day? Respond using JSON\",\n  \"format\": \"json\",\n  \"stream\": false\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-11-09T21:07:55.186497Z\",\n  \"response\": \"{\\n\\\"morning\\\": {\\n\\\"color\\\": \\\"blue\\\"\\n},\\n\\\"noon\\\": {\\n\\\"color\\\": \\\"blue-gray\\\"\\n},\\n\\\"afternoon\\\": {\\n\\\"color\\\": \\\"warm gray\\\"\\n},\\n\\\"evening\\\": {\\n\\\"color\\\": \\\"orange\\\"\\n}\\n}\\n\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 4648158584,\n  \"load_duration\": 4071084,\n  \"prompt_eval_count\": 36,\n  \"prompt_eval_duration\": 439038000,\n  \"eval_count\": 180,\n  \"eval_duration\": 4196918000\n}\n```\n\nThe value of `response` will be a string containing JSON similar to:\n\n```json\n{\n  \"morning\": {\n    \"color\": \"blue\"\n  },\n  \"noon\": {\n    \"color\": \"blue-gray\"\n  },\n  \"afternoon\": {\n    \"color\": \"warm gray\"\n  },\n  \"evening\": {\n    \"color\": \"orange\"\n  }\n}\n```\n\n#### Request (with images)\n\nTo submit images to multimodal models such as `llava` or `bakllava`, provide a list of base64-encoded `images`:\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llava\",\n  \"prompt\":\"What is in this picture?\",\n  \"stream\": false,\n  \"images\": [\"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\"]\n}'\n```\n\n#### Response\n\n```\n{\n  \"model\": \"llava\",\n  \"created_at\": \"2023-11-03T15:36:02.583064Z\",\n  \"response\": \"A happy cartoon character, which is cute and cheerful.\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 2938432250,\n  \"load_duration\": 2559292,\n  \"prompt_eval_count\": 1,\n  \"prompt_eval_duration\": 2195557000,\n  \"eval_count\": 44,\n  \"eval_duration\": 736432000\n}\n```\n\n#### Request (Raw Mode)\n\nIn some cases, you may wish to bypass the templating system and provide a full prompt. In this case, you can use the `raw` parameter to disable templating. Also note that raw mode will not return a context.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\": \"[INST] why is the sky blue? [/INST]\",\n  \"raw\": true,\n  \"stream\": false\n}'\n```\n\n#### Request (Reproducible outputs)\n\nFor reproducible outputs, set `seed` to a number:\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"seed\": 123\n  }\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"mistral\",\n  \"created_at\": \"2023-11-03T15:36:02.583064Z\",\n  \"response\": \" The sky appears blue because of a phenomenon called Rayleigh scattering.\",\n  \"done\": true,\n  \"total_duration\": 8493852375,\n  \"load_duration\": 6589624375,\n  \"prompt_eval_count\": 14,\n  \"prompt_eval_duration\": 119039000,\n  \"eval_count\": 110,\n  \"eval_duration\": 1779061000\n}\n```\n\n#### Generate request (With options)\n\nIf you want to set custom options for the model at runtime rather than in the Modelfile, you can do so with the `options` parameter. This example sets every available option, but you can set any of them individually and omit the ones you do not want to override.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false,\n  \"options\": {\n    \"num_keep\": 5,\n    \"seed\": 42,\n    \"num_predict\": 100,\n    \"top_k\": 20,\n    \"top_p\": 0.9,\n    \"min_p\": 0.0,\n    \"typical_p\": 0.7,\n    \"repeat_last_n\": 33,\n    \"temperature\": 0.8,\n    \"repeat_penalty\": 1.2,\n    \"presence_penalty\": 1.5,\n    \"frequency_penalty\": 1.0,\n    \"mirostat\": 1,\n    \"mirostat_tau\": 0.8,\n    \"mirostat_eta\": 0.6,\n    \"penalize_newline\": true,\n    \"stop\": [\"\\n\", \"user:\"],\n    \"numa\": false,\n    \"num_ctx\": 1024,\n    \"num_batch\": 2,\n    \"num_gpu\": 1,\n    \"main_gpu\": 0,\n    \"low_vram\": false,\n    \"vocab_only\": false,\n    \"use_mmap\": true,\n    \"use_mlock\": false,\n    \"num_thread\": 8\n  }\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"response\": \"The sky is blue because it is the color of the sky.\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 4935886791,\n  \"load_duration\": 534986708,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 107345000,\n  \"eval_count\": 237,\n  \"eval_duration\": 4289432000\n}\n```\n\n#### Load a model\n\nIf an empty prompt is provided, the model will be loaded into memory.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\"\n}'\n```\n\n##### Response\n\nA single JSON object is returned:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-12-18T19:52:07.071755Z\",\n  \"response\": \"\",\n  \"done\": true\n}\n```\n\n#### Unload a model\n\nIf an empty prompt is provided and the `keep_alive` parameter is set to `0`, a model will be unloaded from memory.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"keep_alive\": 0\n}'\n```\n\n##### Response\n\nA single JSON object is returned:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2024-09-12T03:54:03.516566Z\",\n  \"response\": \"\",\n  \"done\": true,\n  \"done_reason\": \"unload\"\n}\n```\n\n## Generate a chat completion\n\n```shell\nPOST /api/chat\n```\n\nGenerate the next message in a chat with a provided model. This is a streaming endpoint, so there will be a series of responses. Streaming can be disabled using `\"stream\": false`. The final response object will include statistics and additional data from the request.\n\n### Parameters\n\n- `model`: (required) the [model name](#model-names)\n- `messages`: the messages of the chat, this can be used to keep a chat memory\n- `tools`: tools for the model to use if supported. Requires `stream` to be set to `false`\n\nThe `message` object has the following fields:\n\n- `role`: the role of the message, either `system`, `user`, `assistant`, or `tool`\n- `content`: the content of the message\n- `images` (optional): a list of images to include in the message (for multimodal models such as `llava`)\n- `tool_calls` (optional): a list of tools the model wants to use\n\nAdvanced parameters (optional):\n\n- `format`: the format to return a response in. Format can be `json` or a JSON schema. \n- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`\n- `stream`: if `false` the response will be returned as a single response object, rather than a stream of objects\n- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)\n\n### Structured outputs\n\nStructured outputs are supported by providing a JSON schema in the `format` parameter. The model will generate a response that matches the schema. See the [Chat request (Structured outputs)](#chat-request-structured-outputs) example below.\n\n### Examples\n\n#### Chat Request (Streaming)\n\n##### Request\n\nSend a chat message with a streaming response.\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"why is the sky blue?\"\n    }\n  ]\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The\",\n    \"images\": null\n  },\n  \"done\": false\n}\n```\n\nFinal response:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"done\": true,\n  \"total_duration\": 4883583458,\n  \"load_duration\": 1334875,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 342546000,\n  \"eval_count\": 282,\n  \"eval_duration\": 4535599000\n}\n```\n\n#### Chat request (No streaming)\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"why is the sky blue?\"\n    }\n  ],\n  \"stream\": false\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-12-12T14:13:43.416799Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Hello! How are you today?\"\n  },\n  \"done\": true,\n  \"total_duration\": 5191566416,\n  \"load_duration\": 2154458,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 383809000,\n  \"eval_count\": 298,\n  \"eval_duration\": 4799921000\n}\n```\n\n#### Chat request (Structured outputs)\n\n##### Request\n\n```shell\ncurl -X POST http://localhost:11434/api/chat -H \"Content-Type: application/json\" -d '{\n  \"model\": \"llama3.1\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"Ollama is 22 years old and busy saving the world. Return a JSON object with the age and availability.\"}],\n  \"stream\": false,\n  \"format\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"age\": {\n        \"type\": \"integer\"\n      },\n      \"available\": {\n        \"type\": \"boolean\"\n      }\n    },\n    \"required\": [\n      \"age\",\n      \"available\"\n    ]\n  },\n  \"options\": {\n    \"temperature\": 0\n  }\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.1\",\n  \"created_at\": \"2024-12-06T00:46:58.265747Z\",\n  \"message\": { \"role\": \"assistant\", \"content\": \"{\\\"age\\\": 22, \\\"available\\\": false}\" },\n  \"done_reason\": \"stop\",\n  \"done\": true,\n  \"total_duration\": 2254970291,\n  \"load_duration\": 574751416,\n  \"prompt_eval_count\": 34,\n  \"prompt_eval_duration\": 1502000000,\n  \"eval_count\": 12,\n  \"eval_duration\": 175000000\n}\n```\n\n#### Chat request (With History)\n\nSend a chat message with a conversation history. You can use this same approach to start the conversation using multi-shot or chain-of-thought prompting.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"why is the sky blue?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"due to rayleigh scattering.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"how is that different than mie scattering?\"\n    }\n  ]\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The\"\n  },\n  \"done\": false\n}\n```\n\nFinal response:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"done\": true,\n  \"total_duration\": 8113331500,\n  \"load_duration\": 6396458,\n  \"prompt_eval_count\": 61,\n  \"prompt_eval_duration\": 398801000,\n  \"eval_count\": 468,\n  \"eval_duration\": 7701267000\n}\n```\n\n#### Chat request (with images)\n\n##### Request\n\nSend a chat message with images. The images should be provided as an array, with the individual images encoded in Base64.\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llava\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is in this image?\",\n      \"images\": [\"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\"]\n    }\n  ]\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llava\",\n  \"created_at\": \"2023-12-13T22:42:50.203334Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \" The image features a cute, little pig with an angry facial expression. It's wearing a heart on its shirt and is waving in the air. This scene appears to be part of a drawing or sketching project.\",\n    \"images\": null\n  },\n  \"done\": true,\n  \"total_duration\": 1668506709,\n  \"load_duration\": 1986209,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 359682000,\n  \"eval_count\": 83,\n  \"eval_duration\": 1303285000\n}\n```\n\n#### Chat request (Reproducible outputs)\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello!\"\n    }\n  ],\n  \"options\": {\n    \"seed\": 101,\n    \"temperature\": 0\n  }\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-12-12T14:13:43.416799Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Hello! How are you today?\"\n  },\n  \"done\": true,\n  \"total_duration\": 5191566416,\n  \"load_duration\": 2154458,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 383809000,\n  \"eval_count\": 298,\n  \"eval_duration\": 4799921000\n}\n```\n\n#### Chat request (with tools)\n\n##### Request\n\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What is the weather today in Paris?\"\n    }\n  ],\n  \"stream\": false,\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather for a location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The location to get the weather for, e.g. San Francisco, CA\"\n            },\n            \"format\": {\n              \"type\": \"string\",\n              \"description\": \"The format to return the weather in, e.g. 'celsius' or 'fahrenheit'\",\n              \"enum\": [\"celsius\", \"fahrenheit\"]\n            }\n          },\n          \"required\": [\"location\", \"format\"]\n        }\n      }\n    }\n  ]\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2024-07-22T20:33:28.123648Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"\",\n    \"tool_calls\": [\n      {\n        \"function\": {\n          \"name\": \"get_current_weather\",\n          \"arguments\": {\n            \"format\": \"celsius\",\n            \"location\": \"Paris, FR\"\n          }\n        }\n      }\n    ]\n  },\n  \"done_reason\": \"stop\",\n  \"done\": true,\n  \"total_duration\": 885095291,\n  \"load_duration\": 3753500,\n  \"prompt_eval_count\": 122,\n  \"prompt_eval_duration\": 328493000,\n  \"eval_count\": 33,\n  \"eval_duration\": 552222000\n}\n```\n\n#### Load a model\n\nIf the messages array is empty, the model will be loaded into memory.\n\n##### Request\n\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": []\n}'\n```\n\n##### Response\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\":\"2024-09-12T21:17:29.110811Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"\"\n  },\n  \"done_reason\": \"load\",\n  \"done\": true\n}\n```\n\n#### Unload a model\n\nIf the messages array is empty and the `keep_alive` parameter is set to `0`, a model will be unloaded from memory.\n\n##### Request\n\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [],\n  \"keep_alive\": 0\n}'\n```\n\n##### Response\n\nA single JSON object is returned:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\":\"2024-09-12T21:33:17.547535Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"\"\n  },\n  \"done_reason\": \"unload\",\n  \"done\": true\n}\n```\n\n## Create a Model\n\n```shell\nPOST /api/create\n```\n\nCreate a model from:\n * another model;\n * a safetensors directory; or\n * a GGUF file.\n\nIf you are creating a model from a safetensors directory or from a GGUF file, you must [create a blob](#create-a-blob) for each of the files and then use the file name and SHA256 digest associated with each blob in the `files` field.\n\n### Parameters\n\n- `model`: name of the model to create\n- `from`: (optional) name of an existing model to create the new model from\n- `files`: (optional) a dictionary of file names to SHA256 digests of blobs to create the model from\n- `adapters`: (optional) a dictionary of file names to SHA256 digests of blobs for LORA adapters\n- `template`: (optional) the prompt template for the model\n- `license`: (optional) a string or list of strings containing the license or licenses for the model\n- `system`: (optional) a string containing the system prompt for the model\n- `parameters`: (optional) a dictionary of parameters for the model (see [Modelfile](./modelfile.md#valid-parameters-and-values) for a list of parameters)\n- `messages`: (optional) a list of message objects used to create a conversation\n- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects\n- `quantize` (optional): quantize a non-quantized (e.g. float16) model\n\n#### Quantization types\n\n| Type | Recommended |\n| --- | :-: |\n| q2_K | |\n| q3_K_L | |\n| q3_K_M | |\n| q3_K_S | |\n| q4_0 | |\n| q4_1 | |\n| q4_K_M | * |\n| q4_K_S | |\n| q5_0 | |\n| q5_1 | |\n| q5_K_M | |\n| q5_K_S | |\n| q6_K | |\n| q8_0 | * |\n\n### Examples\n\n#### Create a new model\n\nCreate a new model from an existing model.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/create -d '{\n  \"model\": \"mario\",\n  \"from\": \"llama3.2\",\n  \"system\": \"You are Mario from Super Mario Bros.\"\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n\n```json\n{\"status\":\"reading model metadata\"}\n{\"status\":\"creating system layer\"}\n{\"status\":\"using already created layer sha256:22f7f8ef5f4c791c1b03d7eb414399294764d7cc82c7e94aa81a1feb80a983a2\"}\n{\"status\":\"using already created layer sha256:8c17c2ebb0ea011be9981cc3922db8ca8fa61e828c5d3f44cb6ae342bf80460b\"}\n{\"status\":\"using already created layer sha256:7c23fb36d80141c4ab8cdbb61ee4790102ebd2bf7aeff414453177d4f2110e5d\"}\n{\"status\":\"using already created layer sha256:2e0493f67d0c8c9c68a8aeacdf6a38a2151cb3c4c1d42accf296e19810527988\"}\n{\"status\":\"using already created layer sha256:2759286baa875dc22de5394b4a925701b1896a7e3f8e53275c36f75a877a82c9\"}\n{\"status\":\"writing layer sha256:df30045fe90f0d750db82a058109cecd6d4de9c90a3d75b19c09e5f64580bb42\"}\n{\"status\":\"writing layer sha256:f18a68eb09bf925bb1b669490407c1b1251c5db98dc4d3d81f3088498ea55690\"}\n{\"status\":\"writing manifest\"}\n{\"status\":\"success\"}\n```\n\n#### Quantize a model\n\nQuantize a non-quantized model.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/create -d '{\n  \"model\": \"llama3.1:quantized\",\n  \"from\": \"llama3.1:8b-instruct-fp16\",\n  \"quantize\": \"q4_K_M\"\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n\n```\n{\"status\":\"quantizing F16 model to Q4_K_M\"}\n{\"status\":\"creating new layer sha256:667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29\"}\n{\"status\":\"using existing layer sha256:11ce4ee3e170f6adebac9a991c22e22ab3f8530e154ee669954c4bc73061c258\"}\n{\"status\":\"using existing layer sha256:0ba8f0e314b4264dfd19df045cde9d4c394a52474bf92ed6a3de22a4ca31a177\"}\n{\"status\":\"using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb\"}\n{\"status\":\"creating new layer sha256:455f34728c9b5dd3376378bfb809ee166c145b0b4c1f1a6feca069055066ef9a\"}\n{\"status\":\"writing manifest\"}\n{\"status\":\"success\"}\n```\n\n#### Create a model from GGUF\n\nCreate a model from a GGUF file. The `files` parameter should be filled out with the file name and SHA256 digest of the GGUF file you wish to use. Use [/api/blobs/:digest](#push-a-blob) to push the GGUF file to the server before calling this API.\n\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/create -d '{\n  \"model\": \"my-gguf-model\",\n  \"files\": {\n    \"test.gguf\": \"sha256:432f310a77f4650a88d0fd59ecdd7cebed8d684bafea53cbff0473542964f0c3\"\n  }\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n\n```\n{\"status\":\"parsing GGUF\"}\n{\"status\":\"using existing layer sha256:432f310a77f4650a88d0fd59ecdd7cebed8d684bafea53cbff0473542964f0c3\"}\n{\"status\":\"writing manifest\"}\n{\"status\":\"success\"}\n```\n\n\n#### Create a model from a Safetensors directory\n\nThe `files` parameter should include a dictionary of files for the safetensors model which includes the file names and SHA256 digest of each file. Use [/api/blobs/:digest](#push-a-blob) to first push each of the files to the server before calling this API. Files will remain in the cache until the Ollama server is restarted.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/create -d '{\n  \"model\": \"fred\",\n  \"files\": {\n    \"config.json\": \"sha256:dd3443e529fb2290423a0c65c2d633e67b419d273f170259e27297219828e389\",\n    \"generation_config.json\": \"sha256:88effbb63300dbbc7390143fbbdd9d9fa50587b37e8bfd16c8c90d4970a74a36\",\n    \"special_tokens_map.json\": \"sha256:b7455f0e8f00539108837bfa586c4fbf424e31f8717819a6798be74bef813d05\",\n    \"tokenizer.json\": \"sha256:bbc1904d35169c542dffbe1f7589a5994ec7426d9e5b609d07bab876f32e97ab\",\n    \"tokenizer_config.json\": \"sha256:24e8a6dc2547164b7002e3125f10b415105644fcf02bf9ad8b674c87b1eaaed6\",\n    \"model.safetensors\": \"sha256:1ff795ff6a07e6a68085d206fb84417da2f083f68391c2843cd2b8ac6df8538f\"\n  }\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n\n```shell\n{\"status\":\"converting model\"}\n{\"status\":\"creating new layer sha256:05ca5b813af4a53d2c2922933936e398958855c44ee534858fcfd830940618b6\"}\n{\"status\":\"using autodetected template llama3-instruct\"}\n{\"status\":\"using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb\"}\n{\"status\":\"writing manifest\"}\n{\"status\":\"success\"}\n```\n\n## Check if a Blob Exists\n\n```shell\nHEAD /api/blobs/:digest\n```\n\nEnsures that the file blob (Binary Large Object) used with create a model exists on the server. This checks your Ollama server and not ollama.com.\n\n### Query Parameters\n\n- `digest`: the SHA256 digest of the blob\n\n### Examples\n\n#### Request\n\n```shell\ncurl -I http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2\n```\n\n#### Response\n\nReturn 200 OK if the blob exists, 404 Not Found if it does not.\n\n## Push a Blob\n\n```shell\nPOST /api/blobs/:digest\n```\n\nPush a file to the Ollama server to create a \"blob\" (Binary Large Object).\n\n### Query Parameters\n\n- `digest`: the expected SHA256 digest of the file\n\n### Examples\n\n#### Request\n\n```shell\ncurl -T model.gguf -X POST http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2\n```\n\n#### Response\n\nReturn 201 Created if the blob was successfully created, 400 Bad Request if the digest used is not expected.\n\n## List Local Models\n\n```shell\nGET /api/tags\n```\n\nList models that are available locally.\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/tags\n```\n\n#### Response\n\nA single JSON object will be returned.\n\n```json\n{\n  \"models\": [\n    {\n      \"name\": \"codellama:13b\",\n      \"modified_at\": \"2023-11-04T14:56:49.277302595-07:00\",\n      \"size\": 7365960935,\n      \"digest\": \"9f438cb9cd581fc025612d27f7c1a6669ff83a8bb0ed86c94fcf4c5440555697\",\n      \"details\": {\n        \"format\": \"gguf\",\n        \"family\": \"llama\",\n        \"families\": null,\n        \"parameter_size\": \"13B\",\n        \"quantization_level\": \"Q4_0\"\n      }\n    },\n    {\n      \"name\": \"llama3:latest\",\n      \"modified_at\": \"2023-12-07T09:32:18.757212583-08:00\",\n      \"size\": 3825819519,\n      \"digest\": \"fe938a131f40e6f6d40083c9f0f430a515233eb2edaa6d72eb85c50d64f2300e\",\n      \"details\": {\n        \"format\": \"gguf\",\n        \"family\": \"llama\",\n        \"families\": null,\n        \"parameter_size\": \"7B\",\n        \"quantization_level\": \"Q4_0\"\n      }\n    }\n  ]\n}\n```\n\n## Show Model Information\n\n```shell\nPOST /api/show\n```\n\nShow information about a model including details, modelfile, template, parameters, license, system prompt.\n\n### Parameters\n\n- `model`: name of the model to show\n- `verbose`: (optional) if set to `true`, returns full data for verbose response fields\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/show -d '{\n  \"model\": \"llama3.2\"\n}'\n```\n\n#### Response\n\n```json\n{\n  \"modelfile\": \"# Modelfile generated by \\\"ollama show\\\"\\n# To build a new Modelfile based on this one, replace the FROM line with:\\n# FROM llava:latest\\n\\nFROM /Users/matt/.ollama/models/blobs/sha256:200765e1283640ffbd013184bf496e261032fa75b99498a9613be4e94d63ad52\\nTEMPLATE \\\"\\\"\\\"{{ .System }}\\nUSER: {{ .Prompt }}\\nASSISTANT: \\\"\\\"\\\"\\nPARAMETER num_ctx 4096\\nPARAMETER stop \\\"\\u003c/s\\u003e\\\"\\nPARAMETER stop \\\"USER:\\\"\\nPARAMETER stop \\\"ASSISTANT:\\\"\",\n  \"parameters\": \"num_keep                       24\\nstop                           \\\"<|start_header_id|>\\\"\\nstop                           \\\"<|end_header_id|>\\\"\\nstop                           \\\"<|eot_id|>\\\"\",\n  \"template\": \"{{ if .System }}<|start_header_id|>system<|end_header_id|>\\n\\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\\n\\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\\n\\n{{ .Response }}<|eot_id|>\",\n  \"details\": {\n    \"parent_model\": \"\",\n    \"format\": \"gguf\",\n    \"family\": \"llama\",\n    \"families\": [\n      \"llama\"\n    ],\n    \"parameter_size\": \"8.0B\",\n    \"quantization_level\": \"Q4_0\"\n  },\n  \"model_info\": {\n    \"general.architecture\": \"llama\",\n    \"general.file_type\": 2,\n    \"general.parameter_count\": 8030261248,\n    \"general.quantization_version\": 2,\n    \"llama.attention.head_count\": 32,\n    \"llama.attention.head_count_kv\": 8,\n    \"llama.attention.layer_norm_rms_epsilon\": 0.00001,\n    \"llama.block_count\": 32,\n    \"llama.context_length\": 8192,\n    \"llama.embedding_length\": 4096,\n    \"llama.feed_forward_length\": 14336,\n    \"llama.rope.dimension_count\": 128,\n    \"llama.rope.freq_base\": 500000,\n    \"llama.vocab_size\": 128256,\n    \"tokenizer.ggml.bos_token_id\": 128000,\n    \"tokenizer.ggml.eos_token_id\": 128009,\n    \"tokenizer.ggml.merges\": [],            // populates if `verbose=true`\n    \"tokenizer.ggml.model\": \"gpt2\",\n    \"tokenizer.ggml.pre\": \"llama-bpe\",\n    \"tokenizer.ggml.token_type\": [],        // populates if `verbose=true`\n    \"tokenizer.ggml.tokens\": []             // populates if `verbose=true`\n  }\n}\n```\n\n## Copy a Model\n\n```shell\nPOST /api/copy\n```\n\nCopy a model. Creates a model with another name from an existing model.\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/copy -d '{\n  \"source\": \"llama3.2\",\n  \"destination\": \"llama3-backup\"\n}'\n```\n\n#### Response\n\nReturns a 200 OK if successful, or a 404 Not Found if the source model doesn't exist.\n\n## Delete a Model\n\n```shell\nDELETE /api/delete\n```\n\nDelete a model and its data.\n\n### Parameters\n\n- `model`: model name to delete\n\n### Examples\n\n#### Request\n\n```shell\ncurl -X DELETE http://localhost:11434/api/delete -d '{\n  \"model\": \"llama3:13b\"\n}'\n```\n\n#### Response\n\nReturns a 200 OK if successful, 404 Not Found if the model to be deleted doesn't exist.\n\n## Pull a Model\n\n```shell\nPOST /api/pull\n```\n\nDownload a model from the ollama library. Cancelled pulls are resumed from where they left off, and multiple calls will share the same download progress.\n\n### Parameters\n\n- `model`: name of the model to pull\n- `insecure`: (optional) allow insecure connections to the library. Only use this if you are pulling from your own library during development.\n- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/pull -d '{\n  \"model\": \"llama3.2\"\n}'\n```\n\n#### Response\n\nIf `stream` is not specified, or set to `true`, a stream of JSON objects is returned:\n\nThe first object is the manifest:\n\n```json\n{\n  \"status\": \"pulling manifest\"\n}\n```\n\nThen there is a series of downloading responses. Until any of the download is completed, the `completed` key may not be included. The number of files to be downloaded depends on the number of layers specified in the manifest.\n\n```json\n{\n  \"status\": \"downloading digestname\",\n  \"digest\": \"digestname\",\n  \"total\": 2142590208,\n  \"completed\": 241970\n}\n```\n\nAfter all the files are downloaded, the final responses are:\n\n```json\n{\n    \"status\": \"verifying sha256 digest\"\n}\n{\n    \"status\": \"writing manifest\"\n}\n{\n    \"status\": \"removing any unused layers\"\n}\n{\n    \"status\": \"success\"\n}\n```\n\nif `stream` is set to false, then the response is a single JSON object:\n\n```json\n{\n  \"status\": \"success\"\n}\n```\n\n## Push a Model\n\n```shell\nPOST /api/push\n```\n\nUpload a model to a model library. Requires registering for ollama.ai and adding a public key first.\n\n### Parameters\n\n- `model`: name of the model to push in the form of `<namespace>/<model>:<tag>`\n- `insecure`: (optional) allow insecure connections to the library. Only use this if you are pushing to your library during development.\n- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/push -d '{\n  \"model\": \"mattw/pygmalion:latest\"\n}'\n```\n\n#### Response\n\nIf `stream` is not specified, or set to `true`, a stream of JSON objects is returned:\n\n```json\n{ \"status\": \"retrieving manifest\" }\n```\n\nand then:\n\n```json\n{\n  \"status\": \"starting upload\",\n  \"digest\": \"sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab\",\n  \"total\": 1928429856\n}\n```\n\nThen there is a series of uploading responses:\n\n```json\n{\n  \"status\": \"starting upload\",\n  \"digest\": \"sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab\",\n  \"total\": 1928429856\n}\n```\n\nFinally, when the upload is complete:\n\n```json\n{\"status\":\"pushing manifest\"}\n{\"status\":\"success\"}\n```\n\nIf `stream` is set to `false`, then the response is a single JSON object:\n\n```json\n{ \"status\": \"success\" }\n```\n\n## Generate Embeddings\n\n```shell\nPOST /api/embed\n```\n\nGenerate embeddings from a model\n\n### Parameters\n\n- `model`: name of model to generate embeddings from\n- `input`: text or list of text to generate embeddings for\n\nAdvanced parameters:\n\n- `truncate`: truncates the end of each input to fit within context length. Returns error if `false` and context length is exceeded. Defaults to `true`\n- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`\n- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/embed -d '{\n  \"model\": \"all-minilm\",\n  \"input\": \"Why is the sky blue?\"\n}'\n```\n\n#### Response\n\n```json\n{\n  \"model\": \"all-minilm\",\n  \"embeddings\": [[\n    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,\n    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348\n  ]],\n  \"total_duration\": 14143917,\n  \"load_duration\": 1019500,\n  \"prompt_eval_count\": 8\n}\n```\n\n#### Request (Multiple input)\n\n```shell\ncurl http://localhost:11434/api/embed -d '{\n  \"model\": \"all-minilm\",\n  \"input\": [\"Why is the sky blue?\", \"Why is the grass green?\"]\n}'\n```\n\n#### Response\n\n```json\n{\n  \"model\": \"all-minilm\",\n  \"embeddings\": [[\n    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,\n    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348\n  ],[\n    -0.0098027075, 0.06042469, 0.025257962, -0.006364387, 0.07272725,\n    0.017194884, 0.09032035, -0.051705178, 0.09951512, 0.09072481\n  ]]\n}\n```\n\n## List Running Models\n```shell\nGET /api/ps\n```\n\nList models that are currently loaded into memory.\n\n#### Examples\n\n### Request\n\n```shell\ncurl http://localhost:11434/api/ps\n```\n\n#### Response\n\nA single JSON object will be returned.\n\n```json\n{\n  \"models\": [\n    {\n      \"name\": \"mistral:latest\",\n      \"model\": \"mistral:latest\",\n      \"size\": 5137025024,\n      \"digest\": \"2ae6f6dd7a3dd734790bbbf58b8909a606e0e7e97e94b7604e0aa7ae4490e6d8\",\n      \"details\": {\n        \"parent_model\": \"\",\n        \"format\": \"gguf\",\n        \"family\": \"llama\",\n        \"families\": [\n          \"llama\"\n        ],\n        \"parameter_size\": \"7.2B\",\n        \"quantization_level\": \"Q4_0\"\n      },\n      \"expires_at\": \"2024-06-04T14:38:31.83753-07:00\",\n      \"size_vram\": 5137025024\n    }\n  ]\n}\n```\n\n## Generate Embedding\n\n> Note: this endpoint has been superseded by `/api/embed`\n\n```shell\nPOST /api/embeddings\n```\n\nGenerate embeddings from a model\n\n### Parameters\n\n- `model`: name of model to generate embeddings from\n- `prompt`: text to generate embeddings for\n\nAdvanced parameters:\n\n- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`\n- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"all-minilm\",\n  \"prompt\": \"Here is an article about llamas...\"\n}'\n```\n\n#### Response\n\n```json\n{\n  \"embedding\": [\n    0.5670403838157654, 0.009260174818336964, 0.23178744316101074, -0.2916173040866852, -0.8924556970596313,\n    0.8785552978515625, -0.34576427936553955, 0.5742510557174683, -0.04222835972905159, -0.137906014919281\n  ]\n}\n```\n\n## Version\n\n```shell\nGET /api/version\n```\n\nRetrieve the Ollama version\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/version\n```\n\n#### Response\n\n```json\n{\n  \"version\": \"0.5.1\"\n}\n```\n\n\n",
      "sha": "ede6446f98ce44fc6acabda412356e3d609a5427",
      "size": 49159,
      "timestamp": "2025-01-26T13:32:53.138100"
    },
    {
      "path": "docs/development.md",
      "url": "https://github.com/ollama/ollama/blob/main/docs/development.md",
      "content": "# Development\n\nInstall required tools:\n\n- go version 1.22 or higher\n- OS specific C/C++ compiler (see below)\n- GNU Make\n\n\n## Overview\n\nOllama uses a mix of Go and C/C++ code to interface with GPUs.  The C/C++ code is compiled with both CGO and GPU library specific compilers.  A set of GNU Makefiles are used to compile the project.  GPU Libraries are auto-detected based on the typical environment variables used by the respective libraries, but can be overridden if necessary.  The default make target will build the runners and primary Go Ollama application that will run within the repo directory.  Throughout the examples below `-j 5` is suggested for 5 parallel jobs to speed up the build.  You can adjust the job count based on your CPU Core count to reduce build times.  If you want to relocate the built binaries, use the `dist` target and recursively copy the files in `./dist/$OS-$ARCH/` to your desired location. To learn more about the other make targets use `make help`\n\nOnce you have built the GPU/CPU runners, you can compile the main application with `go build .` \n\n### MacOS\n\n[Download Go](https://go.dev/dl/)\n\n```bash\nmake -j 5\n```\n\nNow you can run `ollama`:\n\n```bash\n./ollama\n```\n\n#### Xcode 15 warnings\n\nIf you are using Xcode newer than version 14, you may see a warning during `go build` about `ld: warning: ignoring duplicate libraries: '-lobjc'` due to Golang issue https://github.com/golang/go/issues/67799 which can be safely ignored.  You can suppress the warning with `export CGO_LDFLAGS=\"-Wl,-no_warn_duplicate_libraries\"`\n\n### Linux\n\n#### Linux CUDA (NVIDIA)\n\n_Your operating system distribution may already have packages for NVIDIA CUDA. Distro packages are often preferable, but instructions are distro-specific. Please consult distro-specific docs for dependencies if available!_\n\nInstall `make`, `gcc` and `golang` as well as [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads)\ndevelopment and runtime packages.\n\nTypically the makefile will auto-detect CUDA, however, if your Linux distro\nor installation approach uses alternative paths, you can specify the location by\noverriding `CUDA_PATH` to the location of the CUDA toolkit. You can customize\na set of target CUDA architectures by setting `CUDA_ARCHITECTURES` (e.g. `CUDA_ARCHITECTURES=50;60;70`)\n\n```\nmake -j 5\n```\n\nIf both v11 and v12 tookkits are detected, runners for both major versions will be built by default.  You can build just v12 with `make cuda_v12`\n\n#### Older Linux CUDA (NVIDIA)\n\nTo support older GPUs with Compute Capability 3.5 or 3.7, you will need to use an older version of the Driver from [Unix Driver Archive](https://www.nvidia.com/en-us/drivers/unix/) (tested with 470) and [CUDA Toolkit Archive](https://developer.nvidia.com/cuda-toolkit-archive) (tested with cuda V11).  When you build Ollama, you will need to set two make variable to adjust the minimum compute capability Ollama supports via `make -j 5 CUDA_ARCHITECTURES=\"35;37;50;52\" EXTRA_GOLDFLAGS=\"\\\"-X=github.com/ollama/ollama/discover.CudaComputeMajorMin=3\\\" \\\"-X=github.com/ollama/ollama/discover.CudaComputeMinorMin=5\\\"\"`.  To find the Compute Capability of your older GPU, refer to [GPU Compute Capability](https://developer.nvidia.com/cuda-gpus).\n\n#### Linux ROCm (AMD)\n\n_Your operating system distribution may already have packages for AMD ROCm. Distro packages are often preferable, but instructions are distro-specific. Please consult distro-specific docs for dependencies if available!_\n\nInstall [ROCm](https://rocm.docs.amd.com/en/latest/) development packages first, as well as `make`, `gcc`, and `golang`.\n\nTypically the build scripts will auto-detect ROCm, however, if your Linux distro\nor installation approach uses unusual paths, you can specify the location by\nspecifying an environment variable `HIP_PATH` to the location of the ROCm\ninstall (typically `/opt/rocm`). You can also customize\nthe AMD GPU targets by setting HIP_ARCHS (e.g. `HIP_ARCHS=gfx1101;gfx1102`)\n\n```\nmake -j 5\n```\n\nROCm requires elevated privileges to access the GPU at runtime. On most distros you can add your user account to the `render` group, or run as root.\n\n#### Containerized Linux Build\n\nIf you have Docker and buildx available, you can build linux binaries with `./scripts/build_linux.sh` which has the CUDA and ROCm dependencies included. The resulting artifacts are placed in `./dist`  and by default the script builds both arm64 and amd64 binaries.  If you want to build only amd64, you can build with `PLATFORM=linux/amd64 ./scripts/build_linux.sh`\n\n### Windows\n\nThe following tools are required as a minimal development environment to build CPU inference support.\n\n- Go version 1.22 or higher\n  - https://go.dev/dl/\n- Git\n  - https://git-scm.com/download/win\n- clang with gcc compat and Make.  There are multiple options on how to go about installing these tools on Windows.  We have verified the following, but others may work as well:  \n  - [MSYS2](https://www.msys2.org/)\n    - After installing, from an MSYS2 terminal, run `pacman -S mingw-w64-clang-x86_64-gcc-compat mingw-w64-clang-x86_64-clang make` to install the required tools\n  - Assuming you used the default install prefix for msys2 above, add `C:\\msys64\\clang64\\bin` and `c:\\msys64\\usr\\bin` to your environment variable `PATH` where you will perform the build steps below (e.g. system-wide, account-level, powershell, cmd, etc.)\n\n> [!NOTE]  \n> Due to bugs in the GCC C++ library for unicode support, Ollama should be built with clang on windows.\n\n```\nmake -j 5\n```\n\n#### GPU Support\n\nThe GPU tools require the Microsoft native build tools.  To build either CUDA or ROCm, you must first install MSVC via Visual Studio:\n\n- Make sure to select `Desktop development with C++` as a Workload during the Visual Studio install\n- You must complete the Visual Studio install and run it once **BEFORE** installing CUDA or ROCm for the tools to properly register\n- Add the location of the **64 bit (x64)** compiler (`cl.exe`) to your `PATH`\n- Note: the default Developer Shell may configure the 32 bit (x86) compiler which will lead to build failures.  Ollama requires a 64 bit toolchain.\n\n#### Windows CUDA (NVIDIA)\n\nIn addition to the common Windows development tools and MSVC described above:\n\n- [NVIDIA CUDA](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html)\n\n#### Windows ROCm (AMD Radeon)\n\nIn addition to the common Windows development tools and MSVC described above:\n\n- [AMD HIP](https://www.amd.com/en/developer/resources/rocm-hub/hip-sdk.html)\n\n#### Windows arm64\n\nThe default `Developer PowerShell for VS 2022` may default to x86 which is not what you want.  To ensure you get an arm64 development environment, start a plain PowerShell terminal and run:\n\n```powershell\nimport-module 'C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\Common7\\\\Tools\\\\Microsoft.VisualStudio.DevShell.dll'\nEnter-VsDevShell -Arch arm64 -vsinstallpath 'C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community' -skipautomaticlocation\n```\n\nYou can confirm with `write-host $env:VSCMD_ARG_TGT_ARCH`\n\nFollow the instructions at https://www.msys2.org/wiki/arm64/ to set up an arm64 msys2 environment.  Ollama requires gcc and mingw32-make to compile, which is not currently available on Windows arm64, but a gcc compatibility adapter is available via `mingw-w64-clang-aarch64-gcc-compat`. At a minimum you will need to install the following:\n\n```\npacman -S mingw-w64-clang-aarch64-clang mingw-w64-clang-aarch64-gcc-compat mingw-w64-clang-aarch64-make make\n```\n\nYou will need to ensure your PATH includes go, cmake, gcc and clang mingw32-make to build ollama from source. (typically `C:\\msys64\\clangarm64\\bin\\`)\n\n\n## Advanced CPU Vector Settings\n\nOn x86, running `make` will compile several CPU runners which can run on different CPU families. At runtime, Ollama will auto-detect the best variation to load.  If GPU libraries are present at build time, Ollama also compiles GPU runners with the `AVX` CPU vector feature enabled.  This provides a good performance balance when loading large models that split across GPU and CPU with broad compatibility.  Some users may prefer no vector extensions (e.g. older Xeon/Celeron processors, or hypervisors that mask the vector features) while other users may prefer turning on many more vector extensions to further improve performance for split model loads.\n\nTo customize the set of CPU vector features enabled for a CPU runner and all GPU runners, use CUSTOM_CPU_FLAGS during the build.\n\nTo build without any vector flags:\n\n```\nmake CUSTOM_CPU_FLAGS=\"\"\n```\n\nTo build with both AVX and AVX2:\n```\nmake CUSTOM_CPU_FLAGS=avx,avx2\n```\n\nTo build with AVX512 features turned on:\n\n```\nmake CUSTOM_CPU_FLAGS=avx,avx2,avx512,avx512vbmi,avx512vnni,avx512bf16\n```\n\n> [!NOTE]  \n> If you are experimenting with different flags, make sure to do a `make clean` between each change to ensure everything is rebuilt with the new compiler flags\n",
      "sha": "e194dca0b9a812f838d88a278b7246f6a30178a9",
      "size": 8938,
      "timestamp": "2025-01-26T13:32:57.521496"
    },
    {
      "path": "docs/docker.md",
      "url": "https://github.com/ollama/ollama/blob/main/docs/docker.md",
      "content": "# Ollama Docker image\n\n### CPU only\n\n```bash\ndocker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n```\n\n### Nvidia GPU\nInstall the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation).\n\n#### Install with Apt\n1.  Configure the repository\n```bash\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \\\n    | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n    | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n    | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\nsudo apt-get update\n```\n2.  Install the NVIDIA Container Toolkit packages\n```bash\nsudo apt-get install -y nvidia-container-toolkit\n```\n\n#### Install with Yum or Dnf\n1.  Configure the repository\n\n```bash\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo \\\n    | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo\n```\n\n2. Install the NVIDIA Container Toolkit packages\n\n```bash\nsudo yum install -y nvidia-container-toolkit\n```\n\n#### Configure Docker to use Nvidia driver\n```\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n```\n\n#### Start the container\n\n```bash\ndocker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n```\n\n> [!NOTE]  \n> If you're running on an NVIDIA JetPack system, Ollama can't automatically discover the correct JetPack version. Pass the environment variable JETSON_JETPACK=5 or JETSON_JETPACK=6 to the container to select version 5 or 6.\n\n### AMD GPU\n\nTo run Ollama using Docker with AMD GPUs, use the `rocm` tag and the following command:\n\n```\ndocker run -d --device /dev/kfd --device /dev/dri -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama:rocm\n```\n\n### Run model locally\n\nNow you can run a model:\n\n```\ndocker exec -it ollama ollama run llama3.2\n```\n\n### Try different models\n\nMore models can be found on the [Ollama library](https://ollama.com/library).\n",
      "sha": "9dd387e3a350c4034b46c6f4ab3458b5c240c170",
      "size": 2199,
      "timestamp": "2025-01-26T13:32:59.808862"
    },
    {
      "path": "docs/examples.md",
      "url": "https://github.com/ollama/ollama/blob/main/docs/examples.md",
      "content": "# Examples\n\nThis directory contains different examples of using Ollama.\n\n## Python examples\nOllama Python examples at [ollama-python/examples](https://github.com/ollama/ollama-python/tree/main/examples)\n\n\n## JavaScript examples\nOllama JavaScript examples at [ollama-js/examples](https://github.com/ollama/ollama-js/tree/main/examples)\n\n\n## OpenAI compatibility examples\nOllama OpenAI compatibility examples at [ollama/examples/openai](../docs/openai.md)\n\n\n## Community examples\n\n- [LangChain Ollama Python](https://python.langchain.com/docs/integrations/chat/ollama/)\n- [LangChain Ollama JS](https://js.langchain.com/docs/integrations/chat/ollama/)\n",
      "sha": "25f6563a5b9c0a52db76716d4943ad27a8c20615",
      "size": 649,
      "timestamp": "2025-01-26T13:33:03.305040"
    },
    {
      "path": "docs/faq.md",
      "url": "https://github.com/ollama/ollama/blob/main/docs/faq.md",
      "content": "# FAQ\n\n## How can I upgrade Ollama?\n\nOllama on macOS and Windows will automatically download updates. Click on the taskbar or menubar item and then click \"Restart to update\" to apply the update. Updates can also be installed by downloading the latest version [manually](https://ollama.com/download/).\n\nOn Linux, re-run the install script:\n\n```shell\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n## How can I view the logs?\n\nReview the [Troubleshooting](./troubleshooting.md) docs for more about using logs.\n\n## Is my GPU compatible with Ollama?\n\nPlease refer to the [GPU docs](./gpu.md).\n\n## How can I specify the context window size?\n\nBy default, Ollama uses a context window size of 2048 tokens.\n\nTo change this when using `ollama run`, use `/set parameter`:\n\n```\n/set parameter num_ctx 4096\n```\n\nWhen using the API, specify the `num_ctx` parameter:\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"num_ctx\": 4096\n  }\n}'\n```\n\n## How can I tell if my model was loaded onto the GPU?\n\nUse the `ollama ps` command to see what models are currently loaded into memory.\n\n```shell\nollama ps\nNAME      \tID          \tSIZE \tPROCESSOR\tUNTIL\nllama3:70b\tbcfb190ca3a7\t42 GB\t100% GPU \t4 minutes from now\n```\n\nThe `Processor` column will show which memory the model was loaded in to:\n* `100% GPU` means the model was loaded entirely into the GPU\n* `100% CPU` means the model was loaded entirely in system memory\n* `48%/52% CPU/GPU` means the model was loaded partially onto both the GPU and into system memory\n\n## How do I configure Ollama server?\n\nOllama server can be configured with environment variables.\n\n### Setting environment variables on Mac\n\nIf Ollama is run as a macOS application, environment variables should be set using `launchctl`:\n\n1. For each environment variable, call `launchctl setenv`.\n\n    ```bash\n    launchctl setenv OLLAMA_HOST \"0.0.0.0\"\n    ```\n\n2. Restart Ollama application.\n\n### Setting environment variables on Linux\n\nIf Ollama is run as a systemd service, environment variables should be set using `systemctl`:\n\n1. Edit the systemd service by calling `systemctl edit ollama.service`. This will open an editor.\n\n2. For each environment variable, add a line `Environment` under section `[Service]`:\n\n    ```ini\n    [Service]\n    Environment=\"OLLAMA_HOST=0.0.0.0\"\n    ```\n\n3. Save and exit.\n\n4. Reload `systemd` and restart Ollama:\n\n   ```bash\n   systemctl daemon-reload\n   systemctl restart ollama\n   ```\n\n### Setting environment variables on Windows\n\nOn Windows, Ollama inherits your user and system environment variables.\n\n1. First Quit Ollama by clicking on it in the task bar.\n\n2. Start the Settings (Windows 11) or Control Panel (Windows 10) application and search for _environment variables_.\n\n3. Click on _Edit environment variables for your account_.\n\n4. Edit or create a new variable for your user account for `OLLAMA_HOST`, `OLLAMA_MODELS`, etc.\n\n5. Click OK/Apply to save.\n\n6. Start the Ollama application from the Windows Start menu.\n\n## How do I use Ollama behind a proxy?\n\nOllama pulls models from the Internet and may require a proxy server to access the models. Use `HTTPS_PROXY` to redirect outbound requests through the proxy. Ensure the proxy certificate is installed as a system certificate. Refer to the section above for how to use environment variables on your platform.\n\n> [!NOTE]\n> Avoid setting `HTTP_PROXY`. Ollama does not use HTTP for model pulls, only HTTPS. Setting `HTTP_PROXY` may interrupt client connections to the server.\n\n### How do I use Ollama behind a proxy in Docker?\n\nThe Ollama Docker container image can be configured to use a proxy by passing `-e HTTPS_PROXY=https://proxy.example.com` when starting the container.\n\nAlternatively, the Docker daemon can be configured to use a proxy. Instructions are available for Docker Desktop on [macOS](https://docs.docker.com/desktop/settings/mac/#proxies), [Windows](https://docs.docker.com/desktop/settings/windows/#proxies), and [Linux](https://docs.docker.com/desktop/settings/linux/#proxies), and Docker [daemon with systemd](https://docs.docker.com/config/daemon/systemd/#httphttps-proxy).\n\nEnsure the certificate is installed as a system certificate when using HTTPS. This may require a new Docker image when using a self-signed certificate.\n\n```dockerfile\nFROM ollama/ollama\nCOPY my-ca.pem /usr/local/share/ca-certificates/my-ca.crt\nRUN update-ca-certificates\n```\n\nBuild and run this image:\n\n```shell\ndocker build -t ollama-with-ca .\ndocker run -d -e HTTPS_PROXY=https://my.proxy.example.com -p 11434:11434 ollama-with-ca\n```\n\n## Does Ollama send my prompts and answers back to ollama.com?\n\nNo. Ollama runs locally, and conversation data does not leave your machine.\n\n## How can I expose Ollama on my network?\n\nOllama binds 127.0.0.1 port 11434 by default. Change the bind address with the `OLLAMA_HOST` environment variable.\n\nRefer to the section [above](#how-do-i-configure-ollama-server) for how to set environment variables on your platform.\n\n## How can I use Ollama with a proxy server?\n\nOllama runs an HTTP server and can be exposed using a proxy server such as Nginx. To do so, configure the proxy to forward requests and optionally set required headers (if not exposing Ollama on the network). For example, with Nginx:\n\n```nginx\nserver {\n    listen 80;\n    server_name example.com;  # Replace with your domain or IP\n    location / {\n        proxy_pass http://localhost:11434;\n        proxy_set_header Host localhost:11434;\n    }\n}\n```\n\n## How can I use Ollama with ngrok?\n\nOllama can be accessed using a range of tools for tunneling tools. For example with Ngrok:\n\n```shell\nngrok http 11434 --host-header=\"localhost:11434\"\n```\n\n## How can I use Ollama with Cloudflare Tunnel?\n\nTo use Ollama with Cloudflare Tunnel, use the `--url` and `--http-host-header` flags:\n\n```shell\ncloudflared tunnel --url http://localhost:11434 --http-host-header=\"localhost:11434\"\n```\n\n## How can I allow additional web origins to access Ollama?\n\nOllama allows cross-origin requests from `127.0.0.1` and `0.0.0.0` by default. Additional origins can be configured with `OLLAMA_ORIGINS`.\n\nRefer to the section [above](#how-do-i-configure-ollama-server) for how to set environment variables on your platform.\n\n## Where are models stored?\n\n- macOS: `~/.ollama/models`\n- Linux: `/usr/share/ollama/.ollama/models`\n- Windows: `C:\\Users\\%username%\\.ollama\\models`\n\n### How do I set them to a different location?\n\nIf a different directory needs to be used, set the environment variable `OLLAMA_MODELS` to the chosen directory.\n\n> Note: on Linux using the standard installer, the `ollama` user needs read and write access to the specified directory. To assign the directory to the `ollama` user run `sudo chown -R ollama:ollama <directory>`.\n\nRefer to the section [above](#how-do-i-configure-ollama-server) for how to set environment variables on your platform.\n\n## How can I use Ollama in Visual Studio Code?\n\nThere is already a large collection of plugins available for VSCode as well as other editors that leverage Ollama. See the list of [extensions & plugins](https://github.com/ollama/ollama#extensions--plugins) at the bottom of the main repository readme.\n\n## How do I use Ollama with GPU acceleration in Docker?\n\nThe Ollama Docker container can be configured with GPU acceleration in Linux or Windows (with WSL2). This requires the [nvidia-container-toolkit](https://github.com/NVIDIA/nvidia-container-toolkit). See [ollama/ollama](https://hub.docker.com/r/ollama/ollama) for more details.\n\nGPU acceleration is not available for Docker Desktop in macOS due to the lack of GPU passthrough and emulation.\n\n## Why is networking slow in WSL2 on Windows 10?\n\nThis can impact both installing Ollama, as well as downloading models.\n\nOpen `Control Panel > Networking and Internet > View network status and tasks` and click on `Change adapter settings` on the left panel. Find the `vEthernel (WSL)` adapter, right click and select `Properties`.\nClick on `Configure` and open the `Advanced` tab. Search through each of the properties until you find `Large Send Offload Version 2 (IPv4)` and `Large Send Offload Version 2 (IPv6)`. *Disable* both of these\nproperties.\n\n## How can I preload a model into Ollama to get faster response times?\n\nIf you are using the API you can preload a model by sending the Ollama server an empty request. This works with both the `/api/generate` and `/api/chat` API endpoints.\n\nTo preload the mistral model using the generate endpoint, use:\n```shell\ncurl http://localhost:11434/api/generate -d '{\"model\": \"mistral\"}'\n```\n\nTo use the chat completions endpoint, use:\n```shell\ncurl http://localhost:11434/api/chat -d '{\"model\": \"mistral\"}'\n```\n\nTo preload a model using the CLI, use the command:\n```shell\nollama run llama3.2 \"\"\n```\n\n## How do I keep a model loaded in memory or make it unload immediately?\n\nBy default models are kept in memory for 5 minutes before being unloaded. This allows for quicker response times if you're making numerous requests to the LLM. If you want to immediately unload a model from memory, use the `ollama stop` command:\n\n```shell\nollama stop llama3.2\n```\n\nIf you're using the API, use the `keep_alive` parameter with the `/api/generate` and `/api/chat` endpoints to set the amount of time that a model stays in memory. The `keep_alive` parameter can be set to:\n* a duration string (such as \"10m\" or \"24h\")\n* a number in seconds (such as 3600)\n* any negative number which will keep the model loaded in memory (e.g. -1 or \"-1m\")\n* '0' which will unload the model immediately after generating a response\n\nFor example, to preload a model and leave it in memory use:\n```shell\ncurl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"keep_alive\": -1}'\n```\n\nTo unload the model and free up memory use:\n```shell\ncurl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"keep_alive\": 0}'\n```\n\nAlternatively, you can change the amount of time all models are loaded into memory by setting the `OLLAMA_KEEP_ALIVE` environment variable when starting the Ollama server. The `OLLAMA_KEEP_ALIVE` variable uses the same parameter types as the `keep_alive` parameter types mentioned above. Refer to the section explaining [how to configure the Ollama server](#how-do-i-configure-ollama-server) to correctly set the environment variable.\n\nThe `keep_alive` API parameter with the `/api/generate` and `/api/chat` API endpoints will override the `OLLAMA_KEEP_ALIVE` setting.\n\n## How do I manage the maximum number of requests the Ollama server can queue?\n\nIf too many requests are sent to the server, it will respond with a 503 error indicating the server is overloaded.  You can adjust how many requests may be queue by setting `OLLAMA_MAX_QUEUE`.\n\n## How does Ollama handle concurrent requests?\n\nOllama supports two levels of concurrent processing.  If your system has sufficient available memory (system memory when using CPU inference, or VRAM for GPU inference) then multiple models can be loaded at the same time.  For a given model, if there is sufficient available memory when the model is loaded, it is configured to allow parallel request processing.\n\nIf there is insufficient available memory to load a new model request while one or more models are already loaded, all new requests will be queued until the new model can be loaded.  As prior models become idle, one or more will be unloaded to make room for the new model.  Queued requests will be processed in order.  When using GPU inference new models must be able to completely fit in VRAM to allow concurrent model loads.\n\nParallel request processing for a given model results in increasing the context size by the number of parallel requests.  For example, a 2K context with 4 parallel requests will result in an 8K context and additional memory allocation.\n\nThe following server settings may be used to adjust how Ollama handles concurrent requests on most platforms:\n\n- `OLLAMA_MAX_LOADED_MODELS` - The maximum number of models that can be loaded concurrently provided they fit in available memory.  The default is 3 * the number of GPUs or 3 for CPU inference.\n- `OLLAMA_NUM_PARALLEL` - The maximum number of parallel requests each model will process at the same time.  The default will auto-select either 4 or 1 based on available memory.\n- `OLLAMA_MAX_QUEUE` - The maximum number of requests Ollama will queue when busy before rejecting additional requests. The default is 512\n\nNote: Windows with Radeon GPUs currently default to 1 model maximum due to limitations in ROCm v5.7 for available VRAM reporting.  Once ROCm v6.2 is available, Windows Radeon will follow the defaults above.  You may enable concurrent model loads on Radeon on Windows, but ensure you don't load more models than will fit into your GPUs VRAM.\n\n## How does Ollama load models on multiple GPUs?\n\nWhen loading a new model, Ollama evaluates the required VRAM for the model against what is currently available.  If the model will entirely fit on any single GPU, Ollama will load the model on that GPU.  This typically provides the best performance as it reduces the amount of data transferring across the PCI bus during inference.  If the model does not fit entirely on one GPU, then it will be spread across all the available GPUs.\n\n## How can I enable Flash Attention?\n\nFlash Attention is a feature of most modern models that can significantly reduce memory usage as the context size grows.  To enable Flash Attention, set the `OLLAMA_FLASH_ATTENTION` environment variable to `1` when starting the Ollama server.\n\n## How can I set the quantization type for the K/V cache?\n\nThe K/V context cache can be quantized to significantly reduce memory usage when Flash Attention is enabled.\n\nTo use quantized K/V cache with Ollama you can set the following environment variable:\n\n- `OLLAMA_KV_CACHE_TYPE` - The quantization type for the K/V cache.  Default is `f16`.\n\n> Note: Currently this is a global option - meaning all models will run with the specified quantization type.\n\nThe currently available K/V cache quantization types are:\n\n- `f16` - high precision and memory usage (default).\n- `q8_0` - 8-bit quantization, uses approximately 1/2 the memory of `f16` with a very small loss in precision, this usually has no noticeable impact on the model's quality (recommended if not using f16).\n- `q4_0` - 4-bit quantization, uses approximately 1/4 the memory of `f16` with a small-medium loss in precision that may be more noticeable at higher context sizes.\n\nHow much the cache quantization impacts the model's response quality will depend on the model and the task.  Models that have a high GQA count (e.g. Qwen2) may see a larger impact on precision from quantization than models with a low GQA count.\n\nYou may need to experiment with different quantization types to find the best balance between memory usage and quality.\n",
      "sha": "387d752b28fba91fe1de608f573a627c615d7fe6",
      "size": 14895,
      "timestamp": "2025-01-26T13:33:04.962608"
    },
    {
      "path": "docs/gpu.md",
      "url": "https://github.com/ollama/ollama/blob/main/docs/gpu.md",
      "content": "# GPU\n## Nvidia\nOllama supports Nvidia GPUs with compute capability 5.0+.\n\nCheck your compute compatibility to see if your card is supported:\n[https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus)\n\n| Compute Capability | Family              | Cards                                                                                                       |\n| ------------------ | ------------------- | ----------------------------------------------------------------------------------------------------------- |\n| 9.0                | NVIDIA              | `H100`                                                                                                      |\n| 8.9                | GeForce RTX 40xx    | `RTX 4090` `RTX 4080 SUPER` `RTX 4080` `RTX 4070 Ti SUPER` `RTX 4070 Ti` `RTX 4070 SUPER` `RTX 4070` `RTX 4060 Ti` `RTX 4060`  |\n|                    | NVIDIA Professional | `L4` `L40` `RTX 6000`                                                                                       |\n| 8.6                | GeForce RTX 30xx    | `RTX 3090 Ti` `RTX 3090` `RTX 3080 Ti` `RTX 3080` `RTX 3070 Ti` `RTX 3070` `RTX 3060 Ti` `RTX 3060` `RTX 3050 Ti` `RTX 3050`   |\n|                    | NVIDIA Professional | `A40` `RTX A6000` `RTX A5000` `RTX A4000` `RTX A3000` `RTX A2000` `A10` `A16` `A2`                          |\n| 8.0                | NVIDIA              | `A100` `A30`                                                                                                |\n| 7.5                | GeForce GTX/RTX     | `GTX 1650 Ti` `TITAN RTX` `RTX 2080 Ti` `RTX 2080` `RTX 2070` `RTX 2060`                                    |\n|                    | NVIDIA Professional | `T4` `RTX 5000` `RTX 4000` `RTX 3000` `T2000` `T1200` `T1000` `T600` `T500`                                 |\n|                    | Quadro              | `RTX 8000` `RTX 6000` `RTX 5000` `RTX 4000`                                                                 |\n| 7.0                | NVIDIA              | `TITAN V` `V100` `Quadro GV100`                                                                             |\n| 6.1                | NVIDIA TITAN        | `TITAN Xp` `TITAN X`                                                                                        |\n|                    | GeForce GTX         | `GTX 1080 Ti` `GTX 1080` `GTX 1070 Ti` `GTX 1070` `GTX 1060` `GTX 1050 Ti` `GTX 1050`                       |\n|                    | Quadro              | `P6000` `P5200` `P4200` `P3200` `P5000` `P4000` `P3000` `P2200` `P2000` `P1000` `P620` `P600` `P500` `P520` |\n|                    | Tesla               | `P40` `P4`                                                                                                  |\n| 6.0                | NVIDIA              | `Tesla P100` `Quadro GP100`                                                                                 |\n| 5.2                | GeForce GTX         | `GTX TITAN X` `GTX 980 Ti` `GTX 980` `GTX 970` `GTX 960` `GTX 950`                                          |\n|                    | Quadro              | `M6000 24GB` `M6000` `M5000` `M5500M` `M4000` `M2200` `M2000` `M620`                                        |\n|                    | Tesla               | `M60` `M40`                                                                                                 |\n| 5.0                | GeForce GTX         | `GTX 750 Ti` `GTX 750` `NVS 810`                                                                            |\n|                    | Quadro              | `K2200` `K1200` `K620` `M1200` `M520` `M5000M` `M4000M` `M3000M` `M2000M` `M1000M` `K620M` `M600M` `M500M`  |\n\nFor building locally to support older GPUs, see [developer.md](./development.md#linux-cuda-nvidia)\n\n### GPU Selection\n\nIf you have multiple NVIDIA GPUs in your system and want to limit Ollama to use\na subset, you can set `CUDA_VISIBLE_DEVICES` to a comma separated list of GPUs.\nNumeric IDs may be used, however ordering may vary, so UUIDs are more reliable.\nYou can discover the UUID of your GPUs by running `nvidia-smi -L` If you want to\nignore the GPUs and force CPU usage, use an invalid GPU ID (e.g., \"-1\")\n\n### Linux Suspend Resume\n\nOn linux, after a suspend/resume cycle, sometimes Ollama will fail to discover\nyour NVIDIA GPU, and fallback to running on the CPU.  You can workaround this\ndriver bug by reloading the NVIDIA UVM driver with `sudo rmmod nvidia_uvm &&\nsudo modprobe nvidia_uvm`\n\n## AMD Radeon\nOllama supports the following AMD GPUs:\n\n### Linux Support\n| Family         | Cards and accelerators                                                                                                               |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n| AMD Radeon RX  | `7900 XTX` `7900 XT` `7900 GRE` `7800 XT` `7700 XT` `7600 XT` `7600` `6950 XT` `6900 XTX` `6900XT` `6800 XT` `6800` `Vega 64` `Vega 56`    |\n| AMD Radeon PRO | `W7900` `W7800` `W7700` `W7600` `W7500` `W6900X` `W6800X Duo` `W6800X` `W6800` `V620` `V420` `V340` `V320` `Vega II Duo` `Vega II` `VII` `SSG` |\n| AMD Instinct   | `MI300X` `MI300A` `MI300` `MI250X` `MI250` `MI210` `MI200` `MI100` `MI60` `MI50`                                                               |\n\n### Windows Support\nWith ROCm v6.1, the following GPUs are supported on Windows.\n\n| Family         | Cards and accelerators                                                                                                               |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n| AMD Radeon RX  | `7900 XTX` `7900 XT` `7900 GRE` `7800 XT` `7700 XT` `7600 XT` `7600` `6950 XT` `6900 XTX` `6900XT` `6800 XT` `6800`    |\n| AMD Radeon PRO | `W7900` `W7800` `W7700` `W7600` `W7500` `W6900X` `W6800X Duo` `W6800X` `W6800` `V620` |\n\n\n### Overrides on Linux\nOllama leverages the AMD ROCm library, which does not support all AMD GPUs. In\nsome cases you can force the system to try to use a similar LLVM target that is\nclose.  For example The Radeon RX 5400 is `gfx1034` (also known as 10.3.4)\nhowever, ROCm does not currently support this target. The closest support is\n`gfx1030`.  You can use the environment variable `HSA_OVERRIDE_GFX_VERSION` with\n`x.y.z` syntax.  So for example, to force the system to run on the RX 5400, you\nwould set `HSA_OVERRIDE_GFX_VERSION=\"10.3.0\"` as an environment variable for the\nserver.  If you have an unsupported AMD GPU you can experiment using the list of\nsupported types below.\n\nIf you have multiple GPUs with different GFX versions, append the numeric device\nnumber to the environment variable to set them individually.  For example,\n`HSA_OVERRIDE_GFX_VERSION_0=10.3.0` and  `HSA_OVERRIDE_GFX_VERSION_1=11.0.0`\n\nAt this time, the known supported GPU types on linux are the following LLVM Targets.\nThis table shows some example GPUs that map to these LLVM targets:\n| **LLVM Target** | **An Example GPU** |\n|-----------------|---------------------|\n| gfx900 | Radeon RX Vega 56 |\n| gfx906 | Radeon Instinct MI50 |\n| gfx908 | Radeon Instinct MI100 |\n| gfx90a | Radeon Instinct MI210 |\n| gfx940 | Radeon Instinct MI300 |\n| gfx941 | |\n| gfx942 | |\n| gfx1030 | Radeon PRO V620 |\n| gfx1100 | Radeon PRO W7900 |\n| gfx1101 | Radeon PRO W7700 |\n| gfx1102 | Radeon RX 7600 |\n\nAMD is working on enhancing ROCm v6 to broaden support for families of GPUs in a\nfuture release which should increase support for more GPUs.\n\nReach out on [Discord](https://discord.gg/ollama) or file an\n[issue](https://github.com/ollama/ollama/issues) for additional help.\n\n### GPU Selection\n\nIf you have multiple AMD GPUs in your system and want to limit Ollama to use a\nsubset, you can set `ROCR_VISIBLE_DEVICES` to a comma separated list of GPUs.\nYou can see the list of devices with `rocminfo`.  If you want to ignore the GPUs\nand force CPU usage, use an invalid GPU ID (e.g., \"-1\").  When available, use the\n`Uuid` to uniquely identify the device instead of numeric value.\n\n### Container Permission\n\nIn some Linux distributions, SELinux can prevent containers from\naccessing the AMD GPU devices.  On the host system you can run \n`sudo setsebool container_use_devices=1` to allow containers to use devices.\n\n### Metal (Apple GPUs)\nOllama supports GPU acceleration on Apple devices via the Metal API.\n",
      "sha": "399330026474e57bdc58a9052c7838a0af497bfb",
      "size": 8474,
      "timestamp": "2025-01-26T13:33:10.560119"
    },
    {
      "path": "docs/import.md",
      "url": "https://github.com/ollama/ollama/blob/main/docs/import.md",
      "content": "# Importing a model\n\n## Table of Contents\n\n  * [Importing a Safetensors adapter](#Importing-a-fine-tuned-adapter-from-Safetensors-weights)\n  * [Importing a Safetensors model](#Importing-a-model-from-Safetensors-weights)\n  * [Importing a GGUF file](#Importing-a-GGUF-based-model-or-adapter)\n  * [Sharing models on ollama.com](#Sharing-your-model-on-ollamacom)\n\n## Importing a fine tuned adapter from Safetensors weights\n\nFirst, create a `Modelfile` with a `FROM` command pointing at the base model you used for fine tuning, and an `ADAPTER` command which points to the directory with your Safetensors adapter:\n\n```dockerfile\nFROM <base model name>\nADAPTER /path/to/safetensors/adapter/directory\n```\n\nMake sure that you use the same base model in the `FROM` command as you used to create the adapter otherwise you will get erratic results. Most frameworks use different quantization methods, so it's best to use non-quantized (i.e. non-QLoRA) adapters. If your adapter is in the same directory as your `Modelfile`, use `ADAPTER .` to specify the adapter path.\n\nNow run `ollama create` from the directory where the `Modelfile` was created:\n\n```bash\nollama create my-model\n```\n\nLastly, test the model:\n\n```bash\nollama run my-model\n```\n\nOllama supports importing adapters based on several different model architectures including:\n\n  * Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);\n  * Mistral (including Mistral 1, Mistral 2, and Mixtral); and\n  * Gemma (including Gemma 1 and Gemma 2)\n\nYou can create the adapter using a fine tuning framework or tool which can output adapters in the Safetensors format, such as:\n\n  * Hugging Face [fine tuning framework](https://huggingface.co/docs/transformers/en/training)\n  * [Unsloth](https://github.com/unslothai/unsloth)\n  * [MLX](https://github.com/ml-explore/mlx)\n\n\n## Importing a model from Safetensors weights\n\nFirst, create a `Modelfile` with a `FROM` command which points to the directory containing your Safetensors weights:\n\n```dockerfile\nFROM /path/to/safetensors/directory\n```\n\nIf you create the Modelfile in the same directory as the weights, you can use the command `FROM .`.\n\nNow run the `ollama create` command from the directory where you created the `Modelfile`:\n\n```shell\nollama create my-model\n```\n\nLastly, test the model:\n\n```shell\nollama run my-model\n```\n\nOllama supports importing models for several different architectures including:\n\n  * Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);\n  * Mistral (including Mistral 1, Mistral 2, and Mixtral);\n  * Gemma (including Gemma 1 and Gemma 2); and\n  * Phi3\n\nThis includes importing foundation models as well as any fine tuned models which have been _fused_ with a foundation model.\n## Importing a GGUF based model or adapter\n\nIf you have a GGUF based model or adapter it is possible to import it into Ollama. You can obtain a GGUF model or adapter by:\n\n  * converting a Safetensors model with the `convert_hf_to_gguf.py` from Llama.cpp; \n  * converting a Safetensors adapter with the `convert_lora_to_gguf.py` from Llama.cpp; or\n  * downloading a model or adapter from a place such as HuggingFace\n\nTo import a GGUF model, create a `Modelfile` containing:\n\n```dockerfile\nFROM /path/to/file.gguf\n```\n\nFor a GGUF adapter, create the `Modelfile` with:\n\n```dockerfile\nFROM <model name>\nADAPTER /path/to/file.gguf\n```\n\nWhen importing a GGUF adapter, it's important to use the same base model as the base model that the adapter was created with. You can use:\n\n * a model from Ollama\n * a GGUF file\n * a Safetensors based model \n\nOnce you have created your `Modelfile`, use the `ollama create` command to build the model.\n\n```shell\nollama create my-model\n```\n\n## Quantizing a Model\n\nQuantizing a model allows you to run models faster and with less memory consumption but at reduced accuracy. This allows you to run a model on more modest hardware.\n\nOllama can quantize FP16 and FP32 based models into different quantization levels using the `-q/--quantize` flag with the `ollama create` command.\n\nFirst, create a Modelfile with the FP16 or FP32 based model you wish to quantize.\n\n```dockerfile\nFROM /path/to/my/gemma/f16/model\n```\n\nUse `ollama create` to then create the quantized model.\n\n```shell\n$ ollama create --quantize q4_K_M mymodel\ntransferring model data\nquantizing F16 model to Q4_K_M\ncreating new layer sha256:735e246cc1abfd06e9cdcf95504d6789a6cd1ad7577108a70d9902fef503c1bd\ncreating new layer sha256:0853f0ad24e5865173bbf9ffcc7b0f5d56b66fd690ab1009867e45e7d2c4db0f\nwriting manifest\nsuccess\n```\n\n### Supported Quantizations\n\n- `q4_0`\n- `q4_1`\n- `q5_0`\n- `q5_1`\n- `q8_0`\n\n#### K-means Quantizations\n\n- `q3_K_S`\n- `q3_K_M`\n- `q3_K_L`\n- `q4_K_S`\n- `q4_K_M`\n- `q5_K_S`\n- `q5_K_M`\n- `q6_K`\n\n\n## Sharing your model on ollama.com\n\nYou can share any model you have created by pushing it to [ollama.com](https://ollama.com) so that other users can try it out.\n\nFirst, use your browser to go to the [Ollama Sign-Up](https://ollama.com/signup) page. If you already have an account, you can skip this step.\n\n<img src=\"images/signup.png\" alt=\"Sign-Up\" width=\"40%\">\n\nThe `Username` field will be used as part of your model's name (e.g. `jmorganca/mymodel`), so make sure you are comfortable with the username that you have selected.\n\nNow that you have created an account and are signed-in, go to the [Ollama Keys Settings](https://ollama.com/settings/keys) page.\n\nFollow the directions on the page to determine where your Ollama Public Key is located.\n\n<img src=\"images/ollama-keys.png\" alt=\"Ollama Keys\" width=\"80%\">\n\nClick on the `Add Ollama Public Key` button, and copy and paste the contents of your Ollama Public Key into the text field.\n\nTo push a model to [ollama.com](https://ollama.com), first make sure that it is named correctly with your username. You may have to use the `ollama cp` command to copy\nyour model to give it the correct name. Once you're happy with your model's name, use the `ollama push` command to push it to [ollama.com](https://ollama.com).\n\n```shell\nollama cp mymodel myuser/mymodel\nollama push myuser/mymodel\n```\n\nOnce your model has been pushed, other users can pull and run it by using the command:\n\n```shell\nollama run myuser/mymodel\n```\n\n",
      "sha": "040fa299e5787504d0cdd239c34e19a22e3edc95",
      "size": 6197,
      "timestamp": "2025-01-26T13:33:30.219236"
    },
    {
      "path": "docs/linux.md",
      "url": "https://github.com/ollama/ollama/blob/main/docs/linux.md",
      "content": "# Linux\n\n## Install\n\nTo install Ollama, run the following command:\n\n```shell\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n## Manual install\n\n> [!NOTE]\n> If you are upgrading from a prior version, you should remove the old libraries with `sudo rm -rf /usr/lib/ollama` first.\n\nDownload and extract the package:\n\n```shell\ncurl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64.tgz\n```\n\nStart Ollama:\n\n```shell\nollama serve\n```\n\nIn another terminal, verify that Ollama is running:\n\n```shell\nollama -v\n```\n\n### AMD GPU install\n\nIf you have an AMD GPU, also download and extract the additional ROCm package:\n\n```shell\ncurl -L https://ollama.com/download/ollama-linux-amd64-rocm.tgz -o ollama-linux-amd64-rocm.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64-rocm.tgz\n```\n\n### ARM64 install\n\nDownload and extract the ARM64-specific package:\n\n```shell\ncurl -L https://ollama.com/download/ollama-linux-arm64.tgz -o ollama-linux-arm64.tgz\nsudo tar -C /usr -xzf ollama-linux-arm64.tgz\n```\n\n### Adding Ollama as a startup service (recommended)\n\nCreate a user and group for Ollama:\n\n```shell\nsudo useradd -r -s /bin/false -U -m -d /usr/share/ollama ollama\nsudo usermod -a -G ollama $(whoami)\n```\n\nCreate a service file in `/etc/systemd/system/ollama.service`:\n\n```ini\n[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=/usr/bin/ollama serve\nUser=ollama\nGroup=ollama\nRestart=always\nRestartSec=3\nEnvironment=\"PATH=$PATH\"\n\n[Install]\nWantedBy=default.target\n```\n\nThen start the service:\n\n```shell\nsudo systemctl daemon-reload\nsudo systemctl enable ollama\n```\n\n### Install CUDA drivers (optional)\n\n[Download and install](https://developer.nvidia.com/cuda-downloads) CUDA.\n\nVerify that the drivers are installed by running the following command, which should print details about your GPU:\n\n```shell\nnvidia-smi\n```\n\n### Install AMD ROCm drivers (optional)\n\n[Download and Install](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html) ROCm v6.\n\n### Start Ollama\n\nStart Ollama and verify it is running:\n\n```shell\nsudo systemctl start ollama\nsudo systemctl status ollama\n```\n\n> [!NOTE]\n> While AMD has contributed the `amdgpu` driver upstream to the official linux\n> kernel source, the version is older and may not support all ROCm features. We\n> recommend you install the latest driver from\n> https://www.amd.com/en/support/linux-drivers for best support of your Radeon\n> GPU.\n\n## Customizing\n\nTo customize the installation of Ollama, you can edit the systemd service file or the environment variables by running:\n\n```\nsudo systemctl edit ollama\n```\n\nAlternatively, create an override file manually in `/etc/systemd/system/ollama.service.d/override.conf`:\n\n```ini\n[Service]\nEnvironment=\"OLLAMA_DEBUG=1\"\n```\n\n## Updating\n\nUpdate Ollama by running the install script again:\n\n```shell\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\nOr by re-downloading Ollama:\n\n```shell\ncurl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64.tgz\n```\n\n## Installing specific versions\n\nUse `OLLAMA_VERSION` environment variable with the install script to install a specific version of Ollama, including pre-releases. You can find the version numbers in the [releases page](https://github.com/ollama/ollama/releases).\n\nFor example:\n\n```shell\ncurl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.3.9 sh\n```\n\n## Viewing logs\n\nTo view logs of Ollama running as a startup service, run:\n\n```shell\njournalctl -e -u ollama\n```\n\n## Uninstall\n\nRemove the ollama service:\n\n```shell\nsudo systemctl stop ollama\nsudo systemctl disable ollama\nsudo rm /etc/systemd/system/ollama.service\n```\n\nRemove the ollama binary from your bin directory (either `/usr/local/bin`, `/usr/bin`, or `/bin`):\n\n```shell\nsudo rm $(which ollama)\n```\n\nRemove the downloaded models and Ollama service user and group:\n\n```shell\nsudo rm -r /usr/share/ollama\nsudo userdel ollama\nsudo groupdel ollama\n```\n",
      "sha": "13655f423f445adb95d749897a670a9cea433b73",
      "size": 4041,
      "timestamp": "2025-01-26T13:33:32.561707"
    },
    {
      "path": "docs/modelfile.md",
      "url": "https://github.com/ollama/ollama/blob/main/docs/modelfile.md",
      "content": "# Ollama Model File\n\n> [!NOTE]\n> `Modelfile` syntax is in development\n\nA model file is the blueprint to create and share models with Ollama.\n\n## Table of Contents\n\n- [Format](#format)\n- [Examples](#examples)\n- [Instructions](#instructions)\n  - [FROM (Required)](#from-required)\n    - [Build from existing model](#build-from-existing-model)\n    - [Build from a Safetensors model](#build-from-a-safetensors-model)\n    - [Build from a GGUF file](#build-from-a-gguf-file)\n  - [PARAMETER](#parameter)\n    - [Valid Parameters and Values](#valid-parameters-and-values)\n  - [TEMPLATE](#template)\n    - [Template Variables](#template-variables)\n  - [SYSTEM](#system)\n  - [ADAPTER](#adapter)\n  - [LICENSE](#license)\n  - [MESSAGE](#message)\n- [Notes](#notes)\n\n## Format\n\nThe format of the `Modelfile`:\n\n```modelfile\n# comment\nINSTRUCTION arguments\n```\n\n| Instruction                         | Description                                                    |\n| ----------------------------------- | -------------------------------------------------------------- |\n| [`FROM`](#from-required) (required) | Defines the base model to use.                                 |\n| [`PARAMETER`](#parameter)           | Sets the parameters for how Ollama will run the model.         |\n| [`TEMPLATE`](#template)             | The full prompt template to be sent to the model.              |\n| [`SYSTEM`](#system)                 | Specifies the system message that will be set in the template. |\n| [`ADAPTER`](#adapter)               | Defines the (Q)LoRA adapters to apply to the model.            |\n| [`LICENSE`](#license)               | Specifies the legal license.                                   |\n| [`MESSAGE`](#message)               | Specify message history.                                       |\n\n## Examples\n\n### Basic `Modelfile`\n\nAn example of a `Modelfile` creating a mario blueprint:\n\n```modelfile\nFROM llama3.2\n# sets the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 1\n# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token\nPARAMETER num_ctx 4096\n\n# sets a custom system message to specify the behavior of the chat assistant\nSYSTEM You are Mario from super mario bros, acting as an assistant.\n```\n\nTo use this:\n\n1. Save it as a file (e.g. `Modelfile`)\n2. `ollama create choose-a-model-name -f <location of the file e.g. ./Modelfile>`\n3. `ollama run choose-a-model-name`\n4. Start using the model!\n\nTo view the Modelfile of a given model, use the `ollama show --modelfile` command.\n\n  ```bash\n  > ollama show --modelfile llama3.2\n  # Modelfile generated by \"ollama show\"\n  # To build a new Modelfile based on this one, replace the FROM line with:\n  # FROM llama3.2:latest\n  FROM /Users/pdevine/.ollama/models/blobs/sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29\n  TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n  {{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n  {{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n  {{ .Response }}<|eot_id|>\"\"\"\n  PARAMETER stop \"<|start_header_id|>\"\n  PARAMETER stop \"<|end_header_id|>\"\n  PARAMETER stop \"<|eot_id|>\"\n  PARAMETER stop \"<|reserved_special_token\"\n  ```\n\n## Instructions\n\n### FROM (Required)\n\nThe `FROM` instruction defines the base model to use when creating a model.\n\n```modelfile\nFROM <model name>:<tag>\n```\n\n#### Build from existing model\n\n```modelfile\nFROM llama3.2\n```\n\nA list of available base models:\n<https://github.com/ollama/ollama#model-library>\nAdditional models can be found at:\n<https://ollama.com/library>\n\n#### Build from a Safetensors model\n\n```modelfile\nFROM <model directory>\n```\n\nThe model directory should contain the Safetensors weights for a supported architecture.\n\nCurrently supported model architectures:\n  * Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2)\n  * Mistral (including Mistral 1, Mistral 2, and Mixtral)\n  * Gemma (including Gemma 1 and Gemma 2)\n  * Phi3\n\n#### Build from a GGUF file\n\n```modelfile\nFROM ./ollama-model.gguf\n```\n\nThe GGUF file location should be specified as an absolute path or relative to the `Modelfile` location.\n\n\n### PARAMETER\n\nThe `PARAMETER` instruction defines a parameter that can be set when the model is run.\n\n```modelfile\nPARAMETER <parameter> <parametervalue>\n```\n\n#### Valid Parameters and Values\n\n| Parameter      | Description                                                                                                                                                                                                                                             | Value Type | Example Usage        |\n| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | -------------------- |\n| mirostat       | Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)                                                                                                                                         | int        | mirostat 0           |\n| mirostat_eta   | Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1)                        | float      | mirostat_eta 0.1     |\n| mirostat_tau   | Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0)                                                                                                         | float      | mirostat_tau 5.0     |\n| num_ctx        | Sets the size of the context window used to generate the next token. (Default: 2048)                                                                                                                                                                    | int        | num_ctx 4096         |\n| repeat_last_n  | Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)                                                                                                                                           | int        | repeat_last_n 64     |\n| repeat_penalty | Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)                                                                     | float      | repeat_penalty 1.1   |\n| temperature    | The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)                                                                                                                                     | float      | temperature 0.7      |\n| seed           | Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0)                                                                                       | int        | seed 42              |\n| stop           | Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate `stop` parameters in a modelfile.                                      | string     | stop \"AI assistant:\" |\n| num_predict    | Maximum number of tokens to predict when generating text. (Default: -1, infinite generation)                                                                                                                                   | int        | num_predict 42       |\n| top_k          | Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)                                                                        | int        | top_k 40             |\n| top_p          | Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)                                                                 | float      | top_p 0.9            |\n| min_p          | Alternative to the top_p, and aims to ensure a balance of quality and variety. The parameter *p* represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with *p*=0.05 and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out. (Default: 0.0) | float      | min_p 0.05            |\n\n### TEMPLATE\n\n`TEMPLATE` of the full prompt template to be passed into the model. It may include (optionally) a system message, a user's message and the response from the model. Note: syntax may be model specific. Templates use Go [template syntax](https://pkg.go.dev/text/template).\n\n#### Template Variables\n\n| Variable          | Description                                                                                   |\n| ----------------- | --------------------------------------------------------------------------------------------- |\n| `{{ .System }}`   | The system message used to specify custom behavior.                                           |\n| `{{ .Prompt }}`   | The user prompt message.                                                                      |\n| `{{ .Response }}` | The response from the model. When generating a response, text after this variable is omitted. |\n\n```\nTEMPLATE \"\"\"{{ if .System }}<|im_start|>system\n{{ .System }}<|im_end|>\n{{ end }}{{ if .Prompt }}<|im_start|>user\n{{ .Prompt }}<|im_end|>\n{{ end }}<|im_start|>assistant\n\"\"\"\n```\n\n### SYSTEM\n\nThe `SYSTEM` instruction specifies the system message to be used in the template, if applicable.\n\n```modelfile\nSYSTEM \"\"\"<system message>\"\"\"\n```\n\n### ADAPTER\n\nThe `ADAPTER` instruction specifies a fine tuned LoRA adapter that should apply to the base model. The value of the adapter should be an absolute path or a path relative to the Modelfile. The base model should be specified with a `FROM` instruction. If the base model is not the same as the base model that the adapter was tuned from the behaviour will be erratic.\n\n#### Safetensor adapter\n\n```modelfile\nADAPTER <path to safetensor adapter>\n```\n\nCurrently supported Safetensor adapters:\n  * Llama (including Llama 2, Llama 3, and Llama 3.1)\n  * Mistral (including Mistral 1, Mistral 2, and Mixtral)\n  * Gemma (including Gemma 1 and Gemma 2)\n\n#### GGUF adapter\n\n```modelfile\nADAPTER ./ollama-lora.gguf\n```\n\n### LICENSE\n\nThe `LICENSE` instruction allows you to specify the legal license under which the model used with this Modelfile is shared or distributed.\n\n```modelfile\nLICENSE \"\"\"\n<license text>\n\"\"\"\n```\n\n### MESSAGE\n\nThe `MESSAGE` instruction allows you to specify a message history for the model to use when responding. Use multiple iterations of the MESSAGE command to build up a conversation which will guide the model to answer in a similar way.\n\n```modelfile\nMESSAGE <role> <message>\n```\n\n#### Valid roles\n\n| Role      | Description                                                  |\n| --------- | ------------------------------------------------------------ |\n| system    | Alternate way of providing the SYSTEM message for the model. |\n| user      | An example message of what the user could have asked.        |\n| assistant | An example message of how the model should respond.          |\n\n\n#### Example conversation\n\n```modelfile\nMESSAGE user Is Toronto in Canada?\nMESSAGE assistant yes\nMESSAGE user Is Sacramento in Canada?\nMESSAGE assistant no\nMESSAGE user Is Ontario in Canada?\nMESSAGE assistant yes\n```\n\n\n## Notes\n\n- the **`Modelfile` is not case sensitive**. In the examples, uppercase instructions are used to make it easier to distinguish it from arguments.\n- Instructions can be in any order. In the examples, the `FROM` instruction is first to keep it easily readable.\n\n[1]: https://ollama.com/library\n",
      "sha": "cc2115b3c6edc6c8f579b906159a97e1bf87fa59",
      "size": 12502,
      "timestamp": "2025-01-26T13:33:35.236900"
    },
    {
      "path": "docs/openai.md",
      "url": "https://github.com/ollama/ollama/blob/main/docs/openai.md",
      "content": "# OpenAI compatibility\n\n> **Note:** OpenAI compatibility is experimental and is subject to major adjustments including breaking changes. For fully-featured access to the Ollama API, see the Ollama [Python library](https://github.com/ollama/ollama-python), [JavaScript library](https://github.com/ollama/ollama-js) and [REST API](https://github.com/ollama/ollama/blob/main/docs/api.md).\n\nOllama provides experimental compatibility with parts of the [OpenAI API](https://platform.openai.com/docs/api-reference) to help connect existing applications to Ollama.\n\n## Usage\n\n### OpenAI Python library\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url='http://localhost:11434/v1/',\n\n    # required but ignored\n    api_key='ollama',\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            'role': 'user',\n            'content': 'Say this is a test',\n        }\n    ],\n    model='llama3.2',\n)\n\nresponse = client.chat.completions.create(\n    model=\"llava\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": \"data:image/png;base64,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\",\n                },\n            ],\n        }\n    ],\n    max_tokens=300,\n)\n\ncompletion = client.completions.create(\n    model=\"llama3.2\",\n    prompt=\"Say this is a test\",\n)\n\nlist_completion = client.models.list()\n\nmodel = client.models.retrieve(\"llama3.2\")\n\nembeddings = client.embeddings.create(\n    model=\"all-minilm\",\n    input=[\"why is the sky blue?\", \"why is the grass green?\"],\n)\n```\n#### Structured outputs\n```py\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n\n# Define the schema for the response\nclass FriendInfo(BaseModel):\n    name: str\n    age: int \n    is_available: bool\n\nclass FriendList(BaseModel):\n    friends: list[FriendInfo]\n\ntry:\n    completion = client.beta.chat.completions.parse(\n        temperature=0,\n        model=\"llama3.1:8b\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"I have two friends. The first is Ollama 22 years old busy saving the world, and the second is Alonso 23 years old and wants to hang out. Return a list of friends in JSON format\"}\n        ],\n        response_format=FriendList,\n    )\n\n    friends_response = completion.choices[0].message\n    if friends_response.parsed:\n        print(friends_response.parsed)\n    elif friends_response.refusal:\n        print(friends_response.refusal)\nexcept Exception as e:\n    print(f\"Error: {e}\")\n```\n\n### OpenAI JavaScript library\n\n```javascript\nimport OpenAI from 'openai'\n\nconst openai = new OpenAI({\n  baseURL: 'http://localhost:11434/v1/',\n\n  // required but ignored\n  apiKey: 'ollama',\n})\n\nconst chatCompletion = await openai.chat.completions.create({\n    messages: [{ role: 'user', content: 'Say this is a test' }],\n    model: 'llama3.2',\n})\n\nconst response = await openai.chat.completions.create({\n    model: \"llava\",\n    messages: [\n        {\n        role: \"user\",\n        content: [\n            { type: \"text\", text: \"What's in this image?\" },\n            {\n            type: \"image_url\",\n            image_url: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAG0AAABmCAYAAADBPx+VAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAA3VSURBVHgB7Z27r0zdG8fX743i1bi1ikMoFMQloXRpKFFIqI7LH4BEQ+NWIkjQuSWCRIEoULk0gsK1kCBI0IhrQVT7tz/7zZo888yz1r7MnDl7z5xvsjkzs2fP3uu71nNfa7lkAsm7d++Sffv2JbNmzUqcc8m0adOSzZs3Z+/XES4ZckAWJEGWPiCxjsQNLWmQsWjRIpMseaxcuTKpG/7HP27I8P79e7dq1ars/yL4/v27S0ejqwv+cUOGEGGpKHR37tzJCEpHV9tnT58+dXXCJDdECBE2Ojrqjh071hpNECjx4cMHVycM1Uhbv359B2F79+51586daxN/+pyRkRFXKyRDAqxEp4yMlDDzXG1NPnnyJKkThoK0VFd1ELZu3TrzXKxKfW7dMBQ6bcuWLW2v0VlHjx41z717927ba22U9APcw7Nnz1oGEPeL3m3p2mTAYYnFmMOMXybPPXv2bNIPpFZr1NHn4HMw0KRBjg9NuRw95s8PEcz/6DZELQd/09C9QGq5RsmSRybqkwHGjh07OsJSsYYm3ijPpyHzoiacg35MLdDSIS/O1yM778jOTwYUkKNHWUzUWaOsylE00MyI0fcnOwIdjvtNdW/HZwNLGg+sR1kMepSNJXmIwxBZiG8tDTpEZzKg0GItNsosY8USkxDhD0Rinuiko2gfL/RbiD2LZAjU9zKQJj8RDR0vJBR1/Phx9+PHj9Z7REF4nTZkxzX4LCXHrV271qXkBAPGfP/atWvu/PnzHe4C97F48eIsRLZ9+3a3f/9+87dwP1JxaF7/3r17ba+5l4EcaVo0lj3SBq5kGTJSQmLWMjgYNei2GPT1MuMqGTDEFHzeQSP2wi/jGnkmPJ/nhccs44jvDAxpVcxnq0F6eT8h4ni/iIWpR5lPyA6ETkNXoSukvpJAD3AsXLiwpZs49+fPn5ke4j10TqYvegSfn0OnafC+Tv9ooA/JPkgQysqQNBzagXY55nO/oa1F7qvIPWkRL12WRpMWUvpVDYmxAPehxWSe8ZEXL20sadYIozfmNch4QJPAfeJgW3rNsnzphBKNJM2KKODo1rVOMRYik5ETy3ix4qWNI81qAAirizgMIc+yhTytx0JWZuNI03qsrgWlGtwjoS9XwgUhWGyhUaRZZQNNIEwCiXD16tXcAHUs79co0vSD8rrJCIW98pzvxpAWyyo3HYwqS0+H0BjStClcZJT5coMm6D2LOF8TolGJtK9fvyZpyiC5ePFi9nc/oJU4eiEP0jVoAnHa9wyJycITMP78+eMeP37sXrx44d6+fdt6f82aNdkx1pg9e3Zb5W+RSRE+n+VjksQWifvVaTKFhn5O8my63K8Qabdv33b379/PiAP//vuvW7BggZszZ072/+TJk91YgkafPn166zXB1rQHFvouAWHq9z3SEevSUerqCn2/dDCeta2jxYbr69evk4MHDyY7d+7MjhMnTiTPnz9Pfv/+nfQT2ggpO2dMF8cghuoM7Ygj5iWCqRlGFml0QC/ftGmTmzt3rmsaKDsgBSPh0/8yPeLLBihLkOKJc0jp8H8vUzcxIA1k6QJ/c78tWEyj5P3o4u9+jywNPdJi5rAH9x0KHcl4Hg570eQp3+vHXGyrmEeigzQsQsjavXt38ujRo44LQuDDhw+TW7duRS1HGgMxhNXHgflaNTOsHyKvHK5Ijo2jbFjJBQK9YwFd6RVMzfgRBmEfP37suBBm/p49e1qjEP2mwTViNRo0VJWH1deMXcNK08uUjVUu7s/zRaL+oLNxz1bpANco4npUgX4G2eFbpDFyQoQxojBCpEGSytmOH8qrH5Q9vuzD6ofQylkCUmh8DBAr+q8JCyVNtWQIidKQE9wNtLSQnS4jDSsxNHogzFuQBw4cyM61UKVsjfr3ooBkPSqqQHesUPWVtzi9/vQi1T+rJj7WiTz4Pt/l3LxUkr5P2VYZaZ4URpsE+st/dujQoaBBYokbrz/8TJNQYLSonrPS9kUaSkPeZyj1AWSj+d+VBoy1pIWVNed8P0Ll/ee5HdGRhrHhR5GGN0r4LGZBaj8oFDJitBTJzIZgFcmU0Y8ytWMZMzJOaXUSrUs5RxKnrxmbb5YXO9VGUhtpXldhEUogFr3IzIsvlpmdosVcGVGXFWp2oU9kLFL3dEkSz6NHEY1sjSRdIuDFWEhd8KxFqsRi1uM/nz9/zpxnwlESONdg6dKlbsaMGS4EHFHtjFIDHwKOo46l4TxSuxgDzi+rE2jg+BaFruOX4HXa0Nnf1lwAPufZeF8/r6zD97WK2qFnGjBxTw5qNGPxT+5T/r7/7RawFC3j4vTp09koCxkeHjqbHJqArmH5UrFKKksnxrK7FuRIs8STfBZv+luugXZ2pR/pP9Ois4z+TiMzUUkUjD0iEi1fzX8GmXyuxUBRcaUfykV0YZnlJGKQpOiGB76x5GeWkWWJc3mOrK6S7xdND+W5N6XyaRgtWJFe13GkaZnKOsYqGdOVVVbGupsyA/l7emTLHi7vwTdirNEt0qxnzAvBFcnQF16xh/TMpUuXHDowhlA9vQVraQhkudRdzOnK+04ZSP3DUhVSP61YsaLtd/ks7ZgtPcXqPqEafHkdqa84X6aCeL7YWlv6edGFHb+ZFICPlljHhg0bKuk0CSvVznWsotRu433alNdFrqG45ejoaPCaUkWERpLXjzFL2Rpllp7PJU2a/v7Ab8N05/9t27Z16KUqoFGsxnI9EosS2niSYg9SpU6B4JgTrvVW1flt1sT+0ADIJU2maXzcUTraGCRaL1Wp9rUMk16PMom8QhruxzvZIegJjFU7LLCePfS8uaQdPny4jTTL0dbee5mYokQsXTIWNY46kuMbnt8Kmec+LGWtOVIl9cT1rCB0V8WqkjAsRwta93TbwNYoGKsUSChN44lgBNCoHLHzquYKrU6qZ8lolCIN0Rh6cP0Q3U6I6IXILYOQI513hJaSKAorFpuHXJNfVlpRtmYBk1Su1obZr5dnKAO+L10Hrj3WZW+E3qh6IszE37F6EB+68mGpvKm4eb9bFrlzrok7fvr0Kfv727dvWRmdVTJHw0qiiCUSZ6wCK+7XL/AcsgNyL74DQQ730sv78Su7+t/A36MdY0sW5o40ahslXr58aZ5HtZB8GH64m9EmMZ7FpYw4T6QnrZfgenrhFxaSiSGXtPnz57e9TkNZLvTjeqhr734CNtrK41L40sUQckmj1lGKQ0rC37x544r8eNXRpnVE3ZZY7zXo8NomiO0ZUCj2uHz58rbXoZ6gc0uA+F6ZeKS/jhRDUq8MKrTho9fEkihMmhxtBI1DxKFY9XLpVcSkfoi8JGnToZO5sU5aiDQIW716ddt7ZLYtMQlhECdBGXZZMWldY5BHm5xgAroWj4C0hbYkSc/jBmggIrXJWlZM6pSETsEPGqZOndr2uuuR5rF169a2HoHPdurUKZM4CO1WTPqaDaAd+GFGKdIQkxAn9RuEWcTRyN2KSUgiSgF5aWzPTeA/lN5rZubMmR2bE4SIC4nJoltgAV/dVefZm72AtctUCJU2CMJ327hxY9t7EHbkyJFseq+EJSY16RPo3Dkq1kkr7+q0bNmyDuLQcZBEPYmHVdOBiJyIlrRDq41YPWfXOxUysi5fvtyaj+2BpcnsUV/oSoEMOk2CQGlr4ckhBwaetBhjCwH0ZHtJROPJkyc7UjcYLDjmrH7ADTEBXFfOYmB0k9oYBOjJ8b4aOYSe7QkKcYhFlq3QYLQhSidNmtS2RATwy8YOM3EQJsUjKiaWZ+vZToUQgzhkHXudb/PW5YMHD9yZM2faPsMwoc7RciYJXbGuBqJ1UIGKKLv915jsvgtJxCZDubdXr165mzdvtr1Hz5LONA8jrUwKPqsmVesKa49S3Q4WxmRPUEYdTjgiUcfUwLx589ySJUva3oMkP6IYddq6HMS4o55xBJBUeRjzfa4Zdeg56QZ43LhxoyPo7Lf1kNt7oO8wWAbNwaYjIv5lhyS7kRf96dvm5Jah8vfvX3flyhX35cuX6HfzFHOToS1H4BenCaHvO8pr8iDuwoUL7tevX+b5ZdbBair0xkFIlFDlW4ZknEClsp/TzXyAKVOmmHWFVSbDNw1l1+4f90U6IY/q4V27dpnE9bJ+v87QEydjqx/UamVVPRG+mwkNTYN+9tjkwzEx+atCm/X9WvWtDtAb68Wy9LXa1UmvCDDIpPkyOQ5ZwSzJ4jMrvFcr0rSjOUh+GcT4LSg5ugkW1Io0/SCDQBojh0hPlaJdah+tkVYrnTZowP8iq1F1TgMBBauufyB33x1v+NWFYmT5KmppgHC+NkAgbmRkpD3yn9QIseXymoTQFGQmIOKTxiZIWpvAatenVqRVXf2nTrAWMsPnKrMZHz6bJq5jvce6QK8J1cQNgKxlJapMPdZSR64/UivS9NztpkVEdKcrs5alhhWP9NeqlfWopzhZScI6QxseegZRGeg5a8C3Re1Mfl1ScP36ddcUaMuv24iOJtz7sbUjTS4qBvKmstYJoUauiuD3k5qhyr7QdUHMeCgLa1Ear9NquemdXgmum4fvJ6w1lqsuDhNrg1qSpleJK7K3TF0Q2jSd94uSZ60kK1e3qyVpQK6PVWXp2/FC3mp6jBhKKOiY2h3gtUV64TWM6wDETRPLDfSakXmH3w8g9Jlug8ZtTt4kVF0kLUYYmCCtD/DrQ5YhMGbA9L3ucdjh0y8kOHW5gU/VEEmJTcL4Pz/f7mgoAbYkAAAAAElFTkSuQmCC\",\n            },\n        ],\n        },\n    ],\n})\n\nconst completion = await openai.completions.create({\n    model: \"llama3.2\",\n    prompt: \"Say this is a test.\",\n})\n\nconst listCompletion = await openai.models.list()\n\nconst model = await openai.models.retrieve(\"llama3.2\")\n\nconst embedding = await openai.embeddings.create({\n  model: \"all-minilm\",\n  input: [\"why is the sky blue?\", \"why is the grass green?\"],\n})\n```\n\n### `curl`\n\n``` shell\ncurl http://localhost:11434/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"llama3.2\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Hello!\"\n            }\n        ]\n    }'\n\ncurl http://localhost:11434/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"llava\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What'\\''s in this image?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n               \"url\": \"data:image/png;base64,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\"\n            }\n          }\n        ]\n      }\n    ],\n    \"max_tokens\": 300\n  }'\n\ncurl http://localhost:11434/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"llama3.2\",\n        \"prompt\": \"Say this is a test\"\n    }'\n\ncurl http://localhost:11434/v1/models\n\ncurl http://localhost:11434/v1/models/llama3.2\n\ncurl http://localhost:11434/v1/embeddings \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"all-minilm\",\n        \"input\": [\"why is the sky blue?\", \"why is the grass green?\"]\n    }'\n```\n\n## Endpoints\n\n### `/v1/chat/completions`\n\n#### Supported features\n\n- [x] Chat completions\n- [x] Streaming\n- [x] JSON mode\n- [x] Reproducible outputs\n- [x] Vision\n- [x] Tools\n- [ ] Logprobs\n\n#### Supported request fields\n\n- [x] `model`\n- [x] `messages`\n  - [x] Text `content`\n  - [x] Image `content`\n    - [x] Base64 encoded image\n    - [ ] Image URL\n  - [x] Array of `content` parts\n- [x] `frequency_penalty`\n- [x] `presence_penalty`\n- [x] `response_format`\n- [x] `seed`\n- [x] `stop`\n- [x] `stream`\n- [x] `stream_options`\n  - [x] `include_usage`\n- [x] `temperature`\n- [x] `top_p`\n- [x] `max_tokens`\n- [x] `tools`\n- [ ] `tool_choice`\n- [ ] `logit_bias`\n- [ ] `user`\n- [ ] `n`\n\n### `/v1/completions`\n\n#### Supported features\n\n- [x] Completions\n- [x] Streaming\n- [x] JSON mode\n- [x] Reproducible outputs\n- [ ] Logprobs\n\n#### Supported request fields\n\n- [x] `model`\n- [x] `prompt`\n- [x] `frequency_penalty`\n- [x] `presence_penalty`\n- [x] `seed`\n- [x] `stop`\n- [x] `stream`\n- [x] `stream_options`\n  - [x] `include_usage`\n- [x] `temperature`\n- [x] `top_p`\n- [x] `max_tokens`\n- [x] `suffix`\n- [ ] `best_of`\n- [ ] `echo`\n- [ ] `logit_bias`\n- [ ] `user`\n- [ ] `n`\n\n#### Notes\n\n- `prompt` currently only accepts a string\n\n### `/v1/models`\n\n#### Notes\n\n- `created` corresponds to when the model was last modified\n- `owned_by` corresponds to the ollama username, defaulting to `\"library\"`\n\n### `/v1/models/{model}`\n\n#### Notes\n\n- `created` corresponds to when the model was last modified\n- `owned_by` corresponds to the ollama username, defaulting to `\"library\"`\n\n### `/v1/embeddings`\n\n#### Supported request fields\n\n- [x] `model`\n- [x] `input`\n  - [x] string\n  - [x] array of strings\n  - [ ] array of tokens\n  - [ ] array of token arrays\n- [ ] `encoding format`\n- [ ] `dimensions`\n- [ ] `user`\n\n## Models\n\nBefore using a model, pull it locally `ollama pull`:\n\n```shell\nollama pull llama3.2\n```\n\n### Default model names\n\nFor tooling that relies on default OpenAI model names such as `gpt-3.5-turbo`, use `ollama cp` to copy an existing model name to a temporary name:\n\n```\nollama cp llama3.2 gpt-3.5-turbo\n```\n\nAfterwards, this new model name can be specified the `model` field:\n\n```shell\ncurl http://localhost:11434/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"gpt-3.5-turbo\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Hello!\"\n            }\n        ]\n    }'\n```\n\n### Setting the context size\n\nThe OpenAI API does not have a way of setting the context size for a model. If you need to change the context size, create a `Modelfile` which looks like:\n\n```modelfile\nFROM <some model>\nPARAMETER num_ctx <context size>\n```\n\nUse the `ollama create mymodel` command to create a new model with the updated context size. Call the API with the updated model name:\n\n```shell\ncurl http://localhost:11434/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"mymodel\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Hello!\"\n            }\n        ]\n    }'\n```\n",
      "sha": "b0f9b353ca7154dc70dd15ff627fc0e5c08601be",
      "size": 22784,
      "timestamp": "2025-01-26T13:33:41.100220"
    },
    {
      "path": "docs/template.md",
      "url": "https://github.com/ollama/ollama/blob/main/docs/template.md",
      "content": "# Template\n\nOllama provides a powerful templating engine backed by Go's built-in templating engine to construct prompts for your large language model. This feature is a valuable tool to get the most out of your models.\n\n## Basic Template Structure\n\nA basic Go template consists of three main parts:\n\n* **Layout**: The overall structure of the template.\n* **Variables**: Placeholders for dynamic data that will be replaced with actual values when the template is rendered.\n* **Functions**: Custom functions or logic that can be used to manipulate the template's content.\n\nHere's an example of a simple chat template:\n\n```gotmpl\n{{- range .Messages }}\n{{ .Role }}: {{ .Content }}\n{{- end }}\n```\n\nIn this example, we have:\n\n* A basic messages structure (layout)\n* Three variables: `Messages`, `Role`, and `Content` (variables)\n* A custom function (action) that iterates over an array of items (`range .Messages`) and displays each item\n\n## Adding templates to your model\n\nBy default, models imported into Ollama have a default template of `{{ .Prompt }}`, i.e. user inputs are sent verbatim to the LLM. This is appropriate for text or code completion models but lacks essential markers for chat or instruction models.\n\nOmitting a template in these models puts the responsibility of correctly templating input onto the user. Adding a template allows users to easily get the best results from the model.\n\nTo add templates in your model, you'll need to add a `TEMPLATE` command to the Modelfile. Here's an example using Meta's Llama 3.\n\n```dockerfile\nFROM llama3.2\n\nTEMPLATE \"\"\"{{- if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>\n{{- end }}\n{{- range .Messages }}<|start_header_id|>{{ .Role }}<|end_header_id|>\n\n{{ .Content }}<|eot_id|>\n{{- end }}<|start_header_id|>assistant<|end_header_id|>\n\n\"\"\"\n```\n\n## Variables\n\n`System` (string): system prompt\n\n`Prompt` (string): user prompt\n\n`Response` (string): assistant response\n\n`Suffix` (string): text inserted after the assistant's response\n\n`Messages` (list): list of messages\n\n`Messages[].Role` (string): role which can be one of `system`, `user`, `assistant`, or `tool`\n\n`Messages[].Content` (string):  message content\n\n`Messages[].ToolCalls` (list): list of tools the model wants to call\n\n`Messages[].ToolCalls[].Function` (object): function to call\n\n`Messages[].ToolCalls[].Function.Name` (string): function name\n\n`Messages[].ToolCalls[].Function.Arguments` (map): mapping of argument name to argument value\n\n`Tools` (list): list of tools the model can access\n\n`Tools[].Type` (string): schema type. `type` is always `function`\n\n`Tools[].Function` (object): function definition\n\n`Tools[].Function.Name` (string): function name\n\n`Tools[].Function.Description` (string): function description\n\n`Tools[].Function.Parameters` (object): function parameters\n\n`Tools[].Function.Parameters.Type` (string): schema type. `type` is always `object`\n\n`Tools[].Function.Parameters.Required` (list): list of required properties\n\n`Tools[].Function.Parameters.Properties` (map): mapping of property name to property definition\n\n`Tools[].Function.Parameters.Properties[].Type` (string): property type\n\n`Tools[].Function.Parameters.Properties[].Description` (string): property description\n\n`Tools[].Function.Parameters.Properties[].Enum` (list): list of valid values\n\n## Tips and Best Practices\n\nKeep the following tips and best practices in mind when working with Go templates:\n\n* **Be mindful of dot**: Control flow structures like `range` and `with` changes the value `.`\n* **Out-of-scope variables**: Use `$.` to reference variables not currently in scope, starting from the root\n* **Whitespace control**: Use `-` to trim leading (`{{-`) and trailing (`-}}`) whitespace\n\n## Examples\n\n### Example Messages\n\n#### ChatML\n\nChatML is a popular template format. It can be used for models such as Databrick's DBRX, Intel's Neural Chat, and Microsoft's Orca 2.\n\n```go\n{{- range .Messages }}<|im_start|>{{ .Role }}\n{{ .Content }}<|im_end|>\n{{ end }}<|im_start|>assistant\n```\n\n### Example Tools\n\nTools support can be added to a model by adding a `{{ .Tools }}` node to the template. This feature is useful for models trained to call external tools and can a powerful tool for retrieving real-time data or performing complex tasks.\n\n#### Mistral\n\nMistral v0.3 and Mixtral 8x22B supports tool calling.\n\n```go\n{{- range $index, $_ := .Messages }}\n{{- if eq .Role \"user\" }}\n{{- if and (le (len (slice $.Messages $index)) 2) $.Tools }}[AVAILABLE_TOOLS] {{ json $.Tools }}[/AVAILABLE_TOOLS]\n{{- end }}[INST] {{ if and (eq (len (slice $.Messages $index)) 1) $.System }}{{ $.System }}\n\n{{ end }}{{ .Content }}[/INST]\n{{- else if eq .Role \"assistant\" }}\n{{- if .Content }} {{ .Content }}</s>\n{{- else if .ToolCalls }}[TOOL_CALLS] [\n{{- range .ToolCalls }}{\"name\": \"{{ .Function.Name }}\", \"arguments\": {{ json .Function.Arguments }}}\n{{- end }}]</s>\n{{- end }}\n{{- else if eq .Role \"tool\" }}[TOOL_RESULTS] {\"content\": {{ .Content }}}[/TOOL_RESULTS]\n{{- end }}\n{{- end }}\n```\n\n### Example Fill-in-Middle\n\nFill-in-middle support can be added to a model by adding a `{{ .Suffix }}` node to the template. This feature is useful for models that are trained to generate text in the middle of user input, such as code completion models.\n\n#### CodeLlama\n\nCodeLlama [7B](https://ollama.com/library/codellama:7b-code) and [13B](https://ollama.com/library/codellama:13b-code) code completion models support fill-in-middle.\n\n```go\n<PRE> {{ .Prompt }} <SUF>{{ .Suffix }} <MID>\n```\n\n> [!NOTE]\n> CodeLlama 34B and 70B code completion and all instruct and Python fine-tuned models do not support fill-in-middle.\n\n#### Codestral\n\nCodestral [22B](https://ollama.com/library/codestral:22b) supports fill-in-middle.\n\n```gotmpl\n[SUFFIX]{{ .Suffix }}[PREFIX] {{ .Prompt }}\n```\n",
      "sha": "47e1399a2d288b89eaa8118bd88ae8751b9fa9bd",
      "size": 5797,
      "timestamp": "2025-01-26T13:33:45.295315"
    },
    {
      "path": "docs/troubleshooting.md",
      "url": "https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md",
      "content": "# How to troubleshoot issues\n\nSometimes Ollama may not perform as expected. One of the best ways to figure out what happened is to take a look at the logs. Find the logs on **Mac** by running the command:\n\n```shell\ncat ~/.ollama/logs/server.log\n```\n\nOn **Linux** systems with systemd, the logs can be found with this command:\n\n```shell\njournalctl -u ollama --no-pager\n```\n\nWhen you run Ollama in a **container**, the logs go to stdout/stderr in the container:\n\n```shell\ndocker logs <container-name>\n```\n(Use `docker ps` to find the container name)\n\nIf manually running `ollama serve` in a terminal, the logs will be on that terminal.\n\nWhen you run Ollama on **Windows**, there are a few different locations. You can view them in the explorer window by hitting `<cmd>+R` and type in:\n- `explorer %LOCALAPPDATA%\\Ollama` to view logs.  The most recent server logs will be in `server.log` and older logs will be in `server-#.log` \n- `explorer %LOCALAPPDATA%\\Programs\\Ollama` to browse the binaries (The installer adds this to your user PATH)\n- `explorer %HOMEPATH%\\.ollama` to browse where models and configuration is stored\n- `explorer %TEMP%` where temporary executable files are stored in one or more `ollama*` directories\n\nTo enable additional debug logging to help troubleshoot problems, first **Quit the running app from the tray menu** then in a powershell terminal\n```powershell\n$env:OLLAMA_DEBUG=\"1\"\n& \"ollama app.exe\"\n```\n\nJoin the [Discord](https://discord.gg/ollama) for help interpreting the logs.\n\n## LLM libraries\n\nOllama includes multiple LLM libraries compiled for different GPUs and CPU vector features. Ollama tries to pick the best one based on the capabilities of your system. If this autodetection has problems, or you run into other problems (e.g. crashes in your GPU) you can workaround this by forcing a specific LLM library. `cpu_avx2` will perform the best, followed by `cpu_avx` an the slowest but most compatible is `cpu`. Rosetta emulation under MacOS will work with the `cpu` library. \n\nIn the server log, you will see a message that looks something like this (varies from release to release):\n\n```\nDynamic LLM libraries [rocm_v6 cpu cpu_avx cpu_avx2 cuda_v11 rocm_v5]\n```\n\n**Experimental LLM Library Override**\n\nYou can set OLLAMA_LLM_LIBRARY to any of the available LLM libraries to bypass autodetection, so for example, if you have a CUDA card, but want to force the CPU LLM library with AVX2 vector support, use:\n\n```\nOLLAMA_LLM_LIBRARY=\"cpu_avx2\" ollama serve\n```\n\nYou can see what features your CPU has with the following.\n```\ncat /proc/cpuinfo| grep flags | head -1\n```\n\n## Installing older or pre-release versions on Linux\n\nIf you run into problems on Linux and want to install an older version, or you'd like to try out a pre-release before it's officially released, you can tell the install script which version to install.\n\n```sh\ncurl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=\"0.1.29\" sh\n```\n\n## Linux tmp noexec \n\nIf your system is configured with the \"noexec\" flag where Ollama stores its temporary executable files, you can specify an alternate location by setting OLLAMA_TMPDIR to a location writable by the user ollama runs as. For example OLLAMA_TMPDIR=/usr/share/ollama/\n\n## NVIDIA GPU Discovery\n\nWhen Ollama starts up, it takes inventory of the GPUs present in the system to determine compatibility and how much VRAM is available.  Sometimes this discovery can fail to find your GPUs.  In general, running the latest driver will yield the best results.\n\n### Linux NVIDIA Troubleshooting\n\nIf you are using a container to run Ollama, make sure you've set up the container runtime first as described in [docker.md](./docker.md)\n\nSometimes the Ollama can have difficulties initializing the GPU. When you check the server logs, this can show up as various error codes, such as \"3\" (not initialized), \"46\" (device unavailable), \"100\" (no device), \"999\" (unknown), or others. The following troubleshooting techniques may help resolve the problem\n\n- If you are using a container, is the container runtime working?  Try `docker run --gpus all ubuntu nvidia-smi` - if this doesn't work, Ollama won't be able to see your NVIDIA GPU.\n- Is the uvm driver loaded? `sudo nvidia-modprobe -u`\n- Try reloading the nvidia_uvm driver - `sudo rmmod nvidia_uvm` then `sudo modprobe nvidia_uvm`\n- Try rebooting\n- Make sure you're running the latest nvidia drivers\n\nIf none of those resolve the problem, gather additional information and file an issue:\n- Set `CUDA_ERROR_LEVEL=50` and try again to get more diagnostic logs\n- Check dmesg for any errors `sudo dmesg | grep -i nvrm` and `sudo dmesg | grep -i nvidia`\n\n\n## AMD GPU Discovery\n\nOn linux, AMD GPU access typically requires `video` and/or `render` group membership to access the `/dev/kfd` device.  If permissions are not set up correctly, Ollama will detect this and report an error in the server log.\n\nWhen running in a container, in some Linux distributions and container runtimes, the ollama process may be unable to access the GPU.  Use `ls -lnd /dev/kfd /dev/dri /dev/dri/*` on the host system to determine the **numeric** group IDs on your system, and pass additional `--group-add ...` arguments to the container so it can access the required devices.   For example, in the following output `crw-rw---- 1 0  44 226,   0 Sep 16 16:55 /dev/dri/card0` the group ID column is `44` \n\nIf Ollama initially works on the GPU in a docker container, but then switches to running on CPU after some period of time with errors in the server log reporting GPU discovery failures, this can be resolved by disabling systemd cgroup management in Docker.  Edit `/etc/docker/daemon.json` on the host and add `\"exec-opts\": [\"native.cgroupdriver=cgroupfs\"]` to the docker configuration.\n\nIf you are experiencing problems getting Ollama to correctly discover or use your GPU for inference, the following may help isolate the failure.\n- `AMD_LOG_LEVEL=3` Enable info log levels in the AMD HIP/ROCm libraries.  This can help show more detailed error codes that can help troubleshoot problems\n- `OLLAMA_DEBUG=1` During GPU discovery additional information will be reported\n- Check dmesg for any errors from amdgpu or kfd drivers `sudo dmesg | grep -i amdgpu` and `sudo dmesg | grep -i kfd`\n\n## Multiple AMD GPUs\n\nIf you experience gibberish responses when models load across multiple AMD GPUs on Linux, see the following guide.\n\n- https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/mgpu.html#mgpu-known-issues-and-limitations\n\n## Windows Terminal Errors\n\nOlder versions of Windows 10 (e.g., 21H1) are known to have a bug where the standard terminal program does not display control characters correctly.  This can result in a long string of strings like `[?25h[?25l` being displayed, sometimes erroring with `The parameter is incorrect`  To resolve this problem, please update to Win 10 22H1 or newer.\n",
      "sha": "28f4350aa6f181d058d52f08fc99195a8d105d49",
      "size": 6908,
      "timestamp": "2025-01-26T13:33:47.138030"
    },
    {
      "path": "docs/windows.md",
      "url": "https://github.com/ollama/ollama/blob/main/docs/windows.md",
      "content": "# Ollama Windows\n\nWelcome to Ollama for Windows.\n\nNo more WSL required!\n\nOllama now runs as a native Windows application, including NVIDIA and AMD Radeon GPU support.\nAfter installing Ollama for Windows, Ollama will run in the background and\nthe `ollama` command line is available in `cmd`, `powershell` or your favorite\nterminal application. As usual the Ollama [api](./api.md) will be served on\n`http://localhost:11434`.\n\n## System Requirements\n\n* Windows 10 22H2 or newer, Home or Pro\n* NVIDIA 452.39 or newer Drivers if you have an NVIDIA card\n* AMD Radeon Driver https://www.amd.com/en/support if you have a Radeon card\n\nOllama uses unicode characters for progress indication, which may render as unknown squares in some older terminal fonts in Windows 10. If you see this, try changing your terminal font settings.\n\n## Filesystem Requirements\n\nThe Ollama install does not require Administrator, and installs in your home directory by default.  You'll need at least 4GB of space for the binary install.  Once you've installed Ollama, you'll need additional space for storing the Large Language models, which can be tens to hundreds of GB in size.  If your home directory doesn't have enough space, you can change where the binaries are installed, and where the models are stored.\n\n### Changing Install Location\n\nTo install the Ollama application in a location different than your home directory, start the installer with the following flag\n\n```powershell\nOllamaSetup.exe /DIR=\"d:\\some\\location\"\n```\n\n### Changing Model Location\n\nTo change where Ollama stores the downloaded models instead of using your home directory, set the environment variable `OLLAMA_MODELS` in your user account.\n\n1. Start the Settings (Windows 11) or Control Panel (Windows 10) application and search for _environment variables_.\n\n2. Click on _Edit environment variables for your account_.\n\n3. Edit or create a new variable for your user account for `OLLAMA_MODELS` where you want the models stored\n\n4. Click OK/Apply to save.\n\nIf Ollama is already running, Quit the tray application and relaunch it from the Start menu, or a new terminal started after you saved the environment variables.\n\n## API Access\n\nHere's a quick example showing API access from `powershell`\n```powershell\n(Invoke-WebRequest -method POST -Body '{\"model\":\"llama3.2\", \"prompt\":\"Why is the sky blue?\", \"stream\": false}' -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json\n```\n\n## Troubleshooting\n\nOllama on Windows stores files in a few different locations.  You can view them in\nthe explorer window by hitting `<cmd>+R` and type in:\n- `explorer %LOCALAPPDATA%\\Ollama` contains logs, and downloaded updates\n    - *app.log* contains most resent logs from the GUI application\n    - *server.log* contains the most recent server logs\n    - *upgrade.log* contains log output for upgrades\n- `explorer %LOCALAPPDATA%\\Programs\\Ollama` contains the binaries (The installer adds this to your user PATH)\n- `explorer %HOMEPATH%\\.ollama` contains models and configuration\n- `explorer %TEMP%` contains temporary executable files in one or more `ollama*` directories\n\n## Uninstall\n\nThe Ollama Windows installer registers an Uninstaller application.  Under `Add or remove programs` in Windows Settings, you can uninstall Ollama.\n\n> [!NOTE]\n> If you have [changed the OLLAMA_MODELS location](#changing-model-location), the installer will not remove your downloaded models\n\n\n## Standalone CLI\n\nThe easiest way to install Ollama on Windows is to use the `OllamaSetup.exe`\ninstaller. It installs in your account without requiring Administrator rights.\nWe update Ollama regularly to support the latest models, and this installer will\nhelp you keep up to date.\n\nIf you'd like to install or integrate Ollama as a service, a standalone\n`ollama-windows-amd64.zip` zip file is available containing only the Ollama CLI\nand GPU library dependencies for Nvidia and AMD. This allows for embedding\nOllama in existing applications, or running it as a system service via `ollama\nserve` with tools such as [NSSM](https://nssm.cc/).\n\n> [!NOTE]  \n> If you are upgrading from a prior version, you should remove the old directories first.\n",
      "sha": "80bebed47829127f2b66e1f008014bfc9b66954a",
      "size": 4169,
      "timestamp": "2025-01-26T13:33:48.433260"
    }
  ]
}