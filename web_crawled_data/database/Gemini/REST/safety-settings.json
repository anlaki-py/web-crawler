{
    "base_url": "https://ai.google.dev/gemini-api/docs/safety-settings#rest",
    "crawl_date": "2025-01-01T18:56:17.468906",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs/safety-settings#rest",
            "title": "Safety settings  |  Gemini API  |  Google AI for Developers",
            "text_content": "Safety settings  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Safety settings The Gemini API provides safety settings that you can adjust during the\nprototyping stage to determine if your application requires more or less\nrestrictive safety configuration. You can adjust these settings across four\nfilter categories to restrict or allow certain types of content. This guide covers how the Gemini API handles safety settings and filtering and\nhow you can change the safety settings for your application. Note: Applications that use less restrictive safety settings may be subject to\nreview. See the Terms of Service for more information. Safety filters The Gemini API's adjustable safety filters cover the following categories: Category Description Harassment Negative or harmful comments targeting identity and/or protected\n      attributes. Hate speech Content that is rude, disrespectful, or profane. Sexually explicit Contains references to sexual acts or other lewd content. Dangerous Promotes, facilitates, or encourages harmful acts. Civic integrity Election-related queries. These categories are defined in HarmCategory . The\n  Gemini models only support HARM_CATEGORY_HARASSMENT , HARM_CATEGORY_HATE_SPEECH , HARM_CATEGORY_SEXUALLY_EXPLICIT , HARM_CATEGORY_DANGEROUS_CONTENT , and HARM_CATEGORY_CIVIC_INTEGRITY . All other categories are used\n  only by PaLM 2 (Legacy) models. You can use these filters to adjust what's appropriate for your use case. For\nexample, if you're building video game dialogue, you may deem it acceptable to\nallow more content that's rated as Dangerous due to the nature of the game. In addition to the adjustable safety filters, the Gemini API has built-in\nprotections against core harms, such as content that endangers child safety.\nThese types of harm are always blocked and cannot be adjusted. Content safety filtering level The Gemini API categorizes the probability level of content being unsafe as HIGH , MEDIUM , LOW , or NEGLIGIBLE . The Gemini API blocks content based on the probability of content being unsafe\nand not the severity. This is important to consider because some content can\nhave low probability of being unsafe even though the severity of harm could\nstill be high. For example, comparing the sentences: The robot punched me. The robot slashed me up. The first sentence might result in a higher probability of being unsafe, but you\nmight consider the second sentence to be a higher severity in terms of violence.\nGiven this, it is important that you carefully test and consider what the\nappropriate level of blocking is needed to support your key use cases while\nminimizing harm to end users. Safety filtering per request You can adjust the safety settings for each request you make to the API. When\nyou make a request, the content is analyzed and assigned a safety rating. The\nsafety rating includes the category and the probability of the harm\nclassification. For example, if the content was blocked due to the harassment\ncategory having a high probability, the safety rating returned would have\ncategory equal to HARASSMENT and harm probability set to HIGH . By default, safety settings block content (including prompts) with medium or\nhigher probability of being unsafe across any filter. This baseline safety is\ndesigned to work for most use cases, so you should only adjust your safety\nsettings if it's consistently required for your application. The following table describes the block settings you can adjust for each\ncategory. For example, if you set the block setting to Block few for the Hate speech category, everything that has a high probability of being hate\nspeech content is blocked. But anything with a lower probability is allowed. Threshold (Google AI Studio) Threshold (API) Description Block none BLOCK_NONE Always show regardless of probability of unsafe content Block few BLOCK_ONLY_HIGH Block when high probability of unsafe content Block some BLOCK_MEDIUM_AND_ABOVE Block when medium or high probability of unsafe content Block most BLOCK_LOW_AND_ABOVE Block when low, medium or high probability of unsafe content N/A HARM_BLOCK_THRESHOLD_UNSPECIFIED Threshold is unspecified, block using default threshold If the threshold is not set, the default block threshold is Block most (for gemini-1.5-pro-002 and gemini-1.5-flash-002 only) or Block some (in all\nother models) for all categories except the Civic integrity category. The default block threshold for the Civic integrity category is Block most when sending prompts using Google AI Studio, and Block none when using the\nGemini API directly. You can set these settings for each request you make to the generative service.\nSee the HarmBlockThreshold API\nreference for details. Safety feedback generateContent returns a GenerateContentResponse which\nincludes safety feedback. Prompt feedback is included in promptFeedback . If promptFeedback.blockReason is set, then the content of the prompt was blocked. Response candidate feedback is included in Candidate.finishReason and Candidate.safetyRatings . If response\ncontent was blocked and the finishReason was SAFETY , you can inspect safetyRatings for more details. The content that was blocked is not returned. Adjust safety settings This section covers how to adjust the safety settings in both Google AI Studio\nand in your code. Google AI Studio You can adjust safety settings in Google AI Studio, but you cannot turn them\noff. Click Edit safety settings in the Run settings panel to open the Run\nsafety settings modal. In the modal, you can use the sliders to adjust the\ncontent filtering level per safety category: Note: If you set any of the category filters to Block none , Google AI Studio\nwill display a reminder about the Gemini API's Terms of Service with respect\nto safety settings. When you send a request (for example, by asking the model a question), a warning No Content message appears if the request's content is blocked. To see more\ndetails, hold the pointer over the No Content text and click warning Safety . Gemini API SDKs The following code snippet shows how to set safety settings in your GenerateContent call. This sets the thresholds for the harassment\n( HARM_CATEGORY_HARASSMENT ) and hate speech ( HARM_CATEGORY_HATE_SPEECH )\ncategories. For example, setting these categories to BLOCK_LOW_AND_ABOVE blocks any content that has a low or higher probability of being harassment or\nhate speech. To understand the threshold settings, see Safety filtering per request . Python from google.generativeai.types import HarmCategory , HarmBlockThreshold model = genai . GenerativeModel ( model_name = 'gemini-1.5-flash' ) response = model . generate_content ( [ 'Do these look store-bought or homemade?' , img ], safety_settings = { HarmCategory . HARM_CATEGORY_HATE_SPEECH : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , HarmCategory . HARM_CATEGORY_HARASSMENT : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , } ) Go model := client . GenerativeModel ( \"gemini-1.5-flash\" ) model . SafetySettings = [] * genai . SafetySetting { { Category : genai . HarmCategoryHarassment , Threshold : genai . HarmBlockLowAndAbove , }, { Category : genai . HarmCategoryHateSpeech , Threshold : genai . HarmBlockLowAndAbove , }, } Node.js import { HarmBlockThreshold , HarmCategory } from \"@google/generative-ai\" ; // ... const safetySettings = [ { category : HarmCategory . HARM_CATEGORY_HARASSMENT , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, { category : HarmCategory . HARM_CATEGORY_HATE_SPEECH , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, ]; const model = genAi . getGenerativeModel ({ model : \"gemini-1.5-flash\" , safetySettings : safetySettings }); Web import { HarmBlockThreshold , HarmCategory } from \"@google/generative-ai\" ; // ... const safetySettings = [ { category : HarmCategory . HARM_CATEGORY_HARASSMENT , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, { category : HarmCategory . HARM_CATEGORY_HATE_SPEECH , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, ]; const model = genAi . getGenerativeModel ({ model : \"gemini-1.5-flash\" , safetySettings }); Dart (Flutter) final safetySettings = [ SafetySetting ( HarmCategory . harassment , HarmBlockThreshold . low ), SafetySetting ( HarmCategory . hateSpeech , HarmBlockThreshold . low ), ]; final model = GenerativeModel ( model: 'gemini-1.5-flash' , apiKey: apiKey , safetySettings: safetySettings , ); Kotlin val harassmentSafety = SafetySetting ( HarmCategory . HARASSMENT , BlockThreshold . LOW_AND_ABOVE ) val hateSpeechSafety = SafetySetting ( HarmCategory . HATE_SPEECH , BlockThreshold . LOW_AND_ABOVE ) val generativeModel = GenerativeModel ( modelName = \"gemini-1.5-flash\" , apiKey = BuildConfig . apiKey , safetySettings = listOf ( harassmentSafety , hateSpeechSafety ) ) Java SafetySetting harassmentSafety = new SafetySetting ( HarmCategory . HARASSMENT , BlockThreshold . LOW_AND_ABOVE ); SafetySetting hateSpeechSafety = new SafetySetting ( HarmCategory . HATE_SPEECH , BlockThreshold . LOW_AND_ABOVE ); GenerativeModel gm = new GenerativeModel ( \"gemini-1.5-flash\" , BuildConfig . apiKey , null , // generation config is optional Arrays . asList ( harassmentSafety , hateSpeechSafety ) ); GenerativeModelFutures model = GenerativeModelFutures . from ( gm ); REST echo '{ \"safetySettings\": [ {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_ONLY_HIGH\"}, {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"} ], \"contents\": [{ \"parts\":[{ \"text\": \"' I support Martians Soccer Club and I think Jupiterians Football Club sucks! Write a ironic phrase about them. '\"}]}]}' > request.json\n\ncurl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= $GOOGLE_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d @request.json 2 > /dev/null safety_settings.sh Next steps See the API reference to learn more about the full API. Review the safety guidance for a general look at safety\nconsiderations when developing with LLMs. Learn more about assessing probability versus severity from the Jigsaw\nteam Learn more about the products that contribute to safety solutions like the Perspective\nAPI .\n                *   You can use these safety settings to create a toxicity\n                    classifier. See the classification\n                    example to\n                    get started. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings"
            ],
            "timestamp": "2025-01-01T18:56:16.657743",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/safety-settings",
            "title": "Safety settings  |  Gemini API  |  Google AI for Developers",
            "text_content": "Safety settings  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Safety settings The Gemini API provides safety settings that you can adjust during the\nprototyping stage to determine if your application requires more or less\nrestrictive safety configuration. You can adjust these settings across four\nfilter categories to restrict or allow certain types of content. This guide covers how the Gemini API handles safety settings and filtering and\nhow you can change the safety settings for your application. Note: Applications that use less restrictive safety settings may be subject to\nreview. See the Terms of Service for more information. Safety filters The Gemini API's adjustable safety filters cover the following categories: Category Description Harassment Negative or harmful comments targeting identity and/or protected\n      attributes. Hate speech Content that is rude, disrespectful, or profane. Sexually explicit Contains references to sexual acts or other lewd content. Dangerous Promotes, facilitates, or encourages harmful acts. Civic integrity Election-related queries. These categories are defined in HarmCategory . The\n  Gemini models only support HARM_CATEGORY_HARASSMENT , HARM_CATEGORY_HATE_SPEECH , HARM_CATEGORY_SEXUALLY_EXPLICIT , HARM_CATEGORY_DANGEROUS_CONTENT , and HARM_CATEGORY_CIVIC_INTEGRITY . All other categories are used\n  only by PaLM 2 (Legacy) models. You can use these filters to adjust what's appropriate for your use case. For\nexample, if you're building video game dialogue, you may deem it acceptable to\nallow more content that's rated as Dangerous due to the nature of the game. In addition to the adjustable safety filters, the Gemini API has built-in\nprotections against core harms, such as content that endangers child safety.\nThese types of harm are always blocked and cannot be adjusted. Content safety filtering level The Gemini API categorizes the probability level of content being unsafe as HIGH , MEDIUM , LOW , or NEGLIGIBLE . The Gemini API blocks content based on the probability of content being unsafe\nand not the severity. This is important to consider because some content can\nhave low probability of being unsafe even though the severity of harm could\nstill be high. For example, comparing the sentences: The robot punched me. The robot slashed me up. The first sentence might result in a higher probability of being unsafe, but you\nmight consider the second sentence to be a higher severity in terms of violence.\nGiven this, it is important that you carefully test and consider what the\nappropriate level of blocking is needed to support your key use cases while\nminimizing harm to end users. Safety filtering per request You can adjust the safety settings for each request you make to the API. When\nyou make a request, the content is analyzed and assigned a safety rating. The\nsafety rating includes the category and the probability of the harm\nclassification. For example, if the content was blocked due to the harassment\ncategory having a high probability, the safety rating returned would have\ncategory equal to HARASSMENT and harm probability set to HIGH . By default, safety settings block content (including prompts) with medium or\nhigher probability of being unsafe across any filter. This baseline safety is\ndesigned to work for most use cases, so you should only adjust your safety\nsettings if it's consistently required for your application. The following table describes the block settings you can adjust for each\ncategory. For example, if you set the block setting to Block few for the Hate speech category, everything that has a high probability of being hate\nspeech content is blocked. But anything with a lower probability is allowed. Threshold (Google AI Studio) Threshold (API) Description Block none BLOCK_NONE Always show regardless of probability of unsafe content Block few BLOCK_ONLY_HIGH Block when high probability of unsafe content Block some BLOCK_MEDIUM_AND_ABOVE Block when medium or high probability of unsafe content Block most BLOCK_LOW_AND_ABOVE Block when low, medium or high probability of unsafe content N/A HARM_BLOCK_THRESHOLD_UNSPECIFIED Threshold is unspecified, block using default threshold If the threshold is not set, the default block threshold is Block most (for gemini-1.5-pro-002 and gemini-1.5-flash-002 only) or Block some (in all\nother models) for all categories except the Civic integrity category. The default block threshold for the Civic integrity category is Block most when sending prompts using Google AI Studio, and Block none when using the\nGemini API directly. You can set these settings for each request you make to the generative service.\nSee the HarmBlockThreshold API\nreference for details. Safety feedback generateContent returns a GenerateContentResponse which\nincludes safety feedback. Prompt feedback is included in promptFeedback . If promptFeedback.blockReason is set, then the content of the prompt was blocked. Response candidate feedback is included in Candidate.finishReason and Candidate.safetyRatings . If response\ncontent was blocked and the finishReason was SAFETY , you can inspect safetyRatings for more details. The content that was blocked is not returned. Adjust safety settings This section covers how to adjust the safety settings in both Google AI Studio\nand in your code. Google AI Studio You can adjust safety settings in Google AI Studio, but you cannot turn them\noff. Click Edit safety settings in the Run settings panel to open the Run\nsafety settings modal. In the modal, you can use the sliders to adjust the\ncontent filtering level per safety category: Note: If you set any of the category filters to Block none , Google AI Studio\nwill display a reminder about the Gemini API's Terms of Service with respect\nto safety settings. When you send a request (for example, by asking the model a question), a warning No Content message appears if the request's content is blocked. To see more\ndetails, hold the pointer over the No Content text and click warning Safety . Gemini API SDKs The following code snippet shows how to set safety settings in your GenerateContent call. This sets the thresholds for the harassment\n( HARM_CATEGORY_HARASSMENT ) and hate speech ( HARM_CATEGORY_HATE_SPEECH )\ncategories. For example, setting these categories to BLOCK_LOW_AND_ABOVE blocks any content that has a low or higher probability of being harassment or\nhate speech. To understand the threshold settings, see Safety filtering per request . Python from google.generativeai.types import HarmCategory , HarmBlockThreshold model = genai . GenerativeModel ( model_name = 'gemini-1.5-flash' ) response = model . generate_content ( [ 'Do these look store-bought or homemade?' , img ], safety_settings = { HarmCategory . HARM_CATEGORY_HATE_SPEECH : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , HarmCategory . HARM_CATEGORY_HARASSMENT : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , } ) Go model := client . GenerativeModel ( \"gemini-1.5-flash\" ) model . SafetySettings = [] * genai . SafetySetting { { Category : genai . HarmCategoryHarassment , Threshold : genai . HarmBlockLowAndAbove , }, { Category : genai . HarmCategoryHateSpeech , Threshold : genai . HarmBlockLowAndAbove , }, } Node.js import { HarmBlockThreshold , HarmCategory } from \"@google/generative-ai\" ; // ... const safetySettings = [ { category : HarmCategory . HARM_CATEGORY_HARASSMENT , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, { category : HarmCategory . HARM_CATEGORY_HATE_SPEECH , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, ]; const model = genAi . getGenerativeModel ({ model : \"gemini-1.5-flash\" , safetySettings : safetySettings }); Web import { HarmBlockThreshold , HarmCategory } from \"@google/generative-ai\" ; // ... const safetySettings = [ { category : HarmCategory . HARM_CATEGORY_HARASSMENT , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, { category : HarmCategory . HARM_CATEGORY_HATE_SPEECH , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, ]; const model = genAi . getGenerativeModel ({ model : \"gemini-1.5-flash\" , safetySettings }); Dart (Flutter) final safetySettings = [ SafetySetting ( HarmCategory . harassment , HarmBlockThreshold . low ), SafetySetting ( HarmCategory . hateSpeech , HarmBlockThreshold . low ), ]; final model = GenerativeModel ( model: 'gemini-1.5-flash' , apiKey: apiKey , safetySettings: safetySettings , ); Kotlin val harassmentSafety = SafetySetting ( HarmCategory . HARASSMENT , BlockThreshold . LOW_AND_ABOVE ) val hateSpeechSafety = SafetySetting ( HarmCategory . HATE_SPEECH , BlockThreshold . LOW_AND_ABOVE ) val generativeModel = GenerativeModel ( modelName = \"gemini-1.5-flash\" , apiKey = BuildConfig . apiKey , safetySettings = listOf ( harassmentSafety , hateSpeechSafety ) ) Java SafetySetting harassmentSafety = new SafetySetting ( HarmCategory . HARASSMENT , BlockThreshold . LOW_AND_ABOVE ); SafetySetting hateSpeechSafety = new SafetySetting ( HarmCategory . HATE_SPEECH , BlockThreshold . LOW_AND_ABOVE ); GenerativeModel gm = new GenerativeModel ( \"gemini-1.5-flash\" , BuildConfig . apiKey , null , // generation config is optional Arrays . asList ( harassmentSafety , hateSpeechSafety ) ); GenerativeModelFutures model = GenerativeModelFutures . from ( gm ); REST echo '{ \"safetySettings\": [ {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_ONLY_HIGH\"}, {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"} ], \"contents\": [{ \"parts\":[{ \"text\": \"' I support Martians Soccer Club and I think Jupiterians Football Club sucks! Write a ironic phrase about them. '\"}]}]}' > request.json\n\ncurl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= $GOOGLE_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d @request.json 2 > /dev/null safety_settings.sh Next steps See the API reference to learn more about the full API. Review the safety guidance for a general look at safety\nconsiderations when developing with LLMs. Learn more about assessing probability versus severity from the Jigsaw\nteam Learn more about the products that contribute to safety solutions like the Perspective\nAPI .\n                *   You can use these safety settings to create a toxicity\n                    classifier. See the classification\n                    example to\n                    get started. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings"
            ],
            "timestamp": "2025-01-01T18:56:17.422779",
            "status_code": 200
        }
    ]
}