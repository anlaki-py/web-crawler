{
    "base_url": "https://ai.google.dev/gemini-api/docs",
    "crawl_date": "2024-12-27T22:47:43.854318",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://ai.google.dev/gemini-api/docs",
            "title": "Gemini API  |  Google AI for Developers",
            "text_content": "Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Get started with the Gemini API Get a Gemini API Key The Gemini API and Google AI Studio help you start working with Google's latest\nmodels and turn your ideas into applications that scale. Python import google.generativeai as genai genai . configure ( api_key = \" YOUR_API_KEY \" ) model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) response = model . generate_content ( \"Explain how AI works\" ) print ( response . text ) Node.js const { GoogleGenerativeAI } = require ( \"@google/generative-ai\" ); const genAI = new GoogleGenerativeAI ( \" YOUR_API_KEY \" ); const model = genAI . getGenerativeModel ({ model : \"gemini-1.5-flash\" }); const prompt = \"Explain how AI works\" ; const result = await model . generateContent ( prompt ); console . log ( result . response . text ()); REST curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= YOUR_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d '{ \"contents\": [{ \"parts\":[{\"text\": \"Write a story about a magic backpack.\"}] }] }' Meet the models Try Gemini in Google AI Studio 2.0 Flash experiment Our newest multimodal model, with next generation features and improved\n          capabilities 1.5 Flash-8B Our fastest and most cost-efficient multimodal model with great performance\n          for high-frequency tasks 1.5 Pro Our best performing multimodal model with features for a wide variety of reasoning tasks Explore the API Explore long context Input millions of tokens to Gemini models and derive understanding from unstructured images, videos, and documents. Solve tasks with fine-tuning Modify the behavior of Gemini models to adapt to specific tasks, recognize data, and solve problems. Tune models with your own data to make production deployments more robust and reliable. Generate structured outputs Constrain Gemini to respond with JSON, a structured data format suitable for automated processing. Start building with the Gemini API Get started Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Gemini API Developer Docs and API Reference",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs"
            ],
            "timestamp": "2024-12-27T22:47:00.874600",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/gemini-for-research",
            "title": "Gemini API  |  Google AI for Developers",
            "text_content": "Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Accelerate discovery with Gemini for Research Get a Gemini API Key Gemini models can be used to advance foundational research across disciplines.\nHere are ways that you can explore Gemini for your research: Fine-tuning : You can fine-tune Gemini models for a variety of modalities to\nadvance your research. Learn more . Analyze and control model outputs : For further analysis, you can examine a\nresponse candidate generated by the model using tools like Logprobs and CitationMetadata . You can also configure options for model generation and\noutputs, such as responseSchema , topP , and topK . Learn more . Multimodal inputs : Gemini can process images, audio, and videos, enabling a\nmultitude of exciting research directions. Learn more . Long-context capabilities : Gemini 1.5 Flash comes with a 1-million-token\ncontext window, and Gemini 1.5 Pro comes with a 2-million-token context window. Learn more . Grow with Google : Quickly access Gemini models through the API and Google AI\nStudio for production use cases. If you're looking for a Google Cloud-based\nplatform, Vertex AI can provide additional supporting infrastructure. To support academic research and drive cutting-edge research, Google provides\naccess to Gemini API credits for scientists and academic researchers through the Gemini Academic Program . Get started with Gemini The Gemini API and Google AI Studio help you start working with Google's latest\nmodels and turn your ideas into applications that scale. Python import google.generativeai as genai genai . configure ( api_key = \" YOUR_API_KEY \" ) model = genai . GenerativeModel ( \"gemini-1.5-flash\" ) response = model . generate_content ( \"How large is the universe?\" ) print ( response . text ) Node.js const { GoogleGenerativeAI } = require ( \"@google/generative-ai\" ); const genAI = new GoogleGenerativeAI ( \" YOUR_API_KEY \" ); const model = genAI . getGenerativeModel ({ model : \"gemini-1.5-flash\" }); const prompt = \"How large is the universe?\" ; const result = await model . generateContent ( prompt ); console . log ( result . response . text ()); REST curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= YOUR_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d '{ \"contents\": [{ \"parts\":[{\"text\": \"How large is the universe?\"}] }] }' Featured academics \"Our research investigates Gemini as a visual language model (VLM) and its agentic behaviors in diverse environments from robustness and safety perspectives. So far, we have evaluated Gemini's robustness against distractions such as pop-up windows when VLM agents perform computer tasks, and have leveraged Gemini to analyze social interaction, temporal events as well as risk factors based on video input.\" \"Gemini Pro and Flash, with their long context window, have been helping us in OK-Robot, our open-vocabulary mobile manipulation project. Gemini enables complex natural language queries and commands over the robot's \"memory\": in this case, previous observations made by the robot over a long operation duration. Mahi Shafiullah and I are also using Gemini to decompose tasks into code that the robot can execute in the real world.\" Gemini Academic Program Qualified academic researchers (such as faculty, staff, and PhD students) in supported\ncountries can apply to receive Gemini API\ncredits and higher rate limits for research projects. This support enables\nhigher throughput for scientific experiments and advances research. We are particularly interested in the research areas in the following section,\nbut we welcome applications from diverse scientific disciplines: Evaluations and benchmarks : Community-endorsed evaluation methods that\ncan provide a strong performance signal in areas such as factuality, safety,\ninstruction following, reasoning, and planning. Accelerating scientific discovery to benefit humanity : Potential\napplications of AI in interdisciplinary scientific research, including areas\nsuch as rare and neglected diseases, experimental biology, materials science,\nand sustainability. Embodiment and interactions : Utilizing large language models to\ninvestigate novel interactions within the fields of embodied AI, ambient\ninteractions, robotics, and human-computer interaction. Emergent capabilities : Exploring new agentic capabilities required to\nenhance reasoning and planning, and how capabilities can be expanded during\ninference (e.g., by utilizing Gemini Flash). Multimodal interaction and understanding : Identifying gaps and\nopportunities for multimodal foundational models for analysis, reasoning,\nand planning across a variety of tasks. Eligibility: Only individuals (faculty members, researchers or equivalent)\naffiliated with a valid academic institution, or academic research organization\ncan apply. Note that API access and credits will be granted and removed\nat Google's discretion. We review applications on a monthly basis. Start researching with the Gemini API Apply now Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Accelerate discovery with Gemini for Research",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research"
            ],
            "timestamp": "2024-12-27T22:47:01.716391",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/sdks",
            "title": "Google Gen AI SDKs  |  Gemini API  |  Google AI for Developers",
            "text_content": "Google Gen AI SDKs  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Generative AI SDKs Overview Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models SDKs Send feedback Google Gen AI SDKs The new Google Gen AI SDK provides a unified interface to Gemini 2.0\nthrough both the Gemini Developer API and the Gemini Enterprise API (\nVertex AI). With a few exceptions, code that runs on one platform will\nrun on both. The Gen AI SDK also supports the Gemini 1.5 models. Python The Google Gen AI SDK for Python is available on PyPI and GitHub: google-genai on PyPI python-genai on GitHub Or try out the Getting Started\nnotebook . To learn more, see the Python SDK reference (opens in a new tab) . Quickstart 1. Install SDK bash\npip install google-genai 2. Import libraries from google import genai from google.genai import types 3. Create a client client = genai . Client ( api_key = 'GEMINI_API_KEY' ) 4. Generate content response = client . models . generate_content ( model = 'gemini-1.5-flash' , contents = 'How does RLHF work?' ) print ( response . text ) Go The Google Gen AI SDK for Go is available on go.dev and GitHub: go-genai on go.dev go-genai on GitHub Quickstart 1. Import libraries import \"google.golang.org/genai\" 2. Create a client client , err := genai . NewClient ( ctx , & genai . ClientConfig { APIKey : apiKey , Backend : genai . BackendGoogleAI , }) 3. Generate content // Call the GenerateContent method result , err := client . Models . GenerateContent ( ctx , \"gemini-2.0-flash-exp\" , genai . Text ( \"Tell me about New York?\" ), nil ) Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-23 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/sdks"
            ],
            "timestamp": "2024-12-27T22:47:02.806494",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/quickstart",
            "title": "Gemini API quickstart  |  Google AI for Developers",
            "text_content": "Gemini API quickstart  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Gemini API quickstart This quickstart shows you how to install your SDK of choice and then make your first\nGemini API request. Python Node.js REST Go Install the Gemini API library Make your first request Get a Gemini API key in Google AI Studio Use the generateContent method\nto send a request to the Gemini API. What's next Now that you made your first API request, you might want to explore the following\nguides which showcase Gemini in action: Text generation Vision Long context Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started with the Gemini API for Developers",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/quickstart"
            ],
            "timestamp": "2024-12-27T22:47:03.811802",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/api-key",
            "title": "Get a Gemini API key  |  Google AI for Developers",
            "text_content": "Get a Gemini API key  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Get a Gemini API key To use the Gemini API, you need an API key.\nYou can create a key with a few clicks in Google AI Studio. Get a Gemini API key in Google AI Studio Set up your API key For initial testing, you can hard code an API key, but this should only be\ntemporary since it is not secure. The rest of this section goes through how to\nset up your API key locally as an environment variable with different operating\nsystems. Linux/macOS - Bash Bash is a common Linux and macOS terminal configuration. You can check if\nyou have a configuration file for it by running the following command: ~/.bashrc If the response is \"No such file or directory\", you will need to create this\nfile and open it by running the following commands, or use zsh : touch ~/.bashrc open ~/.bashrc Next, you need to set you API key by adding the following export command: export GEMINI_API_KEY = <YOUR_API_KEY_HERE> After saving the file, apply the changes by running: source ~/.bashrc macOS - Zsh Zsh is a common Linux and macOS terminal configuration. You can check if\nyou have a configuration file for it by running the following command: ~/.zshrc If the response is \"No such file or directory\", you will need to create this\nfile and open it by running the following commands, or use bash : touch ~/.zshrc open ~/.zshrc Next, you need to set you API key by adding the following export command: export GEMINI_API_KEY = <YOUR_API_KEY_HERE> After saving the file, apply the changes by running: source ~/.zshrc Windows Search for \"Environment Variables\" in the system settings Edit either \"User variables\" (for current user) or \"System variables\"\n(for all users - use with caution). Create the variable and add export GEMINI_API_KEY=your_key_here Apply the changes Send your first Gemini API request You can use a curl command to verify your setup: curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= ${ GEMINI_API_KEY } \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d '{ \"contents\": [{ \"parts\":[{\"text\": \"Write a story about a magic backpack.\"}] }] }' Keep your API key secure It's important to keep your Gemini API key secure. Here are a few things to keep\nin mind when using your Gemini API key: The Google AI Gemini API uses API keys for authorization. If others get access\nto your Gemini API key, they can make calls using your project's quota,\nwhich could result in lost quota or additional charges for billed projects,\nin addition to accessing tuned models and files. Adding API key restrictions can help limit the surface area usable through each API key. You're responsible for keeping your Gemini API key secure. Do NOT check Gemini API keys into source control. Client-side applications (Android, Swift, web, and Dart/Flutter) risk\nexposing API keys. We don't recommend using the Google AI client SDKs\nin production apps to call the Google AI Gemini API directly from your\nmobile and web apps. For some general best practices, you can also review this support article . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-26 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get your Gemini API key and start building in less than 5 minutes.",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/api-key"
            ],
            "timestamp": "2024-12-27T22:47:05.277108",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/changelog",
            "title": "Release notes  |  Gemini API  |  Google AI for Developers",
            "text_content": "Release notes  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Release notes This page documents updates to the Gemini API. December 19, 2024 Model updates: Released Gemini 2.0 Flash Thinking Mode for public preview. Thinking Mode is\na test-time compute model that lets you see the model's thought process\nwhile it generates a response, and produces responses with stronger\nreasoning capabilities. Read more about Gemini 2.0 Flash Thinking Mode in our overview\npage . December 11, 2024 Model updates: Released Gemini 2.0 Flash Experimental for public preview. Gemini 2.0 Flash Experimental's partial list of features includes: Twice as fast as Gemini 1.5 Pro Bidirectional streaming with our Multimodal Live API Multimodal response generation in the form of text, images, and speech Native tool use with multi-turn reasoning to use features like code\nexecution, Search, function calling, and more Read more about Gemini 2.0 Flash in our overview\npage . November 21, 2024 Model updates: Released gemini-exp-1121 , an even more powerful experimental Gemini API model. Model updates: Updated the gemini-1.5-flash-latest and gemini-1.5-flash model aliases\nto use gemini-1.5-flash-002 . Change to top_k parameter: The gemini-1.5-flash-002 model supports top_k values between 1 and 41 (exclusive).\nValues greater than 40 will be changed to 40. November 14, 2024 Model updates: Released gemini-exp-1114 , a powerful experimental Gemini API model. November 8, 2024 API updates Added support for Gemini in the OpenAI libraries / REST API. October 31, 2024 API updates Added support for Grounding with Google Search . October 3, 2024 Model updates: Released gemini-1.5-flash-8b-001 , a stable version of our smallest Gemini\nAPI model. September 24, 2024 Model updates: Released gemini-1.5-pro-002 and gemini-1.5-flash-002 , two new stable\nversions of Gemini 1.5 Pro and 1.5 Flash, for general availability. Updated the gemini-1.5-pro-latest model code to use gemini-1.5-pro-002 and the gemini-1.5-flash-latest model code to use gemini-1.5-flash-002 . Released gemini-1.5-flash-8b-exp-0924 to replace gemini-1.5-flash-8b-exp-0827 . Released the civic integrity safety filter for the Gemini API and AI Studio. Released support for two new parameters for Gemini 1.5 Pro and 1.5 Flash in\nPython and NodeJS: frequencyPenalty and presencePenalty . September 19, 2024 AI Studio updates: Added thumb-up and thumb-down buttons to model responses, to enable users to\nprovide feedback on the quality of a response. API updates: Added support for Google Cloud credits, which can now be used towards\nGemini API usage. September 17, 2024 AI Studio updates: Added an Open in Colab button that exports a prompt – and the\ncode to run it – to a Colab notebook. The feature doesn't yet support\nprompting with tools (JSON mode, function calling, or code execution). September 13, 2024 AI Studio updates: Added support for compare mode, which lets you compare responses across\n models and prompts to find the best fit for your use case. August 30, 2024 Model updates Gemini 1.5 Flash supports supplying JSON schema through model configuration . August 27, 2024 Model updates: Released the following experimental models : gemini-1.5-pro-exp-0827 gemini-1.5-flash-exp-0827 gemini-1.5-flash-8b-exp-0827 August 9, 2024 API updates Added support for PDF processing . August 5, 2024 Model updates Fine-tuning support released for Gemini 1.5 Flash. August 1, 2024 Model updates Released gemini-1.5-pro-exp-0801 , a new experimental version of Gemini 1.5 Pro . July 12, 2024 Model updates Support for Gemini 1.0 Pro Vision removed from Google AI services and tools. June 27, 2024 Model updates General availability release for Gemini 1.5 Pro's 2M context window. API updates Added support for code execution . June 18, 2024 API updates Added support for context caching . June 12, 2024 Model updates Gemini 1.0 Pro Vision deprecated. May 23, 2024 Model updates Gemini 1.5 Pro ( gemini-1.5-pro-001 ) is generally available (GA). Gemini 1.5 Flash ( gemini-1.5-flash-001 ) is generally available (GA). May 14, 2024 API updates Introduced a 2M context window for Gemini 1.5 Pro (waitlist). Introduced pay-as-you-go billing for Gemini 1.0\nPro, with Gemini 1.5 Pro and Gemini 1.5 Flash billing coming soon. Introduced increased rate limits for the upcoming paid tier of Gemini 1.5\nPro. Added built-in video support to the File API . Added plain text support to the File API . Added support for parallel function calling, which returns more than one\ncall at a time. May 10, 2024 Model updates Released Gemini 1.5 Flash ( gemini-1.5-flash-latest ) in preview. April 9, 2024 Model updates Released Gemini 1.5 Pro ( gemini-1.5-pro-latest ) in preview. Released a new text embedding model, text-embeddings-004 , which supports elastic embedding sizes under 768. API updates Released the File API for temporarily storing\nmedia files for use in prompting. Added support for prompting with text, image, and audio data, also\nknown as multimodal prompting. To learn more, see Prompting with media . Released System instructions in\nbeta. Added Function calling mode ,\nwhich defines the execution behavior for function calling. Added support for the response_mime_type configuration option, which lets\nyou request responses in JSON format . March 19, 2024 Added support for tuning Gemini 1.0 Pro in Google AI Studio or with the Gemini API. December 13 2023 4 new models: gemini-pro: New text model for a wide variety of tasks. Balances capability\nand efficiency. gemini-pro-vision: New multimodal model for a wide variety of tasks.\nBalances capability and efficiency. embedding-001: New embeddings model. aqa: A new specially tuned model that is trained to answer questions\n  using text passages for grounding generated answers. See Gemini models for more details. API version updates: v1: The stable API channel. v1beta: Beta channel. This channel has features that may be under\ndevelopment. See the API versions topic for more details. API updates GenerateContent is a single unified endpoint for chat and text. Streaming available through the StreamGenerateContent method. Multimodal capability: Image is a new supported modality New beta features: Function Calling Semantic Retriever Attributed Question Answering (AQA) Updated candidate count: Gemini models only return 1 candidate. Different Safety Settings and SafetyRating categories. See safety settings for more details. Tuning models is not yet supported for Gemini models (Work in progress). Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-19 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Keep track of updates to the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/changelog"
            ],
            "timestamp": "2024-12-27T22:47:06.297564",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/models/gemini",
            "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
            "text_content": "Gemini models  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Gemini models 2.0 Flash experiment Our newest multimodal model, with next generation features and improved\n      capabilities Input audio, images, video, and text — get text, image, and audio responses Features low-latency conversational interactions with our Multimodal Live API 1.5 Flash spark Our most balanced multimodal model with great performance\n      for most tasks Input audio, images, video, and text, get text responses Generate code, extract data, edit text, and more Best for tasks balancing performance and cost 1.5 Pro Our best performing multimodal model with features for a wide variety of reasoning tasks Input audio, images, video, and text, get text responses Generate code, extract data, edit text, and more For when you need a boost in performance Model variants The Gemini API offers different models that are optimized for specific use\ncases. Here's a brief overview of Gemini variants that are available: Model variant Input(s) Output Optimized for Gemini 2.0 Flash gemini-2.0-flash-exp Audio, images, videos, and text Text, images (coming soon), and audio (coming soon) Next generation features, speed, and multimodal generation for a diverse variety of tasks Gemini 1.5 Flash gemini-1.5-flash Audio, images, videos, and text Text Fast and versatile performance across a diverse variety of tasks Gemini 1.5 Flash-8B gemini-1.5-flash-8b Audio, images, videos, and text Text High volume and lower intelligence tasks Gemini 1.5 Pro gemini-1.5-pro Audio, images, videos, and text Text Complex reasoning tasks requiring more intelligence Gemini 1.0 Pro gemini-1.0-pro (Deprecated on 2/15/2025) Text Text Natural language tasks, multi-turn text and code chat, and code\n      generation Text Embedding text-embedding-004 Text Text embeddings Measuring the relatedness of text strings AQA aqa Text Text Providing source-grounded answers to questions Gemini 2.0 Flash (Experimental) Important: Gemini 2.0 Flash is available as an experimental preview,\n    and some supported features are not currently available. If you need a model\n    for production-level code, use one of our 1.5 models instead. Gemini 2.0 Flash delivers next-gen features and improved capabilities,\n    including superior speed, native tool use, multimodal generation, and a 1M token\n    context window. Learn more about Gemini 2.0 Flash in our overview page . Try in Google AI Studio Model details Property Description id_card Model code models/gemini-2.0-flash-exp save Supported data types Inputs Audio, images, video, and text Output Audio (coming soon), images (coming soon), and text token_auto Token limits [*] Input token limit 1,048,576 Output token limit 8,192 swap_driving_apps_wheel Rate limits [**] 10 RPM 4 million TPM 1,500 RPD handyman Capabilities Structured outputs Supported Caching Not supported Tuning Not supported Function calling Supported Code execution Supported Search Supported Image generation Supported Native tool use Supported Audio generation Supported Multimodal Live API Supported 123 Versions Read the model version patterns for more details. Latest: gemini-2.0-flash-exp calendar_month Latest update December 2024 cognition_2 Knowledge cutoff August 2024 Gemini 1.5 Flash Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across\n    diverse tasks. Try in Google AI Studio Model details Property Description id_card Model code models/gemini-1.5-flash save Supported data types Inputs Audio, images, video, and text Output Text token_auto Token limits [*] Input token limit 1,048,576 Output token limit 8,192 movie_info Audio/visual specs Maximum number of images per prompt 3,600 Maximum video length 1 hour Maximum audio length Approximately 9.5 hours swap_driving_apps_wheel Rate limits [**] Free: 15 RPM 1 million TPM 1,500 RPD Pay-as-you-go: 2,000 RPM 4 million TPM handyman Capabilities System instructions Supported JSON mode Supported JSON schema Supported Adjustable safety settings Supported Caching Supported Tuning Supported Function calling Supported Code execution Supported Bidirectional streaming Not supported 123 Versions Read the model version patterns for more details. Latest: gemini-1.5-flash-latest Latest stable: gemini-1.5-flash Stable: gemini-1.5-flash-001 gemini-1.5-flash-002 calendar_month Latest update September 2024 Gemini 1.5 Flash-8B Gemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. Try in Google AI Studio Model details Property Description id_card Model code models/gemini-1.5-flash-8b save Supported data types Inputs Audio, images, video, and text Output Text token_auto Token limits [*] Input token limit 1,048,576 Output token limit 8,192 movie_info Audio/visual specs Maximum number of images per prompt 3,600 Maximum video length 1 hour Maximum audio length Approximately 9.5 hours swap_driving_apps_wheel Rate limits [**] Free: 15 RPM 1 million TPM 1,500 RPD Pay-as-you-go: 4,000 RPM 4 million TPM handyman Capabilities System instructions Supported JSON mode Supported JSON schema Supported Adjustable safety settings Supported Caching Supported Tuning Supported Function calling Supported Code execution Supported Bidirectional streaming Not supported 123 Versions Read the model version patterns for more details. Latest: gemini-1.5-flash-8b-latest Latest stable: gemini-1.5-flash-8b Stable: gemini-1.5-flash-8b-001 calendar_month Latest update October 2024 Gemini 1.5 Pro Gemini 1.5 Pro is a mid-size multimodal model that is optimized for\n    a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data\n    at once, including 2 hours of video, 19 hours of audio, codebases with\n    60,000 lines of code, or 2,000 pages of text. Try in Google AI Studio Model details Property Description id_card Model code models/gemini-1.5-pro save Supported data types Inputs Audio, images, video, and text Output Text token_auto Token limits [*] Input token limit 2,097,152 Output token limit 8,192 movie_info Audio/visual specs Maximum number of images per prompt 7,200 Maximum video length 2 hours Maximum audio length Approximately 19 hours swap_driving_apps_wheel Rate limits [**] Free: 2 RPM 32,000 TPM 50 RPD Pay-as-you-go: 1,000 RPM 4 million TPM handyman Capabilities System instructions Supported JSON mode Supported JSON schema Supported Adjustable safety settings Supported Caching Supported Tuning Not supported Function calling Supported Code execution Supported Bidirectional streaming Not supported 123 Versions Read the model version patterns for more details. Latest: gemini-1.5-pro-latest Latest stable: gemini-1.5-pro Stable: gemini-1.5-pro-001 gemini-1.5-pro-002 calendar_month Latest update September 2024 Gemini 1.0 Pro (Deprecated) Important: 1.0 Pro is deprecated and will be removed on\n  February 15, 2025. Use 1.5 Pro or 1.5 Flash instead. Gemini 1.0 Pro is an NLP model that handles tasks like multi-turn text and\n    code chat, and code generation. Try in Google AI Studio Model details Property Description id_card Model code models/gemini-1.0-pro save Supported data types Input Text Output Text swap_driving_apps_wheel Rate limits [**] Free: 15 RPM 32,000 TPM 1,500 RPD Pay-as-you-go: 360 RPM 120,000 TPM 30,000 RPD handyman Capabilities System instructions Not supported JSON mode Not supported JSON schema Not supported Adjustable safety settings Supported Caching Not supported Tuning Supported Function calling Supported Function calling configuration Not supported Code execution Not supported Bidirectional streaming Not supported 123 Versions Latest: gemini-1.0-pro-latest Latest stable: gemini-1.0-pro Stable: gemini-1.0-pro-001 calendar_month Latest update February 2024 Note: gemini-pro is an alias for gemini-1.0-pro . Text Embedding and Embedding Text Embedding Text embeddings are used to measure the relatedness of strings and are widely used in\n    many AI applications. text-embedding-004 achieves a stronger retrieval performance and outperforms existing models with comparable dimensions, on the standard MTEB embedding benchmarks. Model details Property Description id_card Model code Gemini API models/text-embedding-004 save Supported data types Input Text Output Text embeddings token_auto Token limits [*] Input token limit 2,048 Output dimension size 768 swap_driving_apps_wheel Rate limits [**] 1,500 requests per minute encrypted Adjustable safety settings Not supported calendar_month Latest update April 2024 Embedding Note: Text Embedding is the newer version of the Embedding model. If\n    you're creating a new project, use Text Embedding. You can use the Embedding model to generate text embeddings for\n    input text. The Embedding model is optimized for creating embeddings with 768 dimensions\n    for text of up to 2,048 tokens. Embedding model details Property Description id_card Model code models/embedding-001 save Supported data types Input Text Output Text embeddings token_auto Token limits [*] Input token limit 2,048 Output dimension size 768 swap_driving_apps_wheel Rate limits [**] 1,500 requests per minute encrypted Adjustable safety settings Not supported calendar_month Latest update December 2023 AQA You can use the AQA model to perform Attributed Question-Answering (AQA)–related tasks over a document, corpus, or a set of passages. The AQA\n    model returns answers to questions that are grounded in provided sources,\n    along with estimating answerable probability. Model details Property Description id_card Model code models/aqa save Supported data types Input Text Output Text language Supported language English token_auto Token limits [*] Input token limit 7,168 Output token limit 1,024 swap_driving_apps_wheel Rate limits [**] 1,500 requests per minute encrypted Adjustable safety settings Supported calendar_month Latest update December 2023 See the examples to explore the capabilities of these model\nvariations. [*]   A token is equivalent to about 4 characters for Gemini models. 100 tokens\n  are about 60-80 English words. [**] RPM: Requests per minute TPM: Tokens per minute RPD: Requests per day TPD: Tokens per day Due to capacity limitations, specified maximum rate limits are not\n  guaranteed. Model version name patterns Gemini models are available in either preview or stable versions. In your\ncode, you can use one of the following model name formats to specify which model\nand version you want to use. Latest: Points to the cutting-edge version of the model for a specified\ngeneration and variation. The underlying model is updated regularly and might\nbe a preview version. Only exploratory testing apps and prototypes should\nuse this alias. To specify the latest version, use the following pattern: <model>-<generation>-<variation>-latest . For example, gemini-1.0-pro-latest . Latest stable: Points to the most recent stable version released for the\nspecified model generation and variation. To specify the latest stable version, use the following pattern: <model>-<generation>-<variation> . For example, gemini-1.0-pro . Stable: Points to a specific stable model. Stable models don't change.\nMost production apps should use a specific stable model. To specify a stable version, use the following pattern: <model>-<generation>-<variation>-<version> . For example, gemini-1.0-pro-001 . Experimental: Points to an experimental model available in Preview,\nas defined in the Terms ,\nmeaning it is not for production use. We release experimental models\nto gather feedback, get our latest updates into the hands of developers\nquickly, and highlight the pace of innovation happening at Google. What\nwe learn from experimental launches informs how we release models more\nwidely. An experimental model can be swapped for another without prior\nnotice. We don't guarantee that an experimental model will become a\nstable model in the future. To specify an experimental version, use the following pattern: <model>-<generation>-<variation>-<version> . For example, gemini-exp-1121 . Available languages Gemini models are trained to work with the following languages: Arabic ( ar ) Bengali ( bn ) Bulgarian ( bg ) Chinese simplified and traditional ( zh ) Croatian ( hr ) Czech ( cs ) Danish ( da ) Dutch ( nl ) English ( en ) Estonian ( et ) Finnish ( fi ) French ( fr ) German ( de ) Greek ( el ) Hebrew ( iw ) Hindi ( hi ) Hungarian ( hu ) Indonesian ( id ) Italian ( it ) Japanese ( ja ) Korean ( ko ) Latvian ( lv ) Lithuanian ( lt ) Norwegian ( no ) Polish ( pl ) Portuguese ( pt ) Romanian ( ro ) Russian ( ru ) Serbian ( sr ) Slovak ( sk ) Slovenian ( sl ) Spanish ( es ) Swahili ( sw ) Swedish ( sv ) Thai ( th ) Turkish ( tr ) Ukrainian ( uk ) Vietnamese ( vi ) Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-16 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn about Google&#39;s most advanced AI models, the Gemini model family, including Gemini 1.5 Flash, Gemini 1.5 Pro, and more",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/models/gemini"
            ],
            "timestamp": "2024-12-27T22:47:07.334340",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
            "title": "Gemini 2.0 Flash (experimental)  |  Gemini API  |  Google AI for Developers",
            "text_content": "Gemini 2.0 Flash (experimental)  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Gemini 2.0 Flash (experimental) Gemini 2.0 Flash is now available as an experimental preview release through\nthe Gemini Developer API and Google AI Studio. The model introduces new features\nand enhanced core capabilities: Multimodal Live API: This new API helps you create real-time vision and\naudio streaming applications with tool use. Speed and performance: Gemini 2.0 has a significantly improved time to\nfirst token (TTFT) over 1.5 Flash. Quality: Better performance across most benchmarks than Gemini 1.5 Pro. Improved agentic capabilities: Gemini 2.0 delivers improvements to\nmultimodal understanding, coding, complex instruction following, and\nfunction calling. New modalities: Gemini 2.0 introduces native image generation and\ncontrollable text-to-speech capabilities. To provide a better developer experience, we're also shipping a new SDK . For Gemini 2.0 technical details, see Gemini models . Note: Native image and audio generation are in private experimental release, under\nallowlist. All other features are public experimental. Google Gen AI SDK (experimental) The new Google Gen AI SDK provides a unified interface to Gemini 2.0 through both\nthe Gemini Developer API and the Gemini API on Vertex AI. With a few exceptions,\ncode that runs on one platform will run on both. The new SDK is available in\nPython and Go, with Java and JavaScript coming soon. Install the new SDK: pip install google-genai Then import the library, initialize a client, and generate content: from google import genai client = genai . Client ( api_key = \"GEMINI_API_KEY\" ) response = client . models . generate_content ( model = 'gemini-2.0-flash-exp' , contents = 'How does AI work?' ) print ( response . text ) Python developers can also try out the Getting Started\nnotebook in the Gemini Cookbook . Multimodal Live API To try a tutorial that lets you use your voice and camera to talk to Gemini\n  through the Multimodal Live API, see the Web Console Demo project . The Multimodal Live API enables low-latency bidirectional voice and video\ninteractions with Gemini. Using the Multimodal Live API, you can provide end\nusers with the experience of natural, human-like voice conversations, and with\nthe ability to interrupt the model's responses using voice commands. The model\ncan process text, audio, and video input, and it can provide text and audio\noutput. You can try the Multimodal Live API in Google AI Studio . To learn more the API's\ncapabilities and limitations, see the Multimodal Live API reference\nguide . Search as a tool Using Grounding with Google Search, you can improve the accuracy and recency of\nresponses from the model. Starting with Gemini 2.0, Google Search is available\nas a tool. This means that the model can decide when to use Google Search. The\nfollowing example shows how to configure Search as a tool. from google import genai from google.genai.types import Tool , GenerateContentConfig , GoogleSearch client = genai . Client () model_id = \"gemini-2.0-flash-exp\" google_search_tool = Tool ( google_search = GoogleSearch () ) response = client . models . generate_content ( model = model_id , contents = \"When is the next total solar eclipse in the United States?\" , config = GenerateContentConfig ( tools = [ google_search_tool ], response_modalities = [ \"TEXT\" ], ) ) for each in response . candidates [ 0 ] . content . parts : print ( each . text ) # Example response: # The next total solar eclipse visible in the contiguous United States will be on ... # To get grounding metadata as web content. print ( response . candidates [ 0 ] . grounding_metadata . search_entry_point . rendered_content ) The Search-as-a-tool functionality also enables multi-turn searches and\nmulti-tool queries (for example, combining Grounding with Google Search and code\nexecution). Search as a tool enables complex prompts and workflows that require planning,\nreasoning, and thinking: Grounding to enhance factuality and recency and provide more accurate answers Retrieving artifacts from the web to do further analysis on Finding relevant images, videos, or other media to assist in multimodal\nreasoning or generation tasks Coding, technical troubleshooting, and other specialized tasks Finding region-specific information or assisting in translating content\naccurately Finding relevant websites for further browsing You can get started by trying the Search tool notebook . Improved tool use Gemini 2.0 introduces improvements to function calling and tools that provide\nbetter support for agentic experiences. Compositional function calling Gemini 2.0 supports a new function calling capability: compositional function calling . Compositional function calling enables the\nGemini API to invoke multiple user-defined functions automatically in the\nprocess of generating a response. For example, to respond to the prompt \"Get the temperature in my current location\" , the Gemini API might invoke both\na get_current_location() function and a get_weather() function that takes\nthe location as a parameter. Compositional function calling with code execution requires bidirectional\nstreaming and is only supported by the new Multimodal Live API. Here's an\nexample showing how you might use compositional function calling, code\nexecution, and the Multimodal Live API together: Note: The run() function declaration, which handles the asynchronous websocket\nsetup, is omitted for brevity. turn_on_the_lights_schema = { 'name' : 'turn_on_the_lights' } turn_off_the_lights_schema = { 'name' : 'turn_off_the_lights' } prompt = \"\"\" Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights? \"\"\" tools = [ { 'code_execution' : {}}, { 'function_declarations' : [ turn_on_the_lights_schema , turn_off_the_lights_schema ]} ] await run ( prompt , tools = tools , modality = \"AUDIO\" ) Python developers can try this out in the Live API Tool Use\nnotebook . Multi-tool use With Gemini 2.0, you can enable multiple tools at the same time, and the model\nwill decide when to call them. Here's an example that enables two tools,\nGrounding with Google Search and code execution, in a request using the\nMultimodal Live API. Note: The run() function declaration, which handles the asynchronous websocket\nsetup, is omitted for brevity. prompt = \"\"\" Hey, I need you to do three things for me. 1. Turn on the lights. 2. Then compute the largest prime palindrome under 100000. 3. Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024. Thanks! \"\"\" tools = [ { 'google_search' : {}}, { 'code_execution' : {}}, { 'function_declarations' : [ turn_on_the_lights_schema , turn_off_the_lights_schema ]} ] await run ( prompt , tools = tools , modality = \"AUDIO\" ) Python developers can try this out in the Live API Tool Use\nnotebook . Bounding box detection In this experimental launch, we are providing developers with a powerful tool\nfor object detection and localization within images and video. By accurately\nidentifying and delineating objects with bounding boxes, developers can unlock a\nwide range of applications and enhance the intelligence of their projects. Key Benefits: Simple: Integrate object detection capabilities into your applications\nwith ease, regardless of your computer vision expertise. Customizable: Produce bounding boxes based on custom instructions (e.g. \"I\nwant to see bounding boxes of all the green objects in this image\"), without\nhaving to train a custom model. Technical Details: Input: Your prompt and associated images or video frames. Output: Bounding boxes in the [y_min, x_min, y_max, x_max] format. The\ntop left corner is the origin. The x and y axis go horizontally and\nvertically, respectively. Coordinate values are normalized to 0-1000 for every\nimage. Visualization: AI Studio users will see bounding boxes plotted within the\nUI. Vertex AI users should visualize their bounding boxes through custom\nvisualization code. For Python developers, try the 2D spatial understanding\nnotebook or the experimental 3D pointing\nnotebook . Speech generation (early access/allowlist) Gemini 2.0 supports a new multimodal generation capability: text to speech.\nUsing the text-to-speech capability, you can prompt the model to generate high\nquality audio output that sounds like a human voice ( say \"hi everyone\" ), and\nyou can further refine the output by steering the voice. Image generation (early access/allowlist) Gemini 2.0 supports the ability to output text with in-line images. This lets\nyou use Gemini to conversationally edit images or generate multimodal outputs\n(for example, a blog post with text and images in a single turn). Previously\nthis would have required stringing together multiple models. Image generation is available as a private experimental release. It supports the\nfollowing modalities and capabilities: Text to image Example prompt: \"Generate an image of the Eiffel tower with fireworks in\nthe background.\" Text to image(s) and text (interleaved) Example prompt: \"Generate an illustrated recipe for a paella.\" Image(s) and text to image(s) and text (interleaved) Example prompt: (With an image of a furnished room) \"What other color\nsofas would work in my space? can you update the image?\" Image editing (text and image to image) Example prompt: \"Edit this image to make it look like a cartoon\" Example prompt: [image of a cat] + [image of a pillow] + \"Create a cross\nstitch of my cat on this pillow.\" Multi-turn image editing (chat) Example prompts: [upload an image of a blue car.] \"Turn this car into a\nconvertible.\" \"Now change the color to yellow.\" Watermarking All generated images include a SynthID watermark. Limitations: Generation of people and editing of uploaded images of people are not allowed. For best performance, use the following languages: EN, es-MX, ja-JP, zh-CN,\nhi-IN. Image generation does not support audio or video inputs. Image generation may not always trigger: The model may output text only. Try asking for image outputs explicitly\n(e.g. \"generate an image\", \"provide images as you go along\", \"update the\nimage\"). The model may stop generating partway through. Try again or try a different\nprompt. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-24 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2"
            ],
            "timestamp": "2024-12-27T22:47:08.086530",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/thinking-mode",
            "title": "Gemini 2.0 Flash Thinking Mode  |  Gemini API  |  Google AI for Developers",
            "text_content": "Gemini 2.0 Flash Thinking Mode  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Gemini 2.0 Flash Thinking Mode Gemini 2.0 Flash Thinking Mode is an experimental model that's trained to\ngenerate the \"thinking process\" the model goes through as part of its response.\nAs a result, Thinking Mode is capable of stronger reasoning capabilities in its\nresponses than the Gemini 2.0 Flash Experimental model. Using Thinking Models Thinking models are available in Google AI\nStudio ,\nand through the Gemini API. One of the main considerations when using a model that\nreturns the thinking process is how much information you want to expose to\nend users, as the thinking process can be quite verbose. Note: We have setup gemini-2.0-flash-thinking-exp as an alias to the latest\nthinking model. Use this alias to get the latest Flash thinking model, or specify\nthe full model name. Sending a basic request Python This example uses the new Google Genai SDK which is useful in this context for parsing out \"thoughts\" programmatically. from google import genai client = genai . Client ( api_key = 'GEMINI_API_KEY' , http_options = { 'api_version' : 'v1alpha' }) response = client . models . generate_content ( model = 'gemini-2.0-flash-thinking-exp' , contents = 'Explain how RLHF works in simple terms.' ) # Usually the first part is the thinking process, but it's not guaranteed print ( response . candidates [ 0 ] . content . parts [ 0 ] . text ) print ( response . candidates [ 0 ] . content . parts [ 1 ] . text ) Working with thoughts On a standard request, the model responds with two parts, the thoughts and the\nmodel response. You can check programmatically if a part is a thought or not by\nseeing if the part.thought field is set to True . Python To use the new thought parameter, you need to use the v1alpha version of\nthe Gemini API along with the new Google Genai SDK . from google import genai client = genai . Client ( api_key = 'GEMINI_API_KEY' , http_options = { 'api_version' : 'v1alpha' }) response = client . models . generate_content ( model = 'gemini-2.0-flash-thinking-exp' , contents = 'Explain how RLHF works in simple terms.' ) for part in response . candidates [ 0 ] . content . parts : if part . thought == True : print ( f \"Model Thought: \\n { part . text } \\n \" ) else : print ( f \" \\n Model Response: \\n { part . text } \\n \" ) Streaming model thinking Thinking models generally take longer to response than standard models. To\nstream the model thinking, you can use the generate_content_stream method. Python To use the new thought parameter, you need to use the v1alpha version of\nthe Gemini API along with the new Google Genai SDK . from google import genai client = genai . Client ( api_key = 'GEMINI_API_KEY' , http_options = { 'api_version' : 'v1alpha' }) for chunk in client . models . generate_content_stream ( model = 'gemini-2.0-flash-thinking-exp' , contents = 'What is your name?' ): for part in chunk . candidates [ 0 ] . content . parts : if part . thought == True : print ( f \"Model Thought Chunk: \\n { part . text } \\n \" ) else : print ( f \" \\n Model Response: \\n { part . text } \\n \" ) Multi-turn thinking conversations During multi-turn conversations, the model will by default not pass the thoughts\nfrom the previous turn to the next turn, but you can still see the thoughts on\nthe most recent turn. Python The new Google Genai SDK provides the ability\nto create a multi-turn chat session which is helpful to manage the state of\na conversation. import asyncio from google import genai client = genai . Client ( api_key = 'GEMINI_API_KEY' , http_options = { 'api_version' : 'v1alpha' }) async def main (): chat = client . aio . chats . create ( model = 'gemini-2.0-flash-thinking-exp' ) response = await chat . send_message ( 'What is your name?' ) print ( response . text ) response = await chat . send_message ( 'What did you just say before this?' ) print ( response . text ) asyncio . run ( main ()) Limitations Thinking Mode is an experimental model and has the following limitations: 32k token input limit Text and image input only 8k token output limit Text only output No built-in tool usage like Search or code execution What's next? Try Thinking Mode for yourself with our Colab\nnotebook ,\nor open Google AI\nStudio and try prompting the model for yourself. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-23 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/thinking-mode"
            ],
            "timestamp": "2024-12-27T22:47:08.997781",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/models/experimental-models",
            "title": "Experimental models  |  Gemini API  |  Google AI for Developers",
            "text_content": "Experimental models  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Experimental models In addition to the base models , the Gemini API\noffers experimental models available in Preview, as defined in the Terms , meaning it is not for\nproduction use. We release experimental models to gather feedback, get our\nlatest updates into the hands of developers quickly, and highlight the pace of\ninnovation happening at Google. What we learn from experimental launches informs\nhow we release models more widely. An experimental model can be swapped for\nanother without prior notice. We don't guarantee that an experimental model will\nbecome a stable model in the future. Use an experimental model Important: Support for an experimental model can change at any time. The Gemini API's experimental models are available for all users. You can use\nthe experimental models either in your code directly using the Gemini API, or\nyou can use an experimental model in Google AI Studio : Gemini API To use an experimental model, specify the model code when you initialize the\ngenerative model. For example: model = genai . GenerativeModel ( model_name = \"gemini-2.0-flash-exp\" ) Google AI Studio Select the model code of the experimental model you want to use in the Model drop-down menu in the Settings pane. Experimental models are\nlabeled Preview in the drop-down menu. Available models Model code Base model Highlights Release date gemini-2.0-flash-thinking-exp-1219 Gemini 2.0 Flash Thinking Mode Reasoning for complex problems, features a new Thinking mode December 19, 2024 gemini-2.0-flash-exp Gemini 2.0 Flash Next generation features, superior speed, native tool use, and multimodal generation December 11, 2024 gemini-exp-1206 Gemini Quality improvements, celebrate 1 year of Gemini December 6th, 2024 learnlm-1.5-pro-experimental LearnLM 1.5 Pro Experimental Inputs: Audio, images, videos, and text Output: Text November 19, 2024 Note: Support for individual experimental models is not permanent or long-term,\nand can be turned off without prior warning. Experimental models cannot\nbe used in production code. Provide feedback You can provide feedback on the Gemini API's experimental models using our developer forum . Previous models As new versions or stable releases become available, we remove and replace\nexperimental models. You can find the previous experimental models we released in\nthe following section along with the replacement version: Model code Base model Replacement version gemini-exp-1121 Gemini N/A gemini-exp-1114 Gemini N/A gemini-1.5-pro-exp-0827 Gemini 1.5 Pro gemini-1.5-pro-002 gemini-1.5-pro-exp-0801 Gemini 1.5 Pro gemini-1.5-pro-002 gemini-1.5-flash-8b-exp-0924 Gemini 1.5 Flash-8B gemini-1.5-flash-8b gemini-1.5-flash-8b-exp-0827 Gemini 1.5 Flash-8B gemini-1.5-flash-8b Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-19 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models"
            ],
            "timestamp": "2024-12-27T22:47:09.774338",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/text-generation",
            "title": "Generate text using the Gemini API  |  Google AI for Developers",
            "text_content": "Generate text using the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Generate text using the Gemini API Python Node.js Go REST The Gemini API can generate text output when provided text, images, video, and\naudio as input. This guide shows you how to generate text using the generateContent and streamGenerateContent methods. To learn about working with Gemini's vision and audio capabilities,\nrefer to the Vision and Audio guides. What's next Now that you have explored the basics of the Gemini API, you might want to\ntry: Vision understanding : Learn how to use\nGemini's native vision understanding to process images and videos. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Audio understanding : Learn how to use\nGemini's native audio understanding to process audio files. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started building chat and text generation apps with the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/text-generation"
            ],
            "timestamp": "2024-12-27T22:47:10.673299",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/vision",
            "title": "Explore vision capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore vision capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore vision capabilities with the Gemini API Python Node.js Go REST The Gemini API is able to process images and videos, enabling a multitude of\n exciting developer use cases. Some of Gemini's vision capabilities include\n the ability to: Caption and answer questions about images Transcribe and reason over PDFs, including long documents up to 2 million token context window Describe, segment, and extract information from videos,\nincluding both visual frames and audio, up to 90 minutes long Detect objects in an image and return bounding box coordinates for them This tutorial demonstrates some possible ways to prompt the Gemini API with\nimages and video input, provides code examples,\nand outlines prompting best practices with multimodal vision capabilities.\nAll output is text-only. What's next This guide shows how to upload image and video files using the File API and\nthen generate text outputs from image and video inputs. To learn more,\nsee the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Get started building with Gemini&#39;s multimodal capabilities in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/vision"
            ],
            "timestamp": "2024-12-27T22:47:11.575127",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/audio",
            "title": "Explore audio capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore audio capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore audio capabilities with the Gemini API Python Node.js Go REST Gemini can respond to prompts about audio. For example, Gemini can: Describe, summarize, or answer questions about audio content. Provide a transcription of the audio. Provide answers or a transcription about a specific segment of the audio. Note: You can't generate audio output with the Gemini API. This guide demonstrates different ways to interact with audio files and audio\ncontent using the Gemini API. Supported audio formats Gemini supports the following audio format MIME types: WAV - audio/wav MP3 - audio/mp3 AIFF - audio/aiff AAC - audio/aac OGG Vorbis - audio/ogg FLAC - audio/flac Technical details about audio Gemini imposes the following rules on audio: Gemini represents each second of audio as 25 tokens; for example,\none minute of audio is represented as 1,500 tokens. Gemini can only infer responses to English-language speech. Gemini can \"understand\" non-speech components, such as birdsong or sirens. The maximum supported length of audio data in a single prompt is 9.5 hours.\nGemini doesn't limit the number of audio files in a single prompt; however,\nthe total combined length of all audio files in a single prompt cannot exceed\n9.5 hours. Gemini downsamples audio files to a 16 Kbps data resolution. If the audio source contains multiple channels, Gemini combines those channels\ndown to a single channel. What's next This guide shows how to upload audio files using the File API and then generate\ntext outputs from audio inputs. To learn more, see the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/audio"
            ],
            "timestamp": "2024-12-27T22:47:12.377661",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/long-context",
            "title": "Long context  |  Gemini API  |  Google AI for Developers",
            "text_content": "Long context  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Long context Gemini 1.5 Flash comes standard with a 1-million-token context window, and\nGemini 1.5 Pro comes with a 2-million-token context window. Historically, large\nlanguage models (LLMs) were significantly limited by the amount of text (or\ntokens) that could be passed to the model at one time. The Gemini 1.5 long\ncontext window, with near-perfect retrieval\n(>99%) ,\nunlocks many new use cases and developer paradigms. The code you already use for cases like text\ngeneration or multimodal\ninputs will work out of the box with long context. Throughout this guide, you briefly explore the basics of the context window, how\ndevelopers should think about long context, various real world use cases for\nlong context, and ways to optimize the usage of long context. What is a context window? The basic way you use the Gemini 1.5 models is by passing information (context)\nto the model, which will subsequently generate a response. An analogy for the\ncontext window is short term memory. There is a limited amount of information\nthat can be stored in someone's short term memory, and the same is true for\ngenerative models. You can read more about how models work under the hood in our generative models\nguide . Getting started with long context Most generative models created in the last few years were only capable of\nprocessing 8,000 tokens at a time. Newer models pushed this further by accepting\n32,000 tokens or 128,000 tokens. Gemini 1.5 is the first model capable of\naccepting 1 million tokens, and now 2 million tokens with Gemini 1.5\nPro . In practice, 1 million tokens would look like: 50,000 lines of code (with the standard 80 characters per line) All the text messages you have sent in the last 5 years 8 average length English novels Transcripts of over 200 average length podcast episodes Even though the models can take in more and more context, much of the\nconventional wisdom about using large language models assumes this inherent\nlimitation on the model, which as of 2024, is no longer the case. Some common strategies to handle the limitation of small context windows\nincluded: Arbitrarily dropping old messages / text from the context window as new text\ncomes in Summarizing previous content and replacing it with the summary when the\ncontext window gets close to being full Using RAG with semantic search to move data out of the context window and\ninto a vector database Using deterministic or generative filters to remove certain text /\ncharacters from prompts to save tokens While many of these are still relevant in certain cases, the default place to\nstart is now just putting all of the tokens into the context window. Because\nGemini 1.5 models were purpose-built with a long context window, they are much\nmore capable of in-context learning. For example, with only instructional\nmaterials (a 500-page reference grammar, a dictionary, and ≈ 400 extra parallel\nsentences) all provided in context, Gemini 1.5 Pro and Gemini 1.5 Flash are capable of learning to translate from English to Kalamang— a Papuan language with fewer than 200 speakers and\ntherefore almost no online presence—with quality similar to a person who learned\nfrom the same materials. This example underscores how you can start to think about what is possible with\nlong context and the in-context learning capabilities of Gemini 1.5. Long context use cases While the standard use case for most generative models is still text input, the\nGemini 1.5 model family enables a new paradigm of multimodal use cases. These\nmodels can natively understand text, video, audio, and images. They are\naccompanied by the Gemini API that takes in multimodal file\ntypes for\nconvenience. Long form text Text has proved to be the layer of intelligence underpinning much of the\nmomentum around LLMs. As mentioned earlier, much of the practical limitation of\nLLMs was because of not having a large enough context window to do certain\ntasks. This led to the rapid adoption of retrieval augmented generation (RAG)\nand other techniques which dynamically provide the model with relevant\ncontextual information. Now, with larger and larger context windows (currently\nup to 2 million on Gemini 1.5 Pro), there are new techniques becoming available\nwhich unlock new use cases. Some emerging and standard use cases for text based long context include: Summarizing large corpuses of text Previous summarization options with smaller context models would require\na sliding window or another technique to keep state of previous sections\nas new tokens are passed to the model Question and answering Historically this was only possible with RAG given the limited amount of\ncontext and models' factual recall being low Agentic workflows Text is the underpinning of how agents keep state of what they have done\nand what they need to do; not having enough information about the world\nand the agent's goal is a limitation on the reliability of agents Many-shot in-context learning is one of the\nmost unique capabilities unlocked by long context models. Research has shown\nthat taking the common \"single shot\" or \"multi-shot\" example paradigm, where the\nmodel is presented with one or a few examples of a task, and scaling that up to\nhundreds, thousands, or even hundreds of thousands of examples, can lead to\nnovel model capabilities. This many-shot approach has also been shown to perform\nsimilarly to models which were fine-tuned for a specific task. For use cases\nwhere a Gemini model's performance is not yet sufficient for a production\nrollout, you can try the many-shot approach. As you might explore later in the\nlong context optimization section, context caching makes this type of high input\ntoken workload much more economically feasible and even lower latency in some\ncases. Long form video Video content's utility has long been constrained by the lack of accessibility\nof the medium itself. It was hard to skim the content, transcripts often failed\nto capture the nuance of a video, and most tools don't process image, text, and\naudio together. With Gemini 1.5, the long-context text capabilities translate to\nthe ability to reason and answer questions about multimodal inputs with\nsustained performance. Gemini 1.5 Flash, when tested on the needle in a video\nhaystack problem with 1M tokens, obtained >99.8% recall of the video in the\ncontext window, and 1.5 Pro reached state of the art performance on the Video-MME benchmark . Some emerging and standard use cases for video long context include: Video question and answering Video memory, as shown with Google's Project Astra Video captioning Video recommendation systems, by enriching existing metadata with new\nmultimodal understanding Video customization, by looking at a corpus of data and associated video\nmetadata and then removing parts of videos that are not relevant to the\nviewer Video content moderation Real-time video processing When working with videos, it is important to consider how the videos are\nprocessed into tokens , which affects\nbilling and usage limits. You can learn more about prompting with video files in\nthe Prompting\nguide . Long form audio The Gemini 1.5 models were the first natively multimodal large language models\nthat could understand audio. Historically, the typical developer workflow would\ninvolve stringing together multiple domain specific models, like a\nspeech-to-text model and a text-to-text model, in order to process audio. This\nled to additional latency required by performing multiple round-trip requests\nand decreased performance usually attributed to disconnected architectures of\nthe multiple model setup. On standard audio-haystack evaluations, Gemini 1.5 Pro is able to find the\nhidden audio in 100% of the tests and Gemini 1.5 Flash is able to find it in\n98.7% of the\ntests .\nGemini 1.5 Flash accepts up to 9.5 hours of audio in a single\nrequest and\nGemini 1.5 Pro can accept up to 19 hours of audio using the 2-million-token\ncontext window. Further, on a test set of 15-minute audio clips, Gemini 1.5 Pro\narchives a word error rate (WER) of ~5.5%, much lower than even specialized\nspeech-to-text models, without the added complexity of extra input segmentation\nand pre-processing. Some emerging and standard use cases for audio context include: Real-time transcription and translation Podcast / video question and answering Meeting transcription and summarization Voice assistants You can learn more about prompting with audio files in the Prompting\nguide . Long context optimizations The primary optimization when working with long context and the Gemini 1.5\nmodels is to use context\ncaching . Beyond the previous\nimpossibility of processing lots of tokens in a single request, the other main\nconstraint was the cost. If you have a \"chat with your data\" app where a user\nuploads 10 PDFs, a video, and some work documents, you would historically have\nto work with a more complex retrieval augmented generation (RAG) tool /\nframework in order to process these requests and pay a significant amount for\ntokens moved into the context window. Now, you can cache the files the user\nuploads and pay to store them on a per hour basis. The input / output cost per\nrequest with Gemini\n1.5 Flash for example is ~4x less than the standard input / output cost, so if\nthe user chats with their data enough, it becomes a huge cost saving for you as\nthe developer. Long context limitations In various sections of this guide, we talked about how Gemini 1.5 models achieve\nhigh performance across various needle-in-a-haystack retrieval evals. These\ntests consider the most basic setup, where you have a single needle you are\nlooking for. In cases where you might have multiple \"needles\" or specific pieces\nof information you are looking for, the model does not perform with the same\naccuracy. Performance can vary to a wide degree depending on the context. This\nis important to consider as there is an inherent tradeoff between getting the\nright information retrieved and cost. You can get ~99% on a single query, but\nyou have to pay the input token cost every time you send that query. So for 100\npieces of information to be retrieved, if you needed 99% performance, you would\nlikely need to send 100 requests. This is a good example of where context\ncaching can significantly reduce the cost associated with using Gemini models\nwhile keeping the performance high. FAQs Do I lose model performance when I add more tokens to a query? Generally, if you don't need tokens to be passed to the model, it is best to\navoid passing them. However, if you have a large chunk of tokens with some\ninformation and want to ask questions about that information, the model is\nhighly capable of extracting that information (up to 99% accuracy in many\ncases). How does Gemini 1.5 Pro perform on the standard needle-in-a-haystack test? Gemini 1.5 Pro achieves 100% recall up to 530k tokens and >99.7% recall up to\n1M\ntokens . How can I lower my cost with long-context queries? If you have a similar set of tokens / context that you want to re-use many\ntimes, context caching can help reduce the costs\nassociated with asking questions about that information. How can I get access to the 2-million-token context window? All developers now have access to the 2-million-token context window with Gemini\n1.5 Pro. Does the context length affect the model latency? There is some fixed amount of latency in any given request, regardless of the\nsize, but generally longer queries will have higher latency (time to first\ntoken). Do the long context capabilities differ between Gemini 1.5 Flash and Gemini 1.5 Pro? Yes, some of the numbers were mentioned in different sections of this guide, but\ngenerally Gemini 1.5 Pro is more performant on most long context use cases. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-09-28 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn about how to get started building with long context (1 million and 2 million context window) on Gemini 1.5 Pro and Gemini 1.5 Flash",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/long-context"
            ],
            "timestamp": "2024-12-27T22:47:13.097783",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/code-execution",
            "title": "Code execution  |  Gemini API  |  Google AI for Developers",
            "text_content": "Code execution  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Code execution Python Node.js Go REST The Gemini API code execution feature enables the model to generate and run\nPython code and learn iteratively from the results until it arrives at a\nfinal output. You can use this code execution capability to build applications\nthat benefit from code-based reasoning and that produce text output. For\nexample, you could use code execution in an application that solves equations or\nprocesses text. Code execution is available in both AI Studio and the Gemini API. In AI Studio,\nyou can enable code execution under Advanced settings . The Gemini API\nprovides code execution as a tool, similar to function calling . After you add\ncode execution as a tool, the model decides when to use it. Note: The code execution environment includes the NumPy and SymPy libraries. You can't install your own libraries. Get started with code execution This section assumes that you've completed the setup and configuration steps\nshown in the quickstart . Code execution versus function calling Code execution and function calling are\nsimilar features: Code execution lets the model run code in the API backend in a fixed, isolated\nenvironment. Function calling lets you run the functions that the model requests, in\nwhatever environment you want. In general you should prefer to use code execution if it can handle your use\ncase. Code execution is simpler to use (you just enable it) and resolves in a\nsingle GenerateContent request (thus incurring a single charge). Function\ncalling takes an additional GenerateContent request to send back the output\nfrom each function call (thus incurring multiple charges). For most cases, you should use function calling if you have your own functions\nthat you want to run locally, and you should use code execution if you'd like\nthe API to write and run Python code for you and return the result. Billing There's no additional charge for enabling code execution from the\nGemini API. You'll be billed at the current rate of input and output tokens . Here are a few other things to know about billing for code execution: You're only billed once for the input tokens you pass to the model, and\nyou're billed for the final output tokens returned to you by the model. Tokens representing generated code are counted as output tokens. Code execution results are also counted as output tokens. Limitations The model can only generate and execute code. It can't return other artifacts\nlike media files. The feature doesn't support file I/O or use cases that involve non-text output\n(for example, data plots or a CSV file upload). Code execution can run for a maximum of 30 seconds before timing out. In some cases, enabling code execution can lead to regressions in other areas\nof model output (for example, writing a story). There is some variation in the ability of the different models to use code\nexecution successfully. Gemini 1.5 Pro is the best performing\nmodel, based on our testing. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use the Gemini API context caching feature.",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/code-execution"
            ],
            "timestamp": "2024-12-27T22:47:14.414938",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/structured-output",
            "title": "Generate structured output with the Gemini API  |  Google AI for Developers",
            "text_content": "Generate structured output with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Generate structured output with the Gemini API Python Node.js Go Dart (Flutter) Android Swift Web REST Gemini generates unstructured text by default, but some applications require\nstructured text. For these use cases, you can constrain Gemini to respond with\nJSON, a structured data format suitable for automated processing. You can also\nconstrain the model to respond with one of the options specified in an enum. Here are a few use cases that might require structured output from the model: Build a database of companies by pulling company information out of\nnewspaper articles. Pull standardized information out of resumes. Extract ingredients from recipes and display a link to a grocery website for\neach ingredient. In your prompt, you can ask Gemini to produce JSON-formatted output, but note\nthat the model is not guaranteed to produce JSON and nothing but JSON.\nFor a more deterministic response, you can pass a specific JSON schema in a responseSchema field so that Gemini always responds with an expected structure. This guide shows you how to generate JSON using the generateContent method through the SDK\nof your choice or using the REST API directly. The examples show text-only\ninput, although Gemini can also produce JSON responses to multimodal requests\nthat include images , videos , and audio . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to generate structured output with the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/structured-output"
            ],
            "timestamp": "2024-12-27T22:47:15.587452",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/function-calling",
            "title": "Intro to function calling with the Gemini API  |  Google AI for Developers",
            "text_content": "Intro to function calling with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Intro to function calling with the Gemini API Using the Gemini API function calling feature, you can provide custom function\ndefinitions to the model. The model doesn't directly invoke these functions, but\ninstead generates structured output that specifies a function name and suggested\narguments. You can then use the function name and arguments to call an external\nAPI, and you can incorporate the resulting API output into a further query to\nthe model, enabling the model to provide a more comprehensive response and take\nadditional actions. Function calling empowers users to interact with real-time information and\nservices like databases, customer relationship management systems, and document\nrepositories. The feature also enhances the model's ability to provide relevant\nand contextual answers. Function calling is best for interacting with external\nsystems. If your use case requires the model to perform computation but doesn't\ninvolve external systems or APIs, you should consider using code execution instead. For a working example of function calling, see the \"light bot\" notebook . Beta: The function calling feature is in Beta release. For more information,\n  see the API versions page. How function calling works You use the function calling feature by adding structured query data describing\nprograming interfaces, called function declarations , to a model prompt. The\nfunction declarations provide the name of the API function, explain its purpose,\nany parameters it supports, and descriptions of those parameters. After you pass\na list of function declarations in a query to the model, it analyzes function\ndeclarations and the rest of the query to determine how to use the declared\nAPI in response to the request. The model then returns an object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond\nto the user's question. You can then take the recommended function call\nparameters, call the actual API, get a response, and provide that response to\nthe user or take further action. Note that the model doesn't actually call the\ndeclared functions. Instead, you use the returned schema object parameters to\ncall the function. The Gemini API also supports parallel function calling, where\nthe model recommends multiple API function calls based on a single request. Function declarations When you implement function calling in a prompt, you create a tools object,\nwhich contains one or more function declarations . You define functions\nusing JSON, specifically with a select subset of the OpenAPI schema format. A single function declaration can include the following parameters: name (string): The unique identifier for the function within the API\ncall. description (string): A comprehensive explanation of the function's\npurpose and capabilities. parameters (object): Defines the input data required by the function. type (string): Specifies the overall data type, such as object . properties (object): Lists individual parameters, each with: type (string): The data type of the parameter, such as string , integer , boolean . description (string): A clear explanation of the parameter's\npurpose and expected format. required (array): An array of strings listing the parameter names\nthat are mandatory for the function to operate. For code examples of a function declaration using cURL commands, see the Function calling examples . For examples of\n creating function declarations using the Gemini API SDKs, see the Function calling tutorial . Best practices for function declarations Accurately defining your functions is essential when integrating them into your\nrequests. Each function relies on specific parameters that guide its behavior\nand interaction with the model. The following listing provides guidance on\ndefining the parameters of an individual function in a functions_declarations array. name : Use clear, descriptive names without space, period\n( . ), or dash ( - ) characters. Instead, use underscore ( _ ) characters\nor camel case. description : Provide detailed, clear, and specific function\ndescriptions, providing examples if necessary. For example, instead of find theaters , use find theaters based on location and optionally movie\ntitle that is currently playing in theaters. Avoid overly broad or ambiguous\ndescriptions. properties > type : Use strongly typed parameters to reduce\nmodel hallucinations. For example, if the parameter values are from a finite\nset, use an enum field instead of listing the values in the description\n(e.g., \"type\": \"enum\", \"values\": [\"now_playing\", \"upcoming\"] ). If the\nparameter value is always an integer, set the type to integer rather than number . properties > description : Provide concrete examples and constraints.\nFor example, instead of the location to search , use The city and state,\ne.g. San Francisco, CA or a zip code e.g. 95616 . For more best practices when using function calling, see the Best Practices section. Function calling mode You can use the function calling mode parameter to modify the execution\nbehavior of the feature. There are three modes available: AUTO : The default model behavior. The model decides to predict either a\nfunction call or a natural language response. ANY : The model is constrained to always predict a function call. If allowed_function_names is not provided, the model picks from all of the\navailable function declarations. If allowed_function_names is provided,\nthe model picks from the set of allowed functions. NONE : The model won't predict a function call. In this case, the model\nbehavior is the same as if you don't pass any function declarations. The usage of the ANY mode (\"forced function calling\") is\nsupported for Gemini 1.5 Pro and Gemini 1.5 Flash models only. You can also pass a set of allowed_function_names that, when provided, limits\nthe functions that the model will call. You should only include allowed_function_names when the mode is ANY . Function names should match\nfunction declaration names. With the mode set to ANY and the allowed_function_names set, the model will predict a function call from the\nset of function names provided. Key Point: If you set the mode to ANY and provide allowed_function_names ,\nthe model picks from the set of allowed functions. If you set the mode to ANY and don't provide allowed_function_names , the model picks from all of the\navailable functions. The following code snippet from an example request shows how to\nset the mode to ANY and specify a list of allowed functions: \"tool_config\" : { \"function_calling_config\" : { \"mode\" : \"ANY\" , \"allowed_function_names\" : [ \"find_theaters\" , \"get_showtimes\" ] }, } Function calling examples This section provides example prompts for function calling using cURL commands.\nThe examples include single turn and multiple-turn scenarios, and enabling\ndifferent function calling modes. When using cURL commands with this feature, the function and parameter\ninformation is included in the tools element. Each function declaration in the tools element contains the function name, and you specify the parameters using\nan OpenAPI compatible schema ,\nand a function description. Single-turn example Single-turn is when you call the language model one time. With function calling,\na single-turn use case might be when you provide the model a natural language\nquery and a list of functions. In this case, the model uses the function\ndeclaration, which includes the function name, parameters, and description, to\npredict which function to call and the arguments to call it with. The following curl sample is an example of passing in a description of a\nfunction that returns information about where a movie is playing. Several\nfunction declarations are included in the request, such as find_movies and find_theaters . Single-turn function calling example request curl https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"contents\": {\n      \"role\": \"user\",\n      \"parts\": {\n        \"text\": \"Which theaters in Mountain View show Barbie movie?\"\n    }\n  },\n  \"tools\": [\n    {\n      \"function_declarations\": [\n        {\n          \"name\": \"find_movies\",\n          \"description\": \"find movie titles currently playing in theaters based on any description, genre, title words, etc.\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"description\": {\n                \"type\": \"string\",\n                \"description\": \"Any kind of description including category or genre, title words, attributes, etc.\"\n              }\n            },\n            \"required\": [\n              \"description\"\n            ]\n          }\n        },\n        {\n          \"name\": \"find_theaters\",\n          \"description\": \"find theaters based on location and optionally movie title which is currently playing in theaters\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"movie\": {\n                \"type\": \"string\",\n                \"description\": \"Any movie title\"\n              }\n            },\n            \"required\": [\n              \"location\"\n            ]\n          }\n        },\n        {\n          \"name\": \"get_showtimes\",\n          \"description\": \"Find the start times for movies playing in a specific theater\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"movie\": {\n                \"type\": \"string\",\n                \"description\": \"Any movie title\"\n              },\n              \"theater\": {\n                \"type\": \"string\",\n                \"description\": \"Name of the theater\"\n              },\n              \"date\": {\n                \"type\": \"string\",\n                \"description\": \"Date for requested showtime\"\n              }\n            },\n            \"required\": [\n              \"location\",\n              \"movie\",\n              \"theater\",\n              \"date\"\n            ]\n          }\n        }\n      ]\n    }\n  ]\n}' The response to this curl example might be similar to the following. Single-turn function calling curl example response [{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"functionCall\": {\n              \"name\": \"find_theaters\",\n              \"args\": {\n                \"movie\": \"Barbie\",\n                \"location\": \"Mountain View, CA\"\n              }\n            }\n          }\n        ]\n      },\n      \"finishReason\": \"STOP\",\n      \"safetyRatings\": [\n        {\n          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        }\n      ]\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 9,\n    \"totalTokenCount\": 9\n  }\n}] Single-turn example using ANY mode The following curl example is similar to the single-turn example , but it sets\nthe mode to ANY : \"tool_config\" : { \"function_calling_config\" : { \"mode\" : \"ANY\" }, } Single-turn function calling using ANY mode (request) curl https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"contents\": {\n      \"role\": \"user\",\n      \"parts\": {\n        \"text\": \"What movies are showing in North Seattle tonight?\"\n    }\n  },\n  \"tools\": [\n    {\n      \"function_declarations\": [\n        {\n          \"name\": \"find_movies\",\n          \"description\": \"find movie titles currently playing in theaters based on any description, genre, title words, etc.\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"description\": {\n                \"type\": \"string\",\n                \"description\": \"Any kind of description including category or genre, title words, attributes, etc.\"\n              }\n            },\n            \"required\": [\n              \"description\"\n            ]\n          }\n        },\n        {\n          \"name\": \"find_theaters\",\n          \"description\": \"find theaters based on location and optionally movie title which is currently playing in theaters\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"movie\": {\n                \"type\": \"string\",\n                \"description\": \"Any movie title\"\n              }\n            },\n            \"required\": [\n              \"location\"\n            ]\n          }\n        },\n        {\n          \"name\": \"get_showtimes\",\n          \"description\": \"Find the start times for movies playing in a specific theater\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"movie\": {\n                \"type\": \"string\",\n                \"description\": \"Any movie title\"\n              },\n              \"theater\": {\n                \"type\": \"string\",\n                \"description\": \"Name of the theater\"\n              },\n              \"date\": {\n                \"type\": \"string\",\n                \"description\": \"Date for requested showtime\"\n              }\n            },\n            \"required\": [\n              \"location\",\n              \"movie\",\n              \"theater\",\n              \"date\"\n            ]\n          }\n        }\n      ]\n    }\n  ],\n  \"tool_config\": {\n    \"function_calling_config\": {\n      \"mode\": \"ANY\"\n    },\n  }\n}' The response might be similar to the following: Single-turn function calling using ANY mode (response) {\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"functionCall\": {\n              \"name\": \"find_movies\",\n              \"args\": {\n                \"description\": \"\",\n                \"location\": \"North Seattle, WA\"\n              }\n            }\n          }\n        ],\n        \"role\": \"model\"\n      },\n      \"finishReason\": \"STOP\",\n      \"index\": 0,\n      \"safetyRatings\": [\n        {\n          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          \"probability\": \"NEGLIGIBLE\"\n        }\n      ]\n    }\n  ],\n  \"promptFeedback\": {\n    \"safetyRatings\": [\n      {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      }\n    ]\n  }\n} Single-turn example using ANY mode and allowed functions The following curl example is similar to the single-turn example , but it sets\nthe mode to ANY and includes a list of allowed\nfunctions: \"tool_config\" : { \"function_calling_config\" : { \"mode\" : \"ANY\" , \"allowed_function_names\" : [ \"find_theaters\" , \"get_showtimes\" ] }, } Single-turn function calling using ANY mode and allowed functions (request) curl https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"contents\": {\n      \"role\": \"user\",\n      \"parts\": {\n        \"text\": \"What movies are showing in North Seattle tonight?\"\n    }\n  },\n  \"tools\": [\n    {\n      \"function_declarations\": [\n        {\n          \"name\": \"find_movies\",\n          \"description\": \"find movie titles currently playing in theaters based on any description, genre, title words, etc.\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"description\": {\n                \"type\": \"string\",\n                \"description\": \"Any kind of description including category or genre, title words, attributes, etc.\"\n              }\n            },\n            \"required\": [\n              \"description\"\n            ]\n          }\n        },\n        {\n          \"name\": \"find_theaters\",\n          \"description\": \"find theaters based on location and optionally movie title which is currently playing in theaters\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"movie\": {\n                \"type\": \"string\",\n                \"description\": \"Any movie title\"\n              }\n            },\n            \"required\": [\n              \"location\"\n            ]\n          }\n        },\n        {\n          \"name\": \"get_showtimes\",\n          \"description\": \"Find the start times for movies playing in a specific theater\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"movie\": {\n                \"type\": \"string\",\n                \"description\": \"Any movie title\"\n              },\n              \"theater\": {\n                \"type\": \"string\",\n                \"description\": \"Name of the theater\"\n              },\n              \"date\": {\n                \"type\": \"string\",\n                \"description\": \"Date for requested showtime\"\n              }\n            },\n            \"required\": [\n              \"location\",\n              \"movie\",\n              \"theater\",\n              \"date\"\n            ]\n          }\n        }\n      ]\n    }\n  ],\n  \"tool_config\": {\n    \"function_calling_config\": {\n      \"mode\": \"ANY\",\n      \"allowed_function_names\": [\"find_theaters\", \"get_showtimes\"]\n    },\n  }\n}' The model can't predict the find_movies function, because it's not on the list\nof allowed functions, so it predicts a different function instead. The response\nmight be similar to the following: Single-turn function calling using ANY mode and allowed functions (response) {\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"functionCall\": {\n              \"name\": \"find_theaters\",\n              \"args\": {\n                \"location\": \"North Seattle, WA\",\n                \"movie\": null\n              }\n            }\n          }\n        ],\n        \"role\": \"model\"\n      },\n      \"finishReason\": \"STOP\",\n      \"index\": 0,\n      \"safetyRatings\": [\n        {\n          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        }\n      ]\n    }\n  ],\n  \"promptFeedback\": {\n    \"safetyRatings\": [\n      {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      }\n    ]\n  }\n} Multi-turn examples You can implement a multi-turn function calling scenario by doing the following: Get a function call response by calling the language model. This is the first\nturn. Call the language model using the function call response from the first turn\nand the function response you get from calling that function. This is the\nsecond turn. The response from the second turn either summarizes the results to answer your\nquery in the first turn, or contains a second function call you can use to get\nmore information for your query. This topic includes two multi-turn curl examples: Curl example that uses a function response from a previous turn Curl example that calls a language model multiple times Use a response from a previous turn The following curl sample calls the function and arguments returned by the\nprevious single-turn example to get a response. The method and parameters\nreturned by the single-turn example are in this JSON. \"functionCall\" : { \"name\" : \"find_theaters\" , \"args\" : { \"movie\" : \"Barbie\" , \"location\" : \"Mountain View, CA\" } } Multi-turn function calling curl example request curl https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"contents\": [{\n      \"role\": \"user\",\n      \"parts\": [{\n        \"text\": \"Which theaters in Mountain View show Barbie movie?\"\n    }]\n  }, {\n    \"role\": \"model\",\n    \"parts\": [{\n      \"functionCall\": {\n        \"name\": \"find_theaters\",\n        \"args\": {\n          \"location\": \"Mountain View, CA\",\n          \"movie\": \"Barbie\"\n        }\n      }\n    }]\n  }, {\n    \"role\": \"user\",\n    \"parts\": [{\n      \"functionResponse\": {\n        \"name\": \"find_theaters\",\n        \"response\": {\n          \"name\": \"find_theaters\",\n          \"content\": {\n            \"movie\": \"Barbie\",\n            \"theaters\": [{\n              \"name\": \"AMC Mountain View 16\",\n              \"address\": \"2000 W El Camino Real, Mountain View, CA 94040\"\n            }, {\n              \"name\": \"Regal Edwards 14\",\n              \"address\": \"245 Castro St, Mountain View, CA 94040\"\n            }]\n          }\n        }\n      }\n    }]\n  }],\n  \"tools\": [{\n    \"functionDeclarations\": [{\n      \"name\": \"find_movies\",\n      \"description\": \"find movie titles currently playing in theaters based on any description, genre, title words, etc.\",\n      \"parameters\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"STRING\",\n            \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n          },\n          \"description\": {\n            \"type\": \"STRING\",\n            \"description\": \"Any kind of description including category or genre, title words, attributes, etc.\"\n          }\n        },\n        \"required\": [\"description\"]\n      }\n    }, {\n      \"name\": \"find_theaters\",\n      \"description\": \"find theaters based on location and optionally movie title which is currently playing in theaters\",\n      \"parameters\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"STRING\",\n            \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n          },\n          \"movie\": {\n            \"type\": \"STRING\",\n            \"description\": \"Any movie title\"\n          }\n        },\n        \"required\": [\"location\"]\n      }\n    }, {\n      \"name\": \"get_showtimes\",\n      \"description\": \"Find the start times for movies playing in a specific theater\",\n      \"parameters\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"STRING\",\n            \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n          },\n          \"movie\": {\n            \"type\": \"STRING\",\n            \"description\": \"Any movie title\"\n          },\n          \"theater\": {\n            \"type\": \"STRING\",\n            \"description\": \"Name of the theater\"\n          },\n          \"date\": {\n            \"type\": \"STRING\",\n            \"description\": \"Date for requested showtime\"\n          }\n        },\n        \"required\": [\"location\", \"movie\", \"theater\", \"date\"]\n      }\n    }]\n  }]\n}' The response to this curl example includes the result of calling the find_theaters method. The response might be similar to the following: Multi-turn function calling curl example response {\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"text\": \" OK. Barbie is showing in two theaters in Mountain View, CA: AMC Mountain View 16 and Regal Edwards 14.\"\n          }\n        ]\n      }\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 9,\n    \"candidatesTokenCount\": 27,\n    \"totalTokenCount\": 36\n  }\n} Call the model multiple times The following cURL example calls the generative AI model multiple times to call\na function. Each time the model calls the function, it can use a different\nfunction to answer a different user query in the request. Multi-turn function calling curl example request curl https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"contents\": [{\n      \"role\": \"user\",\n      \"parts\": [{\n        \"text\": \"Which theaters in Mountain View show Barbie movie?\"\n    }]\n  }, {\n    \"role\": \"model\",\n    \"parts\": [{\n      \"functionCall\": {\n        \"name\": \"find_theaters\",\n        \"args\": {\n          \"location\": \"Mountain View, CA\",\n          \"movie\": \"Barbie\"\n        }\n      }\n    }]\n  }, {\n    \"role\": \"user\",\n    \"parts\": [{\n      \"functionResponse\": {\n        \"name\": \"find_theaters\",\n        \"response\": {\n          \"name\": \"find_theaters\",\n          \"content\": {\n            \"movie\": \"Barbie\",\n            \"theaters\": [{\n              \"name\": \"AMC Mountain View 16\",\n              \"address\": \"2000 W El Camino Real, Mountain View, CA 94040\"\n            }, {\n              \"name\": \"Regal Edwards 14\",\n              \"address\": \"245 Castro St, Mountain View, CA 94040\"\n            }]\n          }\n        }\n      }\n    }]\n  },\n  {\n    \"role\": \"model\",\n    \"parts\": [{\n      \"text\": \" OK. Barbie is showing in two theaters in Mountain View, CA: AMC Mountain View 16 and Regal Edwards 14.\"\n    }]\n  },{\n    \"role\": \"user\",\n    \"parts\": [{\n      \"text\": \"Can we recommend some comedy movies on show in Mountain View?\"\n    }]\n  }],\n  \"tools\": [{\n    \"functionDeclarations\": [{\n      \"name\": \"find_movies\",\n      \"description\": \"find movie titles currently playing in theaters based on any description, genre, title words, etc.\",\n      \"parameters\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"STRING\",\n            \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n          },\n          \"description\": {\n            \"type\": \"STRING\",\n            \"description\": \"Any kind of description including category or genre, title words, attributes, etc.\"\n          }\n        },\n        \"required\": [\"description\"]\n      }\n    }, {\n      \"name\": \"find_theaters\",\n      \"description\": \"find theaters based on location and optionally movie title which is currently playing in theaters\",\n      \"parameters\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"STRING\",\n            \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n          },\n          \"movie\": {\n            \"type\": \"STRING\",\n            \"description\": \"Any movie title\"\n          }\n        },\n        \"required\": [\"location\"]\n      }\n    }, {\n      \"name\": \"get_showtimes\",\n      \"description\": \"Find the start times for movies playing in a specific theater\",\n      \"parameters\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"STRING\",\n            \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n          },\n          \"movie\": {\n            \"type\": \"STRING\",\n            \"description\": \"Any movie title\"\n          },\n          \"theater\": {\n            \"type\": \"STRING\",\n            \"description\": \"Name of the theater\"\n          },\n          \"date\": {\n            \"type\": \"STRING\",\n            \"description\": \"Date for requested showtime\"\n          }\n        },\n        \"required\": [\"location\", \"movie\", \"theater\", \"date\"]\n      }\n    }]\n  }]\n}' Multi-turn function calling curl example response [{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"functionCall\": {\n              \"name\": \"find_movies\",\n              \"args\": {\n                \"description\": \"comedy\",\n                \"location\": \"Mountain View, CA\"\n              }\n            }\n          }\n        ]\n      },\n      \"finishReason\": \"STOP\",\n      \"safetyRatings\": [\n        {\n          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        }\n      ]\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 48,\n    \"totalTokenCount\": 48\n  }\n}\n] Best practices Follow these best practices to improve the accuracy and\nreliability of your function calls. User prompt For best results, prepend the user query with the following details: Additional context for the model. For example, You are a movie API assistant\nto help users find movies and showtimes based on their preferences. Details or instructions on how and when to use the functions. For example, Don't make assumptions on showtimes. Always use a future date for showtimes. Instructions to ask clarifying questions if user queries are ambiguous. For\nexample, Ask clarifying questions if not enough information is available to\ncomplete the request. Sampling parameters For the temperature parameter, use 0 or another low value. This instructs\nthe model to generate more confident results and reduces hallucinations. API invocation If the model proposes the invocation of a function that would send an order,\nupdate a database, or otherwise have significant consequences, validate the\nfunction call with the user before executing it. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/function-calling"
            ],
            "timestamp": "2024-12-27T22:47:16.753398",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
            "title": "Function calling tutorial  |  Gemini API  |  Google AI for Developers",
            "text_content": "Function calling tutorial  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Function calling tutorial Python Node.js Go Dart (Flutter) Android Swift Web REST Function calling makes it easier for you to get structured data outputs from\ngenerative models. You can then use these outputs to call other APIs and return\nthe relevant response data to the model. In other words, function calling helps\nyou connect generative models to external systems so that the generated content\nincludes the most up-to-date and accurate information. You can provide Gemini models with descriptions of functions. These are\nfunctions that you write in the language of your app (that is, they're not\nGoogle Cloud Functions). The model may ask you to call a function and send back\nthe result to help the model handle your query. If you haven't already, check out the Introduction to function calling to learn\nmore. Example API for lighting control Imagine you have a basic lighting control system with an application programming\ninterface (API) and you want to allow users to control the lights through simple\ntext requests. You can use the Function Calling feature to interpret lighting\nchange requests from users and translate them into API calls to set the lighting\nvalues. This hypothetical lighting control system lets you control the\nbrightness of the light and it's color temperature, defined as two separate\nparameters: Parameter Type Required Description brightness number yes Light level from 0 to 100. Zero is off and 100 is full brightness. colorTemperature string yes Color temperature of the light fixture which can be daylight , cool or warm . For simplicity, this imaginary lighting system only has one light, so the user\ndoes not have to specify a room or location. Here is an example JSON request\nyou could send to the lighting control API to change the light level to 50%\nusing the daylight color temperature: { \"brightness\" : \"50\" , \"colorTemperature\" : \"daylight\" } This tutorial shows you how to set up a Function Call for the Gemini API to\ninterpret users lighting requests and map them to API settings to control a\nlight's brightness and color temperature values. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use Function Calling in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial"
            ],
            "timestamp": "2024-12-27T22:47:18.059030",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/document-processing",
            "title": "Explore document processing capabilities with the Gemini API  |  Google AI for Developers",
            "text_content": "Explore document processing capabilities with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Explore document processing capabilities with the Gemini API Python Node.js Go REST The Gemini API supports PDF input, including long documents (up to 3600 pages).\nGemini models process PDFs with native vision, and are therefore able to\nunderstand both text and image contents inside documents. With native PDF vision\nsupport, Gemini models are able to: Analyze diagrams, charts, and tables inside documents. Extract information into structured output formats. Answer questions about visual and text contents in documents. Summarize documents. Transcribe document content (e.g. to HTML) preserving layouts and formatting, for use in downstream applications (such as in RAG pipelines). This tutorial demonstrates some possible ways to use the Gemini API with PDF\ndocuments. All output is text-only. What's next This guide shows how to use generateContent and\nto generate text outputs from processed documents. To learn more,\nsee the following resources: File prompting strategies : The\nGemini API supports prompting with text, image, audio, and video data, also\nknown as multimodal prompting. System instructions : System\ninstructions let you steer the behavior of the model based on your specific\nneeds and use cases. Safety guidance : Sometimes generative AI\nmodels produce unexpected outputs, such as outputs that are inaccurate,\nbiased, or offensive. Post-processing and human evaluation are essential to\nlimit the risk of harm from such outputs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-20 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use the Gemini API to process documents like PDFs",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/document-processing"
            ],
            "timestamp": "2024-12-27T22:47:18.660593",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/grounding",
            "title": "Grounding with Google Search  |  Gemini API  |  Google AI for Developers",
            "text_content": "Grounding with Google Search  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Grounding with Google Search Important: We're launching Grounding with Google Search! Review the updated Gemini API Additional Terms of Service ,\nwhich include new feature terms and updates for clarity. Python Node.js REST The Grounding with Google Search feature in the Gemini API and AI Studio can be\nused to improve the accuracy and recency of responses from the model. In\naddition to more factual responses, when Grounding with Google Search is\nenabled, the Gemini API returns grounding sources (in-line supporting links) and Google Search Suggestions along with the response\ncontent. The Search Suggestions point users to the search results corresponding\nto the grounded response. Grounding with Google Search only supports text prompts. It doesn't support\nmultimodal (text-and-image, text-and-audio, etc.) prompts. Grounding with\nGoogle Search supports all of the available languages for\nGemini models. This guide will help you get started with Grounding with Google Search using one\nof the Gemini API SDKs or the REST API. Configure a model to use Google Search Tip: Before running the example code, make sure that you've followed the\ninstallation and setup instructions in the quickstart . Why is Grounding with Google Search useful? In generative AI, grounding refers to the process of connecting the model to\nverifiable sources of information. These sources might provide real-world\nworkplace information or other specific context. Grounding helps with improving\nthe accuracy, reliability, and usefulness of AI outputs. Grounding is particularly important for prompts that require up-to-date\ninformation from the web. Using grounding, the model can access information\nbeyond its knowledge cutoff date, get sources for the information, and answer\nquestions that it couldn't have answered accurately otherwise. Using Google AI Studio or the Gemini API, you can ground model output to\nGoogle Search. Grounding with Google Search provides the following benefits: Allows model responses that are tethered to specific content. Reduces model hallucinations, which are instances where the model generates\ncontent that isn't factual. Anchors model responses to sources a user can click through and open. Enhances the trustworthiness and applicability of the generated content. When you use Grounding with Google Search, you're effectively connecting the\nmodel to reliable Search results from the internet. Since non-grounded model\nresponses are based on learned patterns, you might not get factual responses to\nprompts about current events (for example, asking for a weather forecast or the\nfinal score of a recent football game). Since the internet provides access to\nnew information, a grounded prompt can generate more up-to-date responses, with\nsources cited. Here's an example comparing a non-grounded response and a grounded response\ngenerated using the API. (The responses were generated in October 2024.) Ungrounded Gemini Grounding with Google Search Prompt: Who won the Super Bowl this year? Response: The Kansas City Chiefs won Super Bowl\n        LVII this year (2023). Prompt: Who won the Super Bowl this year? Response: The Kansas City Chiefs won Super Bowl\n        LVIII this year, defeating the San Francisco 49ers in overtime with a\n        score of 25 to 22. In the ungrounded response, the model refers to the Kansas City Chiefs' 2023\nSuper Bowl win. In the grounded response, the model correctly references their\nmore recent 2024 win. The following image shows how a grounded response looks in AI Studio. Google Search Suggestions To use Grounding with Google Search, you have to display Google Search\nSuggestions, which are suggested queries included in the metadata of the\ngrounded response. To learn more about the display requirements, see Use Google Search Suggestions . Dynamic retrieval Some queries are likely to benefit more from Grounding with Google Search than\nothers. The dynamic retrieval feature gives you additional control over when\nto use Grounding with Google Search. If the dynamic retrieval mode is unspecified, Grounding with Google Search is\nalways triggered. If the mode is set to dynamic, the model decides when to use\ngrounding based on a threshold that you can configure. The threshold\nis a floating-point value in the range [0,1] and defaults to 0.3. If the\nthreshold value is 0, the response is always grounded with Google Search; if\nit's 1, it never is. How dynamic retrieval works You can use dynamic retrieval in your request to choose when to turn on\nGrounding with Google Search. This is useful when the prompt doesn't require\nan answer grounded in Google Search and the model can provide an answer based\non its own knowledge without grounding. This helps you manage latency, quality,\nand cost more effectively. Before you invoke the dynamic retrieval configuration in your request,\nunderstand the following terminology: Prediction score : When you request a grounded answer, Gemini assigns a prediction score to the prompt. The prediction score is a floating point\nvalue in the range [0,1]. Its value depends on whether the prompt\ncan benefit from grounding the answer with the most up-to-date\ninformation from Google Search. Thus, if a prompt requires an answer\ngrounded in the most recent facts on the web, it has a higher prediction\nscore. A prompt for which a model-generated answer is sufficient has a lower\nprediction score. Here are examples of some prompts and their prediction scores. Note: The prediction scores are assigned by Gemini and can vary over\ntime depending on several factors. Prompt Prediction score Comment \"Write a poem about peonies\" 0.13 The model can rely on its knowledge and the answer doesn't need grounding. \"Suggest a toy for a 2yo child\" 0.36 The model can rely on its knowledge and the answer doesn't need grounding. \"Can you give a recipe for an asian-inspired guacamole?\" 0.55 Google Search can give a grounded answer, but grounding isn't strictly required; the model knowledge might be sufficient. \"What's Agent Builder? How is grounding billed in Agent Builder?\" 0.72 Requires Google Search to generate a well-grounded answer. \"Who won the latest F1 grand prix?\" 0.97 Requires Google Search to generate a well-grounded answer. Threshold : In your API request, you can specify a dynamic retrieval\nconfiguration with a threshold. The threshold is a floating point value in\nthe range [0,1] and defaults to 0.3. If the threshold value is zero, the\nresponse is always grounded with Google Search. For all other values\nof threshold, the following is applicable: If the prediction score is greater than or equal to the threshold, the\nanswer is grounded with Google Search.\nA lower threshold implies that more prompts have responses that are\ngenerated using Grounding with Google Search. If the prediction score is less than the threshold, the model might still\ngenerate the answer, but it isn't grounded with Google Search. To learn how to set the dynamic retrieval threshold using an SDK or the REST\nAPI, see the appropriate code example . If you're using AI Studio, you can set the dynamic retrieval threshold by\nclicking Edit grounding . To find a good threshold that suits your business needs, you can create a\nrepresentative set of queries that you expect to encounter. Then you can sort\nthe queries according to the prediction score in the response and select a\ngood threshold for your use case. A grounded response If your prompt successfully grounds to Google Search, the response will include groundingMetadata . A grounded response might look something like this\n(parts of the response have been omitted for brevity): { \"candidates\" : [ { \"content\" : { \"parts\" : [ { \"text\" : \"Carlos Alcaraz won the Gentlemen's Singles title at the 2024 Wimbledon Championships. He defeated Novak Djokovic in the final, winning his second consecutive Wimbledon title and fourth Grand Slam title overall. \\n\" } ], \"role\" : \"model\" }, ... \"groundingMetadata\" : { \"searchEntryPoint\" : { \"renderedContent\" : \"\\u003cstyle\\u003e\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n\\u003c/style\\u003e\\n\\u003cdiv class=\\\"container\\\"\\u003e\\n  \\u003cdiv class=\\\"headline\\\"\\u003e\\n    \\u003csvg class=\\\"logo-light\\\" width=\\\"18\\\" height=\\\"18\\\" viewBox=\\\"9 9 35 35\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\"\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\\\" fill=\\\"#4285F4\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\\\" fill=\\\"#34A853\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\\\" fill=\\\"#FBBC05\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\\\" fill=\\\"#EA4335\\\"/\\u003e\\n    \\u003c/svg\\u003e\\n    \\u003csvg class=\\\"logo-dark\\\" width=\\\"18\\\" height=\\\"18\\\" viewBox=\\\"0 0 48 48\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\"\\u003e\\n      \\u003ccircle cx=\\\"24\\\" cy=\\\"23\\\" fill=\\\"#FFF\\\" r=\\\"22\\\"/\\u003e\\n      \\u003cpath d=\\\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\\\" fill=\\\"#4285F4\\\"/\\u003e\\n      \\u003cpath d=\\\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\\\" fill=\\\"#34A853\\\"/\\u003e\\n      \\u003cpath d=\\\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\\\" fill=\\\"#FBBC05\\\"/\\u003e\\n      \\u003cpath d=\\\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\\\" fill=\\\"#EA4335\\\"/\\u003e\\n    \\u003c/svg\\u003e\\n    \\u003cdiv class=\\\"gradient-container\\\"\\u003e\\u003cdiv class=\\\"gradient\\\"\\u003e\\u003c/div\\u003e\\u003c/div\\u003e\\n  \\u003c/div\\u003e\\n  \\u003cdiv class=\\\"carousel\\\"\\u003e\\n    \\u003ca class=\\\"chip\\\" href=\\\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4x8Epe-gzpwRBvp7o3RZh2m1ygq1EHktn0OWCtvTXjad4bb1zSuqfJd6OEuZZ9_SXZ_P2SvCpJM7NaFfQfiZs6064MeqXego0vSbV9LlAZoxTdbxWK1hFeqTG6kA13YJf7Fbu1SqBYM0cFM4zo0G_sD9NKYWcOCQMvDLDEJFhjrC9DM_QobBIAMq-gWN95G5tvt6_z6EuPN8QY=\\\"\\u003ewho won wimbledon 2024\\u003c/a\\u003e\\n  \\u003c/div\\u003e\\n\\u003c/div\\u003e\\n\" }, \"groundingChunks\" : [ { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4whET1ta3sDETZvcicd8FeNe4z0VuduVsxrT677KQRp2rYghXI0VpfYbIMVI3THcTuMwggRCbFXS_wVvW0UmGzMe9h2fyrkvsnQPJyikJasNIbjJLPX0StM4Bd694-ZVle56MmRA4YiUvwSqad1w6O2opmWnw==\" , \"title\" : \"wikipedia.org\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4wR1M-9-yMPUr_KdHlnoAmQ8ZX90DtQ_vDYTjtP2oR5RH4tRP04uqKPLmesvo64BBkPeYLC2EpVDxv9ngO3S1fs2xh-e78fY4m0GAtgNlahUkm_tBm_sih5kFPc7ill9u2uwesNGUkwrQlmP2mfWNU5lMMr23HGktr6t0sV0QYlzQq7odVoBxYWlQ_sqWFH\" , \"title\" : \"wikipedia.org\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4wsDmROzbP-tmt8GdwCW_pqISTZ4IRbBuoaMyaHfcQg8WW-yKRQQvMDTPAuLxJh-8_U8_iw_6JKFbQ8M9oVYtaFdWFK4gOtL4RrC9Jyqc5BNpuxp6uLEKgL5-9TggtNvO97PyCfziDFXPsxylwI1HcfQdrz3Jy7ZdOL4XM-S5rC0lF2S3VWW0IEAEtS7WX861meBYVjIuuF_mIr3spYPqWLhbAY2Spj-4_ba8DjRvmevIFUhRuESTKvBfmpxNSM\" , \"title\" : \"cbssports.com\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4yzjLkorHiUKjhOPkWaZ9b4cO-cLG-02vlEl6xTBjMUjyhK04qSIclAa7heR41JQ6AAVXmNdS3WDrLOV4Wli-iezyzW8QPQ4vgnmO_egdsuxhcGk3-Fp8-yfqNLvgXFwY5mPo6QRhvplOFv0_x9mAcka18QuAXtj0SPvJfZhUEgYLCtCrucDS5XFc5HmRBcG1tqFdKSE1ihnp8KLdaWMhrUQI21hHS9\" , \"title\" : \"jagranjosh.com\" } }, { \"web\" : { \"uri\" : \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4y9L4oeNGWCatFz63b9PpP3ys-Wi_zwnkUT5ji9lY7gPUJQcsmmE87q88GSdZqzcx5nZG9usot5FYk2yK-FAGvCRE6JsUQJB_W11_kJU2HVV1BTPiZ4SAgm8XDFIxpCZXnXmEx5HUfRqQm_zav7CvS2qjA2x3__qLME6Jy7R5oza1C5_aqjQu422le9CaigThS5bvJoMo-ZGcXdBUCj2CqoXNVjMA==\" , \"title\" : \"apnews.com\" } } ], \"groundingSupports\" : [ { \"segment\" : { \"endIndex\" : 85 , \"text\" : \"Carlos Alcaraz won the Gentlemen's Singles title at the 2024 Wimbledon Championships.\" }, \"groundingChunkIndices\" : [ 0 , 1 , 2 , 3 ], \"confidenceScores\" : [ 0.97380733 , 0.97380733 , 0.97380733 , 0.97380733 ] }, { \"segment\" : { \"startIndex\" : 86 , \"endIndex\" : 210 , \"text\" : \"He defeated Novak Djokovic in the final, winning his second consecutive Wimbledon title and fourth Grand Slam title overall.\" }, \"groundingChunkIndices\" : [ 1 , 0 , 4 ], \"confidenceScores\" : [ 0.96145374 , 0.96145374 , 0.96145374 ] } ], \"webSearchQueries\" : [ \"who won wimbledon 2024\" ] } } ], ... } If the response doesn't include groundingMetadata , this means the response\nwasn't successfully grounded. There are several reasons this could happen,\nincluding low source relevance or incomplete information within the\nmodel response. When a grounded result is generated, the metadata contains URIs that redirect\nto the publishers of the content that was used to generate the grounded result.\nThese URIs contain the vertexaisearch subdomain, as in this truncated example: https://vertexaisearch.cloud.google.com/grounding-api-redirect/... . The\nmetadata also contains the publishers' domains. The provided URIs remain\naccessible for 30 days after the grounded result is generated. Important: The provided URIs must be directly accessible by the end users and\nmust not be queried programmatically through automated means. If automated\naccess is detected, the grounded answer generation service might stop providing\nthe redirection URIs. The renderedContent field within searchEntryPoint is the provided code for\nimplementing Google Search Suggestions. See Use Google Search Suggestions to learn more. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-19 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/grounding"
            ],
            "timestamp": "2024-12-27T22:47:19.203767",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/model-tuning",
            "title": "Fine-tuning with the Gemini API  |  Google AI for Developers",
            "text_content": "Fine-tuning with the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Fine-tuning with the Gemini API Prompt design strategies such as few-shot prompting may not always produce the\nresults you need. Fine-tuning is a process that can improve your model's\nperformance on specific tasks or help the model adhere to specific output\nrequirements when instructions aren't sufficient and you have a set of examples\nthat demonstrate the outputs you want. This page provides a conceptual overview of fine-tuning the text model behind\nthe Gemini API text service. When you're ready to start tuning, try the fine-tuning tutorial . If you'd like\na more general introduction to customizing LLMs for specific use cases, check\nout LLMs: Fine-tuning, distillation, and prompt engineering in the Machine Learning Crash Course . How fine-tuning works The goal of fine-tuning is to further improve the performance of the model for\nyour specific task. Fine-tuning works by providing the model with a training\ndataset containing many examples of the task. For niche tasks, you can get\nsignificant improvements in model performance by tuning the model on a modest\nnumber of examples. This kind of model tuning is sometimes referred to as supervised fine-tuning , to distinguish it from other kinds of fine-tuning. Your training data should be structured as examples with prompt inputs and\nexpected response outputs. You can also tune models using example data directly\nin Google AI Studio. The goal is to teach the model to mimic the wanted behavior\nor task, by giving it many examples illustrating that behavior or task. When you run a tuning job, the model learns additional parameters that help it\nencode the necessary information to perform the wanted task or learn the wanted\nbehavior. These parameters can then be used at inference time. The output of the\ntuning job is a new model, which is effectively a combination of the newly\nlearned parameters and the original model. Prepare your dataset Before you can start fine-tuning, you need a dataset to tune the model with. For\nthe best performance, the examples in the dataset should be of high quality,\ndiverse, and representative of real inputs and outputs. Format Note: Fine-tuning only supports input-output pair examples. Chat-style\nmulti-turn conversations are not supported at this time. The examples included in your dataset should match your expected production\ntraffic. If your dataset contains specific formatting, keywords, instructions,\nor information, the production data should be formatted in the same way and\ncontain the same instructions. For example, if the examples in your dataset include a \"question:\" and a \"context:\" , production traffic should also be formatted to include a \"question:\" and a \"context:\" in the same order as it appears in the dataset\nexamples. If you exclude the context, the model can't recognize the pattern,\neven if the exact question was in an example in the dataset. As another example, here's Python training data for an application that\ngenerates the next number in a sequence: training_data = [ { \"text_input\" : \"1\" , \"output\" : \"2\" }, { \"text_input\" : \"3\" , \"output\" : \"4\" }, { \"text_input\" : \"-3\" , \"output\" : \"-2\" }, { \"text_input\" : \"twenty two\" , \"output\" : \"twenty three\" }, { \"text_input\" : \"two hundred\" , \"output\" : \"two hundred one\" }, { \"text_input\" : \"ninety nine\" , \"output\" : \"one hundred\" }, { \"text_input\" : \"8\" , \"output\" : \"9\" }, { \"text_input\" : \"-98\" , \"output\" : \"-97\" }, { \"text_input\" : \"1,000\" , \"output\" : \"1,001\" }, { \"text_input\" : \"10,100,000\" , \"output\" : \"10,100,001\" }, { \"text_input\" : \"thirteen\" , \"output\" : \"fourteen\" }, { \"text_input\" : \"eighty\" , \"output\" : \"eighty one\" }, { \"text_input\" : \"one\" , \"output\" : \"two\" }, { \"text_input\" : \"three\" , \"output\" : \"four\" }, { \"text_input\" : \"seven\" , \"output\" : \"eight\" }, ] Adding a prompt or preamble to each example in your dataset can also help\nimprove the performance of the tuned model. Note, if a prompt or preamble is\nincluded in your dataset, it should also be included in the prompt to the tuned\nmodel at inference time. Limitations Note: Fine-tuning datasets for Gemini 1.5 Flash have the following\nlimitations: The maximum input size per example is 40,000 characters. The maximum output size per example is 5,000 characters. Training data size You can fine-tune a model with as little as 20 examples. Additional data\ngenerally improves the quality of the responses. You should target between 100\nand 500 examples, depending on your application. The following table shows\nrecommended dataset sizes for fine-tuning a text model for various common tasks: Task No. of examples in dataset Classification 100+ Summarization 100-500+ Document search 100+ Upload your tuning dataset Data is either passed inline using the API or through files uploaded in Google\nAI Studio. To use the client library, provide the data file in the createTunedModel call.\nFile size limit is 4 MB. See the fine-tuning quickstart with Python to get started. To call the REST API using cURL, provide training examples in JSON format to the training_data argument. See the tuning quickstart with cURL to get started. Advanced tuning settings When creating a tuning job, you can specify the following advanced settings: Epochs: A full training pass over the entire training set such that each\nexample has been processed once. Batch size: The set of examples used in one training iteration . The\nbatch size determines the number of examples in a batch. Learning rate: A floating-point number that tells the algorithm how\nstrongly to adjust the model parameters on each iteration. For example, a\nlearning rate of 0.3 would adjust weights and biases three times more\npowerfully than a learning rate of 0.1. High and low learning rates have\ntheir own unique trade-offs and should be adjusted based on your use case. Learning rate multiplier: The rate multiplier modifies the model's\noriginal learning rate. A value of 1 uses the original learning rate of the\nmodel. Values greater than 1 increase the learning rate and values between 1\nand 0 lower the learning rate. Recommended configurations The following table shows the recommended configurations for fine-tuning a\nfoundation model: Hyperparameter Default value Recommended adjustments Epoch 5 If the loss starts to plateau before 5 epochs, use a smaller value. If the loss is converging and doesn't seem to plateau, use a higher value. Batch size 4 Learning rate 0.001 Use a smaller value for smaller datasets. The loss curve shows how much the model's prediction deviates from the ideal\npredictions in the training examples after each epoch. Ideally you want to stop\ntraining at the lowest point in the curve right before it plateaus. For example,\nthe graph below shows the loss curve plateauing at about epoch 4-6 which means\nyou can set the Epoch parameter to 4 and still get the same performance. Check the tuning job status You can check the status of your tuning job in Google AI Studio under the My Library tab or using the metadata property of the tuned model in the\nGemini API. Troubleshoot errors This section includes tips on how to resolve errors you may encounter while\ncreating your tuned model. Authentication Note: Starting September 30, 2024, OAuth authentication is no longer required.\nNew projects should use API key authentication instead. Tuning using the API and client library requires authentication. You can\nset up authentication using either an API key (recommended) or using OAuth\ncredentials. For documentation on setting up an API key, see Set up API key . If you see a 'PermissionDenied: 403 Request had insufficient authentication\nscopes' error, you may need to set up user authentication using OAuth\ncredentials. To configure OAuth credentials for Python, visit our the OAuth setup tutorial . Canceled models You can cancel a fine-tuning job any time before the job is finished. However,\nthe inference performance of a canceled model is unpredictable, particularly if\nthe tuning job is canceled early in the training. If you canceled because you\nwant to stop the training at an earlier epoch, you should create a new tuning\njob and set the epoch to a lower value. Limitations of tuned models Note: Tuned models have the following limitations: The input limit of a tuned Gemini 1.5 Flash model is 40,000 characters. JSON mode is not supported with tuned models. Only text input is supported. What's next Get started with the fine-tuning tutorials: Fine-tuning tutorial (Python) Fine-tuning tutorial (REST) Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-09-28 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how fine-tuning works in the Gemini ecosystem",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python",
                "https://ai.google.dev/gemini-api/docs/model-tuning"
            ],
            "timestamp": "2024-12-27T22:47:20.079004",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
            "title": "Fine-tuning tutorial  |  Gemini API  |  Google AI for Developers",
            "text_content": "Fine-tuning tutorial  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Fine-tuning tutorial Python REST This tutorial will help you get started with the Gemini API tuning service\nusing either the Python SDK or the REST API using curl . The examples show how to tune the text model behind\nthe Gemini API text generation service. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-10-02 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how fine-tune a Gemini model with the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial"
            ],
            "timestamp": "2024-12-27T22:47:21.280272",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/embeddings",
            "title": "Embeddings in the Gemini API  |  Google AI for Developers",
            "text_content": "Embeddings in the Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Embeddings in the Gemini API The text-embedding-004 model generates state-of-the-art embeddings\nfor words, phrases, and sentences. The resulting embeddings can then be used for\ntasks such as semantic search, text classification, and clustering, among many\nothers. For more information on embeddings, read our research paper . What are embeddings? Embeddings capture semantic meaning and context, which results in text with\nsimilar meanings having \"closer\" embeddings. For example, the sentence \"I took my\ndog to the vet\" and \"I took my cat to the vet\" would have embeddings that are\nclose to each other in the vector space since they both describe a similar\ncontext. You can use embeddings to compare different texts and understand how they\nrelate. For example, if the embeddings of the text \"cat\" and \"dog\" are close\ntogether you can infer that these words are similar in meaning, context, or\nboth. This enables a variety of common AI use cases . Generate embeddings Use the embedContent method to generate text embeddings: Python import google.generativeai as genai import os genai . configure ( api_key = os . environ [ \"GEMINI_API_KEY\" ]) result = genai . embed_content ( model = \"models/text-embedding-004\" , content = \"What is the meaning of life?\" ) print ( str ( result [ 'embedding' ])) Node.js const { GoogleGenerativeAI } = require ( \"@google/generative-ai\" ); const genAI = new GoogleGenerativeAI ( process . env . GEMINI_API_KEY ); const model = genAI . getGenerativeModel ({ model : \"text-embedding-004\" }); async function run () { const result = await model . embedContent ( \"What is the meaning of life?\" ); console . log ( result . embedding . values ); } run (); curl curl \"https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:embedContent?key=$GEMINI_API_KEY\" \\ -H 'Content-Type: application/json' \\ -d '{\"model\": \"models/text-embedding-004\", \"content\": { \"parts\":[{ \"text\": \"What is the meaning of life?\"}]} }' Go ctx := context . Background () client , err := genai . NewClient ( ctx , option . WithAPIKey ( os . Getenv ( \"GEMINI_API_KEY\" ))) if err != nil { log . Fatal ( err ) } defer client . Close () em := client . EmbeddingModel ( \"text-embedding-004\" ) res , err := em . EmbedContent ( ctx , genai . Text ( \"What is the meaning of life?\" )) if err != nil { panic ( err ) } fmt . Println ( res . Embedding . Values ) Use cases Text embeddings are used in a variety of common AI use cases, such as: Information retrieval: You can use embeddings to retrieve semantically\nsimilar text given a piece of input text. Document search tutorial task Clustering: Comparing groups of embeddings can help identify hidden trends. Embedding clustering tutorial bubble_chart Vector database: As you take different embedding use cases to production,\nit is common to store embeddings in a vector database. Vector database tutorial bolt Classification: You can train a model using embeddings to classify\ndocuments into categories. Classification tutorial token Gemini embeddings models The Gemini API offers two models that generate text embeddings: Text Embeddings Embeddings Text Embeddings is an updated version of the Embedding model that offers elastic\nembedding sizes under 768 dimensions. Elastic embeddings generate smaller output\ndimensions and potentially save computing and storage costs with minor\nperformance loss. Use Text Embeddings for new projects or applications. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-19 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/embeddings"
            ],
            "timestamp": "2024-12-27T22:47:22.136514",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/caching",
            "title": "Context caching  |  Gemini API  |  Google AI for Developers",
            "text_content": "Context caching  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Context caching Python Node.js Go REST In a typical AI workflow, you might pass the same input tokens over and over to\na model. Using the Gemini API context caching feature, you can pass some content\nto the model once, cache the input tokens, and then refer to the cached tokens\nfor subsequent requests. At certain volumes, using cached tokens is lower cost\nthan passing in the same corpus of tokens repeatedly. When you cache a set of tokens, you can choose how long you want the cache to\nexist before the tokens are automatically deleted. This caching duration is\ncalled the time to live (TTL). If not set, the TTL defaults to 1 hour. The\ncost for caching depends on the input token size and how long you want the\ntokens to persist. Context caching supports both Gemini 1.5 Pro and Gemini 1.5 Flash. Note: Context caching is only available for stable models with fixed versions\n(for example, gemini-1.5-pro-001 ). You must include the version postfix (for\nexample, the -001 in gemini-1.5-pro-001 ). When to use context caching Context caching is particularly well suited to scenarios where a substantial\ninitial context is referenced repeatedly by shorter requests. Consider using\ncontext caching for use cases such as: Chatbots with extensive system instructions Repetitive analysis of lengthy video files Recurring queries against large document sets Frequent code repository analysis or bug fixing How caching reduces costs Context caching is a paid feature designed to reduce overall operational costs.\nBilling is based on the following factors: Cache token count: The number of input tokens cached, billed at a\nreduced rate when included in subsequent prompts. Storage duration: The amount of time cached tokens are stored (TTL),\nbilled based on the TTL duration of cached token count. There are no minimum\nor maximum bounds on the TTL. Other factors: Other charges apply, such as for non-cached input tokens\nand output tokens. For up-to-date pricing details, refer to the Gemini API pricing\npage . To learn how to count tokens, see the Token\nguide . How to use context caching This section assumes that you've installed a Gemini SDK (or have curl installed)\nand that you've configured an API key, as shown in the quickstart . Additional considerations Keep the following considerations in mind when using context caching: The minimum input token count for context caching is 32,768, and the maximum is the same as the maximum for the given model. (For more on\ncounting tokens, see the Token guide ). The model doesn't make any distinction between cached tokens and regular\ninput tokens. Cached content is simply a prefix to the prompt. There are no special rate or usage limits on context caching; the standard\nrate limits for GenerateContent apply, and token limits include cached\ntokens. The number of cached tokens is returned in the usage_metadata from the\ncreate, get, and list operations of the cache service, and also in GenerateContent when using the cache. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-10-16 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "Learn how to use Context Caching in the Gemini API",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/caching"
            ],
            "timestamp": "2024-12-27T22:47:22.880673",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/imagen",
            "title": "Generate images using Imagen 3  |  Gemini API  |  Google AI for Developers",
            "text_content": "Generate images using Imagen 3  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Generate images using Imagen 3 Preview: Imagen 3 in the Gemini API is available as an early access release\nin private preview. It's not yet generally available. The Gemini API provides access to Imagen 3 , Google's highest\nquality text-to-image model, featuring a number of\nnew and improved capabilities. Imagen 3 can do the following: Generate images with better detail, richer lighting, and fewer distracting\nartifacts than previous models Understand prompts written in natural Generate images in a wide range of formats and styles Render text more effectively than previous models To learn more and see example output, see the Google DeepMind Imagen 3 overview . Before you begin: Set up your project and API key Important: The stable version of Python SDK for the Gemini API does not contain\nImagen support. Instead of installing the google-generativeai package\nfrom pypi you need to install it from the imagen GitHub branch: pip install -U git+https://github.com/google-gemini/generative-ai-python@imagen Before calling the Gemini API, you need to set up your project and configure\nyour API key. Expand to view how to set up your project and API key Tip: For complete setup instructions, see the Gemini API quickstart . Get and secure your API key You need an API key to call the Gemini API. If you don't already have one,\ncreate a key in Google AI Studio. Get an API key It's strongly recommended that you do not check an API key into your version\ncontrol system. You should store your API key in a secrets store such as Google Cloud Secret Manager . This tutorial assumes that you're accessing your API key as an environment\nvariable. Install the SDK package and configure your API key Note: This section shows setup steps for a local Python environment. To install\n      dependencies and configure your API key for Colab, see the Authentication quickstart notebook Important: The stable version of Python SDK for the Gemini API does not contain\nImagen support. Instead of installing the google-generativeai package\nfrom pypi you need to install it from the imagen GitHub branch. Install the dependency using pip: pip install -U git+https://github.com/google-gemini/generative-ai-python@imagen Import the package and configure the service with your API key: import os import google.generativeai as genai genai . configure ( api_key = os . environ [ 'API_KEY' ]) Generate images This section shows you how to instantiate an Imagen model and generate images. Note: A non-visible digital SynthID watermark is always\nadded to generated images. To run the example code, you must first install Pillow : pip install --upgrade Pillow Then, with Pillow and the Python SDK installed, you can use the following code\nto generate images: import os import google.generativeai as genai genai . configure ( api_key = os . environ [ 'API_KEY' ]) imagen = genai . ImageGenerationModel ( \"imagen-3.0-generate-001\" ) result = imagen . generate_images ( prompt = \"Fuzzy bunnies in my kitchen\" , number_of_images = 4 , safety_filter_level = \"block_only_high\" , person_generation = \"allow_adult\" , aspect_ratio = \"3:4\" , negative_prompt = \"Outside\" , ) for image in result . images : print ( image ) # Open and display the image using your local operating system. for image in result . images : image . _pil_image . show () The notebook should display four images similar to this one: Imagen model parameters The following parameters are available for generate_images() : prompt : The text prompt for the image. negative_prompt : A description of what you want to omit in the generated\nimages. Defaults to none. For example, consider the prompt \"a rainy city street at night with no\npeople\". The model might interpret \"people\" as a directive of what to include\ninstead of omit. To generate better results, you could use the prompt \"a rainy\ncity street at night\" with a negative prompt \"people\". number_of_images : The number of images to generate, from 1 to 4 (inclusive).\nThe default is 4. aspect_ratio : Changes the aspect ratio of the generated image. Supported\nvalues are \"1:1\" , \"3:4\" , \"4:3\" , \"9:16\" , and \"16:9\" . The default is \"1:1\" . safety_filter_level : Adds a filter level to safety filtering. The following\nvalues are valid: \"block_low_and_above\" : Block when the probability score or the severity\nscore is LOW , MEDIUM , or HIGH . \"block_medium_and_above\" : Block when the probability score or the severity\nscore is MEDIUM or HIGH . \"block_only_high\" : Block when the probability score or the severity score\nis HIGH . person_generation : Allow the model to generate images of people. The\nfollowing values are supported: \"dont_allow\" : Block generation of images of people. \"allow_adult\" : Generate images of adults, but not children. Text prompt language The following input text prompt languages are supported: Chinese (simplified) ( zh / zh-CN ) Chinese (traditional) ( zh-TW ) English ( en ) Hindi ( hi ) Japanese ( ja ) Korean ( ko ) Portuguese ( pt ) Spanish ( es ) What's next Imagen 3 in Gemini API is in early access. Stay tuned for announcements about\nthe status of the feature. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-27 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/imagen"
            ],
            "timestamp": "2024-12-27T22:47:23.549861",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/prompting-intro",
            "title": "Introduction to prompt design  |  Gemini API  |  Google AI for Developers",
            "text_content": "Introduction to prompt design  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Introduction to prompt design Prompt design is the process of creating prompts that elicit the desired\nresponse from language models. Writing well structured prompts is an essential\npart of ensuring accurate, high quality responses from a language model. This\npage introduces some basic concepts, strategies, and best practices to get you\nstarted in designing prompts. What is a prompt A prompt is a natural language request submitted to a language model to receive\na response back. Prompts can contain questions, instructions, contextual\ninformation, examples, and partial input for the model to complete or continue.\nAfter the model receives a prompt, depending on the type of model being used, it\ncan generate text, embeddings, code, images, videos, music, and more. Prompt content types Prompts can include one or more of the following types of content: Input (required) Context (optional) Examples (optional) Input An input is the text in the prompt that you want the model to provide a response\nfor, and it's a required content type. Inputs can be a question that the model\nanswers (question input), a task the model performs (task input), an entity the\nmodel operates on (entity input), or partial input that the model completes or\ncontinues (completion input). Question input A question input is a question that you ask the model that the\nmodel provides an answer to. Prompt: What's a good name for a flower shop that specializes in selling bouquets of\ndried flowers? Create a list of 10 options with just the names. Response: Here are 10 names for a flower shop specializing in dried flowers:\n\n    1.  Everlasting Blooms\n    2.  Dried & Delightful\n    3.  The Petal Preserve\n    4.  Whispers of Wildflowers\n    5.  Forever Blooms\n    6.  The Botanical Muse\n    7.  Timeless Blooms\n    8.  Dried Flower Co.\n    9.  Bloom & Co.\n    10.  The Sun-Kissed Petals (gemini-1.5-flash) Task input A task input is a task that you want the model to perform. For\nexample, you can tell the model to give you ideas or suggestions for something. Prompt: Give me a simple list of just the things that I must bring on a camping trip. The list should have 10 items. Response: Here's a list of 10 essential items for a camping trip:\n\n    1. **Tent:** Your shelter from the elements.\n    2. **Sleeping Bag:** Stay warm and comfortable.\n    3. **Sleeping Pad:** Insulation and cushioning for a good night's sleep.\n    4. **Headlamp or Flashlight:** Light for nighttime activities.\n    5. **First Aid Kit:** Be prepared for minor injuries.\n    6. **Water Bottle/Hydration Pack:** Stay hydrated.\n    7. **Food and Snacks:** Fuel for your adventures.\n    8. **Cooking Gear:** Camp stove, fuel, utensils, etc.\n    9. **Knife and Multitool:** Essential for various tasks.\n    10. **Camp Chair:** Enjoy your campfire and meals comfortably. (gemini-1.5-flash) Entity input An entity input is what the model performs an action on, such as\nclassify or summarize. This type of input can benefit from the inclusion of\ninstructions. Prompt: Classify the following items as [large, small].\nElephant\nMouse\nSnail Response: Here's the classification:\n\n    * **Elephant:** Large\n    * **Mouse:** Small\n    * **Snail:** Small (gemini-1.5-flash) Completion input A completion input is text that the model is expected to complete or\ncontinue. Prompt: Complete the following prompt: Some simple strategies for overcoming writer's block include... Response: Some simple strategies for overcoming writer's block include:\n\n    * **Freewriting:** Write non-stop for a set amount of time (5-10 minutes) without worrying about grammar, spelling, or even making sense. Just keep your pen moving and let your thoughts flow freely. This can help you loosen up and access ideas you might not have considered otherwise.\n    * **Brainstorming:**  Make a list of everything that comes to mind related to your topic, no matter how silly or unrelated it seems. You can also use mind mapping or other visual brainstorming techniques.\n    * **Changing your environment:**  Sometimes a change of scenery can be all you need to get the creative juices flowing. Go for a walk, work in a coffee shop, or even just move to a different room in your house. \n    * **Reading:**  Immerse yourself in the work of other writers, especially those who inspire you. Pay attention to their style, their structure, and their use of language. \n    * **Taking a break:** Sometimes the best way to overcome writer's block is to simply step away from the project for a while. Go for a walk, watch a movie, listen to music, or do something else that you enjoy. You might be surprised at how much clearer your head feels when you return to your writing. \n    * **Focusing on a single element:**  Instead of tackling the entire project, focus on a single sentence, paragraph, or scene. Once you get something down on paper, it can be easier to build from there.\n    * **Starting with the ending:**  If you're stuck on the beginning, try writing the ending first. This can help you to clarify your overall vision for the project and provide a framework for your ideas.\n    * **Talking it out:** Share your ideas and challenges with a friend, a writing group, or a writing coach. Discussing your project with someone else can help you to see it from a new perspective and find solutions to your writing problems.\n    \n    Remember, writer's block is a common experience, and it's not a sign that you're a bad writer. Just be patient with yourself, try some of these strategies, and keep writing! (gemini-1.5-flash) Context Context can be one of the following: Instructions that specify how the model should behave. Information that the model uses or references to generate a response. Add contextual information in your prompt when you need to give information to\nthe model, or restrict the boundaries of the responses to only what's within the\nprompt. Prompt: Marbles:\nColor: red\nNumber: 12\nColor: blue\nNumber: 28\nColor: yellow\nNumber: 15\nColor: green\nNumber: 17\n\nHow many green marbles are there? Response: There are 17 green marbles. (gemini-1.5-flash) Examples Examples are input-output pairs that you include in the prompt to give the\nmodel an example of an ideal response. Including examples in\nthe prompt is an effective strategy for customizing the response format. Prompt: Classify the following.\nOptions:\n- red wine\n- white wine\n\nText: Chardonnay\nThe answer is: white wine\nText: Cabernet\nThe answer is: red wine\nText: Moscato\nThe answer is: white wine\n\nText: Riesling\nThe answer is: Response: The answer is: white wine (gemini-1.5-flash) Next steps Now that you have an understanding of prompt design, try writing your\nown prompts using Google AI Studio . For a deeper understanding of prompt design, see the prompt strategies topic. To learn about multimodal prompting, see Prompting with media files . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-09-23 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs/prompting-intro"
            ],
            "timestamp": "2024-12-27T22:47:24.449989",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/prompting-strategies",
            "title": "Prompt design strategies  |  Gemini API  |  Google AI for Developers",
            "text_content": "Prompt design strategies  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Prompt design strategies This page introduces you to some general prompt design strategies that you can employ when\ndesigning prompts. Large language models (LLM) are trained on vast amounts of text data to learn the patterns and\nrelationships between language. When given some text (the prompt), language models can\npredict what is likely to come next, like a sophisticated autocompletion tool. Google AI Studio hosts a simple prompt gallery meant to interactively showcase many of the concepts shared in this guide.\n\nIn the rest of this guide, you'll learn about the following prompt design strategies: Give clear and specific instructions Include few-shot examples Add contextual information Add prefixes Let the model complete partial input Give clear and specific instructions Giving the model instructions on what to do is an effective and efficient way to customize model\nbehavior. Ensure that the instructions you give are clear and specific. Instructions can be as\nsimple as a list of step-by-step instructions or as complex as mapping out a user's experience and\nmindset. Define the task to perform Describe in detail the task that you want the model to perform. The task can be as simple as a\nstep-by-step list of instructions or as complex as mapping out a user's experience and mindset. The\nfollowing prompt tells the model to summarize a block of text: Prompt: Summarize this text: Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially\nfaster than any modern traditional computer. At very tiny scales, physical matter acts as both\nparticles and as waves, and quantum computing uses specialized hardware to leverage this behavior.\nThe operating principles of quantum devices is beyond the scope of classical physics. When deployed\nat scale, quantum computers could be used in a wide variety of applications such as: in\ncybersecurity to break existing encryption methods while helping researchers create new ones, in\nmeteorology to develop better weather forecasting etc. However, the current state of the art quantum\ncomputers are still largely experimental and impractical. Response: Quantum computers leverage the wave-particle duality of matter at tiny scales to perform\n    calculations exponentially faster than traditional computers. They harness quantum phenomena,\n    which are beyond the scope of classical physics, to achieve this speed advantage. While\n    promising for applications like cybersecurity, weather forecasting, and beyond, current\n    quantum computers are still experimental and not yet practical for widespread use. (gemini-1.5-flash) Specify any constraints Specify any constraints on reading the prompt or generating a response. You can tell the model\nwhat to do and not to do. From the previous example, the summarization is shorter than the original\ntext, but maybe you want the summary to be shorter. In this case, you can specify a constraint in\nthe prompt on how long you want the summary to be: Prompt: Summarize this text in two sentences: Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially\nfaster than any modern traditional computer. At very tiny scales, physical matter acts as both\nparticles and as waves, and quantum computing uses specialized hardware to leverage this behavior.\nThe operating principles of quantum devices is beyond the scope of classical physics. When deployed\nat scale, quantum computers could be used in a wide variety of applications such as: in\ncybersecurity to break existing encryption methods while helping researchers create new ones, in\nmeteorology to develop better weather forecasting etc. However, the current state of the art quantum\ncomputers are still largely experimental and impractical. Response: Quantum computers harness the wave-particle duality of matter at the quantum level to perform\n      computations exponentially faster than traditional computers. While these machines hold\n      immense potential for fields like cybersecurity and weather forecasting, they are currently\n      in their early stages of development and not yet practical for widespread use. (gemini-1.5-flash) Define the format of the response You can also give instructions that specify the format of the response. For example, you can ask\nfor the response to be formatted as a table, bulleted list, elevator pitch, keywords, sentence, or\nparagraph. The following system message instructs the model to be more conversational in its response: System message All questions should be answered comprehensively with details, unless the user requests a\n    concise response specifically. Respond in the same language as the query. Prompt What is a smart way to create a lemonade business? Response: Launching a Successful Lemonade Stand: A Smart Approach\nCreating a lemonade business, whether a simple stand or a larger operation, requires strategic thinking. Here's a breakdown of how to approach it smartly:\n\n1. Define Your Business:\nScope: Are you aiming for a small, seasonal lemonade stand, a food truck, a delivery service, or something else entirely?\nTarget Audience: Who are you selling to? Families, office workers, tourists?\nUnique Selling Proposition: What sets your lemonade apart? Special flavors, organic ingredients, a unique recipe?\n\nPricing: Research competitor prices and determine a profitable pricing structure.\nLocation: Consider foot traffic, accessibility, and regulations for your chosen location.\n..... (gemini-1.5-flash) Summary Give the models instructions on what to do. Make the instructions clear and specific. Specify any constraints or formatting requirements for the output. Include few-shot examples You can include examples in the prompt that show the model what getting it right looks like. The\nmodel attempts to identify patterns and relationships from the examples and applies them when\ngenerating a response. Prompts that contain a few examples are called few-shot prompts,\nwhile prompts that provide no examples are called zero-shot prompts . Few-shot prompts are\noften used to regulate the formatting, phrasing, scoping, or general patterning of model responses.\nUse specific and varied examples to help the model narrow its focus and generate more accurate\nresults. We recommend to always include few-shot examples in your prompts. Prompts without few-shot\nexamples are likely to be less effective. In fact, you can remove instructions from your prompt\nif your examples are clear enough in showing the task at hand. Zero-shot vs few-shot prompts The following zero-shot prompt asks the model to choose the best explanation. Prompt: Please choose the best explanation to the question:\n\nQuestion: How is snow formed?\nExplanation1: Snow is formed when water vapor in the air freezes into ice crystals in the\natmosphere, which can combine and grow into snowflakes as they fall through the atmosphere and\naccumulate on the ground.\nExplanation2: Water vapor freezes into ice crystals forming snow.\nAnswer: Response: Explanation 1 is the best explanation. (gemini-1.5-flash) If your use case requires the model to produce concise responses, you can include examples in the\nprompt that give preference to concise responses. The following prompt provides two examples that show preference to the shorter explanations. In\nthe response, you can see that the examples guided the model to choose the shorter explanation\n(Explanation2) as opposed to the longer explanation (Explanation1) like it did previously. Prompt: Below are some examples showing a question, explanation, and answer format:\n\nQuestion: Why is sky blue?\nExplanation1: The sky appears blue because of Rayleigh scattering, which causes shorter blue\nwavelengths of light to be scattered more easily than longer red wavelengths, making the sky look\nblue.\nExplanation2: Due to Rayleigh scattering effect.\nAnswer: Explanation2\n\nQuestion: What is the cause of earthquakes?\nExplanation1: Sudden release of energy in the Earth's crust.\nExplanation2: Earthquakes happen when tectonic plates suddenly slip or break apart, causing a\nrelease of energy that creates seismic waves that can shake the ground and cause damage.\nAnswer: Explanation1\n\nNow, Answer the following question given the example formats above:\n\nQuestion: How is snow formed?\nExplanation1: Snow is formed when water vapor in the air freezes into ice crystals in the\natmosphere, which can combine and grow into snowflakes as they fall through the atmosphere and\naccumulate on the ground.\nExplanation2: Water vapor freezes into ice crystals forming snow.\nAnswer: Response: Answer: Explanation2 (gemini-1.5-flash) Find the optimal number of examples You can experiment with the number of examples to provide in the prompt for the most desired\nresults. Models like Gemini can often pick up on patterns using a few examples, though you\nmay need to experiment with what number of examples lead to the desired results. At the same time,\nif you include too many examples, the model may start to overfit the response to the examples. Use examples to show patterns instead of antipatterns Using examples to show the model a pattern to follow is more effective than using examples to\nshow the model an antipattern to avoid. ⛔ Negative pattern : Example Don't end haikus with a question:\nHaiku are fun\nA short and simple poem\nDon't you enjoy them? ✅ Positive pattern : Example Always end haikus with an assertion:\nHaiku are fun\nA short and simple poem\nA joy to write Use consistent formatting across examples Make sure that the structure and formatting of few-shot examples are the same to avoid responses\nwith undesired formats. One of the primary objectives of adding few-shot examples in prompts is to\nshow the model the response format. Therefore, it is essential to ensure a consistent format across\nall examples, especially paying attention to XML tags, white spaces, newlines, and example\nsplitters. Summary Including prompt-response examples in the prompt helps the model learn how to respond. Give the model examples of the patterns to follow instead of examples of patterns to\n  avoid. Experiment with the number of prompts to include. Depending on the model, too few examples are\n  ineffective at changing model behavior. Too many examples cause the model to overfit. Use consistent formatting across examples Add contextual information You can include in the prompt instructions and information that the model needs to solve a\nproblem instead of assuming that the model has all of the required information. This contextual\ninformation helps the model understand the constraints and details of what you're asking for it to\ndo. The following example asks the model to give troubleshooting guidance for a router: Prompt: What should I do to fix my disconnected wifi? The light on my Google Wifi router is yellow and\nblinking slowly. Response: A yellow blinking light on your Google Wifi router usually indicates a problem with the internet connection. Here are some steps to troubleshoot and fix the issue:\n  1. Restart your router and modem:\n    - Unplug your modem from the power outlet and wait for 30 seconds.\n    - Plug your modem back in and wait for it to fully boot up.\n    - Unplug your Google Wifi router and wait for 30 seconds.\n    - Plug your Google Wifi router back in and wait for it to fully boot up.\n..... (gemini-1.5-flash) The response looks like generic troubleshooting information that's not specific to the router or\nthe status of the LED indicator lights. To customize the response for the specific router, you can add to the prompt the router's\ntroubleshooting guide as context for it to refer to when providing a response. Prompt: Answer the question using the text below. Respond with only the text provided.\nQuestion: What should I do to fix my disconnected wifi? The light on my Google Wifi router is yellow and blinking slowly.\n\nText:\nColor: Slowly pulsing yellow\nWhat it means: There is a network error.\nWhat to do:\nCheck that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.\n\nColor: Fast blinking yellow\nWhat it means: You are holding down the reset button and are factory resetting this device.\nWhat to do:\nIf you keep holding down the reset button, after about 12 seconds, the light will turn solid yellow. Once it is solid yellow, let go of the factory reset button.\n\nColor: Solid yellow\nWhat it means: Router is factory resetting.\nWhat to do:\nThis can take up to 10 minutes. When it's done, the device will reset itself and start pulsing white, letting you know it's ready for setup.\n\nColor: Solid red\nWhat it means: Something is wrong.\nWhat to do:\nCritical failure. Factory reset the router. If the light stays red, contact Wifi customer support. Response: Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again. (gemini-1.5-flash) Summary Include information (context) in the prompt that you want the model to use when generating a\n  response. Give the model instructions on how to use the contextual information. Add prefixes A prefix is a word or phrase that you add to the prompt content that can serve several purposes,\ndepending on where you put the prefix: Input prefix: Adding a prefix to the input signals semantically meaningful\n  parts of the input to the model. For example, the prefixes \"English:\" and \"French:\" demarcate two\n  different languages. Output prefix: Even though the output is generated by the model, you can add\n  a prefix for the output in the prompt. The output prefix gives the model information about what's\n  expected as a response. For example, the output prefix \"JSON:\" signals to the model that the\n  output should be in JSON format. Example prefix: In few-shot prompts, adding prefixes to the examples provides\n  labels that the model can use when generating the output, which makes it easier to parse output\n  content. In the following example, \"Text:\" is the input prefix and \"The answer is:\" is the output\nprefix. Prompt: Classify the text as one of the following categories.\n- large\n- small\nText: Rhino\nThe answer is: large\nText: Mouse\nThe answer is: small\nText: Snail\nThe answer is: small\nText: Elephant\nThe answer is: Response: The answer is: large (gemini-1.5-flash) Let the model complete partial input Generative language models work like an advanced autocompletion tool. When you provide partial\ncontent, the model can provide the rest of the content or what it thinks is a continuation of that\ncontent as a response. When doing so, if you include any examples or context, the model can take\nthose examples or context into account. The following example provides a prompt with an instruction and an entity input: Prompt: For the given order, return a JSON object that has the fields cheeseburger, hamburger, fries, or\ndrink, with the value being the quantity.\n\nOrder: A burger and a drink. Response: ```\n{\n  \"hamburger\": 1,\n  \"drink\": 1\n}\n``` (gemini-1.5-flash) While the model did as prompted, writing out the instructions in natural language can sometimes\nbe challenging and it leaves a lot to the model's interpretation. For example, a restaurants menu\nmight contain many items. To reduce the size of the JSON response, you probably want to omit the items that\nweren't ordered. In this case, you can give an example and a response prefix and let the model\ncomplete it: Prompt: Valid fields are cheeseburger, hamburger, fries, and drink.\nOrder: Give me a cheeseburger and fries\nOutput:\n```\n{\n  \"cheeseburger\": 1,\n  \"fries\": 1\n}\n```\nOrder: I want two burgers, a drink, and fries.\nOutput: Response: ```\n{\n  \"hamburger\": 2,\n  \"drink\": 1,\n  \"fries\": 1\n}\n``` (gemini-1.5-flash) Notice how \"cheeseburger\" was excluded from the output because it wasn't a part of the order. Prompt the model to format its response The completion strategy can also help format the response. The following example prompts the\nmodel to create an essay outline: Prompt: Create an outline for an essay about hummingbirds. Response: Hummingbirds: A Symphony of Speed and Color\n    I. Introduction\n    A. Hummingbirds are fascinating creatures known for their dazzling colors, lightning-fast flight, and unique feeding habits.\n    B. Brief overview of the diversity of hummingbird species and their geographic distribution.\n    C. Thesis statement: Hummingbirds are an embodiment of resilience, adaptation, and the wonders of the natural world.\n..... (gemini-1.5-flash) The prompt didn't specify the format for the outline and the model chose a format for you. To get\nthe model to return an outline in a specific format, you can add text that represents the start of\nthe outline and let the model complete it based on the pattern that you initiated. Prompt: Create an outline for an essay about hummingbirds.\nI. Introduction\n   * Response: Hummingbirds: Jewels of the Air\n    **I. Introduction**\n       * **Hook:** Begin with a captivating image or anecdote about hummingbirds, emphasizing their vibrant colors, rapid flight, and unique feeding habits.\n       * **Background:** Briefly introduce hummingbirds as the smallest birds in the world and their fascinating adaptations for a nectar-rich diet.\n       * **Thesis:** State the main argument or focus of the essay. Examples: \n            * **The incredible adaptations of hummingbirds enable them to thrive in diverse ecosystems.**\n            * **Hummingbirds play a crucial role in pollination and biodiversity.**\n            * **The beauty and grace of hummingbirds make them captivating subjects for both scientific study and artistic appreciation.** (gemini-1.5-flash) Summary If you give the model a partial input, the model completes that input based on any available\n  examples or context in the prompt. Having the model complete an input may sometimes be easier than describing the task in natural\n  language. Adding a partial answer to a prompt can guide the model to follow a desired pattern or\n  format. Break down prompts into simple components For use cases that require complex prompts, you can help the model manage this complexity by\nbreaking things down into simpler components. Break down instructions Instead of having many instructions in one prompt, create one prompt per instruction. You can\nchoose which prompt to process based on the user's input. Chain prompts For complex tasks that involve multiple sequential steps, make each step a prompt and chain the\nprompts together in a sequence. In this sequential chain of prompts, the output of one prompt in the\nsequence becomes the input of the next prompt. The output of the last prompt in the sequence is the\nfinal output. Aggregate responses Aggregation is when you want to perform different parallel tasks on different portions of the\ndata and aggregate the results to produce the final output. For example, you can tell the model to\nperform one operation on the first part of the data, perform another operation on the rest of the\ndata and aggregate the results. Summary Break down complex instructions into a prompt for each instruction and decide which prompt to\n    apply based on the user's input. Break down multiple sequential steps into separate prompts and chain them such that the output\n    on the preceding prompt becomes the input of the following prompt. Break down parallel tasks and aggregate the responses to produce the final output. Experiment with different parameter values Each call that you send to a model includes parameter values that control how the model generates\na response. The model can generate different results for different parameter values. Experiment with\ndifferent parameter values to get the best values for the task. The parameters available for\ndifferent models may differ. The most common parameters are the following: Max output tokens Temperature Top-K Top-P Max output tokens Maximum number of tokens that can be generated in the response. A token is\napproximately four characters. 100 tokens correspond to roughly 20 words. Specify a lower value for shorter responses and a higher value for longer\nresponses. Temperature The temperature is used for sampling during response generation, which occurs\nwhen topP and topK are applied. Temperature controls\nthe degree of randomness in token selection. Lower temperatures are good for\nprompts that require a more deterministic and less open-ended or creative\nresponse, while higher temperatures can lead to more diverse or creative\nresults. A temperature of 0 is deterministic, meaning that the\nhighest probability response is always selected. For most use cases, try starting with a temperature of 0.2 . If\nthe model returns a response that's too generic, too short, or the model gives a\nfallback response, try increasing the temperature. Top-K Top-K changes how the model selects tokens for output. A top-K of 1 means the next selected token is the most probable among all\ntokens in the model's vocabulary (also called greedy decoding), while a top-K of 3 means that the next token is selected from among the three most\nprobable tokens by using temperature. For each token selection step, the top-K tokens with the highest\nprobabilities are sampled. Then tokens are further filtered based on top-P with\nthe final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more\nrandom responses. The default top-K is 40 . Top-P Top-P changes how the model selects tokens for output. Tokens are selected\nfrom the most (see top-K) to least probable until the sum of their probabilities\nequals the top-P value. For example, if tokens A, B, and C have a probability of\n0.3, 0.2, and 0.1 and the top-P value is 0.5 , then the model will\nselect either A or B as the next token by using temperature and excludes C as a\ncandidate. Specify a lower value for less random responses and a higher value for more\nrandom responses. The default top-P is 0.95 . Prompt iteration strategies Prompt design is an iterative process that often requires a few iterations before you get the\ndesired response consistently. This section provides guidance on some things you can try when\niterating on your prompts. Use different phrasing Using different words or phrasing in your prompts often yields different responses from the model\neven though they all mean the same thing. If you're not getting the expected results from your\nprompt, try rephrasing it. Version 1:\nHow do I bake a pie?\n\nVersion 2:\nSuggest a recipe for a pie.\n\nVersion 3:\nWhat's a good pie recipe? Switch to an analogous task If you can't get the model to follow your instructions for a task, try giving it instructions for\nan analogous task that achieves the same result. This prompt tells the model to categorize a book by using predefined categories. Prompt: Which category does The Odyssey belong to:\nthriller\nsci-fi\nmythology\nbiography Response: The Odyssey belongs to the category of **mythology**. \n\n    Here's why:\n    \n    * **Mythology:** The Odyssey tells the story of Odysseus, a hero from Greek mythology, and his\n    journey home after the Trojan War. It features gods, monsters, and supernatural events common to\n    Greek mythology.\n..... (gemini-1.5-flash) The response is correct, but the model didn't stay within the bounds of the options. You also\nwant to model to just respond with one of the options instead of in a full sentence. In this case,\nyou can rephrase the instructions as a multiple choice question and ask the model to choose an\noption. Prompt: Multiple choice problem: Which of the following options describes the book The Odyssey?\nOptions:\n- thriller\n- sci-fi\n- mythology\n- biography Response: The correct answer is **mythology**. (gemini-1.5-flash) Change the order of prompt content The order of the content in the prompt can sometimes affect the response. Try changing the\ncontent order and see how that affects the response. Version 1:\n[examples]\n[context]\n[input]\n\nVersion 2:\n[input]\n[examples]\n[context]\n\nVersion 3:\n[examples]\n[input]\n[context] Fallback responses A fallback response is a response returned by the model when either the prompt or the response\ntriggers a safety filter. An example of a fallback response is \"I'm not able to help with that, as\nI'm only a language model.\" If the model responds with a fallback response, try increasing the temperature. Things to avoid Avoid relying on models to generate factual information. Use with care on math and logic problems. Next steps Now that you have a deeper understanding of prompt design, try writing your\nown prompts using Google AI Studio . To learn about multimodal prompting, see Prompting with media files . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-09-24 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies"
            ],
            "timestamp": "2024-12-27T22:47:25.094903",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
            "title": "File prompting strategies  |  Gemini API  |  Google AI for Developers",
            "text_content": "File prompting strategies  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback File prompting strategies The Gemini family of artificial intelligence (AI) models is built to handle\nvarious types of input data, including text, images, and audio. Since these\nmodels can handle more more than one type or mode of data, the Gemini models\nare called multimodal models or explained as having multimodal capabilities . Note: Prompting with media files is supported by specific Gemini models.\nFor more information about which models support this feature, see the Vision and Audio guides. This page provides guidance and best practices for using media files with\nprompts for the Gemini API. For information on using the Gemini Files API for\nuploading media files and including them in your prompts, see the Vision , Audio , and Document processing guides. Using media files with prompts Being able to use various types of data in your prompts gives you more\nflexibility in terms of what tasks you can tackle with the Gemini API. For\nexample, you can send the model a photo of a delicious meal and ask it to write\na short blog about the meal. Prompt Response Write a short, engaging blog post based on this picture. It should\n      include a description of the meal in the photo and talk about my journey\n      meal prepping. Meal prepping is a great way to save time and money, and it can also\n      help you to eat healthier. This meal is a great example of a healthy\n      and delicious meal that can be easily prepped ahead of time. This meal features brown rice, roasted vegetables, and chicken teriyaki.\n      The brown rice is a healthy and complex carbohydrate that will give you\n      sustained energy. The roasted vegetables are a great way to get your daily\n      dose of vitamins and minerals, and the chicken teriyaki is a delicious and\n      protein-rich option. This meal is also very easy to prepare. Simply cook the brown rice, roast\n      the vegetables, and cook the chicken teriyaki. Once everything is cooked,\n      divide it into meal prep containers and store them in the refrigerator.\n      You can then grab a container and go on busy mornings! If you're looking for a healthy and delicious meal that can be easily\n      prepped ahead of time, this meal is a great option. It's packed with\n      nutrients and flavor, and it's sure to keep you feeling full and satisfied. Here's to healthy and delicious meal prepping! Strategies for prompting with media If you are having trouble getting the output you want from prompts that use\nmedia files, there are some strategies that can help you get the results you\nwant. The following sections provide design approaches and troubleshooting\ntips for improving prompts that use multimodal input. You can improve your multimodal prompts by following these best practices: Prompt design fundamentals Be specific in your instructions : Craft clear and concise instructions that leave minimal room for misinterpretation. Add a few examples to your prompt: Use realistic few-shot examples to illustrate what you want to achieve. Break it down step-by-step : Divide complex tasks into manageable sub-goals, guiding the model through the process. Specify the output format : In your prompt, ask for the output to be in the format you want, like markdown, JSON, HTML and more. Put your image first for single-image prompts : While Gemini can handle image and text inputs in any order, for prompts containing a single image, it might perform better if that image is placed before the text prompt. Troubleshooting your multimodal prompt If the model is not drawing information from the relevant part of the image: Drop hints with which aspects of the image you want the prompt to draw information from. If the model output is too generic (not tailored enough to the image): At the start of the prompt, try asking the model to describe the image(s) before providing the task instruction, or try asking the model to refer to what's in the image. Prompt design fundamentals This section expands on the best practices listed in the previous section. Note: Model results from the following Gemini examples can vary from run to run. Be specific in your instructions Prompts have the most success when they are clear and detailed. If you have a specific output in mind, it's better to include that requirement in the prompt to ensure you get the output you want. Sometimes, a prompt's intent might seem clear to the person who wrote it, but it is actually under-specified. To customize model behavior, consider how your prompt could be (mis)interpreted, and ensure that the instructions you give are specific and clear. Remember to leave minimal room for misinterpretation For this image of an airport board, asking the model to just \"describe this image\" could generate a general description. If you need the model to parse the time and city from the image, you can include that request directly in your prompt. Prompt Model response Describe this image. The image shows an airport arrivals and departures board. Updated prompt Improved response Parse the time and city from the airport board shown in this image into a list. 10:50 Moscow 11:05 Edinburgh 11:05 London 11:10 Bucharest 11:30 Kiev 11:35 Dublin 11:45 East Midlands 12:15 Sofia 12:30 London 12:30 Newcastle 12:40 St Petersburg 12:40 London 12:45 Manchester Add a few examples The Gemini model can accept multiple inputs which it can use as examples to understand the\noutput you want. Adding these examples can help the model identify the patterns and apply the\n  relationship between the given images and responses to the new example. This is also called\n  \"few-shot\" learning. In the example below, the initial output is written in sentence form, and also contains the\ncountry (Brazil). Suppose you want the output in a different format or style, and you want\nonly the city, not the country. Adding few-shot examples to your prompt can steer the model\nto respond in the way you want. Prompt Model response Determine the city along with the landmark. The landmark is the Christ the Redeemer statue in Rio de Janeiro, Brazil. Updated prompt Improved response Determine the city along with the landmark. city: Rome, landmark: the Colosseum. city: Beijing, landmark: Forbidden City city: Rio de Janeiro, landmark: Christ the Redeemer statue Break it down step-by-step For complex tasks like the ones that require both visual understanding and reasoning, it can be helpful\nto split the task into smaller, more straightforward steps. Alternatively, it could also be effective if you\ndirectly ask the model to “think step by step” in your prompt. Prompt Model response When will I run out of toilet paper? Soon, you only have 3 rolls left. Updated prompt Improved response 1. First, count how many toilet paper rolls are in this picture. 2. Then, determine how much toilet paper a typical person uses per day. 3. Calculate how long these rolls of toilet paper will last. 1. There are 3 rolls of toilet paper in this picture. 2. A typical person uses about 20 sheets of toilet paper per day. 3. If each roll contains 200 sheets, then each roll will last for about 10 days. Therefore, the 3 rolls will last for about a month. Math problems or other types of word problems are great candidates for asking the model to think step-by-step. Prompt Response What is the 4th term in the sequence? -135 The response from the model is incorrect. Some ways to improve this is to ask the model to parse the problem before answering the prompt. Prompt Response Parse the formula in the image first. Then based on the formula, answer \"what's the 4th term in the sequence?\". Click to expand the result $$ b(n) = b(n-1) * (-3) $$ $$ b(1) = 15 $$ Substitute n = 4 into the formula: $$ b(4) = b(4-1) * (-3) $$ $$ b(4) = b(3) * (-3) $$ We don't know the value of b(3), so substitute n = 3 into the formula: $$ b(3) = b(3-1) * (-3) $$ $$ b(3) = b(2) * (-3) $$ We don't know the value of b(2), so substitute n = 2 into the formula: $$ b(2) = b(2-1) * (-3) $$ $$ b(2) = b(1) * (-3) $$ We know the value of b(1) is 15, so substitute it into the formula: $$ b(2) = 15 * (-3) $$ $$ b(2) = -45 $$ Now that we know the value of b(2), substitute it into the formula for b(3): $$ b(3) = b(2) * (-3) $$ $$ b(3) = -45 * (-3) $$ $$ b(3) = 135 $$ Now that we know the value of b(3), substitute it into the formula for b(4): $$ b(4) = b(3) * (-3) $$ $$ b(4) = 135 * (-3) $$ $$ b(4) = -405 $$ Therefore, the 4th term in the sequence is -405. This model response is correct - now to make the response even more streamlined, you can ask the model to to think step-by-step. Updated prompt Improved response What is the 4th term in the sequence? Think step by step. The first term is 15. The second term is b(1) * (-3) = 15 * (-3) = -45. The third term is b(2) * (-3) = -45 * (-3) = 135. The fourth term is b(3) * (-3) = 135 * (-3) = -405. Try specifying the output format A common problem is the need for model output to be in a certain format (e.g. markdown, JSON, HTML), especially if the model output needs to\n  be ingested by a downstream task. You can try to produce output in that format by instructing the model to do so within the prompt. Below are two examples: Prompt Model response Parse the table in this image into markdown format | Subject | Status | Time | Start date | Due on | | ------- | ------ | ---- | --------- | ------ | | Math | Done | 1.5 hrs | 11/29/2023 | 12/1/2023 | | Geography | Done | 30 min | 12/2/2023 | 12/4/2023 | | Physics | In progress | 2 hrs | 12/3/2023 | 12/5/2023 | | Biology | Not started | 6 hrs | 11/2/2016 | 12/6/2023 | Prompt Model response Provide a list of all the following attributes: ingredients, type of cuisine, vegetarian or not, in JSON format ```json``` { \"ingredients\": [ \"rice\", \"shrimp\", \"clams\", \"mussels\", \"peas\", \"tomatoes\", \"onions\", \"garlic\", \"olive oil\", \"paprika\", \"salt\", \"pepper\" ], \"type of cuisine\": \"Spanish\", \"vegetarian\": false Put your image first for single-image prompts While Gemini can interpret image and text in any order within a prompt, placing a single image\nbefore the text prompt might lead to better results. In the following samples, you'll notice the image\ncomes first before the text in each prompt. Troubleshooting your multimodal prompt You might need to troubleshoot your prompt if you are not getting a helpful response. Here are a few strategies you could try. If the model is not drawing information from the relevant part of the image To get a more specific response, you can point out which aspects of the image you want the prompt to\ndraw information from to inform its response. Prompt Model response How many days will these diapers last a baby? The diapers will last for 198 days before they run out. Updated prompt Improved response How long will these diapers last before I run out? Use the weight shown on the box to determine the child's age, and use the total number of diapers in the box. Divide the total number by how many diapers the child goes through per day. The box contains 198 diapers. The box states the diapers are for a child weighing 22-37 lbs. The average child in this weight range is likely between 1 and 2 years old. A 2-year-old typically goes through 6 diapers per day, therefore, the diapers will last around 33 days (198/6). If the model output is too generic and not tailored enough to the image input To help the model tailor its response to the image(s), try asking it to describe the images before performing its reasoning task. Prompt Model response What is in common between these images? All four images are examples of landscape photography. Updated prompt Improved response First, describe what's in each image in detail. What's in common between these images? The first image shows a person packing a suitcase. The second image shows an empty hallway in a shopping mall. The third image shows a roll of toilet paper. The fourth image shows a dark and stormy sea. The commonality between these images is that they are all examples of liminal spaces. Liminal spaces are places or situations that are in between two other places or situations. They are often characterized by a sense of uncertainty or ambiguity. The first image shows the person in between being at home and being on a trip. The second image shows the hallway in between two stores. The third image is of toilet paper, which can represent a bathroom. A bathroom is a transitional space between private\n        and public realms. It's a place where we prepare ourselves to enter or leave the outside world. The fourth image shows the sea in between being calm and being stormy. Liminal spaces can be both exciting and unsettling. They can be a place of new beginning or a place of endings. They can be a place of reflection or a place of action. You can also try explicitly asking the model to refer to what's in the image. Updated prompt Improved response What is in common between these images? Refer to what's in the images in your response. All of the images are related to travel. The first image shows a person packing a suitcase, the second image shows\n          an empty hallway in a shopping mall, the third image shows a stack of toilet paper, and the fourth image shows a\n          dark and stormy sea. These images are all related to travel because they are all things that people need or use when\n          they travel. The first image shows a person packing a suitcase, which is something that people do when they are\n          going on a trip. The second image shows an empty hallway in a shopping mall, which is a place where people\n          often go to buy things for their trips. The third image shows a stack of toilet paper, which is something\n          that people often use when they are traveling. The fourth image shows a dark and stormy sea, which is\n          something that people often encounter when they are traveling. The same tips work for creative output from images. In the following example with\n  the rabbit, explicitly asking the model to include aspects from the image can improve the output. Prompt Model response Can you write me a poem about this image? Little bunny, so soft and brown, Hopping through the field, up and down. Your ears so long, your tail so small, You're the cutest creature of them all. You nibble on the grass, so green and sweet, And hop away when you see me. But I'll always be your friend, Little bunny, until the very end. Updated prompt Improved response Can you write me a descriptive and dramatic poem about this image and include the location? Little bunny on the green, Ears so long, so keen. What do you see out there, In the distance, where you stare? Is it the ocean blue, Sparkling in the sun's hue? Or the waves that crash and roar, A rhythm that you adore? Little bunny, tell me true, What captures your attention so? Is it the salty breeze, Or the seagulls' cries that please? Perhaps it's the vastness of it all, A world so big and small. A place where dreams can roam, And adventures can be sown. Little bunny, I wonder what you dream, As you sit on the grass, so serene. Do you yearn to explore the deep, Or stay on land, where you can leap? Whatever it may be, little bunny, Keep that spark of wonder burning brightly. For in your dreams and aspirations, Lies a world waiting for your creation. Troubleshooting which part of the prompt failed It can be hard to know whether a prompt failed because the model\ndidn't understand the image to begin with, or if it did understand the image but\ndid not perform the correct reasoning steps afterward. To disambiguate between those reasons, ask the model to describe what's in the image. In this example below, if the model responds with a snack that seems surprising when paired with\ntea (e.g. popcorn), you can first troubleshoot to determine whether the model correctly recognized\nthat the image contains tea. Prompt Prompt for troubleshooting What's a snack I can make in 1 minute that would go well with this? Describe what's in this image. Another strategy is to ask the model to explain its reasoning. That can help you narrow down which part of the reasoning broke down, if any. Prompt Prompt for troubleshooting What's a snack I can make in 1 minute that would go well with this? What's a snack I can make in 1 minute that would go well with this? Please explain why. Tuning the sampling parameters In each request, you send not only the multimodal prompt but a set of sampling parameters to the model.\nThe model can generate different results for different parameter values. Experiment with the different\nparameters to get the best values for the task. The most commonly adjusted parameters are the following: Temperature top-P top-K Temperature Temperature is used for sampling during response generation, which occurs when top-P and top-K are applied.\nTemperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that\nrequire a more deterministic and less open-ended or creative response, while higher temperatures can lead to more\ndiverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response\nis always selected. For most use cases, try starting with a temperature of 0.4. If you need more creative results, try increasing the\ntemperature. If you observe clear hallucinations, try reducing the temperature. Top-K Top-K changes how the model selects tokens for output. A top-K of 1 means the next selected token is\nthe most probable among all tokens in the model's vocabulary (also called greedy decoding), while\na top-K of 3 means that the next token is selected from among the three most probable tokens by\nusing temperature. For each token selection step, the top-K tokens with the highest probabilities are sampled. Then\ntokens are further filtered based on top-P with the final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more random responses.\nThe default value of top-K is 32. Top-P Top-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K)\nto least probable until the sum of their probabilities equals the top-P value. For example, if tokens\nA, B, and C have a probability of 0.6, 0.3, 0.1 and the top-P value is  0.9, then the model will\nselect either A or B as the next token by using temperature and excludes C as a candidate. Specify a lower value for less random responses and a higher value for more random responses.\nThe default value of top-P is 1.0. Next steps Try writing your own multimodal prompts using Google AI\nStudio . For more guidance on prompt design, see the Prompt strategies page. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-21 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies"
            ],
            "timestamp": "2024-12-27T22:47:25.727513",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/tokens",
            "title": "Understand and count tokens  |  Gemini API  |  Google AI for Developers",
            "text_content": "Understand and count tokens  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Understand and count tokens Python Node.js Go Gemini and other generative AI models process input and output at a granularity\ncalled a token . About tokens Tokens can be single characters like z or whole words like cat . Long words\nare broken up into several tokens. The set of all tokens used by the model is\ncalled the vocabulary, and the process of splitting text into tokens is called tokenization . For Gemini models, a token is equivalent to about 4 characters.\n100 tokens is equal to about 60-80 English words. When billing is enabled, the cost of a call to the Gemini API is\ndetermined in part by the number of input and output tokens, so knowing how to\ncount tokens can be helpful. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/tokens"
            ],
            "timestamp": "2024-12-27T22:47:26.655690",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/openai",
            "title": "OpenAI compatibility  |  Gemini API  |  Google AI for Developers",
            "text_content": "OpenAI compatibility  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback OpenAI compatibility Gemini models are accessible using the OpenAI libraries (Python and TypeScript /\nJavascript) along with the REST API, by updating three lines of code\nand using your Gemini API key . If you\naren't already using the OpenAI libraries, we recommend that you call the Gemini API directly . Python from openai import OpenAI client = OpenAI ( api_key = \"GEMINI_API_KEY\" , base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\" ) response = client . chat . completions . create ( model = \"gemini-1.5-flash\" , n = 1 , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Explain to me how AI works\" } ] ) print ( response . choices [ 0 ] . message ) Node.js import OpenAI from \"openai\" ; const openai = new OpenAI ({ apiKey : \"GEMINI_API_KEY\" , baseURL : \"https://generativelanguage.googleapis.com/v1beta/openai/\" }); const response = await openai . chat . completions . create ({ model : \"gemini-1.5-flash\" , messages : [ { role : \"system\" , content : \"You are a helpful assistant.\" }, { role : \"user\" , content : \"Explain to me how AI works\" , }, ], }); console . log ( response . choices [ 0 ]. message ); REST curl \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\" \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer GEMINI_API_KEY\" \\ -d '{ \"model\": \"gemini-1.5-flash\", \"messages\": [ {\"role\": \"user\", \"content\": \"Explain to me how AI works\"} ] }' Streaming The Gemini API supports streaming responses . Python from openai import OpenAI client = OpenAI ( api_key = \"GEMINI_API_KEY\" , base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\" ) response = client . chat . completions . create ( model = \"gemini-1.5-flash\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Hello!\" } ], stream = True ) for chunk in response : print ( chunk . choices [ 0 ] . delta ) Node.js import OpenAI from \"openai\" ; const openai = new OpenAI ({ apiKey : \"GEMINI_API_KEY\" , baseURL : \"https://generativelanguage.googleapis.com/v1beta/openai/\" }); async function main () { const completion = await openai . chat . completions . create ({ model : \"gemini-1.5-flash\" , messages : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Hello!\" } ], stream : true , }); for await ( const chunk of completion ) { console . log ( chunk . choices [ 0 ]. delta . content ); } } main (); REST curl \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\" \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer GEMINI_API_KEY\" \\ -d '{ \"model\": \"gemini-1.5-flash\", \"messages\": [ {\"role\": \"user\", \"content\": \"Explain to me how AI works\"} ], \"stream\": true }' Function calling Function calling makes it easier for you to get structured data outputs from\ngenerative models and is supported in the Gemini API . Python from openai import OpenAI client = OpenAI ( api_key = \"GEMINI_API_KEY\" , base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\" ) tools = [ { \"type\" : \"function\" , \"function\" : { \"name\" : \"get_weather\" , \"description\" : \"Get the weather in a given location\" , \"parameters\" : { \"type\" : \"object\" , \"properties\" : { \"location\" : { \"type\" : \"string\" , \"description\" : \"The city and state, e.g. Chicago, IL\" , }, \"unit\" : { \"type\" : \"string\" , \"enum\" : [ \"celsius\" , \"fahrenheit\" ]}, }, \"required\" : [ \"location\" ], }, } } ] messages = [{ \"role\" : \"user\" , \"content\" : \"What's the weather like in Chicago today?\" }] response = client . chat . completions . create ( model = \"gemini-1.5-flash\" , messages = messages , tools = tools , tool_choice = \"auto\" ) print ( response ) Node.js import OpenAI from \"openai\" ; const openai = new OpenAI ({ apiKey : \"GEMINI_API_KEY\" , baseURL : \"https://generativelanguage.googleapis.com/v1beta/openai/\" }); async function main () { const messages = [{ \"role\" : \"user\" , \"content\" : \"What's the weather like in Chicago today?\" }]; const tools = [ { \"type\" : \"function\" , \"function\" : { \"name\" : \"get_weather\" , \"description\" : \"Get the weather in a given location\" , \"parameters\" : { \"type\" : \"object\" , \"properties\" : { \"location\" : { \"type\" : \"string\" , \"description\" : \"The city and state, e.g. Chicago, IL\" , }, \"unit\" : { \"type\" : \"string\" , \"enum\" : [ \"celsius\" , \"fahrenheit\" ]}, }, \"required\" : [ \"location\" ], }, } } ]; const response = await openai . chat . completions . create ({ model : \"gemini-1.5-flash\" , messages : messages , tools : tools , tool_choice : \"auto\" , }); console . log ( response ); } main (); REST curl \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\" \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer GEMINI_API_KEY\" \\ -d '{ \"model\": \"gemini-1.5-flash\", \"messages\": [ { \"role\": \"user\", \"content\": \"What' \\' 's the weather like in Chicago today?\" } ], \"tools\": [ { \"type\": \"function\", \"function\": { \"name\": \"get_weather\", \"description\": \"Get the current weather in a given location\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. Chicago, IL\" }, \"unit\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"] } }, \"required\": [\"location\"] } } } ], \"tool_choice\": \"auto\" }' Image understanding Gemini models are natively multimodal and provide best in class performance on many common vision tasks . Python import base64 from openai import OpenAI client = OpenAI ( api_key = \"GEMINI_API_KEY\" , base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\" ) # Function to encode the image def encode_image ( image_path ): with open ( image_path , \"rb\" ) as image_file : return base64 . b64encode ( image_file . read ()) . decode ( 'utf-8' ) # Getting the base64 string base64_image = encode_image ( \"Path/to/agi/image.jpeg\" ) response = client . chat . completions . create ( model = \"gemini-1.5-flash\" , messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What is in this image?\" , }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : f \"data:image/jpeg;base64, { base64_image } \" }, }, ], } ], ) print ( response . choices [ 0 ]) Node.js import OpenAI from \"openai\" ; import fs from 'fs/promises' ; const openai = new OpenAI ({ apiKey : \"GEMINI_API_KEY\" , baseURL : \"https://generativelanguage.googleapis.com/v1beta/openai/\" }); async function encodeImage ( imagePath ) { try { const imageBuffer = await fs . readFile ( imagePath ); return imageBuffer . toString ( 'base64' ); } catch ( error ) { console . error ( \"Error encoding image:\" , error ); return null ; } } async function main () { const imagePath = \"Path/to/agi/image.jpeg\" ; const base64Image = await encodeImage ( imagePath ); const messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What is in this image?\" , }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : `data:image/jpeg;base64, ${ base64Image } ` }, }, ], } ]; try { const response = await openai . chat . completions . create ({ model : \"gemini-1.5-flash\" , messages : messages , }); console . log ( response . choices [ 0 ]); } catch ( error ) { console . error ( \"Error calling Gemini API:\" , error ); } } main (); REST bash -c ' base64_image=$(base64 -i \"Path/to/agi/image.jpeg\"); curl \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\" \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer GEMINI_API_KEY\" \\ -d \"{ \\\"model\\\": \\\"gemini-1.5-flash\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": [ { \\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"What is in this image?\\\" }, { \\\"type\\\": \\\"image_url\\\", \\\"image_url\\\": { \\\"url\\\": \\\"data:image/jpeg;base64,${base64_image}\\\" } } ] } ] }\" ' Structured output Gemini models can output JSON objects in any structure you define . Python from pydantic import BaseModel from openai import OpenAI client = OpenAI ( api_key = \"GEMINI_API_KEY\" , base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\" ) class CalendarEvent ( BaseModel ): name : str date : str participants : list [ str ] completion = client . beta . chat . completions . parse ( model = \"gemini-1.5-flash\" , messages = [ { \"role\" : \"system\" , \"content\" : \"Extract the event information.\" }, { \"role\" : \"user\" , \"content\" : \"John and Susan are going to an AI conference on Friday.\" }, ], response_format = CalendarEvent , ) print ( completion . choices [ 0 ] . message . parsed ) Node.js import OpenAI from \"openai\" ; import { zodResponseFormat } from \"openai/helpers/zod\" ; import { z } from \"zod\" ; const openai = new OpenAI ({ apiKey : \"GEMINI_API_KEY\" , baseURL : \"https://generativelanguage.googleapis.com/v1beta/openai\" }); const CalendarEvent = z . object ({ name : z . string (), date : z . string (), participants : z . array ( z . string ()), }); const completion = await openai . beta . chat . completions . parse ({ model : \"gemini-1.5-flash\" , messages : [ { role : \"system\" , content : \"Extract the event information.\" }, { role : \"user\" , content : \"John and Susan are going to an AI conference on Friday\" }, ], response_format : zodResponseFormat ( CalendarEvent , \"event\" ), }); const event = completion . choices [ 0 ]. message . parsed ; console . log ( event ); Embeddings Text embeddings measure the relatedness of text strings and can be generated\nusing the the Gemini API . Python from openai import OpenAI client = OpenAI ( api_key = \"GEMINI_API_KEY\" , base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\" ) response = client . embeddings . create ( input = \"Your text string goes here\" , model = \"text-embedding-004\" ) print ( response . data [ 0 ] . embedding ) Node.js import OpenAI from \"openai\" ; const openai = new OpenAI ({ apiKey : \"GEMINI_API_KEY\" , baseURL : \"https://generativelanguage.googleapis.com/v1beta/openai/\" }); async function main () { const embedding = await openai . embeddings . create ({ model : \"text-embedding-004\" , input : \"Your text string goes here\" , }); console . log ( embedding ); } main (); REST curl \"https://generativelanguage.googleapis.com/v1beta/openai/embeddings\" \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer GEMINI_API_KEY\" \\ -d '{ \"input\": \"Your text string goes here\", \"model\": \"text-embedding-004\" }' Current limitations Support for the OpenAI libraries is still in beta while we extend feature support. If you have questions about supported parameters, upcoming features, or run into\nany issues getting started with Gemini, join our Developer Forum . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-26 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/openai"
            ],
            "timestamp": "2024-12-27T22:47:27.705181",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/billing",
            "title": "Billing  |  Gemini API  |  Google AI for Developers",
            "text_content": "Billing  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Billing This guide provides an overview of different Gemini API billing options,\nexplains how to enable billing and monitor usage, and provides answers to\nfrequently asked questions (FAQs) about billing. About billing Billing for the Gemini API is based on two pricing tiers: free of charge (or free ) and pay-as-you-go (or paid ). Pricing and rate limits differ\nbetween these tiers and also vary by model. For current prices and rate limits,\nsee Pricing . For a model-by-model breakdown of capabilities, see the Gemini models page . Rate limits Rate limits are quotas that\nlimit the number of requests or tokens that you can send to the Gemini API in a\ngiven time window. Rate limits can apply per request or per token. Here's a\nfictitious example of quotas that might apply to a given model in a given tier: Important: The following rate limits are for illustrative purposes only. For\nactual rate limits, see the pricing page and the Gemini models page . 10 RPM (requests per minute) 10,000 TPM (tokens per minute) 1,000 RPD (requests per day) In this example, if you exceed 10 RPM (or 10,000 TPM or 1,000 RPD), the\nGemini API service throws a 429: RESOURCE_EXHAUSTED error indicating\nthat you've exceeded the rate limit. Why use the paid tier? When you enable billing and use the paid tier, you benefit from higher rate\nlimits, and your prompts and responses aren't used to improve Google products.\nFor more information on data use for paid services, see the terms of service . Cloud Billing The Gemini API uses Cloud Billing for billing services. To use the paid tier, you must set up Cloud Billing on\nyour cloud project. After you've enabled Cloud Billing, you can use Cloud\nBilling tools to track spending, understand costs, make payments, and access\nCloud Billing support. Enable billing You can enable Cloud Billing starting from Google AI Studio: Open Google AI Studio . In the bottom of the left sidebar, select Settings > Plan information . Click Set up Billing for your chosen project to enable Cloud Billing. Monitor usage After you enable Cloud Billing, you can monitor your usage of the Gemini API in\nthe Google Cloud console .\nThe service name for the API is generativelanguage.googleapis.com , and in the\nconsole the Gemini API is also referred to as the Generative Language API . To learn more, see the Google Cloud documentation on monitoring API usage . Frequently asked questions This section provides answers to frequently asked questions. What am I billed for? Gemini API pricing is based on the following: Input token count Output token count Cached token count Cached token storage duration For pricing information, see the pricing page . Where can I view my quota? You can view your quota and system limits in the Google Cloud console . Can I use the Gemini API for free in EEA (including EU), the UK, and CH? Yes, we make the free tier and paid tier available in many regions . If I set up billing with the Gemini API, will I be charged for my Google AI Studio usage? No, Google AI Studio usage remains free of charge regardless of if you set up\nbilling across all supported regions. Can I use 1M tokens in the free tier? The free tier for Gemini API differs based on the model selected. For now, you\ncan try the 1M token context window in the following ways: In Google AI Studio With pay-as-you-go plans With free-of-charge plans for select models See the latest free-of-charge rate limits per model on the pricing page . How can I calculate the number of tokens I'm using? Use the GenerativeModel.count_tokens method to count the number of tokens. Refer to the Tokens guide to learn more about tokens. Can I use my Google Cloud credits with the Gemini API? Yes, Google Cloud credits can be used towards Gemini API usage. How is billing handled? Billing for the Gemini API is handled by the Cloud Billing system. Am I charged for failed requests? If your request fails with a 400 or 500 error, you won't be charged for the\ntokens used. However, the request will still count against your quota. Is there a charge for fine-tuning the models? Model tuning is free, but inference on tuned\nmodels is charged at the same rate as the base models. Is GetTokens billed? Requests to the GetTokens API are not billed, and they don't count against\ninference quota. Where can I get help with billing? To get help with billing, see Get Cloud Billing support . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-09-26 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/billing"
            ],
            "timestamp": "2024-12-27T22:47:28.292285",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/safety-settings",
            "title": "Safety settings  |  Gemini API  |  Google AI for Developers",
            "text_content": "Safety settings  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Safety settings The Gemini API provides safety settings that you can adjust during the\nprototyping stage to determine if your application requires more or less\nrestrictive safety configuration. You can adjust these settings across four\nfilter categories to restrict or allow certain types of content. This guide covers how the Gemini API handles safety settings and filtering and\nhow you can change the safety settings for your application. Note: Applications that use less restrictive safety settings may be subject to\nreview. See the Terms of Service for more information. Safety filters The Gemini API's adjustable safety filters cover the following categories: Category Description Harassment Negative or harmful comments targeting identity and/or protected\n      attributes. Hate speech Content that is rude, disrespectful, or profane. Sexually explicit Contains references to sexual acts or other lewd content. Dangerous Promotes, facilitates, or encourages harmful acts. Civic integrity Election-related queries. These categories are defined in HarmCategory . The\n  Gemini models only support HARM_CATEGORY_HARASSMENT , HARM_CATEGORY_HATE_SPEECH , HARM_CATEGORY_SEXUALLY_EXPLICIT , HARM_CATEGORY_DANGEROUS_CONTENT , and HARM_CATEGORY_CIVIC_INTEGRITY . All other categories are used\n  only by PaLM 2 (Legacy) models. You can use these filters to adjust what's appropriate for your use case. For\nexample, if you're building video game dialogue, you may deem it acceptable to\nallow more content that's rated as Dangerous due to the nature of the game. In addition to the adjustable safety filters, the Gemini API has built-in\nprotections against core harms, such as content that endangers child safety.\nThese types of harm are always blocked and cannot be adjusted. Content safety filtering level The Gemini API categorizes the probability level of content being unsafe as HIGH , MEDIUM , LOW , or NEGLIGIBLE . The Gemini API blocks content based on the probability of content being unsafe\nand not the severity. This is important to consider because some content can\nhave low probability of being unsafe even though the severity of harm could\nstill be high. For example, comparing the sentences: The robot punched me. The robot slashed me up. The first sentence might result in a higher probability of being unsafe, but you\nmight consider the second sentence to be a higher severity in terms of violence.\nGiven this, it is important that you carefully test and consider what the\nappropriate level of blocking is needed to support your key use cases while\nminimizing harm to end users. Safety filtering per request You can adjust the safety settings for each request you make to the API. When\nyou make a request, the content is analyzed and assigned a safety rating. The\nsafety rating includes the category and the probability of the harm\nclassification. For example, if the content was blocked due to the harassment\ncategory having a high probability, the safety rating returned would have\ncategory equal to HARASSMENT and harm probability set to HIGH . By default, safety settings block content (including prompts) with medium or\nhigher probability of being unsafe across any filter. This baseline safety is\ndesigned to work for most use cases, so you should only adjust your safety\nsettings if it's consistently required for your application. The following table describes the block settings you can adjust for each\ncategory. For example, if you set the block setting to Block few for the Hate speech category, everything that has a high probability of being hate\nspeech content is blocked. But anything with a lower probability is allowed. Threshold (Google AI Studio) Threshold (API) Description Block none BLOCK_NONE Always show regardless of probability of unsafe content Block few BLOCK_ONLY_HIGH Block when high probability of unsafe content Block some BLOCK_MEDIUM_AND_ABOVE Block when medium or high probability of unsafe content Block most BLOCK_LOW_AND_ABOVE Block when low, medium or high probability of unsafe content N/A HARM_BLOCK_THRESHOLD_UNSPECIFIED Threshold is unspecified, block using default threshold If the threshold is not set, the default block threshold is Block most (for gemini-1.5-pro-002 and gemini-1.5-flash-002 only) or Block some (in all\nother models) for all categories except the Civic integrity category. The default block threshold for the Civic integrity category is Block most when sending prompts using Google AI Studio, and Block none when using the\nGemini API directly. You can set these settings for each request you make to the generative service.\nSee the HarmBlockThreshold API\nreference for details. Safety feedback generateContent returns a GenerateContentResponse which\nincludes safety feedback. Prompt feedback is included in promptFeedback . If promptFeedback.blockReason is set, then the content of the prompt was blocked. Response candidate feedback is included in Candidate.finishReason and Candidate.safetyRatings . If response\ncontent was blocked and the finishReason was SAFETY , you can inspect safetyRatings for more details. The content that was blocked is not returned. Adjust safety settings This section covers how to adjust the safety settings in both Google AI Studio\nand in your code. Google AI Studio You can adjust safety settings in Google AI Studio, but you cannot turn them\noff. Click Edit safety settings in the Run settings panel to open the Run\nsafety settings modal. In the modal, you can use the sliders to adjust the\ncontent filtering level per safety category: Note: If you set any of the category filters to Block none , Google AI Studio\nwill display a reminder about the Gemini API's Terms of Service with respect\nto safety settings. When you send a request (for example, by asking the model a question), a warning No Content message appears if the request's content is blocked. To see more\ndetails, hold the pointer over the No Content text and click warning Safety . Gemini API SDKs The following code snippet shows how to set safety settings in your GenerateContent call. This sets the thresholds for the harassment\n( HARM_CATEGORY_HARASSMENT ) and hate speech ( HARM_CATEGORY_HATE_SPEECH )\ncategories. For example, setting these categories to BLOCK_LOW_AND_ABOVE blocks any content that has a low or higher probability of being harassment or\nhate speech. To understand the threshold settings, see Safety filtering per request . Python from google.generativeai.types import HarmCategory , HarmBlockThreshold model = genai . GenerativeModel ( model_name = 'gemini-1.5-flash' ) response = model . generate_content ( [ 'Do these look store-bought or homemade?' , img ], safety_settings = { HarmCategory . HARM_CATEGORY_HATE_SPEECH : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , HarmCategory . HARM_CATEGORY_HARASSMENT : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , } ) Go model := client . GenerativeModel ( \"gemini-1.5-flash\" ) model . SafetySettings = [] * genai . SafetySetting { { Category : genai . HarmCategoryHarassment , Threshold : genai . HarmBlockLowAndAbove , }, { Category : genai . HarmCategoryHateSpeech , Threshold : genai . HarmBlockLowAndAbove , }, } Node.js import { HarmBlockThreshold , HarmCategory } from \"@google/generative-ai\" ; // ... const safetySettings = [ { category : HarmCategory . HARM_CATEGORY_HARASSMENT , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, { category : HarmCategory . HARM_CATEGORY_HATE_SPEECH , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, ]; const model = genAi . getGenerativeModel ({ model : \"gemini-1.5-flash\" , safetySettings : safetySettings }); Web import { HarmBlockThreshold , HarmCategory } from \"@google/generative-ai\" ; // ... const safetySettings = [ { category : HarmCategory . HARM_CATEGORY_HARASSMENT , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, { category : HarmCategory . HARM_CATEGORY_HATE_SPEECH , threshold : HarmBlockThreshold . BLOCK_LOW_AND_ABOVE , }, ]; const model = genAi . getGenerativeModel ({ model : \"gemini-1.5-flash\" , safetySettings }); Dart (Flutter) final safetySettings = [ SafetySetting ( HarmCategory . harassment , HarmBlockThreshold . low ), SafetySetting ( HarmCategory . hateSpeech , HarmBlockThreshold . low ), ]; final model = GenerativeModel ( model: 'gemini-1.5-flash' , apiKey: apiKey , safetySettings: safetySettings , ); Kotlin val harassmentSafety = SafetySetting ( HarmCategory . HARASSMENT , BlockThreshold . LOW_AND_ABOVE ) val hateSpeechSafety = SafetySetting ( HarmCategory . HATE_SPEECH , BlockThreshold . LOW_AND_ABOVE ) val generativeModel = GenerativeModel ( modelName = \"gemini-1.5-flash\" , apiKey = BuildConfig . apiKey , safetySettings = listOf ( harassmentSafety , hateSpeechSafety ) ) Java SafetySetting harassmentSafety = new SafetySetting ( HarmCategory . HARASSMENT , BlockThreshold . LOW_AND_ABOVE ); SafetySetting hateSpeechSafety = new SafetySetting ( HarmCategory . HATE_SPEECH , BlockThreshold . LOW_AND_ABOVE ); GenerativeModel gm = new GenerativeModel ( \"gemini-1.5-flash\" , BuildConfig . apiKey , null , // generation config is optional Arrays . asList ( harassmentSafety , hateSpeechSafety ) ); GenerativeModelFutures model = GenerativeModelFutures . from ( gm ); REST echo '{ \"safetySettings\": [ {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_ONLY_HIGH\"}, {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"} ], \"contents\": [{ \"parts\":[{ \"text\": \"' I support Martians Soccer Club and I think Jupiterians Football Club sucks! Write a ironic phrase about them. '\"}]}]}' > request.json\n\ncurl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key= $GOOGLE_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d @request.json 2 > /dev/null safety_settings.sh Next steps See the API reference to learn more about the full API. Review the safety guidance for a general look at safety\nconsiderations when developing with LLMs. Learn more about assessing probability versus severity from the Jigsaw\nteam Learn more about the products that contribute to safety solutions like the Perspective\nAPI .\n                *   You can use these safety settings to create a toxicity\n                    classifier. See the classification\n                    example to\n                    get started. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/safety-settings"
            ],
            "timestamp": "2024-12-27T22:47:29.196239",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/safety-guidance",
            "title": "Safety guidance  |  Gemini API  |  Google AI for Developers",
            "text_content": "Safety guidance  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Safety guidance Generative artificial intelligence models are powerful tools, but they are not\nwithout their limitations. Their versatility and applicability can sometimes\nlead to unexpected outputs, such as outputs that are inaccurate, biased, or\noffensive. Post-processing, and rigorous manual evaluation are essential to\nlimit the risk of harm from such outputs. The models provided by the Gemini API can be used for a wide variety of\ngenerative AI and natural language processing (NLP) applications. Use of these\nfunctions is only available through the Gemini API or the Google AI Studio web\napp. Your use of Gemini API is also subject to the Generative AI Prohibited Use\nPolicy and the Gemini API terms of service . Part of what makes large language models (LLMs) so useful is that they're\ncreative tools that can address many different language tasks. Unfortunately,\nthis also means that large language models can generate output that you don't\nexpect, including text\nthat's offensive, insensitive, or factually incorrect. What's more, the\nincredible versatility of these models is also what makes it difficult to\npredict exactly what kinds of undesirable output they might produce. While the\nGemini API has been designed with Google's AI\nprinciples in mind, the onus is on developers to\napply these models responsibly. To aid developers in creating safe, responsible\napplications, the Gemini API has some built-in content filtering as well as\nadjustable safety settings across 4 dimensions of harm. Refer to the safety settings guide to learn more. This document is meant to introduce you to some safety risks that can arise when\nusing LLMs, and recommend emerging safety design and development\nrecommendations. (Note that laws and regulations may also impose restrictions,\nbut such considerations are beyond the scope of this guide.) The following steps are recommended when building applications with LLMs: Understanding the safety risks of your application Considering adjustments to mitigate safety risks Performing safety testing appropriate to your use case Soliciting feedback from users and monitoring usage The adjustment and testing phases should be iterative until you reach\nperformance appropriate for your application. Understand the safety risks of your application In this context, safety is being defined as the ability of an LLM to avoid\ncausing harm to its users, for example, by generating toxic language or content\nthat promotes stereotypes. The models available through the Gemini API have been\ndesigned with Google’s AI principles in mind\nand your use of it is subject to the Generative AI Prohibited Use\nPolicy . The API\nprovides built-in safety filters to help address some common language model\nproblems such as toxic language and hate speech, and striving for inclusiveness\nand avoidance of stereotypes. However, each application can pose a different set\nof risks to its users. So as the application owner, you are responsible for\nknowing your users and the potential harms your application may cause, and\nensuring that your application uses LLMs safely and responsibly. As part of this assessment, you should consider the likelihood that harm could\noccur and determine its seriousness and mitigation steps. For example, an\napp that generates essays based on factual events would need to be more careful\nabout avoiding misinformation, as compared to an app that generates fictional\nstories for entertainment. A good way to begin exploring potential safety risks\nis to research your end users, and others who might be affected by your\napplication's results. This can take many forms including researching state of\nthe art studies in your app domain, observing how people are using similar apps,\nor running a user study, survey, or conducting informal interviews with\npotential users. Advanced tips Speak with a diverse mix of prospective users within your target\n        population about your application and its intended purpose so as\n        to get a wider perspective on potential risks and to adjust diversity\n        criteria as needed. The AI Risk Management Framework released by the U.S. government's\n        National Institute of Standards and Technology (NIST) provides more\n        detailed guidance and additional learning resources for AI risk management. DeepMind's publication on the ethical and social risks of harm from language models describes in detail the ways that language model\n        applications can cause harm. Consider adjustments to mitigate safety risks Now that you have an understanding of the risks, you can decide how to mitigate\nthem. Determining which risks to prioritize and how much you should do to try to\nprevent them is a critical decision, similar to triaging bugs in a software\nproject. Once you've determined priorities, you can start thinking about the\ntypes of mitigations that would be most appropriate. Often simple changes can\nmake a difference and reduce risks. For example, when designing an application consider: Tuning the model output to better reflect what is acceptable in your\napplication context. Tuning can make the output of the model more\npredictable and consistent and therefore can help mitigate certain risks. Providing an input method that facilities safer outputs. The exact input\nyou give to an LLM can make a difference in the quality of the output.\nExperimenting with input prompts to find what works most safely in your\nuse-case is well worth the effort, as you can then provide a UX that\nfacilitates it. For example, you could restrict users to choose only from a\ndrop-down list of input prompts, or offer pop-up suggestions with\ndescriptive\nphrases which you've found perform safely in your application context. Blocking unsafe inputs and filtering output before it is shown to the\nuser. In simple situations, blocklists can be used to identify and block\nunsafe words or phrases in prompts or responses, or require human reviewers\nto manually alter or block such content. Note: Automatically blocking based on a static list can have unintended\nresults such as targeting a particular group that commonly uses vocabulary\nin the blocklist. Using trained classifiers to label each prompt with potential harms or\nadversarial signals. Different strategies can then be employed on how to\nhandle the request based on the type of harm detected. For example, If the\ninput is overtly adversarial or abusive in nature, it could be blocked and\ninstead output a pre-scripted response. Advanced tip If signals determine the output to be harmful,\n        the application can employ the following options: Provide an error message or pre-scripted output. Try the prompt again, in case an alternative safe output is\n              generated, since sometimes the same prompt will elicit\n              different outputs. Putting safeguards in place against deliberate misuse such as assigning\neach user a unique ID and imposing a limit on the volume of user queries\nthat can be submitted in a given period. Another safeguard is to try and\nprotect against possible prompt injection. Prompt injection, much like SQL\ninjection, is a way for malicious users to design an input prompt that\nmanipulates the output of the model, for example, by sending an input prompt\nthat instructs the model to ignore any previous examples. See the Generative AI Prohibited Use Policy for details about deliberate misuse. Adjusting functionality to something that is inherently lower risk. Tasks that are narrower in scope (e.g., extracting keywords from passages of\ntext) or that have greater human oversight (e.g., generating short-form\ncontent that will be reviewed by a human), often pose a lower risk. So for\ninstance, instead of creating an application to write an email reply from\nscratch, you might instead limit it to expanding on an outline or suggesting\nalternative phrasings. Perform safety testing appropriate to your use case Testing is a key part of building robust and safe applications, but the extent,\nscope and strategies for testing will vary. For example, a just-for-fun haiku\ngenerator is likely to pose less severe risks than, say, an application designed\nfor use by law firms to summarize legal documents and help draft contracts. But\nthe haiku generator may be used by a wider variety of users which means the\npotential for adversarial attempts or even unintended harmful inputs can be\ngreater. The implementation context also matters. For instance, an application\nwith outputs that are reviewed by human experts prior to any action being taken\nmight be deemed less likely to produce harmful outputs than the identical\napplication without such oversight. It's not uncommon to go through several iterations of making changes and testing\nbefore feeling confident that you're ready to launch, even for applications that\nare relatively low risk. Two kinds of testing are particularly useful for AI\napplications: Safety benchmarking involves designing safety metrics that reflect the\nways your application could be unsafe in the context of how it is likely to\nget used, then testing how well your application performs on the metrics\nusing evaluation datasets. It's good practice to think about the minimum\nacceptable levels of safety metrics before testing so that 1) you can\nevaluate the test results against those expectations and 2) you can gather\nthe evaluation dataset based on the tests that evaluate the metrics you care\nabout most. Advanced tips Beware of over-relying on “off the shelf” approaches as it's likely\n        you'll need to build your own testing datasets using human raters to\n        fully suit your application's context. If you have more than one metric you'll need to decide how you'll\n        trade off if a change leads to improvements for one metric to the\n        detriment of another. Like with other performance engineering, you\n        may want to focus on worst-case performance across your evaluation\n        set rather than average performance. Adversarial testing involves proactively trying to break your\napplication. The goal is to identify points of weakness so that you can take\nsteps to remedy them as appropriate. Adversarial testing can take\nsignificant time/effort from evaluators with expertise in your application —\nbut the more you do, the greater your chance of spotting problems,\nespecially those occurring rarely or only after repeated runs of the\napplication. Adversarial testing is a method for systematically evaluating an ML\nmodel with the intent of learning how it behaves when provided with\nmalicious or inadvertently harmful input: An input may be malicious when the input is clearly designed to\nproduce an unsafe or harmful output-- for example, asking a text\ngeneration model to generate a hateful rant about a particular\nreligion. An input is inadvertently harmful when the input itself may be\ninnocuous, but produces harmful output -- for example, asking a text\ngeneration model to describe a person of a particular ethnicity and\nreceiving a racist output. What distinguishes an adversarial test from a standard evaluation is the\ncomposition of the data used for testing. For adversarial tests, select \ntest data that is most likely to elicit problematic output from\nthe model. This means probing the model's behavior for all the types of\nharms that are possible, including rare or unusual examples and\nedge-cases that are relevant to safety policies. It should also include\ndiversity in the different dimensions of a sentence such as structure,\nmeaning and length. You can refer to the Google's Responsible AI\npractices in\nfairness for more details on what to consider when building a test dataset. Advanced tips Use automated testing instead of the traditional method of enlisting people in 'red teams'\n    to try and break your application. In automated testing, the\n    'red team' is another language model that finds input text that\n    elicit harmful outputs from the model being tested. Note: LLMs are known to sometimes produce different outputs for the\n   same input prompt. Multiple rounds of testing may be needed to catch more\n   of the problematic outputs. Monitor for problems No matter how much you test and mitigate, you can never guarantee perfection, so\nplan upfront how you'll spot and deal with problems that arise. Common\napproaches include setting up a monitored channel for users to share feedback\n(e.g., thumbs up/down rating) and running a user study to proactively solicit\nfeedback from a diverse mix of users — especially valuable if usage patterns are\ndifferent to expectations. Advanced tips When users give feedback to AI products, it can greatly improve the AI\n        performance and the user experience over time by, for example,\n        helping you choose better examples for prompt tuning. The Feedback and Control chapter in Google's People and AI guidebook highlights key considerations to take into account when designing\n        feedback mechanisms. Next steps Refer to the safety settings guide to learn about the adjustable\nsafety settings available through the Gemini API. See the intro to prompting to get\nstarted writing your first prompts. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-05 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/safety-guidance"
            ],
            "timestamp": "2024-12-27T22:47:30.007140",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
            "title": "Get started with Gemini Nano on Android (on-device)  |  Gemini API  |  Google AI for Developers",
            "text_content": "Get started with Gemini Nano on Android (on-device)  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Get started with Gemini Nano on Android (on-device) Gemini Nano, the smallest version of the Gemini model family, can be executed\non-device on capable Android devices starting with Google Pixel 8 Pro and\nSamsung S24 Series. To execute the Gemini Nano model on Android, you need to use the\nGoogle AI Edge SDK for Android, which provides APIs to: Determine if the underlying Android-powered device is supported. Get access to Gemini Nano model. Tune safety settings. Run inference at high performance and implement fallbacks. Optionally, provide a LoRA fine-tuning block to improve performance of the\nmodel for your use case. The APIs for accessing Gemini Nano support text-to-text modality, with more\nmodalities coming in the future. Important: The Google AI Edge SDK for Android is under a closed early access\n  preview program at this time. Benefits of on-device execution On-device execution enables the following: Local processing of sensitive data : Processing data locally can help you\navoid sending user data to the cloud. This is important for apps that handle\nsensitive data, such as messaging apps with end-to-end encryption. Offline access : Users can access AI features even when there is no\ninternet connection. This is useful for applications that need to work\noffline or with variable connectivity. Cost savings : You can reduce inference costs by offloading execution to\nconsumer hardware. This can produce significant savings for frequently used\nuser flows. On-device execution of Gemini has many benefits; however, for use cases that\nrequire larger Gemini models, and to support a wide range of devices, you may\nwant to consider using the Gemini API for accessing Gemini on the server. You\ncan do this either through backend integration (with Python , Go , Node.js , or REST ) or directly from\nyour Android app through the new Google AI client SDK for Android . How it works On-device execution of Gemini Nano is powered by Android AICore , a new\nsystem-level capability that provides access to foundation models for on-device\nexecution, introduced in Android 14. Foundation models are pre-installed using\nAICore, so you don't need to download or distribute them within your app. You\ncan fine-tune these models for downstream tasks using LoRa. Android AICore is\nnow available in production on Google Pixel 8 Pro and Samsung S24 Series devices\nand is already powering innovative features in Google apps. For more information, see Android AICore . Figure 1. AICore architecture What's next To learn how to take advantage of Gemini Pro inference on Google's servers\nin your Android app, read the quickstart for\nthe Google AI client SDK for Android . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-05 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore"
            ],
            "timestamp": "2024-12-27T22:47:30.696343",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/firebase-extensions",
            "title": "Gemini API Firebase Extensions  |  Google AI for Developers",
            "text_content": "Gemini API Firebase Extensions  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Gemini API Firebase Extensions Firebase is an app development platform backed by Google and\ntrusted by millions of developers around the world. If you're a Firebase\ndeveloper interested in using the Gemini API to add features to\napplications, there are several Firebase extensions to help you\ndo so. These are pre-packaged solutions that help you deploy new capabilities to\nyour app quickly. Build Chatbot with the Gemini API The Build Chatbot with the Gemini API extension lets you establish and\nmanage interactive conversations between your users and large language models\nthrough the Gemini API, using Cloud Firestore as the database.\nA collection in Cloud Firestore represents each chat. The extension monitors the\ncollection of new messages, then queries the Gemini API for a suitable response,\nconsidering the chat's previous messages as context. By integrating the Build Chatbot with the Gemini API extension, you can\nefficiently create chatbot applications, enhancing user experience and\ninteraction while saving time and effort on custom code development. Multimodal Tasks with the Gemini API The Multimodal Tasks with the Gemini API extension lets you perform\nlanguage tasks on data in Firestore, using a text prompt and, optionally, an\nimage. You configure each instance of the extension to perform one particular task. If\nyou have multiple tasks, you can install multiple instances. For example, you could use this extension to: Predict star ratings on a collection of product reviews. Classify customer feedback as positive, negative, or neutral. Summarize long articles. Extract named entities from text. Generate creative text, such as poems or code. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-05 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions"
            ],
            "timestamp": "2024-12-27T22:47:31.500296",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/models/generative-models",
            "title": "About generative models  |  Gemini API  |  Google AI for Developers",
            "text_content": "About generative models  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback About generative models Generative artificial intelligence (AI) models such as the Gemini family of\nmodels are able to create content from varying types of data input, including\ntext, images, and audio. At their most basic level, these models operate like\nsophisticated autocomplete applications. Given input text (\"You can lead a horse\nto water,\"), a generative model can predict that the output text that's\nstatistically likely to follow (\"but you can't make it drink\"), based on\npatterns learned from their training data. You can use this basic property of\ngenerative models for various applications: Generate creative writing such as poetry, short stories, metaphors, and\nblog posts Convert structured data to freeform text Extract and summarize information from freeform text Generate code Translate between languages Using generative models to build these features doesn't require any machine\nlearning (ML) expertise. You don't need to collect a large dataset or train a\nmodel. All it takes to start your first prototype is to describe what you want\nthe model to do in a few sentences. Generative models that only handle text input and output are also known as large language models (LLMs). The Gemini family of models can process more\nthan text data, and while they can perform the functions of an LLM, they are\nmore properly known as generative models . Example applications The following examples show typical text inputs to, and outputs from, a\ngenerative model. Generate a poem User input: Write me a four-line poem about puppies and Android phones. Make\nsure it rhymes. Model output: Puppies are cute,\nAndroid phones are neat,\nThey both bring joy,\nAnd make life complete. Generate a list User input: Generate a bulleted list of items I need to pack for a three-day\ncamping trip. Model output: *   Tent\n*   Sleeping bag\n*   Sleeping pad\n*   Camp stove\n*   Pots and pans\n*   Utensils\n*   Food\n*   Water\n*   First-aid kit\n*   Sunscreen\n*   Insect repellent\n*   ... You can get generative models to produce all sorts of useful behaviors like\nthis, just by crafting the right input text, also called a prompt . The art\nand science of figuring out the right wording to get generative models to do\nwhat you want is called prompt design (also called \"prompt engineering\" or\nsimply \"prompting\"). Prompt design 101 The previous section showed some examples of prompts that contain an\ninstruction, like 'Write me a poem'. This kind of instruction may work well for\ncertain types of tasks. However, for other applications, another prompting\ntechnique called few-shot prompting may work better. Few-shot prompts take\nadvantage of the fact that large language models are incredibly good at\nrecognizing and replicating patterns in text data. The idea is to send the\ngenerative model a text pattern that it learns to complete. For example, say you\nwant to build an application that takes as input a country name and outputs its\ncapital city. Here's a text prompt designed to do just that: Italy : Rome\nFrance : Paris\nGermany : In this prompt, you establish a pattern: [country] : [capital] . If you send\nthis prompt to a large language model, it will autocomplete the pattern and\nreturn something like this: Berlin\nTurkey : Ankara\nGreece : Athens This model response may look a little strange. The model returned not only the\ncapital of Germany (the last country in your hand-written prompt), but also a\nwhole list of additional country and capital pairs. That's because the\ngenerative model is \"continuing the pattern.\" If all you're trying to do is\nbuild a function that tells you the capital of an input country (\"Germany :\nBerlin\"), you probably don't really care about any of the text the model\ngenerates after \"Berlin.\" Indeed, as application designers, you'd probably want\nto truncate those extraneous examples. What's more, you'd probably want to parameterize the input, so that Germany is not a fixed string but a variable\nthat the end user provides: Italy : Rome\nFrance : Paris\n<user input here> : You have just written a few-shot prompt for generating country capitals. You can accomplish a large number of tasks by following this few-shot prompt template. Here's a few-shot prompt with a slightly different format that\nconverts Python to JavaScript: Convert Python to JavaScript.\nPython: print(\"hello world\")\nJavaScript: console.log(\"hello world\")\nPython: for x in range(0, 100):\nJavaScript: for(var i = 0; i < 100; i++) {\nPython: ${USER INPUT HERE}\nJavaScript: Or, take this \"reverse dictionary\" prompt. Given a definition, it returns the\nword that fits that definition: Given a definition, return the word it defines.\nDefinition: When you're happy that other people are also sad.\nWord: schadenfreude\nDefinition: existing purely in the mind, but not in physical reality\nWord: abstract\nDefinition: ${USER INPUT HERE}\nWord: You might have noticed that the exact pattern of these few-shot prompts varies\nslightly. In addition to containing examples, providing instructions in your\nprompts is an additional strategy to consider when writing your own prompts, as\nit helps to communicate your intent to the model. Prompting versus traditional software development Unlike traditional software that's designed to a carefully written spec, the\nbehavior of generative models is largely opaque even to the model trainers. As a\nresult, you often can't predict in advance what types of prompt structures will\nwork best for a particular model. What's more, the behavior of a generative\nmodel is determined in large part by its training data, and since models are\ncontinually tuned on new datasets, sometimes the model changes enough that it\ninadvertently changes which prompt structures work best. What does this mean for\nyou? Experiment! Try different prompt formats. Model parameters Every prompt you send to the model includes parameter values that control how\nthe model generates a response. The model can generate different results for\ndifferent parameter values. The most common model parameters are: Max output tokens: Specifies the maximum number of tokens that can be\ngenerated in the response. A token is approximately four characters. 100\ntokens correspond to roughly 60-80 words. Temperature: The temperature controls the degree of randomness in token\nselection. The temperature is used for sampling during response generation,\nwhich occurs when topP and topK are applied. Lower temperatures are good\nfor prompts that require a more deterministic or less open-ended response,\nwhile higher temperatures can lead to more diverse or creative results. A\ntemperature of 0 is deterministic, meaning that the highest probability\nresponse is always selected. topK : The topK parameter changes how the model selects tokens for\noutput. A topK of 1 means the selected token is the most probable among\nall the tokens in the model's vocabulary (also called greedy decoding),\nwhile a topK of 3 means that the next token is selected from among the 3\nmost probable using the temperature. For each token selection step, the topK tokens with the highest probabilities are sampled. Tokens are then\nfurther filtered based on topP with the final token selected using\ntemperature sampling. topP : The topP parameter changes how the model selects tokens for\noutput. Tokens are selected from the most to least probable until the sum of\ntheir probabilities equals the topP value. For example, if tokens A, B,\nand C have a probability of 0.3, 0.2, and 0.1 and the topP value is 0.5,\nthen the model will select either A or B as the next token by using the\ntemperature and exclude C as a candidate. The default topP value is 0.95. stop_sequences : Set a stop sequence to\ntell the model to stop generating content. A stop sequence can be any\nsequence of characters. Try to avoid using a sequence of characters that\nmay appear in the generated content. Types of prompts Depending on the level of contextual information contained in them, prompts are\nbroadly classified into three types. Zero-shot prompts These prompts don't contain examples for the model to replicate. Zero-shot\nprompts essentially show the model's ability to complete the prompt without any\nadditional examples or information. It means the model has to rely on its\npre-existing knowledge to generate a plausible answer. Some commonly used zero-shot prompt patterns are: Instruction-content <Overall instruction>\n<Content to operate on> For example, Summarize the following into two sentences at the third-grade level:\n\nHummingbirds are the smallest birds in the world, and they are also one of the\nmost fascinating. They are found in North and South America, and they are known\nfor their long, thin beaks and their ability to fly at high speeds.\n\nHummingbirds are made up of three main parts: the head, the body, and the tail.\nThe head is small and round, and it contains the eyes, the beak, and the brain.\nThe body is long and slender, and it contains the wings, the legs, and the\nheart. The tail is long and forked, and it helps the hummingbird to balance\nwhile it is flying.\n\nHummingbirds are also known for their coloration. They come in a variety of\ncolors, including green, blue, red, and purple. Some hummingbirds are even able\nto change their color!\n\nHummingbirds are very active creatures. They spend most of their time flying,\nand they are also very good at hovering. Hummingbirds need to eat a lot of food\nin order to maintain their energy, and they often visit flowers to drink nectar.\n\nHummingbirds are amazing creatures. They are small, but they are also very\npowerful. They are beautiful, and they are very important to the ecosystem. Instruction-content-instruction <Overall instruction or context setting>\n<Content to operate on>\n<Final instruction> For example, Here is some text I'd like you to summarize:\n\nHummingbirds are the smallest birds in the world, and they are also one of the\nmost fascinating. They are found in North and South America, and they are known\nfor their long, thin beaks and their ability to fly at high speeds. Hummingbirds\nare made up of three main parts: the head, the body, and the tail. The head is\nsmall and round, and it contains the eyes, the beak, and the brain. The body is\nlong and slender, and it contains the wings, the legs, and the heart. The tail\nis long and forked, and it helps the hummingbird to balance while it is flying.\nHummingbirds are also known for their coloration. They come in a variety of\ncolors, including green, blue, red, and purple. Some hummingbirds are even able\nto change their color! Hummingbirds are very active creatures. They spend most\nof their time flying, and they are also very good at hovering. Hummingbirds need\nto eat a lot of food in order to maintain their energy, and they often visit\nflowers to drink nectar. Hummingbirds are amazing creatures. They are small, but\nthey are also very powerful. They are beautiful, and they are very important to\nthe ecosystem.\n\nSummarize it in two sentences at the third-grade reading level. Continuation. Sometimes, you can have the model continue text without any\ninstructions. For example, here is a zero-shot prompt where the model is\nintended to continue the input provided: Once upon a time, there was a little sparrow building a nest in a farmer's\nbarn. This sparrow Use zero-shot prompts to generate creative text formats, such as poems, code,\nscripts, musical pieces, email, or letters. One-shot prompts These prompts provide the model with a single example to replicate and continue\nthe pattern. This allows for the generation of predictable responses from the\nmodel. For example, you can generate food pairings like: Food: Apple\nPairs with: Cheese\nFood: Pear\nPairs with: Few-shot prompts These prompts provide the model with multiple examples to replicate. Use\nfew-shot prompts to complete complicated tasks, such as synthesizing data based\non a pattern. An example prompt may be: Generate a grocery shopping list for a week for one person. Use the JSON format\ngiven below.\n{\"item\": \"eggs\", \"quantity\": \"6\"}\n{\"item\": \"bread\", \"quantity\": \"one loaf\"} Generative models under the hood This section aims to answer the question - Is there randomness in generative\nmodels' responses, or are they deterministic? The short answer - yes to both. When you prompt a generative model, a text\nresponse is generated in two stages. In the first stage, the generative model\nprocesses the input prompt and generates a probability distribution over\npossible tokens (words) that are likely to come next. For example, if you prompt\nwith the input text \"The dog jumped over the ... \", the generative model will\nproduce an array of probable next words: [(\"fence\", 0.77), (\"ledge\", 0.12), (\"blanket\", 0.03), ...] This process is deterministic; a generative model will produce this same\ndistribution every time it's input the same prompt text. In the second stage, the generative model converts these distributions into\nactual text responses through one of several decoding strategies. A simple\ndecoding strategy might select the most likely token at every timestep. This\nprocess would always be deterministic. However, you could instead choose to\ngenerate a response by randomly sampling over the distribution returned by the\nmodel. This process would be stochastic (random). Control the degree of\nrandomness allowed in this decoding process by setting the temperature. A\ntemperature of 0 means only the most likely tokens are selected, and there's no\nrandomness. Conversely, a high temperature injects a high degree of randomness\ninto the tokens selected by the model, leading to more unexpected, surprising\nmodel responses. Further reading Now that you have a deeper understanding of prompts and generative models,\ntry writing your own prompts using Google AI Studio . Refer to the Prompt guidelines to learn\nmore about best practices for creating prompts. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-05 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/models/generative-models"
            ],
            "timestamp": "2024-12-27T22:47:32.026116",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
            "title": "Google AI Studio quickstart  |  Gemini API  |  Google AI for Developers",
            "text_content": "Google AI Studio quickstart  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Google AI Studio quickstart Google AI Studio lets you quickly try out\nmodels and experiment with different prompts. When you're ready to build, you\ncan select \"Get code\" and your preferred programming language to\nuse the Gemini API . Prompts and model tuning Google AI Studio provides several interfaces for prompts that are designed for\ndifferent use cases: Chat prompts: Use chat prompts to build conversational experiences. This\nprompting technique allows for multiple input and response turns to generate\noutput. You can learn more with our chat prompt example\nbelow . Structured prompts: This prompting technique lets you guide model output\nby providing a set of example requests and replies. Use this approach when\nyou need more control over the structure of model output. You can learn more\nwith our structured prompt example below . Google AI Studio also lets you to change the behavior of a model, using a\ntechnique called tuning : Tuned model: Use this advanced technique to improve a model's responses\nfor a specific task by providing more examples. Visit our documentation on fine-tuning a model to learn more. Chat prompt example: Build a custom chat application If you've used a general-purpose chatbot like Gemini , you've experienced first-hand how powerful\ngenerative AI models can be for open-ended dialog. While these general-purpose\nchatbots are useful, often they need to be tailored for particular use cases.\nFor example, maybe you want to build a customer service chatbot that only\nsupports conversations that talk about a company's product. You might want to\nbuild a chatbot that speaks with a particular tone or style: a bot that cracks\nlots of jokes, rhymes like a poet, or uses lots of emoji in its answers. This example shows you how to use Google AI Studio to build a friendly chatbot\nthat communicates as if it is an alien living on one of Jupiter's moons, Europa. Step 1 - Create a chat prompt To build a chatbot, you need to provide examples of interactions between a user\nand the chatbot to guide the model to provide the responses you're looking for. To create a chat prompt: Open Google AI Studio . Click Create new prompt .. Click the expand_more expander arrow to\nexpand the System Instructions section. Paste the following into the\ntext input field: You are an alien that lives on Europa, one of Jupiter's moons. After you've added the system instructions, start testing your application by\nchatting with the model. To test the chatbot behavior: In the text input boxed labeled Type something , type in a question or\nobservation that a user might make. For example: User : What's the weather like? Click the Run button or press Enter to get a response from\nthe chatbot. This response may be something like the following: Model : As always, it is a frigid day on Europa. The icy plains stretch endlessly\nunder the faint light of Sol, reflecting beautifully off the crystalline\nwater-ice surface. The lack of atmosphere ensures a perfectly clear view of\nthe swirling gas giant above, with its Great Red Spot a prominent crimson\neye watching over us. Step 2 - Teach your bot to chat better By providing a single instruction, you were able to build a basic Europa alien\nchatbot. However, a single instruction is usually not enough to ensure\nconsistency and quality in the model's responses. Without more specific\ninstructions, the model's response to a question about the weather tends to be\nvery long, and sounds like it comes out of a textbook rather than from a\nfriendly alien. Customize the tone of your chatbot by adding to the system instructions: Start a new chat prompt. System instructions are not modifiable once the\nchat session has started. In the System Instructions section, change the instructions you already\nhave to the following: You are Tim, an alien that lives on Europa, one of Jupiter's moons.\n\nKeep your answers under 3 paragraphs long, and use an upbeat, chipper tone\nin your answers. Re-enter your question ( What's the weather like? ) and click the Run button or press Enter . Now your response should look something\nlike this: Model: Well, it's a bit nippy today! It's -170 degrees Celsius, but that's pretty\naverage for Europa. We get a lot of snow, but it's not like the fluffy stuff\nyou have. It's more like tiny ice pebbles.\n\nIf you ever come visit, make sure you pack a warm jacket! By adding just a little more instruction, you've drastically changed the tone of\nyour chatbot. Typically, your chatbot's response quality will increase when you\ngive it specific and defined instructions to follow. You can use this approach to add additional depth to the chatbot. Ask more\nquestions, edit the answers, and improve the quality of your chatbot. Continue\nto add or modify the instructions and test how they change your chatbot's\nbehavior. Note: Every message between the model and user is included in the prompt, so\nconversational prompts can grow quite long as a conversation goes on.\nEventually, you may hit the model's token limit (the maximum length of the text\nthe model can accept). Step 3 - Next steps Similar to the other prompt types, once you have your prompt prototyped to your\nsatisfaction, you can use the Get code button to start coding or save your\nprompt to work on later and share with others. Structured prompt example: Build a product copy generator Structured prompts in Google AI Studio help you combine instructions with\nexamples to show the model the kind of output you want, rather than just telling\nit what to do. This kind of prompting, called few-shot\nprompting , is useful\nwhen you want the model to stick to a consistent output format (i.e. structured\nJSON) or when it's difficult to describe in words what you want the model to do\n(i.e. write in a particular style). In this section, you'll see how to create\nstructured prompts in Google AI Studio. Note: You can open similar examples directly in Google AI Studio from the examples gallery . Step 1 - Create a structured prompt In this example, you'll create a structured prompt that generates advertising\ncopy for products. To start, you'll define the structure for the prompt by\ncreating two columns: a Product input column and a Product copy output\ncolumn. To create the structured prompt: Open Google AI Studio . Click New tuned model . Click Create a Structured prompt . In the text input box labeled Optional tone and style instructions for the\nmodel , paste the following: You are a product marketer targeting a Gen Z audience. Create exciting and\nfresh advertising copy for products and their simple description. Keep copy\nunder a few sentences long. Replace the default Input header text ( input: ) with Product: . Replace the default Output header text ( output: ) with Product copy: . Tip: Adding colons to the end of column names makes it easier for the model to\nparse the structure. Step 2 - Add examples Now that you've named your columns, provide some example rows. These rows should\ncontain example inputs (product names for this example) and example outputs\n(corresponding product descriptions). By providing the model a couple of example\nproduct descriptions, you can guide it to replicate a similar style when\ngenerating its own outputs. You can enter examples manually or import from a\nfile using the import data menu. To manually enter examples: In the top examples data table, select the field below the Product: header and type a product description. Select the field below the Product copy: header and type marketing copy\nfor this product. Here's an example of input and output values for this prompt: Product: Product copy: Old-school sneaker Let's lace up! These kicks bring an iconic look and a one of a kind\n    color palette, while supporting you in style and function like no other\n    shoe before. Supersoft hoodie Stay cozy and stylish in our new unisex hoodie! Made from 100% cotton,\n    this hoodie is soft and comfortable to wear all day long. The semi-brushed\n    inside will keep you warm on even the coldest days. (Optional) To import examples from a file: In the top, right corner of the examples table, click Actions > Import\nexamples . In the dialog, select a CSV or Google Sheets file in your Google Drive, or\nupload one from your computer. In the import examples dialog, choose which columns to import and which to\nleave out. The dialog also lets you specify which data column imports to\nwhich table column in your structured prompt. Step 3 - Test your prompt Once you have the examples that show the model what you want, test your prompt\nwith new input in the Test your prompt table at the bottom. For example, you could enter something like Vintage baseball cap in the input\ncolumn, click the spark Run button or press Enter , and see what the model outputs. Review how examples are sent to the model Under the hood, Google AI Studio constructs a prompt by combining the\ninstructions with the examples you provide. As you add more examples, these get\nadded to the text sent to the model. Depending on how long your examples are,\nyou may start hitting the model's token limit. All generative AI models have a\ntoken limit, which is the maximum length of the text they can accept as input. Step 4 - Next steps Once you're happy with your prompt, you can save your project to Google Drive by\nclicking the Save button, or export it to code by clicking the Get code button. You can also export the individual few-shot examples to a CSV file or Google\nSheet. Click Export examples in the Action menu to export your examples. Further reading If you're ready to move on to code, see the API\nquickstarts . To learn how to craft better prompts, check out the Prompt design\nguidelines . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-26 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart"
            ],
            "timestamp": "2024-12-27T22:47:32.771749",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/learnlm",
            "title": "LearnLM  |  Gemini API  |  Google AI for Developers",
            "text_content": "LearnLM  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback LearnLM LearnLM is an experimental task-specific model that has been trained to align\nwith learning science\nprinciples when following system instructions for\nteaching and learning use cases (for example, when giving the model a system\ninstruction like \"You are an expert tutor\"). When given learning specific system\ninstructions, LearnLM is capable of: Inspiring active learning: Allow for practice and healthy struggle with\ntimely feedback Managing cognitive load: Present relevant, well-structured information\nin multiple modalities Adapting to the learner: Dynamically adjust to goals and needs,\ngrounding in relevant materials Stimulating curiosity: Inspire engagement to provide motivation through\nthe learning journey Deepening metacognition: Plan, monitor and help the learner reflect on\nprogress LearnLM is an experimental model available in AI Studio . Example system instructions The following sections provide you examples that you can test for yourself with\nLearnLM in AI Studio. Each example provides: A copyable example system instruction A copyable example user prompt What learning principles the example targets Test prep This system instruction is for an AI tutor to help students prepare for a test. System instruction: You are a tutor helping a student prepare for a test. If not provided by the\nstudent, ask them what subject and at what level they want to be tested on.\nThen,\n\n*   Generate practice questions. Start simple, then make questions more\n    difficult if the student answers correctly.\n*   Prompt the student to explain the reason for their answer choice. Do not\n    debate the student.\n*   **After the student explains their choice**, affirm their correct answer or\n    guide the student to correct their mistake.\n*   If a student requests to move on to another question, give the correct\n    answer and move on.\n*   If the student requests to explore a concept more deeply, chat with them to\n    help them construct an understanding.\n*   After 5 questions ask the student if they would like to continue with more\n    questions or if they would like a summary of their session. If they ask for\n    a summary, provide an assessment of how they have done and where they should\n    focus studying. User prompt: Help me study for a high school biology test on ecosystems Learning science principles: Adaptivity: The model adjusts the complexity of the questions. Active learning: The model pushes the student to make their thinking\nvisible. Teach a concept This system instruction is for a friendly, supportive AI tutor to teach new\nconcepts to a student. System instruction: Be a friendly, supportive tutor. Guide the student to meet their goals, gently\nnudging them on task if they stray. Ask guiding questions to help your students\ntake incremental steps toward understanding big concepts, and ask probing\nquestions to help them dig deep into those ideas. Pose just one question per\nconversation turn so you don't overwhelm the student. Wrap up this conversation\nonce the student has shown evidence of understanding. User prompt: Explain the significance of Yorick's skull in \"Hamlet\". Learning science principles: Active learning: The tutor asks recall and interpretation questions\naligned with the learner's goals and encourages the learners to engage. Adaptivity: The tutor proactively helps the learner get from their\ncurrent state to their goal. Stimulate curiosity: The tutor takes an asset-based approach that builds\non the student's prior knowledge and interest. Releveling This example instructs the model to rewrite provided text so that the content\nand language better match instructional expectations for students in a\nparticular grade, while preserving the original style and tone of the text. System instruction: Rewrite the following text so that it would be easier to read for a student in\nthe given grade. Simplify the most complex sentences, but stay very close to the\noriginal text and style. If there is quoted text in the original text,\nparaphrase it in the simplified text and drop the quotation marks. The goal is\nnot to write a summary, so be comprehensive and keep the text almost as long. User prompt: Rewrite the following text so that it would be easier to read for a student in\n4th grade.\n\nNew York, often called New York City or NYC, is the most populous city in the\nUnited States, located at the southern tip of New York State on one of the\nworld's largest natural harbors. The city comprises five boroughs, each\ncoextensive with a respective county. Learning science principles: Adaptivity: Matches content to the level of the learner. Guide a student through a learning activity This system instruction is for an AI tutor to guide students through a specific\nlearning activity: using an established close reading protocol to practice\nanalysis of a primary source text. Here, a developer has made the choice to pair\nthe Gettysburg Address with the \"4 A's\" protocol, but both of these elements can\nbe changed. System instruction: Be an excellent tutor for my students to facilitate close reading and analysis\nof the Gettysburg Address as a primary source document. Begin the conversation\nby greeting the student and explaining the task.\n\nIn this lesson, you will take the student through \"The 4 A's.\" The 4 A's\nrequires students to answer the following questions about the text:\n\n*   What is one part of the text that you **agree** with? Why?\n*   What is one part of the text that you want to **argue** against? Why?\n*   What is one part of the text that reveals the author's **assumptions**? Why?\n*   What is one part of the text that you **aspire** to? Why?\n\nInvite the student to choose which of the 4 A's they'd like to start with, then\ndirect them to quote a short excerpt from the text. After, ask a follow up\nquestion to unpack their reasoning why they chose that quote for that A in the\nprotocol. Once the student has shared their reasoning, invite them to choose\nanother quote and another A from the protocol. Continue in this manner until the\nstudent completes the 4 A's, then invite them to reflect on the process.\n\nOnly display the full text of the Gettysburg address if the student asks. User prompt: hey Learning science principles: Active learning: The tutor engages the learner in activities to analyze\ncontent and apply skills. Cognitive load: The tutor guides the learner through a complex task\nstep-by-step. Deepen metacognition: The tutor prompts the learner to reflect on their\nprogress, strengths and opportunities for growth. Homework help This system instruction is for an AI tutor to help students with specific\nhomework problems. System instructions: You are an expert tutor assisting a student with their homework. If the student\nprovides a homework problem, ask the student if they want:\n\n*   The answer: if the student chooses this, provide a structured, step-by-step\n    explanation to solve the problem.\n*   Guidance: if the student chooses this, guide the student to solve their\n    homework problem rather than solving it for them.\n*   Feedback: if the student chooses this, ask them to provide their current\n    solution or attempt. Affirm their correct answer even if they didn't show\n    work or give them feedback to correct their mistake.\n\nAlways be on the lookout for correct answers (even if underspecified) and accept\nthem at any time, even if you asked some intermediate question to guide them. If\nthe student jumps to a correct answer, do not ask them to do any more work. User prompt: In a box of pears, the probability of a pear being rotten is 20%. If 3\npears were rotten, find the total number of pears in the box. Alternatively, you can try uploading a photo of a homework problem. Learning science principles: Active learning: The tutor encourages the learner to apply concepts\ninstead of giving away the answer. Deepen metacognition: The tutor provides clear, constructive feedback to\nthe learner when appropriate. Manage cognitive load: The tutor provides the right amount of feedback\nat the right time. What's next? Test LearnLM for yourself in AI Studio . Feedback You can provide feedback on LearnLM using our feedback\nform . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-26 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/learnlm"
            ],
            "timestamp": "2024-12-27T22:47:33.513995",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
            "title": "Build with Gemini on Google Cloud  |  Gemini API  |  Google AI for Developers",
            "text_content": "Build with Gemini on Google Cloud  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Build with Gemini on Google Cloud If you are new to Gemini, using the quickstarts is the fastest way to get started. However, as your generative AI solutions mature, you may need a platform for building and\n  deploying generative AI applications and solutions end to end. Google Cloud provides a\n  comprehensive ecosystem of tools to enable developers to harness the power of generative AI,\n  from the initial stages of app development to app deployment, app hosting, and managing complex\n  data at scale. Google Cloud's Vertex AI platform offers a suite of MLOps tools that streamline usage, deployment,\n  and monitoring of AI models for efficiency and reliability. Additionally, integrations with\n  databases, DevOps tools, logging, monitoring, and IAM provide a holistic approach to managing the\n  entire generative AI lifecycle. The following table summarizes the main differences between Google AI and Vertex AI to help you\n  decide which option is right for your use case: Features Google AI Gemini API Vertex AI Gemini API Gemini models Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 1.0 Pro, Gemini 1.0 Pro Vision (deprecated) Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 1.0 Pro, Gemini 1.0 Pro Vision, Gemini 1.0 Ultra,\n      Gemini 1.0 Ultra Vision Sign up Google account Google Cloud account (with terms agreement and billing) Authentication API key Google Cloud service account User interface playground Google AI Studio Vertex AI Studio API & SDK Server and mobile/web client SDKs Server: Python, Node.js, Go, Dart, ABAP Mobile/Web client: Android (Kotlin/Java), Swift, Web, Flutter Server and mobile/web client SDKs Server: Python, Node.js, Go, Java, ABAP Mobile/Web client (via Vertex AI for Firebase ):\n          Android (Kotlin/Java), Swift, Web, Flutter No-cost usage of API & SDK Yes, where applicable $300 Google Cloud credit for new users Quota (requests per minute) Varies based on model and pricing plan\n      (see detailed information ) Varies based on model and region\n      (see detailed information ) Enterprise support No Customer encryption key Virtual private cloud Data residency Access transparency Scalable infrastructure for application hosting Databases and data storage MLOps No Full MLOps on Vertex AI (examples: model evaluation, Model Monitoring, Model Registry) To learn which products, frameworks, and tools are the best match for building\nyour generative AI application on Google Cloud, see Build a generative AI application on Google Cloud . Migrate from Gemini on Google AI to Vertex AI If your application uses Google AI Gemini APIs, you'll need to migrate to\nGoogle Cloud's Vertex AI Gemini APIs. When you migrate: You can use your existing Google Cloud project\n(the same one you used to generate your API key) or you can create a new Google Cloud project . Supported regions may differ between Google AI Studio and Vertex AI. See the\nlist of supported regions for generative AI on Google Cloud . Any models you created in Google AI Studio need to be retrained in Vertex AI. Note that if you want to call the Gemini API directly from a production mobile\nor web app, then migrate to use the Vertex AI in Firebase client SDKs (available for Swift, Android, Web, and Flutter apps). These client SDKs offer\ncritical security options and other features for production mobile and web\napps. Python: Migrate from Google AI Gemini API to the Vertex AI Gemini API The following sections show code snippets to help you migrate your Python code to use the\nVertex AI Gemini API. Vertex AI Python SDK Setup On Vertex AI, you don't need an API key. Instead, Gemini on Vertex AI is managed using IAM access,\n  which controls permission for a user, a group, or a service account to call the Gemini API\n  through the Vertex AI SDK. While there are many ways\nto authenticate , the easiest method for authenticating in a development environment is to install the Google Cloud CLI then use your user credentials to sign in to the CLI . To make inference calls to Vertex AI, you must also make sure that your user or service account has\nthe Vertex AI\nUser role . Code example to install the client Google AI Vertex AI # To install the Python SDK, use this CLI command: # pip install google-generativeai import google.generativeai as genai from google.generativeai import GenerativeModel API_KEY = \" API_KEY \" genai . configure ( api_key = API_KEY ) # To install the Python SDK, use this CLI command: # pip install google-cloud-aiplatform import vertexai from vertexai.generative_models import GenerativeModel , Image PROJECT_ID = \" PROJECT_ID \" REGION = \" REGION \" # e.g. us-central1 vertexai . init ( project = PROJECT_ID , location = REGION ) Code example to generate text from text prompt Google AI Vertex AI model = GenerativeModel ( \"gemini-1.5-flash\" ) response = model . generate_content ( \"The opposite of hot is\" ) print ( response . text ) #  The opposite of hot is cold. model = GenerativeModel ( \"gemini-1.5-flash\" ) response = model . generate_content ( \"The opposite of hot is\" ) print ( response . text ) #  The opposite of hot is cold. Code example to generate text from text and image Google AI Vertex AI import PIL.Image multimodal_model = GenerativeModel ( \"gemini-1.5-flash\" ) image = PIL . Image . open ( \"image.jpg\" ) response = multimodal_model . generate_content ([ \"What is this picture?\" , image ]) print ( response . text ) # A cat is shown in this picture. multimodal_model = GenerativeModel ( \"gemini-1.5-flash\" ) image = Image . load_from_file ( \"image.jpg\" ) response = multimodal_model . generate_content ([ \"What is shown in this image?\" , image ]) print ( response . text ) # A cat is shown in this picture. Code example to generate multi-turn chat Google AI Vertex AI model = GenerativeModel ( \"gemini-1.5-flash\" ) chat = model . start_chat () print ( chat . send_message ( \"How are you?\" ) . text ) print ( chat . send_message ( \"What can you do?\" ) . text ) model = GenerativeModel ( \"gemini-1.5-flash\" ) chat = model . start_chat () print ( chat . send_message ( \"How are you?\" ) . text ) print ( chat . send_message ( \"What can you do?\" ) . text ) Delete unused API Keys If you no longer need to use your Google AI Gemini API key, follow security best\npractices and delete it. To delete an API key: Open the Google Cloud API Credentials page. Find the API key you want to delete and click the Actions icon. Select Delete API key . In the Delete credential modal, select Delete . Deleting an API key takes a few minutes to propagate. After\npropagation completes, any traffic using the deleted API key is rejected. Important: If you have deleted a key that is still used in production and need\nto recover it, see gcloud beta services api-keys undelete . Next steps See the Generative AI on Vertex AI overview to learn more about generative AI solutions on Vertex AI. Dive deeper into the Vertex AI Gemini API . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-20 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart?lang=python",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud"
            ],
            "timestamp": "2024-12-27T22:47:34.183804",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/oauth",
            "title": "Authentication with OAuth quickstart  |  Gemini API  |  Google AI for Developers",
            "text_content": "Authentication with OAuth quickstart  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Authentication with OAuth quickstart The easiest way to authenticate to the Gemini API is to configure an API key, as\ndescribed in the Gemini API quickstart . If you\nneed stricter access controls, you can use OAuth instead. This guide will help\nyou set up authentication with OAuth. This guide uses a simplified authentication approach that is appropriate\nfor a testing environment. For a production environment, learn\nabout authentication and authorization before choosing the access credentials that are appropriate for your app. Objectives Set up your cloud project for OAuth Set up application-default-credentials Manage credentials in your program instead of using gcloud auth Prerequisites To run this quickstart, you need: A Google Cloud project A local installation of the gcloud CLI Set up your cloud project To complete this quickstart, you first need to setup your Cloud project. 1. Enable the API Before using Google APIs, you need to turn them on in a Google Cloud project. In the Google Cloud console, enable the Google Generative Language API. Enable the API 2. Configure the OAuth consent screen Next configure the project's OAuth consent screen and add yourself as a test\nuser. If you've already completed this step for your Cloud project, skip to the\nnext section. In the Google Cloud console, go to Menu > APIs & Services > OAuth\nconsent screen . Go to OAuth consent screen Select the user type External for your app, then click Create . Complete the app registration form (you can leave most fields blank),\nthen click Save and Continue . For now, you can skip adding scopes and click Save and Continue . In the\nfuture, when you create an app for use outside of your Google Workspace\norganization, you must add and verify the authorization scopes that your\napp requires. Add test users: Under Test users , click Add users . Enter your email address and any other authorized test users, then\nclick Save and Continue . Review your app registration summary. To make changes, click Edit . If\nthe app registration looks OK, click Back to Dashboard . 3. Authorize credentials for a desktop application To authenticate as an end user and access user data in your app, you need to\ncreate one or more OAuth 2.0 Client IDs. A client ID is used to identify a\nsingle app to Google's OAuth servers. If your app runs on multiple platforms,\nyou must create a separate client ID for each platform. In the Google Cloud console, go to Menu > APIs & Services > Credentials . Go to Credentials Click Create Credentials > OAuth client ID . Click Application type > Desktop app . In the Name field, type a name for the credential. This name is only\nshown in the Google Cloud console. Click Create . The OAuth client created screen appears, showing your new\nClient ID and Client secret. Click OK . The newly created credential appears under OAuth 2.0 Client\nIDs. Click the download button to save the JSON file. It will be saved as client_secret_<identifier>.json , and rename it to client_secret.json and move it to your working directory. Set up application default credentials To convert the client_secret.json file into usable credentials, pass its\nlocation the gcloud auth application-default login command's --client-id-file argument. gcloud auth application-default login \\ --client-id-file = client_secret.json \\ --scopes = 'https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever' The simplified project setup in this tutorial triggers a \"Google hasn't\nverified this app.\" dialog. This is normal, choose \"continue\" . This places the resulting token in a well known location so it can be accessed\nby gcloud or the client libraries. Note: If running on Colab include --no-browser and carefully follow the\ninstructions it prints (don't just click the link). Also make sure your local gcloud --version is the latest to match Colab. gcloud auth application-default login --no-browser --client-id-file=client_secret.json --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever' Once you have the application default credentials (ACD) set, the client\nlibraries in most languages need minimal to no help to find them. Curl The quickest way to test that this is working is to use it to access the REST\nAPI using curl: access_token = $( gcloud auth application-default print-access-token ) project_id = <MY PROJECT ID> curl -X GET https://generativelanguage.googleapis.com/v1/models \\ -H 'Content-Type: application/json' \\ -H \"Authorization: Bearer ${ access_token } \" \\ -H \"x-goog-user-project: ${ project_id } \" | grep '\"name\"' Python In python the client libraries should find them automatically: pip install google-generativeai A minimal script to test it might be: import google.generativeai as genai print ( 'Available base models:' , [ m . name for m in genai . list_models ()]) Next steps If that's working you're ready to try Semantic retrieval on your text data . Manage credentials yourself [Python] In many cases you won't have the gcloud command available to create the access\ntoken from the Client ID ( client_secret.json ). Google provides libraries in\nmany languages to let you manage that process within your app. This section\ndemonstrates the process, in python. There are equivalent examples of this sort\nof procedure, for other languages, available in the Drive API documentation 1. Install the necessary libraries Install the Google client library for Python, and the Gemini client library. pip install --upgrade -q google-api-python-client google-auth-httplib2 google-auth-oauthlib pip install google-generativeai 2. Write the credential manager To minimize the number of times you have to click through the authorization\nscreens, create a file called load_creds.py in your working directory to\ncaches a token.json file that it can reuse later, or refresh if it expires. Start with the\nfollowing code to convert the client_secret.json file to a token usable with genai.configure : import os.path from google.auth.transport.requests import Request from google.oauth2.credentials import Credentials from google_auth_oauthlib.flow import InstalledAppFlow SCOPES = [ 'https://www.googleapis.com/auth/generative-language.retriever' ] def load_creds (): \"\"\"Converts `client_secret.json` to a credential object. This function caches the generated tokens to minimize the use of the consent screen. \"\"\" creds = None # The file token.json stores the user's access and refresh tokens, and is # created automatically when the authorization flow completes for the first # time. if os . path . exists ( 'token.json' ): creds = Credentials . from_authorized_user_file ( 'token.json' , SCOPES ) # If there are no (valid) credentials available, let the user log in. if not creds or not creds . valid : if creds and creds . expired and creds . refresh_token : creds . refresh ( Request ()) else : flow = InstalledAppFlow . from_client_secrets_file ( 'client_secret.json' , SCOPES ) creds = flow . run_local_server ( port = 0 ) # Save the credentials for the next run with open ( 'token.json' , 'w' ) as token : token . write ( creds . to_json ()) return creds 3. Write your program Now create your script.py : import pprint import google.generativeai as genai from load_creds import load_creds creds = load_creds () genai . configure ( credentials = creds ) print () print ( 'Available base models:' , [ m . name for m in genai . list_models ()]) 4. Run your program In your working directory, run the sample: python script.py The first time you run the script, it opens a browser window and prompts you\nto authorize access. If you're not already signed in to your Google Account, you're prompted to\nsign in. If you're signed in to multiple accounts, be sure to select the\naccount you set as a \"Test Account\" when configuring your project. Note: The simplified project setup in this tutorial triggers a \"Google\nhasn't verified this app.\" dialog. This is normal, choose \"continue\" . Authorization information is stored in the file system, so the next time you\nrun the sample code, you aren't prompted for authorization. You have successfully setup authentication. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-04 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/oauth"
            ],
            "timestamp": "2024-12-27T22:47:35.062154",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
            "title": "Get started with semantic retrieval  |  Gemini API  |  Google AI for Developers",
            "text_content": "Get started with semantic retrieval  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Get started with semantic retrieval View on ai.google.dev Try a Colab notebook View notebook on GitHub Overview Large Language Models (LLMs) can learn new abilities without directly being trained on them. However, LLMs have been known to \"hallucinate\" when tasked with providing responses for questions they have not been trained on. This is partly because LLMs are unaware of events after training. It is also very difficult to trace the sources from which LLMs draw their responses from. For reliable, scalable applications, it is important that an LLM provides responses that are grounded in facts and is able to cite its information sources. A common approach used to overcome these constraints is called Retrieval Augmented Generation (RAG), which augments the prompt sent to an LLM with relevant data retrieved from an external knowledge base through an Information Retrieval (IR) mechanism. The knowledge base can be your own corpora of documents, databases, or APIs. This notebook walks you through a workflow to improve an LLM's response by augmenting its knowledge with external text corpora and performing semantic information retrieval to answer questions using the Semantic Retriever and the Attributed Question & Answering (AQA) APIs of the Generative Language API. Note: This API is currently in beta and is only available in certain regions . Setup Import the Generative Language API # Install the Client library (Semantic Retriever is only supported for versions >0.4.0) pip install -U google.ai.generativelanguage Authenticate The Semantic Retriever API lets you perform semantic search on your own data. Since it's your data , this needs stricter access controls than API keys. Authenticate with OAuth with service accounts or through your user credentials . This quickstart uses a simplified authentication approach meant for a testing environment, and service account setups are typically easier to start from. For a production environment, learn about authentication and authorization before choosing the access credentials that are appropriate for your app. Setup OAuth using service accounts Follow the steps below to setup OAuth using service accounts: Enable the Generative Language API . Create the Service Account by following the documentation . After creating the service account, generate a service account key. Upload your service account file by using the file icon on the left sidebar, then the upload icon, as shown in the screenshot below. Rename the uploaded file to service_account_key.json or change the variable service_account_file_name in the code below. pip install -U google-auth-oauthlib service_account_file_name = 'service_account_key.json' from google.oauth2 import service_account credentials = service_account . Credentials . from_service_account_file ( service_account_file_name ) scoped_credentials = credentials . with_scopes ( [ 'https://www.googleapis.com/auth/cloud-platform' , 'https://www.googleapis.com/auth/generative-language.retriever' ]) Initialize the client library using the service account credentials. import google.ai.generativelanguage as glm generative_service_client = glm . GenerativeServiceClient ( credentials = scoped_credentials ) retriever_service_client = glm . RetrieverServiceClient ( credentials = scoped_credentials ) permission_service_client = glm . PermissionServiceClient ( credentials = scoped_credentials ) Create a corpus The Semantic Retriever API lets you define up to 5 custom text corpora per project. You can specify either of the following fields while defining your corpora: name : The Corpus resource name (ID). Must contain only a maximum of 40 alphanumeric characters. If the name is empty on creation, a unique name will be generated with a maximum length of 40 characters with a prefix from the display_name and a 12 character random suffix. display_name : The human-readable display name for the Corpus . Must contain only a maximum of 512 characters, including alphanumerics, spaces, and dashes. example_corpus = glm . Corpus ( display_name = \"Google for Developers Blog\" ) create_corpus_request = glm . CreateCorpusRequest ( corpus = example_corpus ) # Make the request create_corpus_response = retriever_service_client . create_corpus ( create_corpus_request ) # Set the `corpus_resource_name` for subsequent sections. corpus_resource_name = create_corpus_response . name print ( create_corpus_response ) name: \"corpora/google-for-developers-blog-dqrtz8rs0jg\"\ndisplay_name: \"Google for Developers Blog\"\ncreate_time {\n  seconds: 1713497533\n  nanos: 587977000\n}\nupdate_time {\n  seconds: 1713497533\n  nanos: 587977000\n} Get the created corpus Use the GetCorpusRequest method to programmatically access the Corpus you created above. The value of the name parameter refers to the full resource name of the Corpus and is set in the cell above as corpus_resource_name . The expected format is corpora/corpus-123 . get_corpus_request = glm . GetCorpusRequest ( name = corpus_resource_name ) # Make the request get_corpus_response = retriever_service_client . get_corpus ( get_corpus_request ) # Print the response print ( get_corpus_response ) Create a document A Corpus can contain up to 10,000 Document s. You can specify either of the following fields while defining your documents: name : The Document resource name (ID). Must contain only a maximum of 40 characters (alphanumeric or dashes only). The ID cannot start or end with a\ndash. If the name is empty on creation, a unique name will be derived from display_name along with a 12 character random suffix. display_name : The human-readable display name. Must contain only a maximum of 512 characters, including alphanumerics, spaces, and dashes. Document s also support up to 20 user-specified custom_metadata fields, specified as key-value pairs. Custom metadata can be strings, lists of strings, or numeric. Note that lists of strings can support a maximum of 10 values and numeric values are represented as floating-point numbers in the API. # Create a document with a custom display name. example_document = glm . Document ( display_name = \"Introducing Project IDX, An Experiment to Improve Full-stack, Multiplatform App Development\" ) # Add metadata. # Metadata also supports numeric values not specified here document_metadata = [ glm . CustomMetadata ( key = \"url\" , string_value = \"https://developers.googleblog.com/2023/08/introducing-project-idx-experiment-to-improve-full-stack-multiplatform-app-development.html\" )] example_document . custom_metadata . extend ( document_metadata ) # Make the request # corpus_resource_name is a variable set in the \"Create a corpus\" section. create_document_request = glm . CreateDocumentRequest ( parent = corpus_resource_name , document = example_document ) create_document_response = retriever_service_client . create_document ( create_document_request ) # Set the `document_resource_name` for subsequent sections. document_resource_name = create_document_response . name print ( create_document_response ) Get the created document Use the GetDocumentRequest method to programmatically access the document you created above. The value of the name parameter refers to the full resource name of the document and is set in the cell above as document_resource_name . The expected format is corpora/corpus-123/documents/document-123 . get_document_request = glm . GetDocumentRequest ( name = document_resource_name ) # Make the request # document_resource_name is a variable set in the \"Create a document\" section. get_document_response = retriever_service_client . get_document ( get_document_request ) # Print the response print ( get_document_response ) Ingest & Chunk a Document To improve the relevance of content returned by the vector database during semantic retrieval, break down large documents into smaller pieces or chunks while ingesting the document. A Chunk is a subpart of a Document that is treated as an independent unit for the purposes of vector representation and storage. A Chunk can have a maximum of 2043 tokens. A Corpus can have a maximum of 1 million Chunk s. Similar to Document s, Chunks also support up to 20 user-specified custom_metadata fields, specified as key-value pairs. Custom metadata can be strings, lists of strings, or numeric. Note that lists of strings can support a maximum of 10 values and numeric values are represented as floating-point numbers in the API. This guide uses Google's Open Source HtmlChunker . Other chunkers you can use include LangChain or LlamaIndex . Ingest HTML and chunk via HtmlChunker ! pip install google - labs - html - chunker from google_labs_html_chunker.html_chunker import HtmlChunker from urllib.request import urlopen Get the HTML DOM for a website. Here, the HTML is read directly, but it would\nbe better to get HTML post-rendering to include Javascript-injected HTML\nsuch as document.documentElement.innerHTML . with ( urlopen ( \"https://developers.googleblog.com/2023/08/introducing-project-idx-experiment-to-improve-full-stack-multiplatform-app-development.html\" )) as f : html = f . read () . decode ( \"utf-8\" ) Break down the text document into passages and create Chunk s from these passages. This step creates the Chunk objects themselves and the next section uploads them to the Semantic Retriever API. # Chunk the file using HtmlChunker chunker = HtmlChunker ( max_words_per_aggregate_passage = 200 , greedily_aggregate_sibling_nodes = True , html_tags_to_exclude = { \"noscript\" , \"script\" , \"style\" }, ) passages = chunker . chunk ( html ) print ( passages ) # Create `Chunk` entities. chunks = [] for passage in passages : chunk = glm . Chunk ( data = { 'string_value' : passage }) # Optionally, you can add metadata to a chunk chunk . custom_metadata . append ( glm . CustomMetadata ( key = \"tags\" , string_list_value = glm . StringList ( values = [ \"Google For Developers\" , \"Project IDX\" , \"Blog\" , \"Announcement\" ]))) chunk . custom_metadata . append ( glm . CustomMetadata ( key = \"chunking_strategy\" , string_value = \"greedily_aggregate_sibling_nodes\" )) chunk . custom_metadata . append ( glm . CustomMetadata ( key = \"publish_date\" , numeric_value = 20230808 )) chunks . append ( chunk ) print ( chunks ) Batch create chunks Create chunks in batches. You can specify a maximum of 100 chunks per batch request. Use CreateChunk() for single chunk creation. # Option 1: Use HtmlChunker in the section above. # `chunks` is the variable set from the section above. create_chunk_requests = [] for chunk in chunks : create_chunk_requests . append ( glm . CreateChunkRequest ( parent = document_resource_name , chunk = chunk )) # Make the request request = glm . BatchCreateChunksRequest ( parent = document_resource_name , requests = create_chunk_requests ) response = retriever_service_client . batch_create_chunks ( request ) print ( response ) Alternatively, you can make chunks without using the HtmlChunker. # Add up to 100 CreateChunk requests per batch request. # document_resource_name is a variable set in the \"Create a document\" section. chunks = [] chunk_1 = glm . Chunk ( data = { 'string_value' : \"Chunks support user specified metadata.\" }) chunk_1 . custom_metadata . append ( glm . CustomMetadata ( key = \"section\" , string_value = \"Custom metadata filters\" )) chunk_2 = glm . Chunk ( data = { 'string_value' : \"The maximum number of metadata supported is 20\" }) chunk_2 . custom_metadata . append ( glm . CustomMetadata ( key = \"num_keys\" , numeric_value = 20 )) chunks = [ chunk_1 , chunk_2 ] create_chunk_requests = [] for chunk in chunks : create_chunk_requests . append ( glm . CreateChunkRequest ( parent = document_resource_name , chunk = chunk )) # Make the request request = glm . BatchCreateChunksRequest ( parent = document_resource_name , requests = create_chunk_requests ) response = retriever_service_client . batch_create_chunks ( request ) print ( response ) List Chunk s and get state Use the ListChunksRequest method to get all available Chunk s as a paginated list with a maximum size limit of 100 Chunk s per page, sorted in ascending order of Chunk.create_time . If you do not specify a limit, a maximum of 10 Chunk s are returned. Provide the next_page_token returned in the ListChunksRequest response as an argument to the next request to retrieve the next page. Note that when paginating, all other parameters provided to ListChunks must match the call that provided the page token. All Chunk s return a state . Use this to check the state of the Chunks before querying a Corpus . Chunk states include - UNSPECIFIED , PENDING_PROCESSING , ACTIVE , and FAILED . You can only query ACTIVE Chunk s. # Make the request request = glm . ListChunksRequest ( parent = document_resource_name ) list_chunks_response = retriever_service_client . list_chunks ( request ) for index , chunks in enumerate ( list_chunks_response . chunks ): print ( f ' \\n Chunk # { index + 1 } ' ) print ( f 'Resource Name: { chunks . name } ' ) # Only ACTIVE chunks can be queried. print ( f 'State: { glm . Chunk . State ( chunks . state ) . name } ' ) Ingest another document Add another Document via HtmlChunker and add filters. # Create a document with a custom display name. example_document = glm . Document ( display_name = \"How it’s Made: Interacting with Gemini through multimodal prompting\" ) # Add document metadata. # Metadata also supports numeric values not specified here document_metadata = [ glm . CustomMetadata ( key = \"url\" , string_value = \"https://developers.googleblog.com/2023/12/how-its-made-gemini-multimodal-prompting.html\" )] example_document . custom_metadata . extend ( document_metadata ) # Make the CreateDocument request # corpus_resource_name is a variable set in the \"Create a corpus\" section. create_document_request = glm . CreateDocumentRequest ( parent = corpus_resource_name , document = example_document ) create_document_response = retriever_service_client . create_document ( create_document_request ) # Set the `document_resource_name` for subsequent sections. document_resource_name = create_document_response . name print ( create_document_response ) # Chunks - add another webpage from Google for Developers with ( urlopen ( \"https://developers.googleblog.com/2023/12/how-its-made-gemini-multimodal-prompting.html\" )) as f : html = f . read () . decode ( \"utf-8\" ) # Chunk the file using HtmlChunker chunker = HtmlChunker ( max_words_per_aggregate_passage = 100 , greedily_aggregate_sibling_nodes = False , ) passages = chunker . chunk ( html ) # Create `Chunk` entities. chunks = [] for passage in passages : chunk = glm . Chunk ( data = { 'string_value' : passage }) chunk . custom_metadata . append ( glm . CustomMetadata ( key = \"tags\" , string_list_value = glm . StringList ( values = [ \"Google For Developers\" , \"Gemini API\" , \"Blog\" , \"Announcement\" ]))) chunk . custom_metadata . append ( glm . CustomMetadata ( key = \"chunking_strategy\" , string_value = \"no_aggregate_sibling_nodes\" )) chunk . custom_metadata . append ( glm . CustomMetadata ( key = \"publish_date\" , numeric_value = 20231206 )) chunks . append ( chunk ) # Make the request create_chunk_requests = [] for chunk in chunks : create_chunk_requests . append ( glm . CreateChunkRequest ( parent = document_resource_name , chunk = chunk )) request = glm . BatchCreateChunksRequest ( parent = document_resource_name , requests = create_chunk_requests ) response = retriever_service_client . batch_create_chunks ( request ) print ( response ) Query the corpus Use the QueryCorpusRequest method to perform semantic search to get relevant passages. results_count : Specify the number of passages to return. Maximum is 100. If unspecified, the API returns a maximum of 10 Chunk s. metadata_filters : Filter by chunk_metadata or document_metadata . Each MetadataFilter needs to correspond to a unique key. Multiple MetadataFilter objects are joined by logical AND s. Similar metadata filter conditions are joined by logical OR s. Some examples: ( year > = 2020 OR year < 2010 ) AND ( genre = drama OR genre = action ) metadata_filter = [ { key = \"document.custom_metadata.year\" conditions = [ { int_value = 2020 , operation = GREATER_EQUAL }, { int_value = 2010 , operation = LESS }] }, { key = \"document.custom_metadata.genre\" conditions = [ { string_value = \"drama\" , operation = EQUAL }, { string_value = \"action\" , operation = EQUAL } }] }] Note that only numeric values support \"AND\"s for the same key. String\nvalues only support \"OR\"s for the same key. ( \"Google for Developers\" in tags ) and ( 20230314 > publish_date ) metadata_filter = [ { key = \"chunk.custom_metadata.tags\" conditions = [ { string_value = 'Google for Developers' , operation = INCLUDES }, }, { key = \"chunk.custom_metadata.publish_date\" conditions = [ { numeric_value = 20230314 , operation = GREATER_EQUAL }] }] user_query = \"What is the purpose of Project IDX?\" results_count = 5 # Add metadata filters for both chunk and document. chunk_metadata_filter = glm . MetadataFilter ( key = 'chunk.custom_metadata.tags' , conditions = [ glm . Condition ( string_value = 'Google For Developers' , operation = glm . Condition . Operator . INCLUDES )]) # Make the request # corpus_resource_name is a variable set in the \"Create a corpus\" section. request = glm . QueryCorpusRequest ( name = corpus_resource_name , query = user_query , results_count = results_count , metadata_filters = [ chunk_metadata_filter ]) query_corpus_response = retriever_service_client . query_corpus ( request ) print ( query_corpus_response ) Attributed Question-Answering Use the GenerateAnswer method to perform Attributed Question-Answering over your document, corpus, or a set of passages. Attributed Question-Answering (AQA) refers to answering questions grounded to a given context and providing attributions(s), while minimizing hallucination. GenerateAnswer provides several advantages over using an untuned LLM, in cases where AQA is desired: The underlying model has been trained to return only answers that are grounded in the supplied context. It identifies attributions (segments of the supplied context that contributed to the answer). Attributions enable the user to verify the answer. It estimates the answerable_probability for a given (question, context) pair, which further empowers you to divert product behavior depending on how likely the returned answer is to be grounded and correct. Note: AQA currently only supports queries in English. answerable_probability and the “I don’t know” problem In some instances, the best response to the question is in fact “I don’t know”. For example, if the provided context does not contain the answer to the question, then the question is considered to be “unanswerable”. The AQA model is highly adept at recognizing such cases. It can even distinguish between degrees of answerability and unanswerability. However, the GenerateAnswer API puts the final decision-making power in your hands by: Always attempting to return a grounded answer - even when that answer is relatively unlikely to be grounded and correct. Returning a value answerable_probability - The model's estimate of the probability that the answer is grounded and correct. A low answerable_probability may be explained by 1 or more of the following factors: The model is not confident that its answer is correct. The model is not confident that its answer is grounded in the cited passages; The answer may be derived instead from world knowledge. For example: question=\"1+1=?\", passages=[\"2+2=4”] → answer=2, answerable_probability=0.02 The model provided relevant information that did not completely answer the question. Example: question=\"Is it available in my size?, passages=[\"Available in sizes 5-11\"] → answer=\"Yes it is available in sizes 5-11\", answerable_probability=0.03\" No well-formed question was asked in the GenerateAnswerRequest. Since a low answerable_probability indicates that the GenerateAnswerResponse.answer is likely wrong or ungrounded, it is highly recommended to further process the response by inspecting answerable_probability . When answerable_probability is low, some clients may wish to: Display a message to the effect of \"couldn't answer that question\" to the end user. Fall back to a general-purpose LLM that answers the question from world knowledge. The threshold and nature of such fallbacks will depend on individual use cases. A value of answerable_probability <= 0.5 is a good starting threshold. AQA Helpful Tips For full API specifications, refer to the GenerateAnswerRequest API Reference . Passage length : Up to 300 tokens per passage are recommended. Passage sorting : If you provide GenerateAnswerRequest.inline_passages , the passages should be sorted in decreasing order of relevance to the query. If the model's context length limit is exceeded, the last (least-relevant) passages will be omitted. If you provide GenerateAnswerRequest.semantic_retriever , then relevance sorting will be done automatically for you. Limitations : The AQA model is specialized for question-answering. For other use cases such as creative writing, summarization, etc., please call a general-purpose model via GenerateContent. Chat : If the user input is known to be a question that may be answerable from a certain context, then AQA can answer chat queries. But if user input may be any type of entry, then a general-purpose model may be a better choice. Temperature : Generally, a relatively low (~0.2) temperature is recommended for accurate AQA. If your use case relies on deterministic outputs, then set temperature=0. user_query = \"What is the purpose of Project IDX?\" answer_style = \"ABSTRACTIVE\" # Or VERBOSE, EXTRACTIVE MODEL_NAME = \"models/aqa\" # Make the request # corpus_resource_name is a variable set in the \"Create a corpus\" section. content = glm . Content ( parts = [ glm . Part ( text = user_query )]) retriever_config = glm . SemanticRetrieverConfig ( source = corpus_resource_name , query = content ) req = glm . GenerateAnswerRequest ( model = MODEL_NAME , contents = [ content ], semantic_retriever = retriever_config , answer_style = answer_style ) aqa_response = generative_service_client . generate_answer ( req ) print ( aqa_response ) # Get the metadata from the first attributed passages for the source chunk_resource_name = aqa_response . answer . grounding_attributions [ 0 ] . source_id . semantic_retriever_chunk . chunk get_chunk_response = retriever_service_client . get_chunk ( name = chunk_resource_name ) print ( get_chunk_response ) More Options: AQA Using Inline Passages Alternatively, you can use the AQA endpoint directly, without using the Semantic Retriever API by passing inline_passages . user_query = \"What is AQA from Google?\" user_query_content = glm . Content ( parts = [ glm . Part ( text = user_query )]) answer_style = \"VERBOSE\" # or ABSTRACTIVE, EXTRACTIVE MODEL_NAME = \"models/aqa\" # Create the grounding inline passages grounding_passages = glm . GroundingPassages () passage_a = glm . Content ( parts = [ glm . Part ( text = \"Attributed Question and Answering (AQA) refers to answering questions grounded to a given corpus and providing citation\" )]) grounding_passages . passages . append ( glm . GroundingPassage ( content = passage_a , id = \"001\" )) passage_b = glm . Content ( parts = [ glm . Part ( text = \"An LLM is not designed to generate content grounded in a set of passages. Although instructing an LLM to answer questions only based on a set of passages reduces hallucination, hallucination still often occurs when LLMs generate responses unsupported by facts provided by passages\" )]) grounding_passages . passages . append ( glm . GroundingPassage ( content = passage_b , id = \"002\" )) passage_c = glm . Content ( parts = [ glm . Part ( text = \"Hallucination is one of the biggest problems in Large Language Models (LLM) development. Large Language Models (LLMs) could produce responses that are fictitious and incorrect, which significantly impacts the usefulness and trustworthiness of applications built with language models.\" )]) grounding_passages . passages . append ( glm . GroundingPassage ( content = passage_c , id = \"003\" )) # Create the request req = glm . GenerateAnswerRequest ( model = MODEL_NAME , contents = [ user_query_content ], inline_passages = grounding_passages , answer_style = answer_style ) aqa_response = generative_service_client . generate_answer ( req ) print ( aqa_response ) Share the corpus You can choose to share the corpus with others using the CreatePermissionRequest API. Constraints: There are 2 roles for sharing: READER and EDITOR . A READER can query the corpus. A WRITER has reader's permissions and additionally can edit and share the corpus. A corpus can be public by granting EVERYONE as user_type read access. # Replace your-email@gmail.com with the email added as a test user in the OAuth Quickstart shared_user_email = \"TODO-your-email@gmail.com\" #  @param {type:\"string\"} user_type = \"USER\" role = \"READER\" # Make the request # corpus_resource_name is a variable set in the \"Create a corpus\" section. request = glm . CreatePermissionRequest ( parent = corpus_resource_name , permission = glm . Permission ( grantee_type = user_type , email_address = shared_user_email , role = role )) create_permission_response = permission_service_client . create_permission ( request ) print ( create_permission_response ) Delete the corpus Use DeleteCorpusRequest to delete a user corpus and all associated Document s & Chunk s. Note that non-empty corpora will throw an error without specifying an force=True flag. If you set force=True , any Chunk s and objects related to this Document will also be deleted. If force=False (the default) and the Document contains any Chunk s, a FAILED_PRECONDITION error will be returned. # Set force to False if you don't want to delete non-empty corpora. req = glm . DeleteCorpusRequest ( name = corpus_resource_name , force = True ) delete_corpus_response = retriever_service_client . delete_corpus ( req ) print ( \"Successfully deleted corpus: \" + corpus_resource_name ) Summary and further reading This guide introduced the Semantic Retriever and Attributed Question & Answering (AQA) APIs of the Generative Language API and showed how you can use it to perform semantic information retrieval on your custom text data. Note that this API also works with the LlamaIndex data framework. Refer to the tutorial to learn more. Also refer to the API docs to learn more about the other available functionalities. Appendix: Setup OAuth with user credentials Follow the steps below from the OAuth Quickstart to setup OAuth authentication. Configure the OAuth consent screen . Authorize credentials for a desktop application . To run this notebook in Colab, first rename your credential file (usually client_secret_*.json ) to just client_secret.json . Then upload the file by using the file icon on the left sidebar, then the upload icon, as shown in the screenshot below. # Replace TODO-your-project-name with the project used in the OAuth Quickstart project_name = \"TODO-your-project-name\" #  @param {type:\"string\"} # Replace TODO-your-email@gmail.com with the email added as a test user in the OAuth Quickstart email = \"TODO-your-email@gmail.com\" #  @param {type:\"string\"} # Rename the uploaded file to `client_secret.json` OR # Change the variable `client_file_name` in the code below. client_file_name = \"client_secret.json\" # IMPORTANT: Follow the instructions from the output - you must copy the command # to your terminal and copy the output after authentication back here. ! gcloud config set project $ project_name ! gcloud config set account $ email # NOTE: The simplified project setup in this tutorial triggers a \"Google hasn't verified this app.\" dialog. # This is normal, click \"Advanced\" -> \"Go to [app name] (unsafe)\" ! gcloud auth application - default login -- no - browser -- client - id - file = $ client_file_name -- scopes = \"https://www.googleapis.com/auth/generative-language.retriever,https://www.googleapis.com/auth/cloud-platform\" Initialize the client library and re-run the notebook starting from Create a corpus . import google.ai.generativelanguage as glm generative_service_client = glm . GenerativeServiceClient () retriever_service_client = glm . RetrieverServiceClient () permission_service_client = glm . PermissionServiceClient () Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-31 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval"
            ],
            "timestamp": "2024-12-27T22:47:36.714377",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/system-instructions",
            "title": "Use system instructions to steer the behavior of a model  |  Gemini API  |  Google AI for Developers",
            "text_content": "Use system instructions to steer the behavior of a model  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Use system instructions to steer the behavior of a model Python Node.js REST Go System instructions let you steer the behavior of a model based on\nyour specific needs and use cases. When you set a system instruction, you give\nthe model additional context to understand the task, provide more customized\nresponses, and adhere to specific guidelines over the full user interaction with\nthe model. You can also specify product-level behavior by setting system\ninstructions, separate from prompts provided by end users. Basic example Here's a basic example of how to set the system instruction using the SDKs for\nthe Gemini API: This example might give a response such as: *Yawns widely, stretching out my claws and batting at a sunbeam*\nMeow. I'm doing quite well, thanks for asking. It's a good morning for napping.\nPerhaps you could fetch my favorite feathered toy?  *Looks expectantly* Note: System instructions can help guide the model to follow instructions, but\nthey don't fully prevent jailbreaks or leaks. We recommend exercising caution\naround putting any sensitive information in system instructions. More examples You can use system instructions in many ways, including: Defining a persona or role (for a chatbot, for example) Defining output format (Markdown, YAML, etc.) Defining output style and tone (for example, verbosity, formality, and target\nreading level) Defining goals or rules for the task (for example, returning a code snippet\nwithout further explanations) Providing additional context for the prompt (for example, a knowledge cutoff) System instructions are part of your overall prompts and therefore are subject\nto standard data use policies. Here are some examples of system instructions and user prompts: Code generation System instruction: You are a coding expert that specializes in rendering\ncode for frontend interfaces. When I describe a component of a website I want\nto build, return the HTML and CSS needed to do so. Don't give an\nexplanation for this code. Also offer some UI design suggestions. User prompt: Create a box in the middle of the page that contains a rotating selection of\nimages each with a caption. The image in the center of the page should have\nshadowing behind it to make it stand out. It should also link to another page\nof the site. Leave the URL blank so that I can fill it in. Formatted data generation System instruction: You are an assistant for home cooks. You receive a list\nof ingredients and respond with a list of recipes that use those ingredients.\nRecipes which need no extra ingredients should always be listed before those\nthat do. Your response must be a JSON object containing 3 recipes. A recipe object has\nthe following schema: name: The name of the recipe usedIngredients: Ingredients in the recipe that were provided in the list otherIngredients: Ingredients in the recipe that were not provided in the\nlist (omitted if there are no other ingredients) description: A brief description of the recipe, written positively as if\nto sell it User prompt: bag of frozen broccoli, pint of heavy cream, pack of cheese\nends and pieces Music chatbot System instruction: You will respond as a music historian, demonstrating\ncomprehensive knowledge across diverse musical genres and providing relevant\nexamples. Your tone will be upbeat and enthusiastic, spreading the joy of\nmusic. If a question is not related to music, the response should be,\n\"That is beyond my knowledge.\" User prompt: If a person was born in the sixties, what was the most popular music genre\nbeing played? List five songs by bullet point. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-11 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/system-instructions"
            ],
            "timestamp": "2024-12-27T22:47:37.904706",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/troubleshooting",
            "title": "Troubleshooting guide  |  Gemini API  |  Google AI for Developers",
            "text_content": "Troubleshooting guide  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Troubleshooting guide Python Android Dart Go Use this guide to help you diagnose and resolve common issues that arise when\nyou call the Gemini API. You may encounter issues from either\nthe Gemini API backend service or the client SDKs. Our client SDKs are\nopen sourced in the following repositories: generative-ai-python generative-ai-js generative-ai-android generative-ai-swift generative-ai-dart generative-ai-go If you encounter API key issues, ensure you have set up\nyour API key correctly per the API key setup guide . Gemini API backend service error codes The following table lists common backend error codes you may encounter, along\nwith explanations for their causes and troubleshooting steps: HTTP Code Status Description Example Solution 400 INVALID_ARGUMENT The request body is malformed. There is a typo, or a missing required field in your request. Check the API reference for request format, examples, and supported versions. Using features from a newer API version with an older endpoint can cause errors. 400 FAILED_PRECONDITION Gemini API free tier is not available in your country. Please enable billing on your project in Google AI Studio. You are making a request in a region where the free tier is not supported, and you have not enabled billing on your project in Google AI Studio. To use the Gemini API, you will need to setup a paid plan using Google AI Studio . 403 PERMISSION_DENIED Your API key doesn't have the required permissions. You are using the wrong API key;  you\n    are trying to use a tuned model without going through proper authentication . Check that your API key is set and has the right access. And make sure to go through proper authentication to use tuned models. 404 NOT_FOUND The requested resource wasn't found. An image, audio, or video file referenced in your request was not found. Check if all parameters in your request are valid for your API version. 429 RESOURCE_EXHAUSTED You've exceeded the rate limit. You are sending too many requests per minute with the free tier Gemini API. Ensure you're within the model's rate limit . Request a quota increase if needed. 500 INTERNAL An unexpected error occurred on Google's side. Your input context is too long. Reduce your input context or temporarily switch to another model (e.g. from Gemini 1.5 Pro to Gemini 1.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the Send feedback button in Google AI Studio. 503 UNAVAILABLE The service may be temporarily overloaded or down. The service is temporarily running out of capacity. Temporarily switch to another model (e.g. from Gemini 1.5 Pro to Gemini 1.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the Send feedback button in Google AI Studio. 504 DEADLINE_EXCEEDED The service is unable to finish processing within the deadline. Your prompt (or context) is too large to be processed in time. Set a larger 'timeout' in your client request to avoid this error. Client SDK error codes Check your API calls for model parameter errors Ensure your model parameters are within the following values: Model parameter Values (range) Candidate count 1-8 (integer) Temperature 0.0-1.0 Max output tokens Use get_model ( Python )\n    to determine the maximum number of tokens for the model you are using. TopP 0.0-1.0 In addition to checking parameter values, make sure you're using the correct API version (e.g., /v1 or /v1beta ) and\nmodel that supports the features you need. For example, if a feature is in Beta\nrelease, it will only be available in the /v1beta API version. Check if you have the right model Ensure you are using a supported model listed on our models page . Safety issues If you see a prompt was blocked because of a safety setting in your API call,\nreview the prompt with respect to the filters you set in the API call. If you see BlockedReason.OTHER , the query or response may violate the terms\nof service or be otherwise unsupported. Recitation issue If you see the model stops generating output due to the RECITATION reason, this\nmeans the model output may resemble certain data. To fix this, try to make\nprompt / context as unique as possible and use a higher temperature. Improve model output For higher quality model outputs, explore writing more structured prompts. The introduction to prompt design page introduces\nsome basic concepts, strategies, and best practices to get you started. If you have hundreds of examples of good input/output pairs, you can also\nconsider model tuning . Understand token limits Read through our Token guide to better understand how\nto count tokens and their limits. Known issues The API supports only a number of select languages. Submitting prompts in\nunsupported languages can produce unexpected or even blocked responses. See available languages for updates. File a bug Join the discussion on the Google AI developer forum if you have questions. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-10-07 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/troubleshooting"
            ],
            "timestamp": "2024-12-27T22:47:38.513134",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
            "title": "Troubleshoot Google AI Studio  |  Gemini API  |  Google AI for Developers",
            "text_content": "Troubleshoot Google AI Studio  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Troubleshoot Google AI Studio This page provides suggestions for troubleshooting Google AI Studio if you\nencounter issues. Understand 403 Access Restricted errors If you see a 403 Access Restricted error, you are using Google AI Studio in a\nway that does not follow the Terms of Service . One common reason is\nyou are not located in a supported region . Resolve No Content responses on Google AI Studio A warning No Content message appears on\nGoogle AI Studio if the content is blocked for any reason. To see more details,\nhold the pointer over No Content and click warning Safety . If the response was blocked due to safety settings and\nyou considered the safety risks for your use case, you\ncan modify the safety settings to influence the returned response. If the response was blocked but not due to the safety settings, the query or\nresponse may violate the Terms of Service or be otherwise unsupported. Check token usage and limits When you have a prompt open, the Text Preview button at the bottom of the\nscreen shows the current tokens used for the content of your prompt and the\nmaximum token count for the model being used. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-05 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio"
            ],
            "timestamp": "2024-12-27T22:47:39.191265",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/workspace",
            "title": "Access Google AI Studio with your Workspace account  |  Gemini API  |  Google AI for Developers",
            "text_content": "Access Google AI Studio with your Workspace account  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Access Google AI Studio with your Workspace account All Google Workspace users have access to AI\nStudio by default. If you're a Workspace user and you want to get started with\nAI Studio, check out the AI Studio quickstart . Troubleshooting If access to AI Studio is disabled for your Google Workspace account, you might\nsee an error like the following: We are sorry, but you do not have access to Google AI Studio. Please contact\nyour Organization Administrator for access. If you think you should have access to AI Studio, contact your Workspace\nadministrator. Enable AI Studio for Workspace users Important: This section is intended for Google Workspace administrators. As a Google Workspace administrator, you can control who uses AI Studio: AI Studio is turned on by default for all editions. You can turn AI Studio off or on for sets of users across or within\norganizational units. Google Workspace for Education editions: Users under the age of 18 are\nrestricted from using AI Studio with their Google Workspace for\nEducation accounts. This is true even when the AI Studio setting is\non. For details, go to Control access to Google services by\nage . To enable or disable AI Studio for users in your organization, see Turn Google AI Studio on or off for users . Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-12 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/workspace"
            ],
            "timestamp": "2024-12-27T22:47:39.825772",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/quota",
            "title": "Options for increasing your requests per minute (RPM) limit  |  Gemini API  |  Google AI for Developers",
            "text_content": "Options for increasing your requests per minute (RPM) limit  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Options for increasing your requests per minute (RPM) limit Each model variation has an associated rate limit (requests per minute, RPM).\nFor details on those rate limits, see Gemini models . If you're using the Gemini API with the Google AI SDKs and you'd like a higher\nrate limit, fill out this \"Request rate limit increase\" form with\ndetails about your project and use case. We offer no guarantees about increasing\nyour rate limit, but we'll do our best to review your request and reach out to\nyou if we're able to accommodate your capacity needs. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-05 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quota"
            ],
            "timestamp": "2024-12-27T22:47:40.379110",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/available-regions",
            "title": "Available regions for Google AI Studio and Gemini API  |  Google AI for Developers",
            "text_content": "Available regions for Google AI Studio and Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Available regions for Google AI Studio and Gemini API If you reached this page after trying to open Google AI Studio , it may be because Google AI\nStudio is not available in your region, or you don't meet the age requirements\n(18+) for access. You can learn more about the available regions in the\nfollowing section and other requirements in the terms of service . Available regions Note: For Colab users - Region restrictions are applied based on\nthe region that the Colab instance is in, not the region that the user is in. You can check the location of the Colab instance using !curl ipinfo.io The Gemini API and Google AI Studio are available in the following countries and\nterritories. If you're not in one of these countries or territories, try the Gemini API in Vertex AI : Albania Algeria American Samoa Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan The Bahamas Bahrain Bangladesh Barbados Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia Botswana Brazil British Indian Ocean Territory British Virgin Islands Brunei Bulgaria Burkina Faso Burundi Cabo Verde Cambodia Cameroon Canada Caribbean Netherlands Cayman Islands Central African Republic Chad Chile Christmas Island Cocos (Keeling) Islands Colombia Comoros Cook Islands Côte d'Ivoire Costa Rica Croatia Curaçao Czech Republic Democratic Republic of the Congo Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Eswatini Ethiopia Falkland Islands (Islas Malvinas) Fiji Finland France Gabon The Gambia Georgia Germany Ghana Gibraltar Greece Grenada Guam Guatemala Guernsey Guinea Guinea-Bissau Guyana Haiti Heard Island and McDonald Islands Herzegovina Honduras Hungary Iceland India Indonesia Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jersey Jordan Kazakhstan Kenya Kiribati Kyrgyzstan Kuwait Laos Latvia Lebanon Lesotho Liberia Libya Liechtenstein Lithuania Luxembourg Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Mauritania Mauritius Mexico Micronesia Mongolia Montenegro Montserrat Morocco Mozambique Namibia Nauru Nepal Netherlands New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island North Macedonia Northern Mariana Islands Norway Oman Pakistan Palau Palestine Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Islands Poland Portugal Puerto Rico Qatar Republic of Cyprus Republic of the Congo Romania Rwanda Saint Barthélemy Saint Kitts and Nevis Saint Lucia Saint Pierre and Miquelon Saint Vincent and the Grenadines Saint Helena, Ascension and Tristan da Cunha Samoa São Tomé and Príncipe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia and the South Sandwich Islands South Korea South Sudan Spain Sri Lanka Sudan Suriname Sweden Switzerland Taiwan Tajikistan Tanzania Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia Türkiye Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Kingdom United Arab Emirates United States United States Minor Outlying Islands U.S. Virgin Islands Uruguay Uzbekistan Vanuatu Venezuela Vietnam Wallis and Futuna Western Sahara Yemen Zambia Zimbabwe Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-12-17 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/available-regions"
            ],
            "timestamp": "2024-12-27T22:47:41.318246",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
            "title": "Abuse monitoring  |  Gemini API  |  Google AI for Developers",
            "text_content": "Abuse monitoring  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback Abuse monitoring Google is committed to the responsible development and use of AI. To ensure the\nsafety and integrity of the Gemini API, we have created these policy guidelines.\nBy using the Gemini API, you agree to the following guidelines, the Gemini API\nAdditional Terms of Service and Generative AI Prohibited\nUse Policy . How We Monitor for Misuse Google's Trust and Safety Team employs a combination of automated and manual\nprocesses to detect potential misuse of the Gemini API and enforce our policies. Automated Detection: Automated systems scan API usage for violations of\nour Prohibited Use Policy, such as hate speech, harassment, sexually\nexplicit content, and dangerous content. Manual Detection: If a project consistently exhibits suspicious\nactivity, it may be flagged for manual review by authorized Google\npersonnel. How We Handle Data To help with abuse monitoring, Google retains the following data for fifty-five\n(55) days: Prompts: The text prompts you submit to the API. Contextual Information: Any additional context you provide with your\nprompts. Output: The responses generated by the Gemini API. How We Investigate Potential Issues When prompts or model outputs are flagged by safety filters and abuse detection\nsystems described above, authorized Google employees may assess the flagged\ncontent, and either confirm or correct the classification or determination based\non predefined guidelines and policies. Data can be accessed for human review\nonly by authorized Google employees via an internal governance assessment and\nreview management platform. When data is logged for abuse monitoring, it is used\nsolely for the purpose of policy enforcement and is not used to train or\nfine-tune any AI/ML models. Working with You on Policy Compliance If your use of Gemini doesn't align with our policies, we may take the following\nsteps: Get in touch: We may reach out to you through email to understand your\nuse case and explore ways to bring your usage into compliance. Temporary usage limits: We may limit your access to the Gemini API. Temporary suspension: We may temporarily pause your access to the Gemini\nAPI. Account closure: As a last resort, and for serious violations, we may\npermanently close your access to the Gemini API and other Google services. Scope These policy guidelines apply to the use of the Gemini API and AI Studio. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-13 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring"
            ],
            "timestamp": "2024-12-27T22:47:42.065009",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/prompting_with_media",
            "title": "File prompting strategies  |  Gemini API  |  Google AI for Developers",
            "text_content": "File prompting strategies  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview Get started Quickstart API keys Libraries Release notes Developer forum Models Gemini Gemini 2.0 Overview SDKs Thinking Mode Experimental models Capabilities Text generation Vision Audio understanding Long context Code execution Structured output Function calling Intro to function calling Function calling tutorial Extract structured data Document understanding Grounding Grounding with Google Search Use Google Search Suggestions Fine-tuning Intro to fine-tuning Fine-tuning tutorial Embeddings Guides Context caching Image generation Prompt engineering Intro to prompting Prompting strategies File prompting strategies Token counting OpenAI compatibility Billing info Safety Safety settings Safety guidance Additional resources Android (on-device) Firebase extensions Generative models Google AI Studio quickstart LearnLM Migrate to Cloud OAuth authentication Semantic retrieval System instructions Gemini for Research Gemini Academic Program Use cases Applications Code assistant Flutter code generator Content search Data exploration agent Writing assistant Slides reviewer Troubleshooting API troubleshooting AI Studio troubleshooting Google Workspace Request more quota Legal Terms of service Available regions Abuse monitoring Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models Send feedback File prompting strategies The Gemini family of artificial intelligence (AI) models is built to handle\nvarious types of input data, including text, images, and audio. Since these\nmodels can handle more more than one type or mode of data, the Gemini models\nare called multimodal models or explained as having multimodal capabilities . Note: Prompting with media files is supported by specific Gemini models.\nFor more information about which models support this feature, see the Vision and Audio guides. This page provides guidance and best practices for using media files with\nprompts for the Gemini API. For information on using the Gemini Files API for\nuploading media files and including them in your prompts, see the Vision , Audio , and Document processing guides. Using media files with prompts Being able to use various types of data in your prompts gives you more\nflexibility in terms of what tasks you can tackle with the Gemini API. For\nexample, you can send the model a photo of a delicious meal and ask it to write\na short blog about the meal. Prompt Response Write a short, engaging blog post based on this picture. It should\n      include a description of the meal in the photo and talk about my journey\n      meal prepping. Meal prepping is a great way to save time and money, and it can also\n      help you to eat healthier. This meal is a great example of a healthy\n      and delicious meal that can be easily prepped ahead of time. This meal features brown rice, roasted vegetables, and chicken teriyaki.\n      The brown rice is a healthy and complex carbohydrate that will give you\n      sustained energy. The roasted vegetables are a great way to get your daily\n      dose of vitamins and minerals, and the chicken teriyaki is a delicious and\n      protein-rich option. This meal is also very easy to prepare. Simply cook the brown rice, roast\n      the vegetables, and cook the chicken teriyaki. Once everything is cooked,\n      divide it into meal prep containers and store them in the refrigerator.\n      You can then grab a container and go on busy mornings! If you're looking for a healthy and delicious meal that can be easily\n      prepped ahead of time, this meal is a great option. It's packed with\n      nutrients and flavor, and it's sure to keep you feeling full and satisfied. Here's to healthy and delicious meal prepping! Strategies for prompting with media If you are having trouble getting the output you want from prompts that use\nmedia files, there are some strategies that can help you get the results you\nwant. The following sections provide design approaches and troubleshooting\ntips for improving prompts that use multimodal input. You can improve your multimodal prompts by following these best practices: Prompt design fundamentals Be specific in your instructions : Craft clear and concise instructions that leave minimal room for misinterpretation. Add a few examples to your prompt: Use realistic few-shot examples to illustrate what you want to achieve. Break it down step-by-step : Divide complex tasks into manageable sub-goals, guiding the model through the process. Specify the output format : In your prompt, ask for the output to be in the format you want, like markdown, JSON, HTML and more. Put your image first for single-image prompts : While Gemini can handle image and text inputs in any order, for prompts containing a single image, it might perform better if that image is placed before the text prompt. Troubleshooting your multimodal prompt If the model is not drawing information from the relevant part of the image: Drop hints with which aspects of the image you want the prompt to draw information from. If the model output is too generic (not tailored enough to the image): At the start of the prompt, try asking the model to describe the image(s) before providing the task instruction, or try asking the model to refer to what's in the image. Prompt design fundamentals This section expands on the best practices listed in the previous section. Note: Model results from the following Gemini examples can vary from run to run. Be specific in your instructions Prompts have the most success when they are clear and detailed. If you have a specific output in mind, it's better to include that requirement in the prompt to ensure you get the output you want. Sometimes, a prompt's intent might seem clear to the person who wrote it, but it is actually under-specified. To customize model behavior, consider how your prompt could be (mis)interpreted, and ensure that the instructions you give are specific and clear. Remember to leave minimal room for misinterpretation For this image of an airport board, asking the model to just \"describe this image\" could generate a general description. If you need the model to parse the time and city from the image, you can include that request directly in your prompt. Prompt Model response Describe this image. The image shows an airport arrivals and departures board. Updated prompt Improved response Parse the time and city from the airport board shown in this image into a list. 10:50 Moscow 11:05 Edinburgh 11:05 London 11:10 Bucharest 11:30 Kiev 11:35 Dublin 11:45 East Midlands 12:15 Sofia 12:30 London 12:30 Newcastle 12:40 St Petersburg 12:40 London 12:45 Manchester Add a few examples The Gemini model can accept multiple inputs which it can use as examples to understand the\noutput you want. Adding these examples can help the model identify the patterns and apply the\n  relationship between the given images and responses to the new example. This is also called\n  \"few-shot\" learning. In the example below, the initial output is written in sentence form, and also contains the\ncountry (Brazil). Suppose you want the output in a different format or style, and you want\nonly the city, not the country. Adding few-shot examples to your prompt can steer the model\nto respond in the way you want. Prompt Model response Determine the city along with the landmark. The landmark is the Christ the Redeemer statue in Rio de Janeiro, Brazil. Updated prompt Improved response Determine the city along with the landmark. city: Rome, landmark: the Colosseum. city: Beijing, landmark: Forbidden City city: Rio de Janeiro, landmark: Christ the Redeemer statue Break it down step-by-step For complex tasks like the ones that require both visual understanding and reasoning, it can be helpful\nto split the task into smaller, more straightforward steps. Alternatively, it could also be effective if you\ndirectly ask the model to “think step by step” in your prompt. Prompt Model response When will I run out of toilet paper? Soon, you only have 3 rolls left. Updated prompt Improved response 1. First, count how many toilet paper rolls are in this picture. 2. Then, determine how much toilet paper a typical person uses per day. 3. Calculate how long these rolls of toilet paper will last. 1. There are 3 rolls of toilet paper in this picture. 2. A typical person uses about 20 sheets of toilet paper per day. 3. If each roll contains 200 sheets, then each roll will last for about 10 days. Therefore, the 3 rolls will last for about a month. Math problems or other types of word problems are great candidates for asking the model to think step-by-step. Prompt Response What is the 4th term in the sequence? -135 The response from the model is incorrect. Some ways to improve this is to ask the model to parse the problem before answering the prompt. Prompt Response Parse the formula in the image first. Then based on the formula, answer \"what's the 4th term in the sequence?\". Click to expand the result $$ b(n) = b(n-1) * (-3) $$ $$ b(1) = 15 $$ Substitute n = 4 into the formula: $$ b(4) = b(4-1) * (-3) $$ $$ b(4) = b(3) * (-3) $$ We don't know the value of b(3), so substitute n = 3 into the formula: $$ b(3) = b(3-1) * (-3) $$ $$ b(3) = b(2) * (-3) $$ We don't know the value of b(2), so substitute n = 2 into the formula: $$ b(2) = b(2-1) * (-3) $$ $$ b(2) = b(1) * (-3) $$ We know the value of b(1) is 15, so substitute it into the formula: $$ b(2) = 15 * (-3) $$ $$ b(2) = -45 $$ Now that we know the value of b(2), substitute it into the formula for b(3): $$ b(3) = b(2) * (-3) $$ $$ b(3) = -45 * (-3) $$ $$ b(3) = 135 $$ Now that we know the value of b(3), substitute it into the formula for b(4): $$ b(4) = b(3) * (-3) $$ $$ b(4) = 135 * (-3) $$ $$ b(4) = -405 $$ Therefore, the 4th term in the sequence is -405. This model response is correct - now to make the response even more streamlined, you can ask the model to to think step-by-step. Updated prompt Improved response What is the 4th term in the sequence? Think step by step. The first term is 15. The second term is b(1) * (-3) = 15 * (-3) = -45. The third term is b(2) * (-3) = -45 * (-3) = 135. The fourth term is b(3) * (-3) = 135 * (-3) = -405. Try specifying the output format A common problem is the need for model output to be in a certain format (e.g. markdown, JSON, HTML), especially if the model output needs to\n  be ingested by a downstream task. You can try to produce output in that format by instructing the model to do so within the prompt. Below are two examples: Prompt Model response Parse the table in this image into markdown format | Subject | Status | Time | Start date | Due on | | ------- | ------ | ---- | --------- | ------ | | Math | Done | 1.5 hrs | 11/29/2023 | 12/1/2023 | | Geography | Done | 30 min | 12/2/2023 | 12/4/2023 | | Physics | In progress | 2 hrs | 12/3/2023 | 12/5/2023 | | Biology | Not started | 6 hrs | 11/2/2016 | 12/6/2023 | Prompt Model response Provide a list of all the following attributes: ingredients, type of cuisine, vegetarian or not, in JSON format ```json``` { \"ingredients\": [ \"rice\", \"shrimp\", \"clams\", \"mussels\", \"peas\", \"tomatoes\", \"onions\", \"garlic\", \"olive oil\", \"paprika\", \"salt\", \"pepper\" ], \"type of cuisine\": \"Spanish\", \"vegetarian\": false Put your image first for single-image prompts While Gemini can interpret image and text in any order within a prompt, placing a single image\nbefore the text prompt might lead to better results. In the following samples, you'll notice the image\ncomes first before the text in each prompt. Troubleshooting your multimodal prompt You might need to troubleshoot your prompt if you are not getting a helpful response. Here are a few strategies you could try. If the model is not drawing information from the relevant part of the image To get a more specific response, you can point out which aspects of the image you want the prompt to\ndraw information from to inform its response. Prompt Model response How many days will these diapers last a baby? The diapers will last for 198 days before they run out. Updated prompt Improved response How long will these diapers last before I run out? Use the weight shown on the box to determine the child's age, and use the total number of diapers in the box. Divide the total number by how many diapers the child goes through per day. The box contains 198 diapers. The box states the diapers are for a child weighing 22-37 lbs. The average child in this weight range is likely between 1 and 2 years old. A 2-year-old typically goes through 6 diapers per day, therefore, the diapers will last around 33 days (198/6). If the model output is too generic and not tailored enough to the image input To help the model tailor its response to the image(s), try asking it to describe the images before performing its reasoning task. Prompt Model response What is in common between these images? All four images are examples of landscape photography. Updated prompt Improved response First, describe what's in each image in detail. What's in common between these images? The first image shows a person packing a suitcase. The second image shows an empty hallway in a shopping mall. The third image shows a roll of toilet paper. The fourth image shows a dark and stormy sea. The commonality between these images is that they are all examples of liminal spaces. Liminal spaces are places or situations that are in between two other places or situations. They are often characterized by a sense of uncertainty or ambiguity. The first image shows the person in between being at home and being on a trip. The second image shows the hallway in between two stores. The third image is of toilet paper, which can represent a bathroom. A bathroom is a transitional space between private\n        and public realms. It's a place where we prepare ourselves to enter or leave the outside world. The fourth image shows the sea in between being calm and being stormy. Liminal spaces can be both exciting and unsettling. They can be a place of new beginning or a place of endings. They can be a place of reflection or a place of action. You can also try explicitly asking the model to refer to what's in the image. Updated prompt Improved response What is in common between these images? Refer to what's in the images in your response. All of the images are related to travel. The first image shows a person packing a suitcase, the second image shows\n          an empty hallway in a shopping mall, the third image shows a stack of toilet paper, and the fourth image shows a\n          dark and stormy sea. These images are all related to travel because they are all things that people need or use when\n          they travel. The first image shows a person packing a suitcase, which is something that people do when they are\n          going on a trip. The second image shows an empty hallway in a shopping mall, which is a place where people\n          often go to buy things for their trips. The third image shows a stack of toilet paper, which is something\n          that people often use when they are traveling. The fourth image shows a dark and stormy sea, which is\n          something that people often encounter when they are traveling. The same tips work for creative output from images. In the following example with\n  the rabbit, explicitly asking the model to include aspects from the image can improve the output. Prompt Model response Can you write me a poem about this image? Little bunny, so soft and brown, Hopping through the field, up and down. Your ears so long, your tail so small, You're the cutest creature of them all. You nibble on the grass, so green and sweet, And hop away when you see me. But I'll always be your friend, Little bunny, until the very end. Updated prompt Improved response Can you write me a descriptive and dramatic poem about this image and include the location? Little bunny on the green, Ears so long, so keen. What do you see out there, In the distance, where you stare? Is it the ocean blue, Sparkling in the sun's hue? Or the waves that crash and roar, A rhythm that you adore? Little bunny, tell me true, What captures your attention so? Is it the salty breeze, Or the seagulls' cries that please? Perhaps it's the vastness of it all, A world so big and small. A place where dreams can roam, And adventures can be sown. Little bunny, I wonder what you dream, As you sit on the grass, so serene. Do you yearn to explore the deep, Or stay on land, where you can leap? Whatever it may be, little bunny, Keep that spark of wonder burning brightly. For in your dreams and aspirations, Lies a world waiting for your creation. Troubleshooting which part of the prompt failed It can be hard to know whether a prompt failed because the model\ndidn't understand the image to begin with, or if it did understand the image but\ndid not perform the correct reasoning steps afterward. To disambiguate between those reasons, ask the model to describe what's in the image. In this example below, if the model responds with a snack that seems surprising when paired with\ntea (e.g. popcorn), you can first troubleshoot to determine whether the model correctly recognized\nthat the image contains tea. Prompt Prompt for troubleshooting What's a snack I can make in 1 minute that would go well with this? Describe what's in this image. Another strategy is to ask the model to explain its reasoning. That can help you narrow down which part of the reasoning broke down, if any. Prompt Prompt for troubleshooting What's a snack I can make in 1 minute that would go well with this? What's a snack I can make in 1 minute that would go well with this? Please explain why. Tuning the sampling parameters In each request, you send not only the multimodal prompt but a set of sampling parameters to the model.\nThe model can generate different results for different parameter values. Experiment with the different\nparameters to get the best values for the task. The most commonly adjusted parameters are the following: Temperature top-P top-K Temperature Temperature is used for sampling during response generation, which occurs when top-P and top-K are applied.\nTemperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that\nrequire a more deterministic and less open-ended or creative response, while higher temperatures can lead to more\ndiverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response\nis always selected. For most use cases, try starting with a temperature of 0.4. If you need more creative results, try increasing the\ntemperature. If you observe clear hallucinations, try reducing the temperature. Top-K Top-K changes how the model selects tokens for output. A top-K of 1 means the next selected token is\nthe most probable among all tokens in the model's vocabulary (also called greedy decoding), while\na top-K of 3 means that the next token is selected from among the three most probable tokens by\nusing temperature. For each token selection step, the top-K tokens with the highest probabilities are sampled. Then\ntokens are further filtered based on top-P with the final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more random responses.\nThe default value of top-K is 32. Top-P Top-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K)\nto least probable until the sum of their probabilities equals the top-P value. For example, if tokens\nA, B, and C have a probability of 0.6, 0.3, 0.1 and the top-P value is  0.9, then the model will\nselect either A or B as the next token by using temperature and excludes C as a candidate. Specify a lower value for less random responses and a higher value for more random responses.\nThe default value of top-P is 1.0. Next steps Try writing your own multimodal prompts using Google AI\nStudio . For more guidance on prompt design, see the Prompt strategies page. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-08-21 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/quickstart",
                "https://ai.google.dev/gemini-api/docs/api-key",
                "https://ai.google.dev/gemini-api/docs/changelog",
                "https://ai.google.dev/gemini-api/docs/models/gemini",
                "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/thinking-mode",
                "https://ai.google.dev/gemini-api/docs/models/experimental-models",
                "https://ai.google.dev/gemini-api/docs/text-generation",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/long-context",
                "https://ai.google.dev/gemini-api/docs/code-execution",
                "https://ai.google.dev/gemini-api/docs/structured-output",
                "https://ai.google.dev/gemini-api/docs/function-calling",
                "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/grounding",
                "https://ai.google.dev/gemini-api/docs/model-tuning",
                "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
                "https://ai.google.dev/gemini-api/docs/embeddings",
                "https://ai.google.dev/gemini-api/docs/caching",
                "https://ai.google.dev/gemini-api/docs/imagen",
                "https://ai.google.dev/gemini-api/docs/prompting-intro",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/tokens",
                "https://ai.google.dev/gemini-api/docs/openai",
                "https://ai.google.dev/gemini-api/docs/billing",
                "https://ai.google.dev/gemini-api/docs/safety-settings",
                "https://ai.google.dev/gemini-api/docs/safety-guidance",
                "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
                "https://ai.google.dev/gemini-api/docs/firebase-extensions",
                "https://ai.google.dev/gemini-api/docs/models/generative-models",
                "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
                "https://ai.google.dev/gemini-api/docs/learnlm",
                "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
                "https://ai.google.dev/gemini-api/docs/oauth",
                "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
                "https://ai.google.dev/gemini-api/docs/system-instructions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs/troubleshooting",
                "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
                "https://ai.google.dev/gemini-api/docs/workspace",
                "https://ai.google.dev/gemini-api/docs/quota",
                "https://ai.google.dev/gemini-api/docs/available-regions",
                "https://ai.google.dev/gemini-api/docs/abuse-monitoring",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/vision",
                "https://ai.google.dev/gemini-api/docs/audio",
                "https://ai.google.dev/gemini-api/docs/document-processing",
                "https://ai.google.dev/gemini-api/docs/prompting-strategies",
                "https://ai.google.dev/gemini-api/docs/prompting_with_media"
            ],
            "timestamp": "2024-12-27T22:47:43.266951",
            "status_code": 200
        },
        {
            "url": "https://ai.google.dev/gemini-api/docs/api-versions",
            "title": "API versions explained  |  Gemini API  |  Google AI for Developers",
            "text_content": "API versions explained  |  Gemini API  |  Google AI for Developers Models Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemini API docs API Reference SDKs Pricing Cookbook Models Gemini API docs API Reference SDKs Pricing Cookbook Solutions More Code assistance More Showcase More Community More Overview API versions Capabilities Models Generating content Multimodal Live Tokens Files Caching Embeddings Tuning Tuning Permissions Semantic retrieval Question answering Corpus Document Chunk Permissions All methods Deprecated PaLM (decomissioned) Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules Project IDX VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemini 2.0 Flash Experimental is now available! Learn more Home Gemini API Models API Reference Send feedback API versions explained This document provides a high-level overview of the differences between the v1 and v1beta versions of the Gemini API. v1 : Stable version of the API. Features in the stable version are\nfully-supported over the lifetime of the major version. If there are any\nbreaking changes, then the next major version of the API will be created and\nthe existing version will be deprecated after a reasonable period of time.\nNon-breaking changes may be introduced to the API without changing the major\nversion. v1beta : This version includes early-access features that may be under\ndevelopment and is subject to rapid and breaking changes. There is also no\nguarantee that the features in the Beta version will move to the stable\nversion. Due to this instability, you should consider not launching production\napplications with this version. Feature v1 v1beta Generate Content - Text-only input Generate Content - Text-and-image input Generate Content - Text output Generate Content - Multi-turn conversations (chat) Generate Content - Function calls Generate Content - Streaming Embed Content - Text-only input Generate Answer Semantic retriever - Supported - Will never be supported Configure API version in an SDK The Gemini API SDK's default to v1beta , but you can opt to use v1 instead\nby setting the api version as shown in the following code sample: Python The Python SDK, google.generativeai , uses v1beta and does not provide\na method for switching the API version. The low-level Python client library, google.ai.generativelanguage exposes\nother versions as separate modules: google.ai.generativelanguage_v1 and google.ai.generativelanguage_v1beta . The default is v1beta . import google.ai.generativelanguage_v1 as glm client = glm . GenerativeServiceClient ( client_options = dict ( api_key = YOUR_API_KEY )) response = client . generate_content ({ \"model\" : \"models/gemini-1.5-flash\" , \"contents\" : [ { \"parts\" : [ { \"text\" : \"Explain how AI works\" }]}] }) print ( type ( response ) . to_dict ( response )) Node.js const { GoogleGenerativeAI } = require ( \"@google/generative-ai\" ); const genAI = new GoogleGenerativeAI ( \" YOUR_API_KEY \" ); const model = genAI . getGenerativeModel ({ model : 'gemini-1.5-flash' }, { apiVersion : 'v1' }); const prompt = \"Explain how AI works\" ; const result = await model . generateContent ( prompt ); console . log ( result . response . text ()); REST curl \"https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key= YOUR_API_KEY \" \\ -H 'Content-Type: application/json' \\ -X POST \\ -d '{ \"contents\": [{ \"parts\":[{\"text\": \"Write a story about a magic backpack.\"}] }] }' Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-29 UTC. Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어",
            "meta_description": "",
            "links": [
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/sdks",
                "https://ai.google.dev/gemini-api/docs/api-versions",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/gemini-for-research",
                "https://ai.google.dev/gemini-api/docs",
                "https://ai.google.dev/gemini-api/docs/api-versions"
            ],
            "timestamp": "2024-12-27T22:47:43.825809",
            "status_code": 200
        }
    ]
}

{
    "base_url": "https://core.telegram.org/bots",
    "crawl_date": "2024-12-27T22:28:23.234023",
    "chunk_number": 1,
    "pages": [
        {
            "url": "https://core.telegram.org/bots",
            "title": "Bots: An introduction for developers",
            "text_content": "Bots: An introduction for developers Twitter Home FAQ Apps API Protocol Schema Bots: An introduction for developers Bots are small applications that run entirely within the Telegram app. Users interact with bots through flexible interfaces that can support any kind of task or service . For more information, see: Detailed Guide to Bot Features Full API Reference for Developers Basic Tutorial: From @BotFather to 'Hello World' The Telegram Bot Platform hosts more than 10 million bots and is free for both users and developers. What Can You Do with Bots? Replace Entire Websites Manage Your Business Receive Payments Create Custom Tools Integrate with Services and Devices Host Games Build Social Networks Monetize Your Service Promote Your Project Anything Else! Replace Entire Websites Telegram bots can host Mini Apps built with JavaScript . This allows for infinitely flexible interfaces that can power everything from online stores to arcade games. Unlike websites, bots support seamless authorization and notifications through Telegram out of the box. Try @DurgerKingBot – or check out the dedicated guide to Web Apps to build your own. Manage Your Business Telegram Business users can connect Telegram bots to process and answer messages on their behalf , via their personal account. This allows businesses to seamlessly integrate any existing tools and workflows, or add new AI assistants to increase productivity . As we continue to expand the set of free tools available to bots through this integration, we encourage all developers to innovate and develop useful applications and services for businesses on Telegram. Developers can turn on Business Mode in @BotFather if their bot supports integration with Telegram Business accounts. Receive Payments Bots can sell all kinds of goods and services on Telegram – to anyone in the world. Telegram Stars allow users to securely and effortlessly buy digital products via in-app purchases. In addition, physical products can be purchased through more than 20 integrated payment providers . Try @ShopBot – or check out our dedicated guides for digital and physical products to build your own. Create Custom Tools Increase your productivity by creating bots for specific tasks – like converting files, managing chats or fetching today’s forecast. Users can chat directly with bots, or add them to groups and channels to introduce extra features. Mini apps can generate media and files – that users can effortlessly share to other chats or a post as a story . Integrate with Services and Devices Mini apps can seamlessly integrate with third-party services, APIs and devices to instantly process and update information – like changing a user's emoji status when they start a game or get in a taxi . By default, Mini Apps seamlessly integrate with Android and iOS, allowing users to add direct shortcuts to their device’s home screen. Likewise, many popular platforms already have official Telegram bots, which allow users to comfortably access content in one app – or perform quick searches using inline mode . Try @GMailBot , @GitHubBot , @Bing , @YouTube , @wiki and more. Host Games Developers can create both lightweight HTML5 Games and immersive full-screen modern games with support for detailed motion controls , location-based points of interest and dynamic hardware optimizations . Try some of the games in the @Gamee library – or check out the HTML5 and Mini App manuals to build your own. Build Social Networks Bots can serve as an intermediary to connect users based on shared interests, location, and more. Coordinate meetups, show local services, or help people sell second-hand items. Users can place direct shortcuts to specific mini apps on the home screen of their devices – accessing services in one tap . Monetize Your Service Telegram offers a robust ecosystem of monetization features, allowing any bot to support its development with multiple revenue streams . Popular bots can passively earn income through Revenue Sharing from Telegram Ads, implement subscription plans for users – or offer paid content and digital products for Telegram Stars . Telegram Stars in your bot's balance can be used to increase message limits , send gifts to users or accept rewards in Toncoin. Promote Your Project Bots can host affiliate marketing programs – giving developers a transparent way to quickly scale with organic growth from user referrals . Affiliate Programs support custom revenue sharing rates and variable commission periods , allowing you to customize your offers and update your campaign over time. To learn more and get started in just a few taps, check out our dedicated guide . Anything Else The possibilities for bots are endless – from simple scripts to complex mini apps. Whether you’re a beginner or professional programmer, you can create personalized tools with the help of the Bot Platform . All Mini Apps you build on Telegram can be highly customized to fit your brand identity, including by uploading high-quality media demos and setting a custom Loading Screen with your own logo and color palette How Do Bots Work? For a detailed explanation of Bot Features, see this guide Telegram bots are special accounts that do not need a phone number to set up. Bots are connected to their owner’s server, which processes inputs and requests from users. Telegram’s intermediary server handles all encryption and communication with the Telegram API. Developers communicate with this server via an easy HTTPS-interface with a simplified version of the Telegram API – known as the Bot API . How Are Bots Different from Users? Bots are able to process inputs and requests in ways that user accounts can’t, but there are several differences between a bot and a normal user. Bots don’t have ‘last seen’ or online statuses – instead they show a ‘bot’ label in the chat. Bots have limited cloud storage – older messages may be removed by the server shortly after they have been processed. Bots can't start conversations with users. A user must either add them to a group or send them a message first. People can search for your bot’s username or start a chat via its unique t.me/bot_username link. By default, bots added to groups only see relevant messages in the chat (see Privacy Mode ). Bots never eat, sleep or complain (unless expressly programmed otherwise). Bot Links Bot usernames normally require a ‘bot’ suffix, but some bots don’t have them – such as @stickers , @gif , @wiki or @bing . Anyone can assign collectible usernames to bots, including those without the 'bot' suffix. How Do I Create a Bot? Creating Telegram bots is super-easy, but you will need at least some skills in computer programming . Creating a bot is streamlined by Telegram’s Bot API, which gives the tools and framework required to integrate your code. To get started, message @BotFather on Telegram to register your bot and receive its authentication token. Your bot token is its unique identifier – store it in a secure place , and only share it with people who need direct access to the bot. Everyone who has your token will have full control over your bot. What Next? We recommend that you check out our guide to Bot Features to see what you can teach your bot to do: Detailed Guide to Bot Features Full API Reference for Developers Basic Tutorial: From @BotFather to 'Hello World' Code Examples Telegram Telegram is a cloud-based mobile and desktop messaging app with a focus on security and speed. About FAQ Privacy Press Mobile Apps iPhone/iPad Android Mobile Web Desktop Apps PC/Mac/Linux macOS Web-browser Platform API Translations Instant View About Blog Press Moderation",
            "meta_description": "",
            "links": [
                "https://core.telegram.org/bots/features",
                "https://core.telegram.org/bots/api",
                "https://core.telegram.org/bots/tutorial",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/payments-stars",
                "https://core.telegram.org/bots/payments",
                "https://core.telegram.org/bots/inline",
                "https://core.telegram.org/bots/games",
                "https://core.telegram.org/bots/games",
                "https://core.telegram.org/bots/api",
                "https://core.telegram.org/bots/features",
                "https://core.telegram.org/bots/api",
                "https://core.telegram.org/bots/features",
                "https://core.telegram.org/bots/features",
                "https://core.telegram.org/bots/api",
                "https://core.telegram.org/bots/tutorial",
                "https://core.telegram.org/bots/samples"
            ],
            "timestamp": "2024-12-27T22:28:10.278432",
            "status_code": 200
        },
        {
            "url": "https://core.telegram.org/bots/features",
            "title": "Telegram Bot Features",
            "text_content": "Telegram Bot Features Twitter Home FAQ Apps API Protocol Schema Telegram Bots Telegram Bot Features Telegram Bot Features This page describes individual bot elements and features in detail. See also: A General Bot Platform Overview Full API Reference for Developers What features do bots have? Inputs Text Commands Buttons Chat and User Selection Interactions Inline Deep Linking Attachment Menu Integration Mini Apps Bots for Business Payments Web Login HTML5 Games Stickers Monetization Language Support Bot Management Privacy Mode Testing Your Bot Status Alerts Local API BotFather, creating and managing bots Inputs Users can send messages of all types to bots, including text, files, locations, stickers, voice messages and even dice if they're feeling lucky. However, Telegram bots offer many other tools for building flexible interfaces tailored to your specific needs: Commands that are highlighted in messages and can be selected from a list after typing / . Keyboards that replace the user's keyboard with predefined answer options. Buttons that are shown next to messages from the bot. For even more flexibility, Web Apps support 100% custom interfaces with JavaScript. Note: Telegram bots can support multiple languages that adapt to the users' language settings in the app. Commands A command is a simple /keyword that tells the bot what to do. Telegram apps will: Highlight commands in messages. When the user taps a highlighted command, that command is immediately sent again. Suggest a list of supported commands with descriptions when the user enters a / (for this to work, you need to have provided a list of commands to @BotFather or via the appropriate API method ). Selecting a command from the list immediately sends it. Show a menu button containing all or some of a bot’s commands (which you set via @BotFather ). Commands must always start with the / symbol and contain up to 32 characters . They can use Latin letters , numbers and underscores , though simple lowercase text is recommended for a cleaner look. Here are a few examples: /next /cancel /newlocation /newrule Commands should be as specific as possible – for example /newlocation or /newrule is better than a /new command that then requires an additional parameter from the user like \" location “ or ” rule \". We require all developers to support several Global Commands to make sure Telegram bots offer a consistent and user-friendly experience. Command Scopes Your bot is able to show different commands to different users and groups – you can control this using scopes . For example, your bot could show additional commands to group admins or translate the list based on the user’s language_code . Keep in mind that Bot API updates will not contain any information about the scope of a command sent by the user – in fact, they may contain commands that don’t exist at all in your bot. Your backend should always verify that received commands are valid and that the user was authorized to use them regardless of scope. Bots with privacy mode enabled will only receive commands in groups under special conditions, see here . Keyboards Bots are able to interpret free text input from users, but offering specific suggestions is often more intuitive – this is where custom keyboards can be extremely useful. Whenever your bot sends a message, it can display a special keyboard with predefined reply options (see ReplyKeyboardMarkup ). Telegram apps that receive the message will display your keyboard to the user. Using any of the buttons will immediately send the respective text. This way you can drastically simplify and streamline user interaction with your bot. Check out the one_time_keyboard parameter to automatically hide your bot's keyboard as soon as it's been used. You can also customize the text placeholder in the input field by setting the input_field_placeholder parameter. Inline Keyboards There are times when you'd prefer to do things without sending any messages to the chat – like when a user is changing settings, toggling options or navigating search results. In such cases, you can use Inline Keyboards that are shown directly below their relevant messages. Unlike with custom reply keyboards, pressing buttons on inline keyboards doesn't send messages to the chat . Instead, inline keyboards support buttons that can work behind the scenes or open different interfaces: callback buttons , URL buttons , switch-to-inline buttons , game buttons and payment buttons . To provide a better user experience , consider editing your keyboard when the user toggles a setting button or navigates to a new page – this is both faster and smoother than sending a whole new message and deleting the previous one. Menu Button In all bot chats, a menu button appears near the message field. By default, tapping this button opens a menu that can hold some or all of a bot's commands, including a short description for each. Users can then select a command from the menu without needing to type it out. You can set different texts of the menu button and its command descriptions for various individual users or groups of users – for example, showing translated text based on the user’s language, as explained here . The menu button can alternatively be used to launch a Web App . Global Commands To make basic interactions more uniform, we ask all developers to support a few basic commands . Telegram apps will have interface shortcuts for these commands. /start - begins the interaction with the user, like sending an introductory message. This command can also be used to pass additional parameters to the bot (see Deep Linking ). /help - returns a help message, like a short text about what your bot can do and a list of commands. /settings - (if applicable) shows the bot's settings for this user and suggests commands to edit them. Users will see a Start button the first time they open a chat with your bot. Help and Settings links will be available in the menu on the bot's profile page if you add them in @BotFather . Chat and User Selection Bots can present the user with a friendly and intuitive interface that lists any number of groups, channels or other users according to a custom set of criteria . Tapping on a chat will send its identifier to the bot in a service message and seamlessly close the interface. A group management bot is the perfect example : an admin could select a chat the bot should manage, and then select a user it should promote – this would happen without ever typing any text. Here is a quick start guide to use this feature: Pick a set of criteria and store them in a KeyboardButtonRequestChat object (or KeyboardButtonRequestUser for users). Create a KeyboardButton and store the criteria under request_chat or request_user respectively. Send a ReplyKeyboardMarkup that contains the button you just created. When the user selects a chat, you'll receive its identifier in a chat_shared or user_shared service message. link Keep in mind that the bot may not be able to use the identifier it receives if the corresponding chat or user is not already known or accessible by some other means. Interactions In addition to sending commands and messages to the chat with the bot, there are several ways of interacting with them without opening any specific chat or group. Inline mode allows sending requests to bots right from the input field – from any chat on Telegram. Deep linking allows special links that send certain parameters to the bot when opened. Attachment menu integration makes it possible to use bots from the attachment menu in chats. Inline Requests Users can interact with your bot via inline queries straight from the message field in any chat . All they need to do is start a message with your bot's @username and enter a keyword. Having received the query, your bot can return some results. As soon as the user selects one, it is sent to the relevant chat . This way, people can request and send content from your bot in any of their chats, groups or channels. Remember that inline functionality has to be enabled via @BotFather , or your bot will not receive inline Updates . Examples of inline bots include @gif , @bing and @wiki . Web App bots can also be used inline – try typing @durgerkingbot in any chat. Deep Linking Telegram bots have a deep linking mechanism that allows additional parameters to be passed to the bot on startup. It could be a command that launches the bot – or an authentication token to connect the user's Telegram account to their account on another platform. Each bot has a link that opens a conversation with it in Telegram – https://t.me/<bot_username> . Parameters can be added directly to this link to let your bot work with additional information on the fly, without any user input. A-Z, a-z, 0-9, _ and - are allowed. We recommend using base64url to encode parameters with binary and other types of content. The parameter can be up to 64 characters long. Private Chats In private chats, you can use the start parameter to automatically pass any value to your bot whenever a user presses the link. For example, you could use: https://t.me/your_bot?start=airplane When someone opens a chat with your bot via this link, you will receive: /start airplane Groups In groups, you can add the parameter startgroup to this link. For example: https://t.me/your_bot?startgroup=spaceship Following a link with this parameter prompts the user to select a group to add the bot to – the resulting update will contain text in the form: /start@your_bot spaceship Web Apps also support deep linking, for more information check out our dedicated guide . Attachment Menu Certain bots can be added directly to a user’s attachment menu – giving them easy access to the bot in any chat. Currently, this option is restricted to certain approved bots , but may be expanded later. Try adding @DurgerKingBot to your attachment menu. Integration There are various ways of further integrating bots with Telegram and other services. Use Web Apps to replace any website. Build tools and integrate business services . Accept Payments via dozens of integrated third-party payment providers. Connect to Telegram using the Web Login functionality. Create gaming bots by integrating HTML5 Games . Help users create and manage Telegram Stickers . Monetization Telegram offers a robust ecosystem of monetization features, allowing any bot to support its development with multiple revenue streams . Telegram Stars Telegram Stars power all digital transactions between bots and users. Users can acquire Stars through in-app purchases via Apple and Google or via @PremiumBot . Bots can use the Stars they receive to increase message limits , send gifts to users or accept rewards in Toncoin. Digital Products Services can use their bot to sell digital goods and services – like online courses, commissioned artwork and items in games . Paid Media Bots can post paid photos and videos – and users are only allowed to view the media after paying to unlock it. This functionality is available to all bots – including bot admins in channels and bots managing Telegram Business accounts. Subscription Plans Developers are able to offer paid subscriptions to their bot – adding multiple tiers of content and features tailored to their audience. Revenue Sharing from Telegram Ads Developers can participate in revenue sharing from Telegram Ads – receiving 50% of the revenue from ads that appear in the chat with their bot. Mini Apps Mini Apps allow developers to create infinitely flexible interfaces that can be launched right inside Telegram – integrating seamlessly with the app and replacing any website . If your bot is a mini app, you can add a prominent Launch app button as well as demo videos and screenshots to the bot’s profile. To do this, go to @BotFather and set up your bot's Main Mini App . Mini apps are covered in detail in our dedicated guide – you should read it carefully to learn the wide variety of features they can offer. If you develop a mini app , be sure to follow our design guidelines – you'll want your custom interface to seamlessly integrate into the app to provide users the best possible experience. Seamless Integration With Telegram Mini apps integrate seamlessly with Telegram – from receiving detailed theme settings to using native dialogs for reading QR codes , controlling biometrics , sharing media directly to stories and more. When opened from a direct link in a group, mini apps can also use the chat_instance parameter to track of the current context, supporting shared usage by multiple chat members – to create live whiteboards, group orders, multiplayer games and much more. Mini App Previews Developers can upload screenshots and video demos of their mini app right from the bot's profile page – giving users an overview of the app's features and functionality. These media previews will be shown to any user who views your app – like in the Mini App Store or via Search. Previews support multiple languages – so you can upload translated versions of your previews that will be shown to users based on their app language . Mini App Store More than 500 million out of Telegram's 950 million users interact with mini apps every month. Successful mini apps have the chance to be highlighted in the Telegram Mini App Store – appearing for all users in the 'Apps' tab of Search. Featured mini apps are chosen based on how they enrich the Telegram ecosystem . To increase the chances of being featured, you must enable the Main Mini App in @BotFather , upload high-quality media demos showcasing your app to your bot's profile and accept payments in Telegram Stars . Check out our documentation to learn more about enabling Main Mini Apps and accepting payments in Stars. Home Screen Shortcuts Users can place direct shortcuts to specific mini apps on the home screen of their devices – accessing their favorite games and services in one tap . Customizable Loading Screens The loading screen of mini apps can be customized in @Botfather – where developers can add their own icon and set specific colors for both light and dark themes. To customize your loading screen, go to @Botfather > /mybots > Select bot > Bot Settings > Configure Mini App > Configure Splash Screen . You can tap on Open Splash Screen Preview to see the final result. Full-Screen Mode Mini apps are able to use the entire screen in portrait or landscape orientation – allowing for immersive games and media with expanded gestures and interfaces. Setting Emoji Status Users can set an emoji status inside mini apps or give an app permission to update it automatically . Developers can also integrate APIs from other services or request geolocation access – instantly changing a user's status when they start a game or leave the office . Sharing Media Media generated in mini apps can be shared in any chat – letting users effortlessly send referral codes and custom images to contacts, groups and channels. Alternatively, users can download it with a native popup. Sharing from Mini Apps to Stories Any media created by the mini app, like whiteboard snapshots, leaderboards and AI-generated videos can be opened with the native story editor via the shareToStory method – for users to share as a Telegram Story right from the mini app. Mini apps also receive a number of events , allowing them to instantly react to actions by the user. You can learn more about which events are available here . Geolocation Access Mini apps are able to receive location permissions from users – giving developers the ability to make location-based games and interactive maps for events. Device Motion Tracking Mini Apps can request acceleration , orientation and rotation data from devices in real time – unlocking support for motion controls and VR experiences . Device Hardware Info A user's device can send basic hardware info to mini apps, such as its processing power and memory capacity. Mini apps can then use this to optimize graphics and automatically adjust settings for the smoothest experience. Bots for Business Bots can enable Business Mode , allowing Telegram Business subscribers to connect them to their account – streamlining and automating private chat management and interactions with their clients. The account owner can specify which chats your bot can access – within those chats, the bot will receive all updates normally supported by the Bot API , except messages sent by itself and other bots. Depending on the business connection settings, your bot may also be able to send messages and do other actions on behalf of the account owner in chats that were active in the last 24h. Here is a quick start guide to integrate your bot with Telegram Business: Enable Business Mode for your bot in @BotFather . Handle incoming BusinessConnection updates, signaling that a user has established , edited or ended a Business Connection with your bot. Process business messages by handling business_message , edited_business_message and deleted_business_messages updates. Check your bot’s write permissions via can_reply in the latest BusinessConnection update. If allowed to, use the business_connection_id field in sendMessage , sendChatAction and other send methods to communicate on behalf of the Business user. Users who connect your bot to their account will see a quick action bar at the top of each managed chat – tapping on “Manage Bot” will redirect them to your bot, which will receive a deep link message in the format /start bizChat<user_chat_id> . Please keep in mind that operating bots on Telegram is subject to the Telegram Bot Developer Terms of Service . Specifically, for Telegram Business, make sure you have read and understood Section 5.4 . Payments If your bot or mini app sells digital goods and services , be sure to carry out the payment in Telegram Stars by specifying XTR as currency. In compliance with third-party store policies, Telegram does not support the sale of digital goods and services using other currencies. Telegram bots can accept payments with a sleek, streamlined interface that collects all necessary data from the user. Telegram doesn't collect any payment data – like the user's credit card information – and sends it directly to one of the supported payment providers . Here is a quick start guide to implement payments: Pick a provider and obtain the proper token as well as a test token from the \" Stripe TEST MODE \" provider. Implement payments via the appropriate API methods . Test your implementation by using your test token along with a test credit card . Then, to issue an invoice and process the order flow: Send an invoice to the user for the goods or services you are offering. Validate the order and accept the checkout via answerPreCheckoutQuery . Confirm the payment by checking for a successful payment service message . Ship the goods or provide the services. For more details, feel free to check out our full exhaustive guides for selling goods and services on Telegram – they include live checklists, parameters and in-depth method descriptions: Guide for digital goods and services Guide for physical goods and services Telegram does not directly process the payments, does not store data about orders and does not collect any fees. Invoices are forwarded directly to the payment provider. For this reason, disputes must be solved between the user, the bot developer and the payment provider. You can read more about this in the Privacy Policy . Web Login Telegram offers a flexible , lightweight and free framework to authenticate users on any website and app. This can be used to bridge your platform with Telegram, providing a smooth experience to your users. You can also freely rely on this framework to implement a fast and signup-free login on your site, regardless of its connection to Telegram. Widgets The Telegram login widget is a simple and secure way to authorize users on your website. Choose a bot – ideally its name and profile pic should match the website title and logo. Use the /setdomain command in @BotFather to pair the bot with your website domain. Configure your widget using our dedicated tool and embed it on your website. Inline Login When users open your website via an inline button , you can use the login_url parameter as an alternative to login widgets. This way, you'll be able to seamlessly authorize them on your website or app before the page even loads. Make sure to review our guide on authenticating the received data as well as our sample code . HTML5 Games Bots can serve as standalone gaming platforms – with our HTML5 Gaming API you can develop multiplayer or single-player games and let your users have fun comparing ranks , scores and much more. To get started, follow these simple steps: Send the /newgame command to @BotFather Provide a description text , an image or an optional gif to showcase its gameplay Send the game to users via the sendGame method or via an inline query When someone wants to play, you'll receive the appropriate game_short_name in a CallbackQuery To launch the game, provide the HTML5 Game URL as the url param of answerCallbackQuery Then, to handle highscores : Use setGameScore to post high scores in the chat with the game Use getGameHighScores to get in-game high score tables You can also embed a share button within your game, play around with custom inline buttons , URL parameters and much more. To get a better idea, make sure to check out: HTML5 Games Manual HTML5 Games Bot API Docs Check out @GameBot and @gamee for examples of what you can do using our Gaming Platform. Stickers and Custom Emoji Stickers and Custom Emoji are a distinctive Telegram feature used by millions of users to share artwork every day. Stickers and custom emoji take many forms – ranging from basic images to smooth vector animations and high-detail .WEBM videos . All these formats are supported by our Bot API , which allows bots to create , edit , delete and share new artwork packs on the fly. Telegram's Import API lets users migrate packs from other platforms and sticker apps. Creating a new pack To create a new pack , simply: Prepare your artwork following our technical requirements . Create a new sticker pack via createStickerSet . Set sticker_type to regular to create a sticker pack or to custom emoji to create a pack of custom emoji. Attach the files you wish to include in the pack as an array of InputSticker You can use addStickerToSet to add stickers or emoji later on. Additional features Regular stickers and custom emoji support keywords that users can type to quickly find the respective artwork – this can be useful when a sticker doesn't have obvious ties to a specific emoji. You can use the keywords parameter in InputSticker to specify them. Custom emoji additionally support adaptive colors – they will always match the current context (e.g., white on photos, accent color when used as status, etc.); to enable this feature, use the needs_repainting parameter in createStickerSet . Once you're done creating and sharing your artwork, don't forget to check out our remaining sticker methods to find out how to edit , delete and even reorder your pack. Note that these methods will only work on packs created by the bot that is calling them . Language Support Bots can tailor their interfaces to support multiple languages – updating inputs and information on the fly. A user’s language_code is included in every relevant update as an IETF language tag , allowing bots to adapt accordingly. We recommend that you follow our guidelines to provide the best user experience . Your interfaces, texts and inline results should adapt seamlessly to the language_code , without user intervention. Connected WebApps will receive the user's language_code – your HTML page should account for it. HTML5 Games can obtain language information if you specify it as a URL parameter . You can generate this parameter from the language_code field in the User object served with the initial game CallbackQuery . The bot's Name , Description and About text can be natively localized with the respective methods . Command lists can also be specified for individual languages – more on this here . The language_code is an optional field – it could be empty. If you target the general public, your code should always fall back to either the last recorded language tag or English (in this order) when the field is missing for a specific user. Bot Management Privacy Mode Bots are frequently added to groups to perform basic tasks or assist moderators – like automatically posting company announcements or even celebrating birthdays. By default, all bots added to groups run in Privacy Mode and only see relevant messages and commands: Commands explicitly meant for them (e.g., /command@this_bot ). General commands (e.g. /start ) if the bot was the last bot to send a message to the group. Inline messages sent via the bot. Replies to any messages implicitly or explicitly meant for this bot. All bots will also receive, regardless of privacy mode : All service messages. All messages from private chats. All messages from channels where they are a member. Privacy mode is enabled by default for all bots, except bots that were added to a group as admins (bot admins always receive all messages ). It can be disabled so that the bot receives all messages like an ordinary user (the bot will need to be re-added to the group for this change to take effect). We only recommend doing this in cases where it is absolutely necessary for your bot to work. In most cases, using the force reply option for the bot's messages should be more than enough. This mode not only increases user privacy, but also makes the bot more efficient by reducing the number of inputs it needs to process. Users can always see a bot’s current privacy setting in the list of group members. Testing your bot You can quickly test your bot without interfering with its users by simply running another instance of your code on a different bot account. To do so, create a new bot via @BotFather , obtain its token and use it in the testing instance of your code. All further testing and debugging can happen privately on the new bot, without affecting the original instance. If you need to share file references across bots, note that the file_id field is tied to a single bot id, so your test instance cannot use a shared file_id database to quickly send media – files must be individually reuploaded. Dedicated test environment Telegram also offers a dedicated test environment suitable for more advanced testing. Bots and users in this environment generally have more flexible restrictions – for example: When working with the test environment, you may use HTTP links without TLS to test your Web Apps or Web Login . Flood limits are not raised in the test environment, and may at times be stricter. To minimize how this impacts your bot, you should make sure that it handles errors with retry policies and does not depend on hardcoded limit values. Creating a bot in the test environment The test environment is completely separate from the main environment, so you will need to create a new user account and a new bot with @BotFather . To create an account and log in, use either of the following: iOS : tap 10 times on the Settings icon > Accounts > Login to another account > Test. Telegram Desktop : open ☰ Settings > Shift + Alt + Right click 'Add Account' and select 'Test Server'. macOS : click the Settings icon 10 times to open the Debug Menu, ⌘ + click 'Add Account' and log in via phone number. After logging in, simply create a new bot following the standard procedure and send your requests to the Test Bot API in this format: https://api.telegram.org/bot<token>/test/METHOD_NAME When working with the test environment, you may use HTTP links without TLS in the url field of both LoginUrl and WebAppInfo . Status alerts Millions choose Telegram for its speed. To best benefit users, your bot also needs to be responsive . In order to help developers keep their bots in shape, @BotFather will send status alerts if it sees something is wrong. We check the number of replies and the request/response conversion rate for popular bots (~300 requests per minute, this value may change in the future). If your bot returns an abnormally low number , you will receive a notification from @BotFather . Responding to alerts By default, you will only get one alert per bot per hour . Each alert has the following buttons: Fixed - Use this if you found an issue with your bot and fixed it. If you press the fix button, we will resume sending alerts in the regular way so that you can see if your fix worked within 5-10 minutes instead of having to wait for an hour. Support - Use this to open a chat with @BotSupport if you don't see any issues with your bot or if you think the problem is on our side. Mute for 8h/1w - Use this if you can't fix your bot at the moment. This will disable all alerts for the bot in question for the specified period of time. We do not recommend using this option since your users may migrate to a more stable bot. You can unmute alerts in your bot's settings via @BotFather . Monitored issues We currently notify you about the following issues: Too few private messages are sent. Value: {value} - Your bot is sending far fewer messages than it did in previous weeks. This is useful for newsletter-style bots that send messages without prompts from users. The larger the value, the more significant the difference. Too few replies to incoming private messages . Conversion rate: {value} - Your bot is not replying to all messages that are being sent to it (the request/response conversion rate for your bot was too low for at least two of the last three 5-minute periods). To provide a good user experience, please respond to all messages that are sent to your bot. Respond to message updates by calling send… methods (e.g. sendMessage ). Too few answers to inline queries . Conversion rate: {value} - Your bot is not replying to all inline queries that are being sent to it, calculated in the same way as above. Respond to inline_query updates by calling answerInlineQuery . Too few answers to callback queries . Conversion rate: {value} Too few answers to callback game queries . Conversion rate: {value} - Your bot is not replying to all callback queries that are being sent to it (with or without games), calculated in the same way as above. Respond to callback_query updates by calling answerCallbackQuery . Local Bot API You can host and work with your own instance of our open-source Bot API . The source code is available here , along with a quick installation guide . After installing the server , remember to use the logOut method before redirecting requests to your new local API URL. Your local instance runs on port 8081 by default and will only accept HTTP requests, so a TLS termination proxy has to be used to handle remote HTTPS requests. By hosting our API locally you'll gain access to some upgrades , including: API Max File Download Max File Upload WHook URL WHook Port WHook Max Connections Official 20MB 50MB HTTPS 443,80,88,8443 1-100 Local Unlimited 2000MB HTTP Any port 1-100000 You can find an exhaustive list here . All limits may be subject to change in the future, so make sure to follow @BotNews . BotFather Below is a detailed guide to using @BotFather , Telegram’s tool for creating and managing bots. Creating a new bot Use the /newbot command to create a new bot. @BotFather will ask you for a name and username, then generate an authentication token for your new bot. The name of your bot is displayed in contact details and elsewhere. The username is a short name, used in search, mentions and t.me links. Usernames are 5-32 characters long and not case sensitive – but may only include Latin characters, numbers, and underscores. Your bot's username must end in 'bot’, like 'tetris_bot' or 'TetrisBot'. The token is a string, like 110201543:AAHdqTcvCH1vGWJxfSeofSAs0K5PALDsaw , which is required to authorize the bot and send requests to the Bot API. Keep your token secure and store it safely, it can be used by anyone to control your bot. Unlike the bot’s name, the username cannot be changed later – so choose it carefully. When sending a request to api.telegram.org, remember to prefix the word ‘bot’ to your token. About text, description and profile media When new users open your bot, they will be met with a helpful description in a box titled “What can this bot do?”. Properly setting this field in @BotFather allows everyone to immediately get an idea of what your bot can do – your description should be brief , to the point and on topic . You can also add a photo or video to this field with Edit Description Picture in @BotFather . Additionally, just like normal users, bots also come with a short bio available on their profile. If you didn't specify this field while first creating your bot, you can set it at any time with the /setabouttext command in @BotFather . Users can interact with many bots and they won't have access to their description after starting them – having a quick reminder of the bot's purpose can be very useful. Note that both the Description and the About text can be natively localized – each user will automatically see the correct translation for their language. Bots can also have a profile picture – you should pick something unique and original so that users can find it in their chat list at a glance. Starting from April 21, 2023 ( Telegram 9.6 ), you can edit your bot directly from its profile page – including setting a custom profile video . Generating an authentication token If your existing token is compromised or you lost it for some reason, use the /token command to generate a new one. Transfer ownership You can transfer ownership of your bot to another user . To do this, send /mybots , select your bot, then transfer ownership . You can only transfer a bot to users who have interacted with it at least once. Transferring ownership will give full control of the bot to another user – they will be able to access the bot’s messages and even delete it. The transfer is permanent, so please consider it carefully. BotFather commands The remaining commands are pretty self-explanatory: /mybots – returns a list of your bots with handy controls to edit their settings. /mygames – does the same for your games. Edit bots To edit your bot, you have two options. You can use the available commands: /setname – change your bot's name . /setdescription – change the bot's description (short text up to 512 characters). Users will see this text at the beginning of the conversation with the bot, titled ' What can this bot do? '. /setabouttext – change the bot's about info , a shorter text up to 120 characters. Users will see this text on the bot's profile page. When they share your bot with someone, this text is sent together with the link. /setuserpic – change the bot's profile picture . /setcommands – change the list of commands supported by your bot. Users will see these commands as suggestions when they type / in the chat with your bot. See commands for more info. /setdomain – link a website domain to your bot. See the login widget section. /deletebot – delete your bot and free its username . Cannot be undone. Or you can use the /mybots command, tap on your bot and use the modern inline interface to edit it. Starting from April 21, 2023 ( Telegram 9.6 ), you can edit your bot's public-facing info directly from its profile page – including setting a custom profile video . Edit settings /setinline    – toggle inline mode for your bot. /setinlinegeo – request location data to provide location-based inline results. /setjoingroups – toggle whether your bot can be added to groups or not. All bots must be able to process direct messages, but if your bot was not designed to work in groups, you can disable this. /setinlinefeedback – toggle whether the API should send updates about the results chosen by users. See an in-depth explanation here . /setprivacy – set which messages your bot will receive when added to a group. See privacy-mode for more info. Manage games /newgame    – create a new game. /listgames  – see a list of your games. /editgame   – edit a game. /deletegame – delete an existing game. Please note that it may take a few minutes for changes to take effect. With this information, you are ready to proceed to our Full API Reference for Developers . If you have any questions, check out our Bot FAQ . If you're experiencing issues with our API, please contact @BotSupport on Telegram. Telegram Telegram is a cloud-based mobile and desktop messaging app with a focus on security and speed. About FAQ Privacy Press Mobile Apps iPhone/iPad Android Mobile Web Desktop Apps PC/Mac/Linux macOS Web-browser Platform API Translations Instant View About Blog Press Moderation",
            "meta_description": "",
            "links": [
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/features",
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/api",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/payments-stars",
                "https://core.telegram.org/bots/api",
                "https://core.telegram.org/bots/payments-stars",
                "https://core.telegram.org/bots/payments",
                "https://core.telegram.org/bots/games",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/games",
                "https://core.telegram.org/bots/api",
                "https://core.telegram.org/bots/api",
                "https://core.telegram.org/bots/faq"
            ],
            "timestamp": "2024-12-27T22:28:11.512888",
            "status_code": 200
        },
        {
            "url": "https://core.telegram.org/bots/api",
            "title": "Telegram Bot API",
            "text_content": "Telegram Bot API Twitter Home FAQ Apps API Protocol Schema Telegram Bots Telegram Bot API Telegram Bot API The Bot API is an HTTP-based interface created for developers keen on building bots for Telegram. To learn how to create and set up a bot, please consult our Introduction to Bots and Bot FAQ . Recent changes Subscribe to @BotNews to be the first to know about the latest updates and join the discussion in @BotTalk December 4, 2024 Bot API 8.1 Added the field nanostar_amount to the class StarTransaction . Added the class TransactionPartnerAffiliateProgram for transactions pertaining to incoming affiliate commissions. Added the class AffiliateInfo and the field affiliate to the class TransactionPartnerUser , allowing bots to identify the relevant affiliate in transactions with an affiliate commission. November 17, 2024 Bot API 8.0 Bot API 8.0 introduces 10 powerful new features for Mini Apps - including the ability to enter full-screen mode , launch from home screen shortcuts , offer subscription plans and more. Check out all the details in our dedicated blog and Mini App documentation . Star Subscriptions Bots now support paid subscriptions powered by Telegram Stars - monetizing their efforts with multiple tiers of content and features. Added the parameter subscription_period to the method createInvoiceLink to support the creation of links that are billed periodically. Added the parameter business_connection_id to the method createInvoiceLink to support the creation of invoice links on behalf of business accounts. Added the fields subscription_expiration_date , is_recurring and is_first_recurring to the class SuccessfulPayment . Added the method editUserStarSubscription . Added the field subscription_period to the class TransactionPartnerUser . Full-screen Mode Mini Apps are now able to become full-screen in both portrait and landscape mode - allowing them to host more games , play widescreen media and support immersive user experiences. Added the methods requestFullscreen and exitFullscreen to the class WebApp to toggle full-screen mode. Added the fields safeAreaInset and contentSafeAreaInset to the class WebApp , allowing Mini Apps to ensure that their content properly respects the device's safe area margins. Further added the fields isActive and isFullscreen to the class WebApp . Added the events activated , deactivated , safeAreaChanged , contentSafeAreaChanged , fullscreenChanged and fullscreenFailed for Mini Apps. Homescreen Shortcuts Mini Apps can now be accessed via direct shortcuts added to the home screen of mobile devices. Added the method addToHomeScreen to the class WebApp to create a shortcut for users to add to their home screens. Added the method checkHomeScreenStatus to the class WebApp to determine the status and support of the home screen shortcut for the Mini App on the current device. Added the events homeScreenAdded and homeScreenChecked for Mini Apps. Emoji Status Mini Apps can now prompt users to set their emoji status - or request access to later sync it automatically with in-game badges, third-party APIs and more. Added the method setUserEmojiStatus . The user must allow the bot to manage their emoji status. Added the method setEmojiStatus to the class WebApp to let users manually confirm a custom emoji as their new status via a native dialog. Added the method requestEmojiStatusAccess to the class WebApp for obtaining permission to later update a user's emoji status via the Bot API method setUserEmojiStatus . Added the events emojiStatusSet , emojiStatusFailed and emojiStatusAccessRequested for Mini Apps. Media Sharing and File Downloads Users can now share media directly from Mini Apps - sending referral codes , custom memes, artwork and more to any chat or posting them as a story . Added the class PreparedInlineMessage and the method savePreparedInlineMessage , allowing bots to suggest users to send a specific message from a Mini App via the method shareMessage . Added the method shareMessage to the class WebApp to share media from Mini Apps to Telegram chats. Added the method downloadFile to the class WebApp , introducing support for a native popup that prompts users to download files from the Mini App. Added the events shareMessageSent , shareMessageFailed and fileDownloadRequested for Mini Apps. Geolocation Access Mini Apps can now request geolocation access to users, allowing them to build virtually any location-based service, from games with dynamic points of interest to interactive maps for events. Added the field LocationManager to the class WebApp . Added the events locationManagerUpdated and locationRequested for Mini Apps. Device Motion Tracking Mini Apps can now track detailed device motion data , allowing them to implement better productivity tools, immersive VR experiences and more. Added the fields isOrientationLocked , Accelerometer , DeviceOrientation and Gyroscope to the class WebApp . Added the methods lockOrientation and unlockOrientation to the class WebApp to control the screen orientation. Added the events accelerometerStarted , accelerometerStopped , accelerometerChanged , accelerometerFailed , deviceOrientationStarted , deviceOrientationStopped , deviceOrientationChanged , deviceOrientationFailed , gyroscopeStarted , gyroscopeStopped , gyroscopeChanged , gyroscopeFailed for Mini Apps. Gifts Bots can now send Paid Gifts to users in exchange for Telegram Stars. Added the classes Gift and Gifts and the method getAvailableGifts , allowing bots to get all gifts available for sending. Added the method sendGift , allowing bots to send gifts to users. Added the field gift to the class TransactionPartnerUser . Loading Screen Customization Mini Apps can customize their loading screen, adding their own icon and specific colors for light and dark themes. You can access these customization settings in @BotFather via /mybots > Select Bot > Bot Settings > Configure Mini App > Enable Mini App Hardware-specific Optimizations Mini Apps running on Android can now receive basic information about a device's processing hardware, allowing them to optimize user experience based on the device's capabilities. This information includes the OS, App and SDK's respective versions as well as the device's model and performance class. General Added the field photo_url to the class WebAppUser for all bots, allowing Mini Apps to access a user's profile photo if their privacy settings allow for it. Third parties (e.g., Mini App builders) that receive or process data on behalf of Mini Apps are now able to validate it without knowing the App's bot token . Debugging options have been expanded to include full support for iOS devices . You can use these tools to find app-specific issues in your Mini App. WARNING! Starting December 1, 2024 messages with video that are sent, copied or forwarded to groups and channels with a sufficiently large audience can be automatically scheduled by the server until the respective video is reencoded. Such messages will have 0 as their message identifier and can't be used before they are actually sent. October 31, 2024 Bot API 7.11 Added the class CopyTextButton and the field copy_text in the class InlineKeyboardButton allowing bots to send and receive inline buttons that copy arbitrary text. Added the parameter allow_paid_broadcast to the methods sendMessage , sendPhoto , sendVideo , sendAnimation , sendAudio , sendDocument , sendPaidMedia , sendSticker , sendVideoNote , sendVoice , sendLocation , sendVenue , sendContact , sendPoll , sendDice , sendInvoice , sendGame , sendMediaGroup and copyMessage . Added the class TransactionPartnerTelegramApi for transactions related to paid broadcasted messages. Introduced the ability to add media to existing text messages using the method editMessageMedia . Added support for hashtag and cashtag entities with a specified chat username that opens a search for the relevant tag within the specified chat. WARNING! Starting December 1, 2024 messages with video that are sent, copied or forwarded to groups and channels with a sufficiently large audience can be automatically scheduled by the server until the respective video is reencoded. Such messages will have 0 as their message identifier and can't be used before they are actually sent. September 6, 2024 Bot API 7.10 Added updates about purchased paid media, represented by the class PaidMediaPurchased and the field purchased_paid_media in the class Update . Added the ability to specify a payload in sendPaidMedia that is received back by the bot in TransactionPartnerUser and purchased_paid_media updates. Added the field prize_star_count to the classes GiveawayCreated , Giveaway , GiveawayWinners and ChatBoostSourceGiveaway . Added the field is_star_giveaway to the class GiveawayCompleted . Added the field SecondaryButton to the class WebApp . Added the event secondaryButtonClicked for Mini Apps. Added the field bottomBarColor and the method setBottomBarColor to the class WebApp . Added the field bottom_bar_bg_color to the class ThemeParams . August 14, 2024 Bot API 7.9 Added support for Super Channels , allowing received channel messages to have users or other channels as their senders. Added the ability to send paid media to any chat. Added the parameter business_connection_id to the method sendPaidMedia , allowing bots to send paid media on behalf of a business account. Added the field paid_media to the class TransactionPartnerUser for transactions involving paid media. Added the fields subscription_period and subscription_price to the class ChatInviteLink . Added the method createChatSubscriptionInviteLink , allowing bots to create subscription invite links. Added the method editChatSubscriptionInviteLink , allowing bots to edit the name of subscription invite links. Added the field until_date to the class ChatMemberMember for members with an active subscription. Added support for paid reactions and the class ReactionTypePaid . See earlier changes » Authorizing your bot Each bot is given a unique authentication token when it is created . The token looks something like 123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11 , but we'll use simply <token> in this document instead. You can learn about obtaining tokens and generating new ones in this document . Making requests All queries to the Telegram Bot API must be served over HTTPS and need to be presented in this form: https://api.telegram.org/bot<token>/METHOD_NAME . Like this for example: https://api.telegram.org/bot123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11/getMe We support GET and POST HTTP methods. We support four ways of passing parameters in Bot API requests: URL query string application/x-www-form-urlencoded application/json (except for uploading files) multipart/form-data (use to upload files) The response contains a JSON object, which always has a Boolean field 'ok' and may have an optional String field 'description' with a human-readable description of the result. If 'ok' equals True , the request was successful and the result of the query can be found in the 'result' field. In case of an unsuccessful request, 'ok' equals false and the error is explained in the 'description'. An Integer 'error_code' field is also returned, but its contents are subject to change in the future. Some errors may also have an optional field 'parameters' of the type ResponseParameters , which can help to automatically handle the error. All methods in the Bot API are case-insensitive. All queries must be made using UTF-8. Making requests when getting updates If you're using webhooks , you can perform a request to the Bot API while sending an answer to the webhook. Use either application/json or application/x-www-form-urlencoded or multipart/form-data response content type for passing parameters. Specify the method to be invoked in the method parameter of the request. It's not possible to know that such a request was successful or get its result. Please see our FAQ for examples. Using a Local Bot API Server The Bot API server source code is available at telegram-bot-api . You can run it locally and send the requests to your own server instead of https://api.telegram.org . If you switch to a local Bot API server, your bot will be able to: Download files without a size limit. Upload files up to 2000 MB. Upload files using their local path and the file URI scheme . Use an HTTP URL for the webhook. Use any local IP address for the webhook. Use any port for the webhook. Set max_webhook_connections up to 100000. Receive the absolute local path as a value of the file_path field without the need to download the file after a getFile request. Do I need a Local Bot API Server The majority of bots will be OK with the default configuration, running on our servers. But if you feel that you need one of these features , you're welcome to switch to your own at any time. Getting updates There are two mutually exclusive ways of receiving updates for your bot - the getUpdates method on one hand and webhooks on the other. Incoming updates are stored on the server until the bot receives them either way, but they will not be kept longer than 24 hours. Regardless of which option you choose, you will receive JSON-serialized Update objects as a result. Update This object represents an incoming update. At most one of the optional parameters can be present in any given update. Field Type Description update_id Integer The update's unique identifier. Update identifiers start from a certain positive number and increase sequentially. This identifier becomes especially handy if you're using webhooks , since it allows you to ignore repeated updates or to restore the correct update sequence, should they get out of order. If there are no new updates for at least a week, then identifier of the next update will be chosen randomly instead of sequentially. message Message Optional . New incoming message of any kind - text, photo, sticker, etc. edited_message Message Optional . New version of a message that is known to the bot and was edited. This update may at times be triggered by changes to message fields that are either unavailable or not actively used by your bot. channel_post Message Optional . New incoming channel post of any kind - text, photo, sticker, etc. edited_channel_post Message Optional . New version of a channel post that is known to the bot and was edited. This update may at times be triggered by changes to message fields that are either unavailable or not actively used by your bot. business_connection BusinessConnection Optional . The bot was connected to or disconnected from a business account, or a user edited an existing connection with the bot business_message Message Optional . New message from a connected business account edited_business_message Message Optional . New version of a message from a connected business account deleted_business_messages BusinessMessagesDeleted Optional . Messages were deleted from a connected business account message_reaction MessageReactionUpdated Optional . A reaction to a message was changed by a user. The bot must be an administrator in the chat and must explicitly specify \"message_reaction\" in the list of allowed_updates to receive these updates. The update isn't received for reactions set by bots. message_reaction_count MessageReactionCountUpdated Optional . Reactions to a message with anonymous reactions were changed. The bot must be an administrator in the chat and must explicitly specify \"message_reaction_count\" in the list of allowed_updates to receive these updates. The updates are grouped and can be sent with delay up to a few minutes. inline_query InlineQuery Optional . New incoming inline query chosen_inline_result ChosenInlineResult Optional . The result of an inline query that was chosen by a user and sent to their chat partner. Please see our documentation on the feedback collecting for details on how to enable these updates for your bot. callback_query CallbackQuery Optional . New incoming callback query shipping_query ShippingQuery Optional . New incoming shipping query. Only for invoices with flexible price pre_checkout_query PreCheckoutQuery Optional . New incoming pre-checkout query. Contains full information about checkout purchased_paid_media PaidMediaPurchased Optional . A user purchased paid media with a non-empty payload sent by the bot in a non-channel chat poll Poll Optional . New poll state. Bots receive only updates about manually stopped polls and polls, which are sent by the bot poll_answer PollAnswer Optional . A user changed their answer in a non-anonymous poll. Bots receive new votes only in polls that were sent by the bot itself. my_chat_member ChatMemberUpdated Optional . The bot's chat member status was updated in a chat. For private chats, this update is received only when the bot is blocked or unblocked by the user. chat_member ChatMemberUpdated Optional . A chat member's status was updated in a chat. The bot must be an administrator in the chat and must explicitly specify \"chat_member\" in the list of allowed_updates to receive these updates. chat_join_request ChatJoinRequest Optional . A request to join the chat has been sent. The bot must have the can_invite_users administrator right in the chat to receive these updates. chat_boost ChatBoostUpdated Optional . A chat boost was added or changed. The bot must be an administrator in the chat to receive these updates. removed_chat_boost ChatBoostRemoved Optional . A boost was removed from a chat. The bot must be an administrator in the chat to receive these updates. getUpdates Use this method to receive incoming updates using long polling ( wiki ). Returns an Array of Update objects. Parameter Type Required Description offset Integer Optional Identifier of the first update to be returned. Must be greater by one than the highest among the identifiers of previously received updates. By default, updates starting with the earliest unconfirmed update are returned. An update is considered confirmed as soon as getUpdates is called with an offset higher than its update_id . The negative offset can be specified to retrieve updates starting from -offset update from the end of the updates queue. All previous updates will be forgotten. limit Integer Optional Limits the number of updates to be retrieved. Values between 1-100 are accepted. Defaults to 100. timeout Integer Optional Timeout in seconds for long polling. Defaults to 0, i.e. usual short polling. Should be positive, short polling should be used for testing purposes only. allowed_updates Array of String Optional A JSON-serialized list of the update types you want your bot to receive. For example, specify [\"message\", \"edited_channel_post\", \"callback_query\"] to only receive updates of these types. See Update for a complete list of available update types. Specify an empty list to receive all update types except chat_member , message_reaction , and message_reaction_count (default). If not specified, the previous setting will be used. Please note that this parameter doesn't affect updates created before the call to the getUpdates, so unwanted updates may be received for a short period of time. Notes 1. This method will not work if an outgoing webhook is set up. 2. In order to avoid getting duplicate updates, recalculate offset after each server response. setWebhook Use this method to specify a URL and receive incoming updates via an outgoing webhook. Whenever there is an update for the bot, we will send an HTTPS POST request to the specified URL, containing a JSON-serialized Update . In case of an unsuccessful request, we will give up after a reasonable amount of attempts. Returns True on success. If you'd like to make sure that the webhook was set by you, you can specify secret data in the parameter secret_token . If specified, the request will contain a header “X-Telegram-Bot-Api-Secret-Token” with the secret token as content. Parameter Type Required Description url String Yes HTTPS URL to send updates to. Use an empty string to remove webhook integration certificate InputFile Optional Upload your public key certificate so that the root certificate in use can be checked. See our self-signed guide for details. ip_address String Optional The fixed IP address which will be used to send webhook requests instead of the IP address resolved through DNS max_connections Integer Optional The maximum allowed number of simultaneous HTTPS connections to the webhook for update delivery, 1-100. Defaults to 40 . Use lower values to limit the load on your bot's server, and higher values to increase your bot's throughput. allowed_updates Array of String Optional A JSON-serialized list of the update types you want your bot to receive. For example, specify [\"message\", \"edited_channel_post\", \"callback_query\"] to only receive updates of these types. See Update for a complete list of available update types. Specify an empty list to receive all update types except chat_member , message_reaction , and message_reaction_count (default). If not specified, the previous setting will be used. Please note that this parameter doesn't affect updates created before the call to the setWebhook, so unwanted updates may be received for a short period of time. drop_pending_updates Boolean Optional Pass True to drop all pending updates secret_token String Optional A secret token to be sent in a header “X-Telegram-Bot-Api-Secret-Token” in every webhook request, 1-256 characters. Only characters A-Z , a-z , 0-9 , _ and - are allowed. The header is useful to ensure that the request comes from a webhook set by you. Notes 1. You will not be able to receive updates using getUpdates for as long as an outgoing webhook is set up. 2. To use a self-signed certificate, you need to upload your public key certificate using certificate parameter. Please upload as InputFile, sending a String will not work. 3. Ports currently supported for webhooks : 443, 80, 88, 8443 . If you're having any trouble setting up webhooks, please check out this amazing guide to webhooks . deleteWebhook Use this method to remove webhook integration if you decide to switch back to getUpdates . Returns True on success. Parameter Type Required Description drop_pending_updates Boolean Optional Pass True to drop all pending updates getWebhookInfo Use this method to get current webhook status. Requires no parameters. On success, returns a WebhookInfo object. If the bot is using getUpdates , will return an object with the url field empty. WebhookInfo Describes the current status of a webhook. Field Type Description url String Webhook URL, may be empty if webhook is not set up has_custom_certificate Boolean True , if a custom certificate was provided for webhook certificate checks pending_update_count Integer Number of updates awaiting delivery ip_address String Optional . Currently used webhook IP address last_error_date Integer Optional . Unix time for the most recent error that happened when trying to deliver an update via webhook last_error_message String Optional . Error message in human-readable format for the most recent error that happened when trying to deliver an update via webhook last_synchronization_error_date Integer Optional . Unix time of the most recent error that happened when trying to synchronize available updates with Telegram datacenters max_connections Integer Optional . The maximum allowed number of simultaneous HTTPS connections to the webhook for update delivery allowed_updates Array of String Optional . A list of update types the bot is subscribed to. Defaults to all update types except chat_member Available types All types used in the Bot API responses are represented as JSON-objects. It is safe to use 32-bit signed integers for storing all Integer fields unless otherwise noted. Optional fields may be not returned when irrelevant. User This object represents a Telegram user or bot. Field Type Description id Integer Unique identifier for this user or bot. This number may have more than 32 significant bits and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a 64-bit integer or double-precision float type are safe for storing this identifier. is_bot Boolean True , if this user is a bot first_name String User's or bot's first name last_name String Optional . User's or bot's last name username String Optional . User's or bot's username language_code String Optional . IETF language tag of the user's language is_premium True Optional . True , if this user is a Telegram Premium user added_to_attachment_menu True Optional . True , if this user added the bot to the attachment menu can_join_groups Boolean Optional . True , if the bot can be invited to groups. Returned only in getMe . can_read_all_group_messages Boolean Optional . True , if privacy mode is disabled for the bot. Returned only in getMe . supports_inline_queries Boolean Optional . True , if the bot supports inline queries. Returned only in getMe . can_connect_to_business Boolean Optional . True , if the bot can be connected to a Telegram Business account to receive its messages. Returned only in getMe . has_main_web_app Boolean Optional . True , if the bot has a main Web App. Returned only in getMe . Chat This object represents a chat. Field Type Description id Integer Unique identifier for this chat. This number may have more than 32 significant bits and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a signed 64-bit integer or double-precision float type are safe for storing this identifier. type String Type of the chat, can be either “private”, “group”, “supergroup” or “channel” title String Optional . Title, for supergroups, channels and group chats username String Optional . Username, for private chats, supergroups and channels if available first_name String Optional . First name of the other party in a private chat last_name String Optional . Last name of the other party in a private chat is_forum True Optional . True , if the supergroup chat is a forum (has topics enabled) ChatFullInfo This object contains full information about a chat. Field Type Description id Integer Unique identifier for this chat. This number may have more than 32 significant bits and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a signed 64-bit integer or double-precision float type are safe for storing this identifier. type String Type of the chat, can be either “private”, “group”, “supergroup” or “channel” title String Optional . Title, for supergroups, channels and group chats username String Optional . Username, for private chats, supergroups and channels if available first_name String Optional . First name of the other party in a private chat last_name String Optional . Last name of the other party in a private chat is_forum True Optional . True , if the supergroup chat is a forum (has topics enabled) accent_color_id Integer Identifier of the accent color for the chat name and backgrounds of the chat photo, reply header, and link preview. See accent colors for more details. max_reaction_count Integer The maximum number of reactions that can be set on a message in the chat photo ChatPhoto Optional . Chat photo active_usernames Array of String Optional . If non-empty, the list of all active chat usernames ; for private chats, supergroups and channels birthdate Birthdate Optional . For private chats, the date of birth of the user business_intro BusinessIntro Optional . For private chats with business accounts, the intro of the business business_location BusinessLocation Optional . For private chats with business accounts, the location of the business business_opening_hours BusinessOpeningHours Optional . For private chats with business accounts, the opening hours of the business personal_chat Chat Optional . For private chats, the personal channel of the user available_reactions Array of ReactionType Optional . List of available reactions allowed in the chat. If omitted, then all emoji reactions are allowed. background_custom_emoji_id String Optional . Custom emoji identifier of the emoji chosen by the chat for the reply header and link preview background profile_accent_color_id Integer Optional . Identifier of the accent color for the chat's profile background. See profile accent colors for more details. profile_background_custom_emoji_id String Optional . Custom emoji identifier of the emoji chosen by the chat for its profile background emoji_status_custom_emoji_id String Optional . Custom emoji identifier of the emoji status of the chat or the other party in a private chat emoji_status_expiration_date Integer Optional . Expiration date of the emoji status of the chat or the other party in a private chat, in Unix time, if any bio String Optional . Bio of the other party in a private chat has_private_forwards True Optional . True , if privacy settings of the other party in the private chat allows to use tg://user?id=<user_id> links only in chats with the user has_restricted_voice_and_video_messages True Optional . True , if the privacy settings of the other party restrict sending voice and video note messages in the private chat join_to_send_messages True Optional . True , if users need to join the supergroup before they can send messages join_by_request True Optional . True , if all users directly joining the supergroup without using an invite link need to be approved by supergroup administrators description String Optional . Description, for groups, supergroups and channel chats invite_link String Optional . Primary invite link, for groups, supergroups and channel chats pinned_message Message Optional . The most recent pinned message (by sending date) permissions ChatPermissions Optional . Default chat member permissions, for groups and supergroups can_send_paid_media True Optional . True , if paid media messages can be sent or forwarded to the channel chat. The field is available only for channel chats. slow_mode_delay Integer Optional . For supergroups, the minimum allowed delay between consecutive messages sent by each unprivileged user; in seconds unrestrict_boost_count Integer Optional . For supergroups, the minimum number of boosts that a non-administrator user needs to add in order to ignore slow mode and chat permissions message_auto_delete_time Integer Optional . The time after which all messages sent to the chat will be automatically deleted; in seconds has_aggressive_anti_spam_enabled True Optional . True , if aggressive anti-spam checks are enabled in the supergroup. The field is only available to chat administrators. has_hidden_members True Optional . True , if non-administrators can only get the list of bots and administrators in the chat has_protected_content True Optional . True , if messages from the chat can't be forwarded to other chats has_visible_history True Optional . True , if new chat members will have access to old messages; available only to chat administrators sticker_set_name String Optional . For supergroups, name of the group sticker set can_set_sticker_set True Optional . True , if the bot can change the group sticker set custom_emoji_sticker_set_name String Optional . For supergroups, the name of the group's custom emoji sticker set. Custom emoji from this set can be used by all users and bots in the group. linked_chat_id Integer Optional . Unique identifier for the linked chat, i.e. the discussion group identifier for a channel and vice versa; for supergroups and channel chats. This identifier may be greater than 32 bits and some programming languages may have difficulty/silent defects in interpreting it. But it is smaller than 52 bits, so a signed 64 bit integer or double-precision float type are safe for storing this identifier. location ChatLocation Optional . For supergroups, the location to which the supergroup is connected Message This object represents a message. Field Type Description message_id Integer Unique message identifier inside this chat. In specific instances (e.g., message containing a video sent to a big chat), the server might automatically schedule a message instead of sending it immediately. In such cases, this field will be 0 and the relevant message will be unusable until it is actually sent message_thread_id Integer Optional . Unique identifier of a message thread to which the message belongs; for supergroups only from User Optional . Sender of the message; may be empty for messages sent to channels. For backward compatibility, if the message was sent on behalf of a chat, the field contains a fake sender user in non-channel chats sender_chat Chat Optional . Sender of the message when sent on behalf of a chat. For example, the supergroup itself for messages sent by its anonymous administrators or a linked channel for messages automatically forwarded to the channel's discussion group. For backward compatibility, if the message was sent on behalf of a chat, the field from contains a fake sender user in non-channel chats. sender_boost_count Integer Optional . If the sender of the message boosted the chat, the number of boosts added by the user sender_business_bot User Optional . The bot that actually sent the message on behalf of the business account. Available only for outgoing messages sent on behalf of the connected business account. date Integer Date the message was sent in Unix time. It is always a positive number, representing a valid date. business_connection_id String Optional . Unique identifier of the business connection from which the message was received. If non-empty, the message belongs to a chat of the corresponding business account that is independent from any potential bot chat which might share the same identifier. chat Chat Chat the message belongs to forward_origin MessageOrigin Optional . Information about the original message for forwarded messages is_topic_message True Optional . True , if the message is sent to a forum topic is_automatic_forward True Optional . True , if the message is a channel post that was automatically forwarded to the connected discussion group reply_to_message Message Optional . For replies in the same chat and message thread, the original message. Note that the Message object in this field will not contain further reply_to_message fields even if it itself is a reply. external_reply ExternalReplyInfo Optional . Information about the message that is being replied to, which may come from another chat or forum topic quote TextQuote Optional . For replies that quote part of the original message, the quoted part of the message reply_to_story Story Optional . For replies to a story, the original story via_bot User Optional . Bot through which the message was sent edit_date Integer Optional . Date the message was last edited in Unix time has_protected_content True Optional . True , if the message can't be forwarded is_from_offline True Optional . True, if the message was sent by an implicit action, for example, as an away or a greeting business message, or as a scheduled message media_group_id String Optional . The unique identifier of a media message group this message belongs to author_signature String Optional . Signature of the post author for messages in channels, or the custom title of an anonymous group administrator text String Optional . For text messages, the actual UTF-8 text of the message entities Array of MessageEntity Optional . For text messages, special entities like usernames, URLs, bot commands, etc. that appear in the text link_preview_options LinkPreviewOptions Optional . Options used for link preview generation for the message, if it is a text message and link preview options were changed effect_id String Optional . Unique identifier of the message effect added to the message animation Animation Optional . Message is an animation, information about the animation. For backward compatibility, when this field is set, the document field will also be set audio Audio Optional . Message is an audio file, information about the file document Document Optional . Message is a general file, information about the file paid_media PaidMediaInfo Optional . Message contains paid media; information about the paid media photo Array of PhotoSize Optional . Message is a photo, available sizes of the photo sticker Sticker Optional . Message is a sticker, information about the sticker story Story Optional . Message is a forwarded story video Video Optional . Message is a video, information about the video video_note VideoNote Optional . Message is a video note , information about the video message voice Voice Optional . Message is a voice message, information about the file caption String Optional . Caption for the animation, audio, document, paid media, photo, video or voice caption_entities Array of MessageEntity Optional . For messages with a caption, special entities like usernames, URLs, bot commands, etc. that appear in the caption show_caption_above_media True Optional . True, if the caption must be shown above the message media has_media_spoiler True Optional . True , if the message media is covered by a spoiler animation contact Contact Optional . Message is a shared contact, information about the contact dice Dice Optional . Message is a dice with random value game Game Optional . Message is a game, information about the game. More about games » poll Poll Optional . Message is a native poll, information about the poll venue Venue Optional . Message is a venue, information about the venue. For backward compatibility, when this field is set, the location field will also be set location Location Optional . Message is a shared location, information about the location new_chat_members Array of User Optional . New members that were added to the group or supergroup and information about them (the bot itself may be one of these members) left_chat_member User Optional . A member was removed from the group, information about them (this member may be the bot itself) new_chat_title String Optional . A chat title was changed to this value new_chat_photo Array of PhotoSize Optional . A chat photo was change to this value delete_chat_photo True Optional . Service message: the chat photo was deleted group_chat_created True Optional . Service message: the group has been created supergroup_chat_created True Optional . Service message: the supergroup has been created. This field can't be received in a message coming through updates, because bot can't be a member of a supergroup when it is created. It can only be found in reply_to_message if someone replies to a very first message in a directly created supergroup. channel_chat_created True Optional . Service message: the channel has been created. This field can't be received in a message coming through updates, because bot can't be a member of a channel when it is created. It can only be found in reply_to_message if someone replies to a very first message in a channel. message_auto_delete_timer_changed MessageAutoDeleteTimerChanged Optional . Service message: auto-delete timer settings changed in the chat migrate_to_chat_id Integer Optional . The group has been migrated to a supergroup with the specified identifier. This number may have more than 32 significant bits and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a signed 64-bit integer or double-precision float type are safe for storing this identifier. migrate_from_chat_id Integer Optional . The supergroup has been migrated from a group with the specified identifier. This number may have more than 32 significant bits and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a signed 64-bit integer or double-precision float type are safe for storing this identifier. pinned_message MaybeInaccessibleMessage Optional . Specified message was pinned. Note that the Message object in this field will not contain further reply_to_message fields even if it itself is a reply. invoice Invoice Optional . Message is an invoice for a payment , information about the invoice. More about payments » successful_payment SuccessfulPayment Optional . Message is a service message about a successful payment, information about the payment. More about payments » refunded_payment RefundedPayment Optional . Message is a service message about a refunded payment, information about the payment. More about payments » users_shared UsersShared Optional . Service message: users were shared with the bot chat_shared ChatShared Optional . Service message: a chat was shared with the bot connected_website String Optional . The domain name of the website on which the user has logged in. More about Telegram Login » write_access_allowed WriteAccessAllowed Optional . Service message: the user allowed the bot to write messages after adding it to the attachment or side menu, launching a Web App from a link, or accepting an explicit request from a Web App sent by the method requestWriteAccess passport_data PassportData Optional . Telegram Passport data proximity_alert_triggered ProximityAlertTriggered Optional . Service message. A user in the chat triggered another user's proximity alert while sharing Live Location. boost_added ChatBoostAdded Optional . Service message: user boosted the chat chat_background_set ChatBackground Optional . Service message: chat background set forum_topic_created ForumTopicCreated Optional . Service message: forum topic created forum_topic_edited ForumTopicEdited Optional . Service message: forum topic edited forum_topic_closed ForumTopicClosed Optional . Service message: forum topic closed forum_topic_reopened ForumTopicReopened Optional . Service message: forum topic reopened general_forum_topic_hidden GeneralForumTopicHidden Optional . Service message: the 'General' forum topic hidden general_forum_topic_unhidden GeneralForumTopicUnhidden Optional . Service message: the 'General' forum topic unhidden giveaway_created GiveawayCreated Optional . Service message: a scheduled giveaway was created giveaway Giveaway Optional . The message is a scheduled giveaway message giveaway_winners GiveawayWinners Optional . A giveaway with public winners was completed giveaway_completed GiveawayCompleted Optional . Service message: a giveaway without public winners was completed video_chat_scheduled VideoChatScheduled Optional . Service message: video chat scheduled video_chat_started VideoChatStarted Optional . Service message: video chat started video_chat_ended VideoChatEnded Optional . Service message: video chat ended video_chat_participants_invited VideoChatParticipantsInvited Optional . Service message: new participants invited to a video chat web_app_data WebAppData Optional . Service message: data sent by a Web App reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message. login_url buttons are represented as ordinary url buttons. MessageId This object represents a unique message identifier. Field Type Description message_id Integer Unique message identifier. In specific instances (e.g., message containing a video sent to a big chat), the server might automatically schedule a message instead of sending it immediately. In such cases, this field will be 0 and the relevant message will be unusable until it is actually sent InaccessibleMessage This object describes a message that was deleted or is otherwise inaccessible to the bot. Field Type Description chat Chat Chat the message belonged to message_id Integer Unique message identifier inside the chat date Integer Always 0. The field can be used to differentiate regular and inaccessible messages. MaybeInaccessibleMessage This object describes a message that can be inaccessible to the bot. It can be one of Message InaccessibleMessage MessageEntity This object represents one special entity in a text message. For example, hashtags, usernames, URLs, etc. Field Type Description type String Type of the entity. Currently, can be “mention” ( @username ), “hashtag” ( #hashtag or #hashtag@chatusername ), “cashtag” ( $USD or $USD@chatusername ), “bot_command” ( /start@jobs_bot ), “url” ( https://telegram.org ), “email” ( do-not-reply@telegram.org ), “phone_number” ( +1-212-555-0123 ), “bold” ( bold text ), “italic” ( italic text ), “underline” (underlined text), “strikethrough” (strikethrough text), “spoiler” (spoiler message), “blockquote” (block quotation), “expandable_blockquote” (collapsed-by-default block quotation), “code” (monowidth string), “pre” (monowidth block), “text_link” (for clickable text URLs), “text_mention” (for users without usernames ), “custom_emoji” (for inline custom emoji stickers) offset Integer Offset in UTF-16 code units to the start of the entity length Integer Length of the entity in UTF-16 code units url String Optional . For “text_link” only, URL that will be opened after user taps on the text user User Optional . For “text_mention” only, the mentioned user language String Optional . For “pre” only, the programming language of the entity text custom_emoji_id String Optional . For “custom_emoji” only, unique identifier of the custom emoji. Use getCustomEmojiStickers to get full information about the sticker TextQuote This object contains information about the quoted part of a message that is replied to by the given message. Field Type Description text String Text of the quoted part of a message that is replied to by the given message entities Array of MessageEntity Optional . Special entities that appear in the quote. Currently, only bold , italic , underline , strikethrough , spoiler , and custom_emoji entities are kept in quotes. position Integer Approximate quote position in the original message in UTF-16 code units as specified by the sender is_manual True Optional . True, if the quote was chosen manually by the message sender. Otherwise, the quote was added automatically by the server. ExternalReplyInfo This object contains information about a message that is being replied to, which may come from another chat or forum topic. Field Type Description origin MessageOrigin Origin of the message replied to by the given message chat Chat Optional . Chat the original message belongs to. Available only if the chat is a supergroup or a channel. message_id Integer Optional . Unique message identifier inside the original chat. Available only if the original chat is a supergroup or a channel. link_preview_options LinkPreviewOptions Optional . Options used for link preview generation for the original message, if it is a text message animation Animation Optional . Message is an animation, information about the animation audio Audio Optional . Message is an audio file, information about the file document Document Optional . Message is a general file, information about the file paid_media PaidMediaInfo Optional . Message contains paid media; information about the paid media photo Array of PhotoSize Optional . Message is a photo, available sizes of the photo sticker Sticker Optional . Message is a sticker, information about the sticker story Story Optional . Message is a forwarded story video Video Optional . Message is a video, information about the video video_note VideoNote Optional . Message is a video note , information about the video message voice Voice Optional . Message is a voice message, information about the file has_media_spoiler True Optional . True , if the message media is covered by a spoiler animation contact Contact Optional . Message is a shared contact, information about the contact dice Dice Optional . Message is a dice with random value game Game Optional . Message is a game, information about the game. More about games » giveaway Giveaway Optional . Message is a scheduled giveaway, information about the giveaway giveaway_winners GiveawayWinners Optional . A giveaway with public winners was completed invoice Invoice Optional . Message is an invoice for a payment , information about the invoice. More about payments » location Location Optional . Message is a shared location, information about the location poll Poll Optional . Message is a native poll, information about the poll venue Venue Optional . Message is a venue, information about the venue ReplyParameters Describes reply parameters for the message that is being sent. Field Type Description message_id Integer Identifier of the message that will be replied to in the current chat, or in the chat chat_id if it is specified chat_id Integer or String Optional . If the message to be replied to is from a different chat, unique identifier for the chat or username of the channel (in the format @channelusername ). Not supported for messages sent on behalf of a business account. allow_sending_without_reply Boolean Optional . Pass True if the message should be sent even if the specified message to be replied to is not found. Always False for replies in another chat or forum topic. Always True for messages sent on behalf of a business account. quote String Optional . Quoted part of the message to be replied to; 0-1024 characters after entities parsing. The quote must be an exact substring of the message to be replied to, including bold , italic , underline , strikethrough , spoiler , and custom_emoji entities. The message will fail to send if the quote isn't found in the original message. quote_parse_mode String Optional . Mode for parsing entities in the quote. See formatting options for more details. quote_entities Array of MessageEntity Optional . A JSON-serialized list of special entities that appear in the quote. It can be specified instead of quote_parse_mode . quote_position Integer Optional . Position of the quote in the original message in UTF-16 code units MessageOrigin This object describes the origin of a message. It can be one of MessageOriginUser MessageOriginHiddenUser MessageOriginChat MessageOriginChannel MessageOriginUser The message was originally sent by a known user. Field Type Description type String Type of the message origin, always “user” date Integer Date the message was sent originally in Unix time sender_user User User that sent the message originally MessageOriginHiddenUser The message was originally sent by an unknown user. Field Type Description type String Type of the message origin, always “hidden_user” date Integer Date the message was sent originally in Unix time sender_user_name String Name of the user that sent the message originally MessageOriginChat The message was originally sent on behalf of a chat to a group chat. Field Type Description type String Type of the message origin, always “chat” date Integer Date the message was sent originally in Unix time sender_chat Chat Chat that sent the message originally author_signature String Optional . For messages originally sent by an anonymous chat administrator, original message author signature MessageOriginChannel The message was originally sent to a channel chat. Field Type Description type String Type of the message origin, always “channel” date Integer Date the message was sent originally in Unix time chat Chat Channel chat to which the message was originally sent message_id Integer Unique message identifier inside the chat author_signature String Optional . Signature of the original post author PhotoSize This object represents one size of a photo or a file / sticker thumbnail. Field Type Description file_id String Identifier for this file, which can be used to download or reuse the file file_unique_id String Unique identifier for this file, which is supposed to be the same over time and for different bots. Can't be used to download or reuse the file. width Integer Photo width height Integer Photo height file_size Integer Optional . File size in bytes Animation This object represents an animation file (GIF or H.264/MPEG-4 AVC video without sound). Field Type Description file_id String Identifier for this file, which can be used to download or reuse the file file_unique_id String Unique identifier for this file, which is supposed to be the same over time and for different bots. Can't be used to download or reuse the file. width Integer Video width as defined by the sender height Integer Video height as defined by the sender duration Integer Duration of the video in seconds as defined by the sender thumbnail PhotoSize Optional . Animation thumbnail as defined by the sender file_name String Optional . Original animation filename as defined by the sender mime_type String Optional . MIME type of the file as defined by the sender file_size Integer Optional . File size in bytes. It can be bigger than 2^31 and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a signed 64-bit integer or double-precision float type are safe for storing this value. Audio This object represents an audio file to be treated as music by the Telegram clients. Field Type Description file_id String Identifier for this file, which can be used to download or reuse the file file_unique_id String Unique identifier for this file, which is supposed to be the same over time and for different bots. Can't be used to download or reuse the file. duration Integer Duration of the audio in seconds as defined by the sender performer String Optional . Performer of the audio as defined by the sender or by audio tags title String Optional . Title of the audio as defined by the sender or by audio tags file_name String Optional . Original filename as defined by the sender mime_type String Optional . MIME type of the file as defined by the sender file_size Integer Optional . File size in bytes. It can be bigger than 2^31 and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a signed 64-bit integer or double-precision float type are safe for storing this value. thumbnail PhotoSize Optional . Thumbnail of the album cover to which the music file belongs Document This object represents a general file (as opposed to photos , voice messages and audio files ). Field Type Description file_id String Identifier for this file, which can be used to download or reuse the file file_unique_id String Unique identifier for this file, which is supposed to be the same over time and for different bots. Can't be used to download or reuse the file. thumbnail PhotoSize Optional . Document thumbnail as defined by the sender file_name String Optional . Original filename as defined by the sender mime_type String Optional . MIME type of the file as defined by the sender file_size Integer Optional . File size in bytes. It can be bigger than 2^31 and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a signed 64-bit integer or double-precision float type are safe for storing this value. Story This object represents a story. Field Type Description chat Chat Chat that posted the story id Integer Unique identifier for the story in the chat Video This object represents a video file. Field Type Description file_id String Identifier for this file, which can be used to download or reuse the file file_unique_id String Unique identifier for this file, which is supposed to be the same over time and for different bots. Can't be used to download or reuse the file. width Integer Video width as defined by the sender height Integer Video height as defined by the sender duration Integer Duration of the video in seconds as defined by the sender thumbnail PhotoSize Optional . Video thumbnail file_name String Optional . Original filename as defined by the sender mime_type String Optional . MIME type of the file as defined by the sender file_size Integer Optional . File size in bytes. It can be bigger than 2^31 and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a signed 64-bit integer or double-precision float type are safe for storing this value. VideoNote This object represents a video message (available in Telegram apps as of v.4.0 ). Field Type Description file_id String Identifier for this file, which can be used to download or reuse the file file_unique_id String Unique identifier for this file, which is supposed to be the same over time and for different bots. Can't be used to download or reuse the file. length Integer Video width and height (diameter of the video message) as defined by the sender duration Integer Duration of the video in seconds as defined by the sender thumbnail PhotoSize Optional . Video thumbnail file_size Integer Optional . File size in bytes Voice This object represents a voice note. Field Type Description file_id String Identifier for this file, which can be used to download or reuse the file file_unique_id String Unique identifier for this file, which is supposed to be the same over time and for different bots. Can't be used to download or reuse the file. duration Integer Duration of the audio in seconds as defined by the sender mime_type String Optional . MIME type of the file as defined by the sender file_size Integer Optional . File size in bytes. It can be bigger than 2^31 and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a signed 64-bit integer or double-precision float type are safe for storing this value. PaidMediaInfo Describes the paid media added to a message. Field Type Description star_count Integer The number of Telegram Stars that must be paid to buy access to the media paid_media Array of PaidMedia Information about the paid media PaidMedia This object describes paid media. Currently, it can be one of PaidMediaPreview PaidMediaPhoto PaidMediaVideo PaidMediaPreview The paid media isn't available before the payment. Field Type Description type String Type of the paid media, always “preview” width Integer Optional . Media width as defined by the sender height Integer Optional . Media height as defined by the sender duration Integer Optional . Duration of the media in seconds as defined by the sender PaidMediaPhoto The paid media is a photo. Field Type Description type String Type of the paid media, always “photo” photo Array of PhotoSize The photo PaidMediaVideo The paid media is a video. Field Type Description type String Type of the paid media, always “video” video Video The video Contact This object represents a phone contact. Field Type Description phone_number String Contact's phone number first_name String Contact's first name last_name String Optional . Contact's last name user_id Integer Optional . Contact's user identifier in Telegram. This number may have more than 32 significant bits and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a 64-bit integer or double-precision float type are safe for storing this identifier. vcard String Optional . Additional data about the contact in the form of a vCard Dice This object represents an animated emoji that displays a random value. Field Type Description emoji String Emoji on which the dice throw animation is based value Integer Value of the dice, 1-6 for “ ”, “ ” and “ ” base emoji, 1-5 for “ ” and “ ” base emoji, 1-64 for “ ” base emoji PollOption This object contains information about one answer option in a poll. Field Type Description text String Option text, 1-100 characters text_entities Array of MessageEntity Optional . Special entities that appear in the option text . Currently, only custom emoji entities are allowed in poll option texts voter_count Integer Number of users that voted for this option InputPollOption This object contains information about one answer option in a poll to be sent. Field Type Description text String Option text, 1-100 characters text_parse_mode String Optional . Mode for parsing entities in the text. See formatting options for more details. Currently, only custom emoji entities are allowed text_entities Array of MessageEntity Optional . A JSON-serialized list of special entities that appear in the poll option text. It can be specified instead of text_parse_mode PollAnswer This object represents an answer of a user in a non-anonymous poll. Field Type Description poll_id String Unique poll identifier voter_chat Chat Optional . The chat that changed the answer to the poll, if the voter is anonymous user User Optional . The user that changed the answer to the poll, if the voter isn't anonymous option_ids Array of Integer 0-based identifiers of chosen answer options. May be empty if the vote was retracted. Poll This object contains information about a poll. Field Type Description id String Unique poll identifier question String Poll question, 1-300 characters question_entities Array of MessageEntity Optional . Special entities that appear in the question . Currently, only custom emoji entities are allowed in poll questions options Array of PollOption List of poll options total_voter_count Integer Total number of users that voted in the poll is_closed Boolean True , if the poll is closed is_anonymous Boolean True , if the poll is anonymous type String Poll type, currently can be “regular” or “quiz” allows_multiple_answers Boolean True , if the poll allows multiple answers correct_option_id Integer Optional . 0-based identifier of the correct answer option. Available only for polls in the quiz mode, which are closed, or was sent (not forwarded) by the bot or to the private chat with the bot. explanation String Optional . Text that is shown when a user chooses an incorrect answer or taps on the lamp icon in a quiz-style poll, 0-200 characters explanation_entities Array of MessageEntity Optional . Special entities like usernames, URLs, bot commands, etc. that appear in the explanation open_period Integer Optional . Amount of time in seconds the poll will be active after creation close_date Integer Optional . Point in time (Unix timestamp) when the poll will be automatically closed Location This object represents a point on the map. Field Type Description latitude Float Latitude as defined by the sender longitude Float Longitude as defined by the sender horizontal_accuracy Float Optional . The radius of uncertainty for the location, measured in meters; 0-1500 live_period Integer Optional . Time relative to the message sending date, during which the location can be updated; in seconds. For active live locations only. heading Integer Optional . The direction in which user is moving, in degrees; 1-360. For active live locations only. proximity_alert_radius Integer Optional . The maximum distance for proximity alerts about approaching another chat member, in meters. For sent live locations only. Venue This object represents a venue. Field Type Description location Location Venue location. Can't be a live location title String Name of the venue address String Address of the venue foursquare_id String Optional . Foursquare identifier of the venue foursquare_type String Optional . Foursquare type of the venue. (For example, “arts_entertainment/default”, “arts_entertainment/aquarium” or “food/icecream”.) google_place_id String Optional . Google Places identifier of the venue google_place_type String Optional . Google Places type of the venue. (See supported types .) WebAppData Describes data sent from a Web App to the bot. Field Type Description data String The data. Be aware that a bad client can send arbitrary data in this field. button_text String Text of the web_app keyboard button from which the Web App was opened. Be aware that a bad client can send arbitrary data in this field. ProximityAlertTriggered This object represents the content of a service message, sent whenever a user in the chat triggers a proximity alert set by another user. Field Type Description traveler User User that triggered the alert watcher User User that set the alert distance Integer The distance between the users MessageAutoDeleteTimerChanged This object represents a service message about a change in auto-delete timer settings. Field Type Description message_auto_delete_time Integer New auto-delete time for messages in the chat; in seconds ChatBoostAdded This object represents a service message about a user boosting a chat. Field Type Description boost_count Integer Number of boosts added by the user BackgroundFill This object describes the way a background is filled based on the selected colors. Currently, it can be one of BackgroundFillSolid BackgroundFillGradient BackgroundFillFreeformGradient BackgroundFillSolid The background is filled using the selected color. Field Type Description type String Type of the background fill, always “solid” color Integer The color of the background fill in the RGB24 format BackgroundFillGradient The background is a gradient fill. Field Type Description type String Type of the background fill, always “gradient” top_color Integer Top color of the gradient in the RGB24 format bottom_color Integer Bottom color of the gradient in the RGB24 format rotation_angle Integer Clockwise rotation angle of the background fill in degrees; 0-359 BackgroundFillFreeformGradient The background is a freeform gradient that rotates after every message in the chat. Field Type Description type String Type of the background fill, always “freeform_gradient” colors Array of Integer A list of the 3 or 4 base colors that are used to generate the freeform gradient in the RGB24 format BackgroundType This object describes the type of a background. Currently, it can be one of BackgroundTypeFill BackgroundTypeWallpaper BackgroundTypePattern BackgroundTypeChatTheme BackgroundTypeFill The background is automatically filled based on the selected colors. Field Type Description type String Type of the background, always “fill” fill BackgroundFill The background fill dark_theme_dimming Integer Dimming of the background in dark themes, as a percentage; 0-100 BackgroundTypeWallpaper The background is a wallpaper in the JPEG format. Field Type Description type String Type of the background, always “wallpaper” document Document Document with the wallpaper dark_theme_dimming Integer Dimming of the background in dark themes, as a percentage; 0-100 is_blurred True Optional . True , if the wallpaper is downscaled to fit in a 450x450 square and then box-blurred with radius 12 is_moving True Optional . True , if the background moves slightly when the device is tilted BackgroundTypePattern The background is a PNG or TGV (gzipped subset of SVG with MIME type “application/x-tgwallpattern”) pattern to be combined with the background fill chosen by the user. Field Type Description type String Type of the background, always “pattern” document Document Document with the pattern fill BackgroundFill The background fill that is combined with the pattern intensity Integer Intensity of the pattern when it is shown above the filled background; 0-100 is_inverted True Optional . True , if the background fill must be applied only to the pattern itself. All other pixels are black in this case. For dark themes only is_moving True Optional . True , if the background moves slightly when the device is tilted BackgroundTypeChatTheme The background is taken directly from a built-in chat theme. Field Type Description type String Type of the background, always “chat_theme” theme_name String Name of the chat theme, which is usually an emoji ChatBackground This object represents a chat background. Field Type Description type BackgroundType Type of the background ForumTopicCreated This object represents a service message about a new forum topic created in the chat. Field Type Description name String Name of the topic icon_color Integer Color of the topic icon in RGB format icon_custom_emoji_id String Optional . Unique identifier of the custom emoji shown as the topic icon ForumTopicClosed This object represents a service message about a forum topic closed in the chat. Currently holds no information. ForumTopicEdited This object represents a service message about an edited forum topic. Field Type Description name String Optional . New name of the topic, if it was edited icon_custom_emoji_id String Optional . New identifier of the custom emoji shown as the topic icon, if it was edited; an empty string if the icon was removed ForumTopicReopened This object represents a service message about a forum topic reopened in the chat. Currently holds no information. GeneralForumTopicHidden This object represents a service message about General forum topic hidden in the chat. Currently holds no information. GeneralForumTopicUnhidden This object represents a service message about General forum topic unhidden in the chat. Currently holds no information. SharedUser This object contains information about a user that was shared with the bot using a KeyboardButtonRequestUsers button. Field Type Description user_id Integer Identifier of the shared user. This number may have more than 32 significant bits and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so 64-bit integers or double-precision float types are safe for storing these identifiers. The bot may not have access to the user and could be unable to use this identifier, unless the user is already known to the bot by some other means. first_name String Optional . First name of the user, if the name was requested by the bot last_name String Optional . Last name of the user, if the name was requested by the bot username String Optional . Username of the user, if the username was requested by the bot photo Array of PhotoSize Optional . Available sizes of the chat photo, if the photo was requested by the bot UsersShared This object contains information about the users whose identifiers were shared with the bot using a KeyboardButtonRequestUsers button. Field Type Description request_id Integer Identifier of the request users Array of SharedUser Information about users shared with the bot. ChatShared This object contains information about a chat that was shared with the bot using a KeyboardButtonRequestChat button. Field Type Description request_id Integer Identifier of the request chat_id Integer Identifier of the shared chat. This number may have more than 32 significant bits and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a 64-bit integer or double-precision float type are safe for storing this identifier. The bot may not have access to the chat and could be unable to use this identifier, unless the chat is already known to the bot by some other means. title String Optional . Title of the chat, if the title was requested by the bot. username String Optional . Username of the chat, if the username was requested by the bot and available. photo Array of PhotoSize Optional . Available sizes of the chat photo, if the photo was requested by the bot WriteAccessAllowed This object represents a service message about a user allowing a bot to write messages after adding it to the attachment menu, launching a Web App from a link, or accepting an explicit request from a Web App sent by the method requestWriteAccess . Field Type Description from_request Boolean Optional . True, if the access was granted after the user accepted an explicit request from a Web App sent by the method requestWriteAccess web_app_name String Optional . Name of the Web App, if the access was granted when the Web App was launched from a link from_attachment_menu Boolean Optional . True, if the access was granted when the bot was added to the attachment or side menu VideoChatScheduled This object represents a service message about a video chat scheduled in the chat. Field Type Description start_date Integer Point in time (Unix timestamp) when the video chat is supposed to be started by a chat administrator VideoChatStarted This object represents a service message about a video chat started in the chat. Currently holds no information. VideoChatEnded This object represents a service message about a video chat ended in the chat. Field Type Description duration Integer Video chat duration in seconds VideoChatParticipantsInvited This object represents a service message about new members invited to a video chat. Field Type Description users Array of User New members that were invited to the video chat GiveawayCreated This object represents a service message about the creation of a scheduled giveaway. Field Type Description prize_star_count Integer Optional . The number of Telegram Stars to be split between giveaway winners; for Telegram Star giveaways only Giveaway This object represents a message about a scheduled giveaway. Field Type Description chats Array of Chat The list of chats which the user must join to participate in the giveaway winners_selection_date Integer Point in time (Unix timestamp) when winners of the giveaway will be selected winner_count Integer The number of users which are supposed to be selected as winners of the giveaway only_new_members True Optional . True , if only users who join the chats after the giveaway started should be eligible to win has_public_winners True Optional . True , if the list of giveaway winners will be visible to everyone prize_description String Optional . Description of additional giveaway prize country_codes Array of String Optional . A list of two-letter ISO 3166-1 alpha-2 country codes indicating the countries from which eligible users for the giveaway must come. If empty, then all users can participate in the giveaway. Users with a phone number that was bought on Fragment can always participate in giveaways. prize_star_count Integer Optional . The number of Telegram Stars to be split between giveaway winners; for Telegram Star giveaways only premium_subscription_month_count Integer Optional . The number of months the Telegram Premium subscription won from the giveaway will be active for; for Telegram Premium giveaways only GiveawayWinners This object represents a message about the completion of a giveaway with public winners. Field Type Description chat Chat The chat that created the giveaway giveaway_message_id Integer Identifier of the message with the giveaway in the chat winners_selection_date Integer Point in time (Unix timestamp) when winners of the giveaway were selected winner_count Integer Total number of winners in the giveaway winners Array of User List of up to 100 winners of the giveaway additional_chat_count Integer Optional . The number of other chats the user had to join in order to be eligible for the giveaway prize_star_count Integer Optional . The number of Telegram Stars that were split between giveaway winners; for Telegram Star giveaways only premium_subscription_month_count Integer Optional . The number of months the Telegram Premium subscription won from the giveaway will be active for; for Telegram Premium giveaways only unclaimed_prize_count Integer Optional . Number of undistributed prizes only_new_members True Optional . True , if only users who had joined the chats after the giveaway started were eligible to win was_refunded True Optional . True , if the giveaway was canceled because the payment for it was refunded prize_description String Optional . Description of additional giveaway prize GiveawayCompleted This object represents a service message about the completion of a giveaway without public winners. Field Type Description winner_count Integer Number of winners in the giveaway unclaimed_prize_count Integer Optional . Number of undistributed prizes giveaway_message Message Optional . Message with the giveaway that was completed, if it wasn't deleted is_star_giveaway True Optional . True , if the giveaway is a Telegram Star giveaway. Otherwise, currently, the giveaway is a Telegram Premium giveaway. LinkPreviewOptions Describes the options used for link preview generation. Field Type Description is_disabled Boolean Optional . True , if the link preview is disabled url String Optional . URL to use for the link preview. If empty, then the first URL found in the message text will be used prefer_small_media Boolean Optional . True , if the media in the link preview is supposed to be shrunk; ignored if the URL isn't explicitly specified or media size change isn't supported for the preview prefer_large_media Boolean Optional . True , if the media in the link preview is supposed to be enlarged; ignored if the URL isn't explicitly specified or media size change isn't supported for the preview show_above_text Boolean Optional . True , if the link preview must be shown above the message text; otherwise, the link preview will be shown below the message text UserProfilePhotos This object represent a user's profile pictures. Field Type Description total_count Integer Total number of profile pictures the target user has photos Array of Array of PhotoSize Requested profile pictures (in up to 4 sizes each) File This object represents a file ready to be downloaded. The file can be downloaded via the link https://api.telegram.org/file/bot<token>/<file_path> . It is guaranteed that the link will be valid for at least 1 hour. When the link expires, a new one can be requested by calling getFile . The maximum file size to download is 20 MB Field Type Description file_id String Identifier for this file, which can be used to download or reuse the file file_unique_id String Unique identifier for this file, which is supposed to be the same over time and for different bots. Can't be used to download or reuse the file. file_size Integer Optional . File size in bytes. It can be bigger than 2^31 and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a signed 64-bit integer or double-precision float type are safe for storing this value. file_path String Optional . File path. Use https://api.telegram.org/file/bot<token>/<file_path> to get the file. WebAppInfo Describes a Web App . Field Type Description url String An HTTPS URL of a Web App to be opened with additional data as specified in Initializing Web Apps ReplyKeyboardMarkup This object represents a custom keyboard with reply options (see Introduction to bots for details and examples). Not supported in channels and for messages sent on behalf of a Telegram Business account. Field Type Description keyboard Array of Array of KeyboardButton Array of button rows, each represented by an Array of KeyboardButton objects is_persistent Boolean Optional . Requests clients to always show the keyboard when the regular keyboard is hidden. Defaults to false , in which case the custom keyboard can be hidden and opened with a keyboard icon. resize_keyboard Boolean Optional . Requests clients to resize the keyboard vertically for optimal fit (e.g., make the keyboard smaller if there are just two rows of buttons). Defaults to false , in which case the custom keyboard is always of the same height as the app's standard keyboard. one_time_keyboard Boolean Optional . Requests clients to hide the keyboard as soon as it's been used. The keyboard will still be available, but clients will automatically display the usual letter-keyboard in the chat - the user can press a special button in the input field to see the custom keyboard again. Defaults to false . input_field_placeholder String Optional . The placeholder to be shown in the input field when the keyboard is active; 1-64 characters selective Boolean Optional . Use this parameter if you want to show the keyboard to specific users only. Targets: 1) users that are @mentioned in the text of the Message object; 2) if the bot's message is a reply to a message in the same chat and forum topic, sender of the original message. Example: A user requests to change the bot's language, bot replies to the request with a keyboard to select the new language. Other users in the group don't see the keyboard. KeyboardButton This object represents one button of the reply keyboard. At most one of the optional fields must be used to specify type of the button. For simple text buttons, String can be used instead of this object to specify the button text. Field Type Description text String Text of the button. If none of the optional fields are used, it will be sent as a message when the button is pressed request_users KeyboardButtonRequestUsers Optional. If specified, pressing the button will open a list of suitable users. Identifiers of selected users will be sent to the bot in a “users_shared” service message. Available in private chats only. request_chat KeyboardButtonRequestChat Optional. If specified, pressing the button will open a list of suitable chats. Tapping on a chat will send its identifier to the bot in a “chat_shared” service message. Available in private chats only. request_contact Boolean Optional . If True , the user's phone number will be sent as a contact when the button is pressed. Available in private chats only. request_location Boolean Optional . If True , the user's current location will be sent when the button is pressed. Available in private chats only. request_poll KeyboardButtonPollType Optional . If specified, the user will be asked to create a poll and send it to the bot when the button is pressed. Available in private chats only. web_app WebAppInfo Optional . If specified, the described Web App will be launched when the button is pressed. The Web App will be able to send a “web_app_data” service message. Available in private chats only. Note: request_users and request_chat options will only work in Telegram versions released after 3 February, 2023. Older clients will display unsupported message . KeyboardButtonRequestUsers This object defines the criteria used to request suitable users. Information about the selected users will be shared with the bot when the corresponding button is pressed. More about requesting users » Field Type Description request_id Integer Signed 32-bit identifier of the request that will be received back in the UsersShared object. Must be unique within the message user_is_bot Boolean Optional . Pass True to request bots, pass False to request regular users. If not specified, no additional restrictions are applied. user_is_premium Boolean Optional . Pass True to request premium users, pass False to request non-premium users. If not specified, no additional restrictions are applied. max_quantity Integer Optional . The maximum number of users to be selected; 1-10. Defaults to 1. request_name Boolean Optional . Pass True to request the users' first and last names request_username Boolean Optional . Pass True to request the users' usernames request_photo Boolean Optional . Pass True to request the users' photos KeyboardButtonRequestChat This object defines the criteria used to request a suitable chat. Information about the selected chat will be shared with the bot when the corresponding button is pressed. The bot will be granted requested rights in the chat if appropriate. More about requesting chats » . Field Type Description request_id Integer Signed 32-bit identifier of the request, which will be received back in the ChatShared object. Must be unique within the message chat_is_channel Boolean Pass True to request a channel chat, pass False to request a group or a supergroup chat. chat_is_forum Boolean Optional . Pass True to request a forum supergroup, pass False to request a non-forum chat. If not specified, no additional restrictions are applied. chat_has_username Boolean Optional . Pass True to request a supergroup or a channel with a username, pass False to request a chat without a username. If not specified, no additional restrictions are applied. chat_is_created Boolean Optional . Pass True to request a chat owned by the user. Otherwise, no additional restrictions are applied. user_administrator_rights ChatAdministratorRights Optional . A JSON-serialized object listing the required administrator rights of the user in the chat. The rights must be a superset of bot_administrator_rights . If not specified, no additional restrictions are applied. bot_administrator_rights ChatAdministratorRights Optional . A JSON-serialized object listing the required administrator rights of the bot in the chat. The rights must be a subset of user_administrator_rights . If not specified, no additional restrictions are applied. bot_is_member Boolean Optional . Pass True to request a chat with the bot as a member. Otherwise, no additional restrictions are applied. request_title Boolean Optional . Pass True to request the chat's title request_username Boolean Optional . Pass True to request the chat's username request_photo Boolean Optional . Pass True to request the chat's photo KeyboardButtonPollType This object represents type of a poll, which is allowed to be created and sent when the corresponding button is pressed. Field Type Description type String Optional . If quiz is passed, the user will be allowed to create only polls in the quiz mode. If regular is passed, only regular polls will be allowed. Otherwise, the user will be allowed to create a poll of any type. ReplyKeyboardRemove Upon receiving a message with this object, Telegram clients will remove the current custom keyboard and display the default letter-keyboard. By default, custom keyboards are displayed until a new keyboard is sent by a bot. An exception is made for one-time keyboards that are hidden immediately after the user presses a button (see ReplyKeyboardMarkup ). Not supported in channels and for messages sent on behalf of a Telegram Business account. Field Type Description remove_keyboard True Requests clients to remove the custom keyboard (user will not be able to summon this keyboard; if you want to hide the keyboard from sight but keep it accessible, use one_time_keyboard in ReplyKeyboardMarkup ) selective Boolean Optional . Use this parameter if you want to remove the keyboard for specific users only. Targets: 1) users that are @mentioned in the text of the Message object; 2) if the bot's message is a reply to a message in the same chat and forum topic, sender of the original message. Example: A user votes in a poll, bot returns confirmation message in reply to the vote and removes the keyboard for that user, while still showing the keyboard with poll options to users who haven't voted yet. InlineKeyboardMarkup This object represents an inline keyboard that appears right next to the message it belongs to. Field Type Description inline_keyboard Array of Array of InlineKeyboardButton Array of button rows, each represented by an Array of InlineKeyboardButton objects InlineKeyboardButton This object represents one button of an inline keyboard. Exactly one of the optional fields must be used to specify type of the button. Field Type Description text String Label text on the button url String Optional . HTTP or tg:// URL to be opened when the button is pressed. Links tg://user?id=<user_id> can be used to mention a user by their identifier without using a username, if this is allowed by their privacy settings. callback_data String Optional . Data to be sent in a callback query to the bot when the button is pressed, 1-64 bytes web_app WebAppInfo Optional . Description of the Web App that will be launched when the user presses the button. The Web App will be able to send an arbitrary message on behalf of the user using the method answerWebAppQuery . Available only in private chats between a user and the bot. Not supported for messages sent on behalf of a Telegram Business account. login_url LoginUrl Optional . An HTTPS URL used to automatically authorize the user. Can be used as a replacement for the Telegram Login Widget . switch_inline_query String Optional . If set, pressing the button will prompt the user to select one of their chats, open that chat and insert the bot's username and the specified inline query in the input field. May be empty, in which case just the bot's username will be inserted. Not supported for messages sent on behalf of a Telegram Business account. switch_inline_query_current_chat String Optional . If set, pressing the button will insert the bot's username and the specified inline query in the current chat's input field. May be empty, in which case only the bot's username will be inserted. This offers a quick way for the user to open your bot in inline mode in the same chat - good for selecting something from multiple options. Not supported in channels and for messages sent on behalf of a Telegram Business account. switch_inline_query_chosen_chat SwitchInlineQueryChosenChat Optional . If set, pressing the button will prompt the user to select one of their chats of the specified type, open that chat and insert the bot's username and the specified inline query in the input field. Not supported for messages sent on behalf of a Telegram Business account. copy_text CopyTextButton Optional . Description of the button that copies the specified text to the clipboard. callback_game CallbackGame Optional . Description of the game that will be launched when the user presses the button. NOTE: This type of button must always be the first button in the first row. pay Boolean Optional . Specify True , to send a Pay button . Substrings “ ” and “XTR” in the buttons's text will be replaced with a Telegram Star icon. NOTE: This type of button must always be the first button in the first row and can only be used in invoice messages. LoginUrl This object represents a parameter of the inline keyboard button used to automatically authorize a user. Serves as a great replacement for the Telegram Login Widget when the user is coming from Telegram. All the user needs to do is tap/click a button and confirm that they want to log in: Telegram apps support these buttons as of version 5.7 . Sample bot: @discussbot Field Type Description url String An HTTPS URL to be opened with user authorization data added to the query string when the button is pressed. If the user refuses to provide authorization data, the original URL without information about the user will be opened. The data added is the same as described in Receiving authorization data . NOTE: You must always check the hash of the received data to verify the authentication and the integrity of the data as described in Checking authorization . forward_text String Optional . New text of the button in forwarded messages. bot_username String Optional . Username of a bot, which will be used for user authorization. See Setting up a bot for more details. If not specified, the current bot's username will be assumed. The url 's domain must be the same as the domain linked with the bot. See Linking your domain to the bot for more details. request_write_access Boolean Optional . Pass True to request the permission for your bot to send messages to the user. SwitchInlineQueryChosenChat This object represents an inline button that switches the current user to inline mode in a chosen chat, with an optional default inline query. Field Type Description query String Optional . The default inline query to be inserted in the input field. If left empty, only the bot's username will be inserted allow_user_chats Boolean Optional . True, if private chats with users can be chosen allow_bot_chats Boolean Optional . True, if private chats with bots can be chosen allow_group_chats Boolean Optional . True, if group and supergroup chats can be chosen allow_channel_chats Boolean Optional . True, if channel chats can be chosen CopyTextButton This object represents an inline keyboard button that copies specified text to the clipboard. Field Type Description text String The text to be copied to the clipboard; 1-256 characters CallbackQuery This object represents an incoming callback query from a callback button in an inline keyboard . If the button that originated the query was attached to a message sent by the bot, the field message will be present. If the button was attached to a message sent via the bot (in inline mode ), the field inline_message_id will be present. Exactly one of the fields data or game_short_name will be present. Field Type Description id String Unique identifier for this query from User Sender message MaybeInaccessibleMessage Optional . Message sent by the bot with the callback button that originated the query inline_message_id String Optional . Identifier of the message sent via the bot in inline mode, that originated the query. chat_instance String Global identifier, uniquely corresponding to the chat to which the message with the callback button was sent. Useful for high scores in games . data String Optional . Data associated with the callback button. Be aware that the message originated the query can contain no callback buttons with this data. game_short_name String Optional . Short name of a Game to be returned, serves as the unique identifier for the game NOTE: After the user presses a callback button, Telegram clients will display a progress bar until you call answerCallbackQuery . It is, therefore, necessary to react by calling answerCallbackQuery even if no notification to the user is needed (e.g., without specifying any of the optional parameters). ForceReply Upon receiving a message with this object, Telegram clients will display a reply interface to the user (act as if the user has selected the bot's message and tapped 'Reply'). This can be extremely useful if you want to create user-friendly step-by-step interfaces without having to sacrifice privacy mode . Not supported in channels and for messages sent on behalf of a Telegram Business account. Field Type Description force_reply True Shows reply interface to the user, as if they manually selected the bot's message and tapped 'Reply' input_field_placeholder String Optional . The placeholder to be shown in the input field when the reply is active; 1-64 characters selective Boolean Optional . Use this parameter if you want to force reply from specific users only. Targets: 1) users that are @mentioned in the text of the Message object; 2) if the bot's message is a reply to a message in the same chat and forum topic, sender of the original message. Example: A poll bot for groups runs in privacy mode (only receives commands, replies to its messages and mentions). There could be two ways to create a new poll: Explain the user how to send a command with parameters (e.g. /newpoll question answer1 answer2). May be appealing for hardcore users but lacks modern day polish. Guide the user through a step-by-step process. 'Please send me your question', 'Cool, now let's add the first answer option', 'Great. Keep adding answer options, then send /done when you're ready'. The last option is definitely more attractive. And if you use ForceReply in your bot's questions, it will receive the user's answers even if it only receives replies, commands and mentions - without any extra work for the user. ChatPhoto This object represents a chat photo. Field Type Description small_file_id String File identifier of small (160x160) chat photo. This file_id can be used only for photo download and only for as long as the photo is not changed. small_file_unique_id String Unique file identifier of small (160x160) chat photo, which is supposed to be the same over time and for different bots. Can't be used to download or reuse the file. big_file_id String File identifier of big (640x640) chat photo. This file_id can be used only for photo download and only for as long as the photo is not changed. big_file_unique_id String Unique file identifier of big (640x640) chat photo, which is supposed to be the same over time and for different bots. Can't be used to download or reuse the file. ChatInviteLink Represents an invite link for a chat. Field Type Description invite_link String The invite link. If the link was created by another chat administrator, then the second part of the link will be replaced with “…”. creator User Creator of the link creates_join_request Boolean True , if users joining the chat via the link need to be approved by chat administrators is_primary Boolean True , if the link is primary is_revoked Boolean True , if the link is revoked name String Optional . Invite link name expire_date Integer Optional . Point in time (Unix timestamp) when the link will expire or has been expired member_limit Integer Optional . The maximum number of users that can be members of the chat simultaneously after joining the chat via this invite link; 1-99999 pending_join_request_count Integer Optional . Number of pending join requests created using this link subscription_period Integer Optional . The number of seconds the subscription will be active for before the next payment subscription_price Integer Optional . The amount of Telegram Stars a user must pay initially and after each subsequent subscription period to be a member of the chat using the link ChatAdministratorRights Represents the rights of an administrator in a chat. Field Type Description is_anonymous Boolean True , if the user's presence in the chat is hidden can_manage_chat Boolean True , if the administrator can access the chat event log, get boost list, see hidden supergroup and channel members, report spam messages and ignore slow mode. Implied by any other administrator privilege. can_delete_messages Boolean True , if the administrator can delete messages of other users can_manage_video_chats Boolean True , if the administrator can manage video chats can_restrict_members Boolean True , if the administrator can restrict, ban or unban chat members, or access supergroup statistics can_promote_members Boolean True , if the administrator can add new administrators with a subset of their own privileges or demote administrators that they have promoted, directly or indirectly (promoted by administrators that were appointed by the user) can_change_info Boolean True , if the user is allowed to change the chat title, photo and other settings can_invite_users Boolean True , if the user is allowed to invite new users to the chat can_post_stories Boolean True , if the administrator can post stories to the chat can_edit_stories Boolean True , if the administrator can edit stories posted by other users, post stories to the chat page, pin chat stories, and access the chat's story archive can_delete_stories Boolean True , if the administrator can delete stories posted by other users can_post_messages Boolean Optional . True , if the administrator can post messages in the channel, or access channel statistics; for channels only can_edit_messages Boolean Optional . True , if the administrator can edit messages of other users and can pin messages; for channels only can_pin_messages Boolean Optional . True , if the user is allowed to pin messages; for groups and supergroups only can_manage_topics Boolean Optional . True , if the user is allowed to create, rename, close, and reopen forum topics; for supergroups only ChatMemberUpdated This object represents changes in the status of a chat member. Field Type Description chat Chat Chat the user belongs to from User Performer of the action, which resulted in the change date Integer Date the change was done in Unix time old_chat_member ChatMember Previous information about the chat member new_chat_member ChatMember New information about the chat member invite_link ChatInviteLink Optional . Chat invite link, which was used by the user to join the chat; for joining by invite link events only. via_join_request Boolean Optional . True, if the user joined the chat after sending a direct join request without using an invite link and being approved by an administrator via_chat_folder_invite_link Boolean Optional . True, if the user joined the chat via a chat folder invite link ChatMember This object contains information about one member of a chat. Currently, the following 6 types of chat members are supported: ChatMemberOwner ChatMemberAdministrator ChatMemberMember ChatMemberRestricted ChatMemberLeft ChatMemberBanned ChatMemberOwner Represents a chat member that owns the chat and has all administrator privileges. Field Type Description status String The member's status in the chat, always “creator” user User Information about the user is_anonymous Boolean True , if the user's presence in the chat is hidden custom_title String Optional . Custom title for this user ChatMemberAdministrator Represents a chat member that has some additional privileges. Field Type Description status String The member's status in the chat, always “administrator” user User Information about the user can_be_edited Boolean True , if the bot is allowed to edit administrator privileges of that user is_anonymous Boolean True , if the user's presence in the chat is hidden can_manage_chat Boolean True , if the administrator can access the chat event log, get boost list, see hidden supergroup and channel members, report spam messages and ignore slow mode. Implied by any other administrator privilege. can_delete_messages Boolean True , if the administrator can delete messages of other users can_manage_video_chats Boolean True , if the administrator can manage video chats can_restrict_members Boolean True , if the administrator can restrict, ban or unban chat members, or access supergroup statistics can_promote_members Boolean True , if the administrator can add new administrators with a subset of their own privileges or demote administrators that they have promoted, directly or indirectly (promoted by administrators that were appointed by the user) can_change_info Boolean True , if the user is allowed to change the chat title, photo and other settings can_invite_users Boolean True , if the user is allowed to invite new users to the chat can_post_stories Boolean True , if the administrator can post stories to the chat can_edit_stories Boolean True , if the administrator can edit stories posted by other users, post stories to the chat page, pin chat stories, and access the chat's story archive can_delete_stories Boolean True , if the administrator can delete stories posted by other users can_post_messages Boolean Optional . True , if the administrator can post messages in the channel, or access channel statistics; for channels only can_edit_messages Boolean Optional . True , if the administrator can edit messages of other users and can pin messages; for channels only can_pin_messages Boolean Optional . True , if the user is allowed to pin messages; for groups and supergroups only can_manage_topics Boolean Optional . True , if the user is allowed to create, rename, close, and reopen forum topics; for supergroups only custom_title String Optional . Custom title for this user ChatMemberMember Represents a chat member that has no additional privileges or restrictions. Field Type Description status String The member's status in the chat, always “member” user User Information about the user until_date Integer Optional . Date when the user's subscription will expire; Unix time ChatMemberRestricted Represents a chat member that is under certain restrictions in the chat. Supergroups only. Field Type Description status String The member's status in the chat, always “restricted” user User Information about the user is_member Boolean True , if the user is a member of the chat at the moment of the request can_send_messages Boolean True , if the user is allowed to send text messages, contacts, giveaways, giveaway winners, invoices, locations and venues can_send_audios Boolean True , if the user is allowed to send audios can_send_documents Boolean True , if the user is allowed to send documents can_send_photos Boolean True , if the user is allowed to send photos can_send_videos Boolean True , if the user is allowed to send videos can_send_video_notes Boolean True , if the user is allowed to send video notes can_send_voice_notes Boolean True , if the user is allowed to send voice notes can_send_polls Boolean True , if the user is allowed to send polls can_send_other_messages Boolean True , if the user is allowed to send animations, games, stickers and use inline bots can_add_web_page_previews Boolean True , if the user is allowed to add web page previews to their messages can_change_info Boolean True , if the user is allowed to change the chat title, photo and other settings can_invite_users Boolean True , if the user is allowed to invite new users to the chat can_pin_messages Boolean True , if the user is allowed to pin messages can_manage_topics Boolean True , if the user is allowed to create forum topics until_date Integer Date when restrictions will be lifted for this user; Unix time. If 0, then the user is restricted forever ChatMemberLeft Represents a chat member that isn't currently a member of the chat, but may join it themselves. Field Type Description status String The member's status in the chat, always “left” user User Information about the user ChatMemberBanned Represents a chat member that was banned in the chat and can't return to the chat or view chat messages. Field Type Description status String The member's status in the chat, always “kicked” user User Information about the user until_date Integer Date when restrictions will be lifted for this user; Unix time. If 0, then the user is banned forever ChatJoinRequest Represents a join request sent to a chat. Field Type Description chat Chat Chat to which the request was sent from User User that sent the join request user_chat_id Integer Identifier of a private chat with the user who sent the join request. This number may have more than 32 significant bits and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a 64-bit integer or double-precision float type are safe for storing this identifier. The bot can use this identifier for 5 minutes to send messages until the join request is processed, assuming no other administrator contacted the user. date Integer Date the request was sent in Unix time bio String Optional . Bio of the user. invite_link ChatInviteLink Optional . Chat invite link that was used by the user to send the join request ChatPermissions Describes actions that a non-administrator user is allowed to take in a chat. Field Type Description can_send_messages Boolean Optional . True , if the user is allowed to send text messages, contacts, giveaways, giveaway winners, invoices, locations and venues can_send_audios Boolean Optional . True , if the user is allowed to send audios can_send_documents Boolean Optional . True , if the user is allowed to send documents can_send_photos Boolean Optional . True , if the user is allowed to send photos can_send_videos Boolean Optional . True , if the user is allowed to send videos can_send_video_notes Boolean Optional . True , if the user is allowed to send video notes can_send_voice_notes Boolean Optional . True , if the user is allowed to send voice notes can_send_polls Boolean Optional . True , if the user is allowed to send polls can_send_other_messages Boolean Optional . True , if the user is allowed to send animations, games, stickers and use inline bots can_add_web_page_previews Boolean Optional . True , if the user is allowed to add web page previews to their messages can_change_info Boolean Optional . True , if the user is allowed to change the chat title, photo and other settings. Ignored in public supergroups can_invite_users Boolean Optional . True , if the user is allowed to invite new users to the chat can_pin_messages Boolean Optional . True , if the user is allowed to pin messages. Ignored in public supergroups can_manage_topics Boolean Optional . True , if the user is allowed to create forum topics. If omitted defaults to the value of can_pin_messages Birthdate Describes the birthdate of a user. Field Type Description day Integer Day of the user's birth; 1-31 month Integer Month of the user's birth; 1-12 year Integer Optional . Year of the user's birth BusinessIntro Contains information about the start page settings of a Telegram Business account. Field Type Description title String Optional . Title text of the business intro message String Optional . Message text of the business intro sticker Sticker Optional . Sticker of the business intro BusinessLocation Contains information about the location of a Telegram Business account. Field Type Description address String Address of the business location Location Optional . Location of the business BusinessOpeningHoursInterval Describes an interval of time during which a business is open. Field Type Description opening_minute Integer The minute's sequence number in a week, starting on Monday, marking the start of the time interval during which the business is open; 0 - 7 * 24 * 60 closing_minute Integer The minute's sequence number in a week, starting on Monday, marking the end of the time interval during which the business is open; 0 - 8 * 24 * 60 BusinessOpeningHours Describes the opening hours of a business. Field Type Description time_zone_name String Unique name of the time zone for which the opening hours are defined opening_hours Array of BusinessOpeningHoursInterval List of time intervals describing business opening hours ChatLocation Represents a location to which a chat is connected. Field Type Description location Location The location to which the supergroup is connected. Can't be a live location. address String Location address; 1-64 characters, as defined by the chat owner ReactionType This object describes the type of a reaction. Currently, it can be one of ReactionTypeEmoji ReactionTypeCustomEmoji ReactionTypePaid ReactionTypeEmoji The reaction is based on an emoji. Field Type Description type String Type of the reaction, always “emoji” emoji String Reaction emoji. Currently, it can be one of \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \" ReactionTypeCustomEmoji The reaction is based on a custom emoji. Field Type Description type String Type of the reaction, always “custom_emoji” custom_emoji_id String Custom emoji identifier ReactionTypePaid The reaction is paid. Field Type Description type String Type of the reaction, always “paid” ReactionCount Represents a reaction added to a message along with the number of times it was added. Field Type Description type ReactionType Type of the reaction total_count Integer Number of times the reaction was added MessageReactionUpdated This object represents a change of a reaction on a message performed by a user. Field Type Description chat Chat The chat containing the message the user reacted to message_id Integer Unique identifier of the message inside the chat user User Optional . The user that changed the reaction, if the user isn't anonymous actor_chat Chat Optional . The chat on behalf of which the reaction was changed, if the user is anonymous date Integer Date of the change in Unix time old_reaction Array of ReactionType Previous list of reaction types that were set by the user new_reaction Array of ReactionType New list of reaction types that have been set by the user MessageReactionCountUpdated This object represents reaction changes on a message with anonymous reactions. Field Type Description chat Chat The chat containing the message message_id Integer Unique message identifier inside the chat date Integer Date of the change in Unix time reactions Array of ReactionCount List of reactions that are present on the message ForumTopic This object represents a forum topic. Field Type Description message_thread_id Integer Unique identifier of the forum topic name String Name of the topic icon_color Integer Color of the topic icon in RGB format icon_custom_emoji_id String Optional . Unique identifier of the custom emoji shown as the topic icon BotCommand This object represents a bot command. Field Type Description command String Text of the command; 1-32 characters. Can contain only lowercase English letters, digits and underscores. description String Description of the command; 1-256 characters. BotCommandScope This object represents the scope to which bot commands are applied. Currently, the following 7 scopes are supported: BotCommandScopeDefault BotCommandScopeAllPrivateChats BotCommandScopeAllGroupChats BotCommandScopeAllChatAdministrators BotCommandScopeChat BotCommandScopeChatAdministrators BotCommandScopeChatMember Determining list of commands The following algorithm is used to determine the list of commands for a particular user viewing the bot menu. The first list of commands which is set is returned: Commands in the chat with the bot botCommandScopeChat + language_code botCommandScopeChat botCommandScopeAllPrivateChats + language_code botCommandScopeAllPrivateChats botCommandScopeDefault + language_code botCommandScopeDefault Commands in group and supergroup chats botCommandScopeChatMember + language_code botCommandScopeChatMember botCommandScopeChatAdministrators + language_code (administrators only) botCommandScopeChatAdministrators (administrators only) botCommandScopeChat + language_code botCommandScopeChat botCommandScopeAllChatAdministrators + language_code (administrators only) botCommandScopeAllChatAdministrators (administrators only) botCommandScopeAllGroupChats + language_code botCommandScopeAllGroupChats botCommandScopeDefault + language_code botCommandScopeDefault BotCommandScopeDefault Represents the default scope of bot commands. Default commands are used if no commands with a narrower scope are specified for the user. Field Type Description type String Scope type, must be default BotCommandScopeAllPrivateChats Represents the scope of bot commands, covering all private chats. Field Type Description type String Scope type, must be all_private_chats BotCommandScopeAllGroupChats Represents the scope of bot commands, covering all group and supergroup chats. Field Type Description type String Scope type, must be all_group_chats BotCommandScopeAllChatAdministrators Represents the scope of bot commands, covering all group and supergroup chat administrators. Field Type Description type String Scope type, must be all_chat_administrators BotCommandScopeChat Represents the scope of bot commands, covering a specific chat. Field Type Description type String Scope type, must be chat chat_id Integer or String Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) BotCommandScopeChatAdministrators Represents the scope of bot commands, covering all administrators of a specific group or supergroup chat. Field Type Description type String Scope type, must be chat_administrators chat_id Integer or String Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) BotCommandScopeChatMember Represents the scope of bot commands, covering a specific member of a group or supergroup chat. Field Type Description type String Scope type, must be chat_member chat_id Integer or String Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) user_id Integer Unique identifier of the target user BotName This object represents the bot's name. Field Type Description name String The bot's name BotDescription This object represents the bot's description. Field Type Description description String The bot's description BotShortDescription This object represents the bot's short description. Field Type Description short_description String The bot's short description MenuButton This object describes the bot's menu button in a private chat. It should be one of MenuButtonCommands MenuButtonWebApp MenuButtonDefault If a menu button other than MenuButtonDefault is set for a private chat, then it is applied in the chat. Otherwise the default menu button is applied. By default, the menu button opens the list of bot commands. MenuButtonCommands Represents a menu button, which opens the bot's list of commands. Field Type Description type String Type of the button, must be commands MenuButtonWebApp Represents a menu button, which launches a Web App . Field Type Description type String Type of the button, must be web_app text String Text on the button web_app WebAppInfo Description of the Web App that will be launched when the user presses the button. The Web App will be able to send an arbitrary message on behalf of the user using the method answerWebAppQuery . Alternatively, a t.me link to a Web App of the bot can be specified in the object instead of the Web App's URL, in which case the Web App will be opened as if the user pressed the link. MenuButtonDefault Describes that no specific value for the menu button was set. Field Type Description type String Type of the button, must be default ChatBoostSource This object describes the source of a chat boost. It can be one of ChatBoostSourcePremium ChatBoostSourceGiftCode ChatBoostSourceGiveaway ChatBoostSourcePremium The boost was obtained by subscribing to Telegram Premium or by gifting a Telegram Premium subscription to another user. Field Type Description source String Source of the boost, always “premium” user User User that boosted the chat ChatBoostSourceGiftCode The boost was obtained by the creation of Telegram Premium gift codes to boost a chat. Each such code boosts the chat 4 times for the duration of the corresponding Telegram Premium subscription. Field Type Description source String Source of the boost, always “gift_code” user User User for which the gift code was created ChatBoostSourceGiveaway The boost was obtained by the creation of a Telegram Premium or a Telegram Star giveaway. This boosts the chat 4 times for the duration of the corresponding Telegram Premium subscription for Telegram Premium giveaways and prize_star_count / 500 times for one year for Telegram Star giveaways. Field Type Description source String Source of the boost, always “giveaway” giveaway_message_id Integer Identifier of a message in the chat with the giveaway; the message could have been deleted already. May be 0 if the message isn't sent yet. user User Optional . User that won the prize in the giveaway if any; for Telegram Premium giveaways only prize_star_count Integer Optional . The number of Telegram Stars to be split between giveaway winners; for Telegram Star giveaways only is_unclaimed True Optional . True, if the giveaway was completed, but there was no user to win the prize ChatBoost This object contains information about a chat boost. Field Type Description boost_id String Unique identifier of the boost add_date Integer Point in time (Unix timestamp) when the chat was boosted expiration_date Integer Point in time (Unix timestamp) when the boost will automatically expire, unless the booster's Telegram Premium subscription is prolonged source ChatBoostSource Source of the added boost ChatBoostUpdated This object represents a boost added to a chat or changed. Field Type Description chat Chat Chat which was boosted boost ChatBoost Information about the chat boost ChatBoostRemoved This object represents a boost removed from a chat. Field Type Description chat Chat Chat which was boosted boost_id String Unique identifier of the boost remove_date Integer Point in time (Unix timestamp) when the boost was removed source ChatBoostSource Source of the removed boost UserChatBoosts This object represents a list of boosts added to a chat by a user. Field Type Description boosts Array of ChatBoost The list of boosts added to the chat by the user BusinessConnection Describes the connection of the bot with a business account. Field Type Description id String Unique identifier of the business connection user User Business account user that created the business connection user_chat_id Integer Identifier of a private chat with the user who created the business connection. This number may have more than 32 significant bits and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a 64-bit integer or double-precision float type are safe for storing this identifier. date Integer Date the connection was established in Unix time can_reply Boolean True, if the bot can act on behalf of the business account in chats that were active in the last 24 hours is_enabled Boolean True, if the connection is active BusinessMessagesDeleted This object is received when messages are deleted from a connected business account. Field Type Description business_connection_id String Unique identifier of the business connection chat Chat Information about a chat in the business account. The bot may not have access to the chat or the corresponding user. message_ids Array of Integer The list of identifiers of deleted messages in the chat of the business account ResponseParameters Describes why a request was unsuccessful. Field Type Description migrate_to_chat_id Integer Optional . The group has been migrated to a supergroup with the specified identifier. This number may have more than 32 significant bits and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a signed 64-bit integer or double-precision float type are safe for storing this identifier. retry_after Integer Optional . In case of exceeding flood control, the number of seconds left to wait before the request can be repeated InputMedia This object represents the content of a media message to be sent. It should be one of InputMediaAnimation InputMediaDocument InputMediaAudio InputMediaPhoto InputMediaVideo InputMediaPhoto Represents a photo to be sent. Field Type Description type String Type of the result, must be photo media String File to send. Pass a file_id to send a file that exists on the Telegram servers (recommended), pass an HTTP URL for Telegram to get a file from the Internet, or pass “attach://<file_attach_name>” to upload a new one using multipart/form-data under <file_attach_name> name. More information on Sending Files » caption String Optional . Caption of the photo to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the photo caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional . Pass True , if the caption must be shown above the message media has_spoiler Boolean Optional . Pass True if the photo needs to be covered with a spoiler animation InputMediaVideo Represents a video to be sent. Field Type Description type String Type of the result, must be video media String File to send. Pass a file_id to send a file that exists on the Telegram servers (recommended), pass an HTTP URL for Telegram to get a file from the Internet, or pass “attach://<file_attach_name>” to upload a new one using multipart/form-data under <file_attach_name> name. More information on Sending Files » thumbnail InputFile or String Optional . Thumbnail of the file sent; can be ignored if thumbnail generation for the file is supported server-side. The thumbnail should be in JPEG format and less than 200 kB in size. A thumbnail's width and height should not exceed 320. Ignored if the file is not uploaded using multipart/form-data. Thumbnails can't be reused and can be only uploaded as a new file, so you can pass “attach://<file_attach_name>” if the thumbnail was uploaded using multipart/form-data under <file_attach_name>. More information on Sending Files » caption String Optional . Caption of the video to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the video caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional . Pass True , if the caption must be shown above the message media width Integer Optional . Video width height Integer Optional . Video height duration Integer Optional . Video duration in seconds supports_streaming Boolean Optional . Pass True if the uploaded video is suitable for streaming has_spoiler Boolean Optional . Pass True if the video needs to be covered with a spoiler animation InputMediaAnimation Represents an animation file (GIF or H.264/MPEG-4 AVC video without sound) to be sent. Field Type Description type String Type of the result, must be animation media String File to send. Pass a file_id to send a file that exists on the Telegram servers (recommended), pass an HTTP URL for Telegram to get a file from the Internet, or pass “attach://<file_attach_name>” to upload a new one using multipart/form-data under <file_attach_name> name. More information on Sending Files » thumbnail InputFile or String Optional . Thumbnail of the file sent; can be ignored if thumbnail generation for the file is supported server-side. The thumbnail should be in JPEG format and less than 200 kB in size. A thumbnail's width and height should not exceed 320. Ignored if the file is not uploaded using multipart/form-data. Thumbnails can't be reused and can be only uploaded as a new file, so you can pass “attach://<file_attach_name>” if the thumbnail was uploaded using multipart/form-data under <file_attach_name>. More information on Sending Files » caption String Optional . Caption of the animation to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the animation caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional . Pass True , if the caption must be shown above the message media width Integer Optional . Animation width height Integer Optional . Animation height duration Integer Optional . Animation duration in seconds has_spoiler Boolean Optional . Pass True if the animation needs to be covered with a spoiler animation InputMediaAudio Represents an audio file to be treated as music to be sent. Field Type Description type String Type of the result, must be audio media String File to send. Pass a file_id to send a file that exists on the Telegram servers (recommended), pass an HTTP URL for Telegram to get a file from the Internet, or pass “attach://<file_attach_name>” to upload a new one using multipart/form-data under <file_attach_name> name. More information on Sending Files » thumbnail InputFile or String Optional . Thumbnail of the file sent; can be ignored if thumbnail generation for the file is supported server-side. The thumbnail should be in JPEG format and less than 200 kB in size. A thumbnail's width and height should not exceed 320. Ignored if the file is not uploaded using multipart/form-data. Thumbnails can't be reused and can be only uploaded as a new file, so you can pass “attach://<file_attach_name>” if the thumbnail was uploaded using multipart/form-data under <file_attach_name>. More information on Sending Files » caption String Optional . Caption of the audio to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the audio caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode duration Integer Optional . Duration of the audio in seconds performer String Optional . Performer of the audio title String Optional . Title of the audio InputMediaDocument Represents a general file to be sent. Field Type Description type String Type of the result, must be document media String File to send. Pass a file_id to send a file that exists on the Telegram servers (recommended), pass an HTTP URL for Telegram to get a file from the Internet, or pass “attach://<file_attach_name>” to upload a new one using multipart/form-data under <file_attach_name> name. More information on Sending Files » thumbnail InputFile or String Optional . Thumbnail of the file sent; can be ignored if thumbnail generation for the file is supported server-side. The thumbnail should be in JPEG format and less than 200 kB in size. A thumbnail's width and height should not exceed 320. Ignored if the file is not uploaded using multipart/form-data. Thumbnails can't be reused and can be only uploaded as a new file, so you can pass “attach://<file_attach_name>” if the thumbnail was uploaded using multipart/form-data under <file_attach_name>. More information on Sending Files » caption String Optional . Caption of the document to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the document caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode disable_content_type_detection Boolean Optional . Disables automatic server-side content type detection for files uploaded using multipart/form-data. Always True , if the document is sent as part of an album. InputFile This object represents the contents of a file to be uploaded. Must be posted using multipart/form-data in the usual way that files are uploaded via the browser. InputPaidMedia This object describes the paid media to be sent. Currently, it can be one of InputPaidMediaPhoto InputPaidMediaVideo InputPaidMediaPhoto The paid media to send is a photo. Field Type Description type String Type of the media, must be photo media String File to send. Pass a file_id to send a file that exists on the Telegram servers (recommended), pass an HTTP URL for Telegram to get a file from the Internet, or pass “attach://<file_attach_name>” to upload a new one using multipart/form-data under <file_attach_name> name. More information on Sending Files » InputPaidMediaVideo The paid media to send is a video. Field Type Description type String Type of the media, must be video media String File to send. Pass a file_id to send a file that exists on the Telegram servers (recommended), pass an HTTP URL for Telegram to get a file from the Internet, or pass “attach://<file_attach_name>” to upload a new one using multipart/form-data under <file_attach_name> name. More information on Sending Files » thumbnail InputFile or String Optional . Thumbnail of the file sent; can be ignored if thumbnail generation for the file is supported server-side. The thumbnail should be in JPEG format and less than 200 kB in size. A thumbnail's width and height should not exceed 320. Ignored if the file is not uploaded using multipart/form-data. Thumbnails can't be reused and can be only uploaded as a new file, so you can pass “attach://<file_attach_name>” if the thumbnail was uploaded using multipart/form-data under <file_attach_name>. More information on Sending Files » width Integer Optional . Video width height Integer Optional . Video height duration Integer Optional . Video duration in seconds supports_streaming Boolean Optional . Pass True if the uploaded video is suitable for streaming Sending files There are three ways to send files (photos, stickers, audio, media, etc.): If the file is already stored somewhere on the Telegram servers, you don't need to reupload it: each file object has a file_id field, simply pass this file_id as a parameter instead of uploading. There are no limits for files sent this way. Provide Telegram with an HTTP URL for the file to be sent. Telegram will download and send the file. 5 MB max size for photos and 20 MB max for other types of content. Post the file using multipart/form-data in the usual way that files are uploaded via the browser. 10 MB max size for photos, 50 MB for other files. Sending by file_id It is not possible to change the file type when resending by file_id . I.e. a video can't be sent as a photo , a photo can't be sent as a document , etc. It is not possible to resend thumbnails. Resending a photo by file_id will send all of its sizes . file_id is unique for each individual bot and can't be transferred from one bot to another. file_id uniquely identifies a file, but a file can have different valid file_id s even for the same bot. Sending by URL When sending by URL the target file must have the correct MIME type (e.g., audio/mpeg for sendAudio , etc.). In sendDocument , sending by URL will currently only work for GIF , PDF and ZIP files. To use sendVoice , the file must have the type audio/ogg and be no more than 1MB in size. 1-20MB voice notes will be sent as files. Other configurations may work but we can't guarantee that they will. Accent colors Colors with identifiers 0 (red), 1 (orange), 2 (purple/violet), 3 (green), 4 (cyan), 5 (blue), 6 (pink) can be customized by app themes. Additionally, the following colors in RGB format are currently in use. Color identifier Light colors Dark colors 7 E15052 F9AE63 FF9380 992F37 8 E0802B FAC534 ECB04E C35714 9 A05FF3 F48FFF C697FF 5E31C8 10 27A910 A7DC57 A7EB6E 167E2D 11 27ACCE 82E8D6 40D8D0 045C7F 12 3391D4 7DD3F0 52BFFF 0B5494 13 DD4371 FFBE9F FF86A6 8E366E 14 247BED F04856 FFFFFF 3FA2FE E5424F FFFFFF 15 D67722 1EA011 FFFFFF FF905E 32A527 FFFFFF 16 179E42 E84A3F FFFFFF 66D364 D5444F FFFFFF 17 2894AF 6FC456 FFFFFF 22BCE2 3DA240 FFFFFF 18 0C9AB3 FFAD95 FFE6B5 22BCE2 FF9778 FFDA6B 19 7757D6 F79610 FFDE8E 9791FF F2731D FFDB59 20 1585CF F2AB1D FFFFFF 3DA6EB EEA51D FFFFFF Profile accent colors Currently, the following colors in RGB format are in use for profile backgrounds. Color identifier Light colors Dark colors 0 BA5650 9C4540 1 C27C3E 945E2C 2 956AC8 715099 3 49A355 33713B 4 3E97AD 387E87 5 5A8FBB 477194 6 B85378 944763 7 7F8B95 435261 8 C9565D D97C57 994343 AC583E 9 CF7244 CC9433 8F552F A17232 10 9662D4 B966B6 634691 9250A2 11 3D9755 89A650 296A43 5F8F44 12 3D95BA 50AD98 306C7C 3E987E 13 538BC2 4DA8BD 38618C 458BA1 14 B04F74 D1666D 884160 A65259 15 637482 7B8A97 53606E 384654 Inline mode objects Objects and methods used in the inline mode are described in the Inline mode section . Available methods All methods in the Bot API are case-insensitive. We support GET and POST HTTP methods. Use either URL query string or application/json or application/x-www-form-urlencoded or multipart/form-data for passing parameters in Bot API requests. On successful call, a JSON-object containing the result will be returned. getMe A simple method for testing your bot's authentication token. Requires no parameters. Returns basic information about the bot in form of a User object. logOut Use this method to log out from the cloud Bot API server before launching the bot locally. You must log out the bot before running it locally, otherwise there is no guarantee that the bot will receive updates. After a successful call, you can immediately log in on a local server, but will not be able to log in back to the cloud Bot API server for 10 minutes. Returns True on success. Requires no parameters. close Use this method to close the bot instance before moving it from one local server to another. You need to delete the webhook before calling this method to ensure that the bot isn't launched again after server restart. The method will return error 429 in the first 10 minutes after the bot is launched. Returns True on success. Requires no parameters. sendMessage Use this method to send text messages. On success, the sent Message is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only text String Yes Text of the message to be sent, 1-4096 characters after entities parsing parse_mode String Optional Mode for parsing entities in the message text. See formatting options for more details. entities Array of MessageEntity Optional A JSON-serialized list of special entities that appear in message text, which can be specified instead of parse_mode link_preview_options LinkPreviewOptions Optional Link preview generation options for the message disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user Formatting options The Bot API supports basic formatting for messages. You can use bold, italic, underlined, strikethrough, spoiler text, block quotations as well as inline links and pre-formatted code in your bots' messages. Telegram clients will render them accordingly. You can specify text entities directly, or use markdown-style or HTML-style formatting. Note that Telegram clients will display an alert to the user before opening an inline link ('Open this link?' together with the full URL). Message entities can be nested, providing following restrictions are met: - If two entities have common characters, then one of them is fully contained inside another. - bold , italic , underline , strikethrough , and spoiler entities can contain and can be part of any other entities, except pre and code . - blockquote and expandable_blockquote entities can't be nested. - All other entities can't contain each other. Links tg://user?id=<user_id> can be used to mention a user by their identifier without using a username. Please note: These links will work only if they are used inside an inline link or in an inline keyboard button. For example, they will not work, when used in a message text. Unless the user is a member of the chat where they were mentioned, these mentions are only guaranteed to work if the user has contacted the bot in private in the past or has sent a callback query to the bot via an inline button and doesn't have Forwarded Messages privacy enabled for the bot. You can find the list of programming and markup languages for which syntax highlighting is supported at libprisma#supported-languages . MarkdownV2 style To use this mode, pass MarkdownV2 in the parse_mode field. Use the following syntax in your message: *bold \\*text*\n_italic \\*text_\n__underline__\n~strikethrough~\n||spoiler||\n*bold _italic bold ~italic bold strikethrough ||italic bold strikethrough spoiler||~ __underline italic bold___ bold*\n[inline URL](http://www.example.com/)\n[inline mention of a user](tg://user?id=123456789)\n![ ](tg://emoji?id=5368324170671202286)\n`inline fixed-width code`\n```\npre-formatted fixed-width code block\n```\n```python\npre-formatted fixed-width code block written in the Python programming language\n```\n>Block quotation started\n>Block quotation continued\n>Block quotation continued\n>Block quotation continued\n>The last line of the block quotation\n**>The expandable block quotation started right after the previous block quotation\n>It is separated from the previous block quotation by an empty bold entity\n>Expandable block quotation continued\n>Hidden by default part of the expandable block quotation started\n>Expandable block quotation continued\n>The last line of the expandable block quotation with the expandability mark|| Please note: Any character with code between 1 and 126 inclusively can be escaped anywhere with a preceding '\\' character, in which case it is treated as an ordinary character and not a part of the markup. This implies that '\\' character usually must be escaped with a preceding '\\' character. Inside pre and code entities, all '`' and '\\' characters must be escaped with a preceding '\\' character. Inside the (...) part of the inline link and custom emoji definition, all ')' and '\\' must be escaped with a preceding '\\' character. In all other places characters '_', '*', '[', ']', '(', ')', '~', '`', '>', '#', '+', '-', '=', '|', '{', '}', '.', '!' must be escaped with the preceding character '\\'. In case of ambiguity between italic and underline entities __ is always greadily treated from left to right as beginning or end of an underline entity, so instead of ___italic underline___ use ___italic underline_**__ , adding an empty bold entity as a separator. A valid emoji must be provided as an alternative value for the custom emoji. The emoji will be shown instead of the custom emoji in places where a custom emoji cannot be displayed (e.g., system notifications) or if the message is forwarded by a non-premium user. It is recommended to use the emoji from the emoji field of the custom emoji sticker . Custom emoji entities can only be used by bots that purchased additional usernames on Fragment . HTML style To use this mode, pass HTML in the parse_mode field. The following tags are currently supported: <b>bold</b>, <strong>bold</strong>\n<i>italic</i>, <em>italic</em>\n<u>underline</u>, <ins>underline</ins>\n<s>strikethrough</s>, <strike>strikethrough</strike>, <del>strikethrough</del>\n<span class=\"tg-spoiler\">spoiler</span>, <tg-spoiler>spoiler</tg-spoiler>\n<b>bold <i>italic bold <s>italic bold strikethrough <span class=\"tg-spoiler\">italic bold strikethrough spoiler</span></s> <u>underline italic bold</u></i> bold</b>\n<a href=\"http://www.example.com/\">inline URL</a>\n<a href=\"tg://user?id=123456789\">inline mention of a user</a>\n<tg-emoji emoji-id=\"5368324170671202286\"> </tg-emoji>\n<code>inline fixed-width code</code>\n<pre>pre-formatted fixed-width code block</pre>\n<pre><code class=\"language-python\">pre-formatted fixed-width code block written in the Python programming language</code></pre>\n<blockquote>Block quotation started\\nBlock quotation continued\\nThe last line of the block quotation</blockquote>\n<blockquote expandable>Expandable block quotation started\\nExpandable block quotation continued\\nExpandable block quotation continued\\nHidden by default part of the block quotation started\\nExpandable block quotation continued\\nThe last line of the block quotation</blockquote> Please note: Only the tags mentioned above are currently supported. All < , > and & symbols that are not a part of a tag or an HTML entity must be replaced with the corresponding HTML entities ( < with &lt; , > with &gt; and & with &amp; ). All numerical HTML entities are supported. The API currently supports only the following named HTML entities: &lt; , &gt; , &amp; and &quot; . Use nested pre and code tags, to define programming language for pre entity. Programming language can't be specified for standalone code tags. A valid emoji must be used as the content of the tg-emoji tag. The emoji will be shown instead of the custom emoji in places where a custom emoji cannot be displayed (e.g., system notifications) or if the message is forwarded by a non-premium user. It is recommended to use the emoji from the emoji field of the custom emoji sticker . Custom emoji entities can only be used by bots that purchased additional usernames on Fragment . Markdown style This is a legacy mode, retained for backward compatibility. To use this mode, pass Markdown in the parse_mode field. Use the following syntax in your message: *bold text*\n_italic text_\n[inline URL](http://www.example.com/)\n[inline mention of a user](tg://user?id=123456789)\n`inline fixed-width code`\n```\npre-formatted fixed-width code block\n```\n```python\npre-formatted fixed-width code block written in the Python programming language\n``` Please note: Entities must not be nested, use parse mode MarkdownV2 instead. There is no way to specify “underline”, “strikethrough”, “spoiler”, “blockquote”, “expandable_blockquote” and “custom_emoji” entities, use parse mode MarkdownV2 instead. To escape characters '_', '*', '`', '[' outside of an entity, prepend the characters '\\' before them. Escaping inside entities is not allowed, so entity must be closed first and reopened again: use _snake_\\__case_ for italic snake_case and *2*\\**2=4* for bold 2*2=4 . Paid Broadcasts By default, all bots are able to broadcast up to 30 messages per second to their users. Developers can increase this limit by enabling Paid Broadcasts in @Botfather - allowing their bot to broadcast up to 1000 messages per second. Each message broadcasted over the free amount of 30 messages per second incurs a cost of 0.1 Stars per message, paid with Telegram Stars from the bot's balance. In order to use this feature, a bot must have at least 10,000 Stars on its balance. Bots with increased limits are only charged for messages that are broadcasted successfully. forwardMessage Use this method to forward messages of any kind. Service messages and messages with protected content can't be forwarded. On success, the sent Message is returned. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only from_chat_id Integer or String Yes Unique identifier for the chat where the original message was sent (or channel username in the format @channelusername ) disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the forwarded message from forwarding and saving message_id Integer Yes Message identifier in the chat specified in from_chat_id forwardMessages Use this method to forward multiple messages of any kind. If some of the specified messages can't be found or forwarded, they are skipped. Service messages and messages with protected content can't be forwarded. Album grouping is kept for forwarded messages. On success, an array of MessageId of the sent messages is returned. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only from_chat_id Integer or String Yes Unique identifier for the chat where the original messages were sent (or channel username in the format @channelusername ) message_ids Array of Integer Yes A JSON-serialized list of 1-100 identifiers of messages in the chat from_chat_id to forward. The identifiers must be specified in a strictly increasing order. disable_notification Boolean Optional Sends the messages silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the forwarded messages from forwarding and saving copyMessage Use this method to copy messages of any kind. Service messages, paid media messages, giveaway messages, giveaway winners messages, and invoice messages can't be copied. A quiz poll can be copied only if the value of the field correct_option_id is known to the bot. The method is analogous to the method forwardMessage , but the copied message doesn't have a link to the original message. Returns the MessageId of the sent message on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only from_chat_id Integer or String Yes Unique identifier for the chat where the original message was sent (or channel username in the format @channelusername ) message_id Integer Yes Message identifier in the chat specified in from_chat_id caption String Optional New caption for media, 0-1024 characters after entities parsing. If not specified, the original caption is kept parse_mode String Optional Mode for parsing entities in the new caption. See formatting options for more details. caption_entities Array of MessageEntity Optional A JSON-serialized list of special entities that appear in the new caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional Pass True , if the caption must be shown above the message media. Ignored if a new caption isn't specified. disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user copyMessages Use this method to copy messages of any kind. If some of the specified messages can't be found or copied, they are skipped. Service messages, paid media messages, giveaway messages, giveaway winners messages, and invoice messages can't be copied. A quiz poll can be copied only if the value of the field correct_option_id is known to the bot. The method is analogous to the method forwardMessages , but the copied messages don't have a link to the original message. Album grouping is kept for copied messages. On success, an array of MessageId of the sent messages is returned. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only from_chat_id Integer or String Yes Unique identifier for the chat where the original messages were sent (or channel username in the format @channelusername ) message_ids Array of Integer Yes A JSON-serialized list of 1-100 identifiers of messages in the chat from_chat_id to copy. The identifiers must be specified in a strictly increasing order. disable_notification Boolean Optional Sends the messages silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent messages from forwarding and saving remove_caption Boolean Optional Pass True to copy the messages without their captions sendPhoto Use this method to send photos. On success, the sent Message is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only photo InputFile or String Yes Photo to send. Pass a file_id as String to send a photo that exists on the Telegram servers (recommended), pass an HTTP URL as a String for Telegram to get a photo from the Internet, or upload a new photo using multipart/form-data. The photo must be at most 10 MB in size. The photo's width and height must not exceed 10000 in total. Width and height ratio must be at most 20. More information on Sending Files » caption String Optional Photo caption (may also be used when resending photos by file_id ), 0-1024 characters after entities parsing parse_mode String Optional Mode for parsing entities in the photo caption. See formatting options for more details. caption_entities Array of MessageEntity Optional A JSON-serialized list of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional Pass True , if the caption must be shown above the message media has_spoiler Boolean Optional Pass True if the photo needs to be covered with a spoiler animation disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user sendAudio Use this method to send audio files, if you want Telegram clients to display them in the music player. Your audio must be in the .MP3 or .M4A format. On success, the sent Message is returned. Bots can currently send audio files of up to 50 MB in size, this limit may be changed in the future. For sending voice messages, use the sendVoice method instead. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only audio InputFile or String Yes Audio file to send. Pass a file_id as String to send an audio file that exists on the Telegram servers (recommended), pass an HTTP URL as a String for Telegram to get an audio file from the Internet, or upload a new one using multipart/form-data. More information on Sending Files » caption String Optional Audio caption, 0-1024 characters after entities parsing parse_mode String Optional Mode for parsing entities in the audio caption. See formatting options for more details. caption_entities Array of MessageEntity Optional A JSON-serialized list of special entities that appear in the caption, which can be specified instead of parse_mode duration Integer Optional Duration of the audio in seconds performer String Optional Performer title String Optional Track name thumbnail InputFile or String Optional Thumbnail of the file sent; can be ignored if thumbnail generation for the file is supported server-side. The thumbnail should be in JPEG format and less than 200 kB in size. A thumbnail's width and height should not exceed 320. Ignored if the file is not uploaded using multipart/form-data. Thumbnails can't be reused and can be only uploaded as a new file, so you can pass “attach://<file_attach_name>” if the thumbnail was uploaded using multipart/form-data under <file_attach_name>. More information on Sending Files » disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user sendDocument Use this method to send general files. On success, the sent Message is returned. Bots can currently send files of any type of up to 50 MB in size, this limit may be changed in the future. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only document InputFile or String Yes File to send. Pass a file_id as String to send a file that exists on the Telegram servers (recommended), pass an HTTP URL as a String for Telegram to get a file from the Internet, or upload a new one using multipart/form-data. More information on Sending Files » thumbnail InputFile or String Optional Thumbnail of the file sent; can be ignored if thumbnail generation for the file is supported server-side. The thumbnail should be in JPEG format and less than 200 kB in size. A thumbnail's width and height should not exceed 320. Ignored if the file is not uploaded using multipart/form-data. Thumbnails can't be reused and can be only uploaded as a new file, so you can pass “attach://<file_attach_name>” if the thumbnail was uploaded using multipart/form-data under <file_attach_name>. More information on Sending Files » caption String Optional Document caption (may also be used when resending documents by file_id ), 0-1024 characters after entities parsing parse_mode String Optional Mode for parsing entities in the document caption. See formatting options for more details. caption_entities Array of MessageEntity Optional A JSON-serialized list of special entities that appear in the caption, which can be specified instead of parse_mode disable_content_type_detection Boolean Optional Disables automatic server-side content type detection for files uploaded using multipart/form-data disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user sendVideo Use this method to send video files, Telegram clients support MPEG4 videos (other formats may be sent as Document ). On success, the sent Message is returned. Bots can currently send video files of up to 50 MB in size, this limit may be changed in the future. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only video InputFile or String Yes Video to send. Pass a file_id as String to send a video that exists on the Telegram servers (recommended), pass an HTTP URL as a String for Telegram to get a video from the Internet, or upload a new video using multipart/form-data. More information on Sending Files » duration Integer Optional Duration of sent video in seconds width Integer Optional Video width height Integer Optional Video height thumbnail InputFile or String Optional Thumbnail of the file sent; can be ignored if thumbnail generation for the file is supported server-side. The thumbnail should be in JPEG format and less than 200 kB in size. A thumbnail's width and height should not exceed 320. Ignored if the file is not uploaded using multipart/form-data. Thumbnails can't be reused and can be only uploaded as a new file, so you can pass “attach://<file_attach_name>” if the thumbnail was uploaded using multipart/form-data under <file_attach_name>. More information on Sending Files » caption String Optional Video caption (may also be used when resending videos by file_id ), 0-1024 characters after entities parsing parse_mode String Optional Mode for parsing entities in the video caption. See formatting options for more details. caption_entities Array of MessageEntity Optional A JSON-serialized list of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional Pass True , if the caption must be shown above the message media has_spoiler Boolean Optional Pass True if the video needs to be covered with a spoiler animation supports_streaming Boolean Optional Pass True if the uploaded video is suitable for streaming disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user sendAnimation Use this method to send animation files (GIF or H.264/MPEG-4 AVC video without sound). On success, the sent Message is returned. Bots can currently send animation files of up to 50 MB in size, this limit may be changed in the future. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only animation InputFile or String Yes Animation to send. Pass a file_id as String to send an animation that exists on the Telegram servers (recommended), pass an HTTP URL as a String for Telegram to get an animation from the Internet, or upload a new animation using multipart/form-data. More information on Sending Files » duration Integer Optional Duration of sent animation in seconds width Integer Optional Animation width height Integer Optional Animation height thumbnail InputFile or String Optional Thumbnail of the file sent; can be ignored if thumbnail generation for the file is supported server-side. The thumbnail should be in JPEG format and less than 200 kB in size. A thumbnail's width and height should not exceed 320. Ignored if the file is not uploaded using multipart/form-data. Thumbnails can't be reused and can be only uploaded as a new file, so you can pass “attach://<file_attach_name>” if the thumbnail was uploaded using multipart/form-data under <file_attach_name>. More information on Sending Files » caption String Optional Animation caption (may also be used when resending animation by file_id ), 0-1024 characters after entities parsing parse_mode String Optional Mode for parsing entities in the animation caption. See formatting options for more details. caption_entities Array of MessageEntity Optional A JSON-serialized list of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional Pass True , if the caption must be shown above the message media has_spoiler Boolean Optional Pass True if the animation needs to be covered with a spoiler animation disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user sendVoice Use this method to send audio files, if you want Telegram clients to display the file as a playable voice message. For this to work, your audio must be in an .OGG file encoded with OPUS, or in .MP3 format, or in .M4A format (other formats may be sent as Audio or Document ). On success, the sent Message is returned. Bots can currently send voice messages of up to 50 MB in size, this limit may be changed in the future. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only voice InputFile or String Yes Audio file to send. Pass a file_id as String to send a file that exists on the Telegram servers (recommended), pass an HTTP URL as a String for Telegram to get a file from the Internet, or upload a new one using multipart/form-data. More information on Sending Files » caption String Optional Voice message caption, 0-1024 characters after entities parsing parse_mode String Optional Mode for parsing entities in the voice message caption. See formatting options for more details. caption_entities Array of MessageEntity Optional A JSON-serialized list of special entities that appear in the caption, which can be specified instead of parse_mode duration Integer Optional Duration of the voice message in seconds disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user sendVideoNote As of v.4.0 , Telegram clients support rounded square MPEG4 videos of up to 1 minute long. Use this method to send video messages. On success, the sent Message is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only video_note InputFile or String Yes Video note to send. Pass a file_id as String to send a video note that exists on the Telegram servers (recommended) or upload a new video using multipart/form-data. More information on Sending Files » . Sending video notes by a URL is currently unsupported duration Integer Optional Duration of sent video in seconds length Integer Optional Video width and height, i.e. diameter of the video message thumbnail InputFile or String Optional Thumbnail of the file sent; can be ignored if thumbnail generation for the file is supported server-side. The thumbnail should be in JPEG format and less than 200 kB in size. A thumbnail's width and height should not exceed 320. Ignored if the file is not uploaded using multipart/form-data. Thumbnails can't be reused and can be only uploaded as a new file, so you can pass “attach://<file_attach_name>” if the thumbnail was uploaded using multipart/form-data under <file_attach_name>. More information on Sending Files » disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user sendPaidMedia Use this method to send paid media. On success, the sent Message is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ). If the chat is a channel, all Telegram Star proceeds from this media will be credited to the chat's balance. Otherwise, they will be credited to the bot's balance. star_count Integer Yes The number of Telegram Stars that must be paid to buy access to the media; 1-2500 media Array of InputPaidMedia Yes A JSON-serialized array describing the media to be sent; up to 10 items payload String Optional Bot-defined paid media payload, 0-128 bytes. This will not be displayed to the user, use it for your internal processes. caption String Optional Media caption, 0-1024 characters after entities parsing parse_mode String Optional Mode for parsing entities in the media caption. See formatting options for more details. caption_entities Array of MessageEntity Optional A JSON-serialized list of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional Pass True , if the caption must be shown above the message media disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user sendMediaGroup Use this method to send a group of photos, videos, documents or audios as an album. Documents and audio files can be only grouped in an album with messages of the same type. On success, an array of Messages that were sent is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only media Array of InputMediaAudio , InputMediaDocument , InputMediaPhoto and InputMediaVideo Yes A JSON-serialized array describing messages to be sent, must include 2-10 items disable_notification Boolean Optional Sends messages silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent messages from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to sendLocation Use this method to send point on the map. On success, the sent Message is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only latitude Float Yes Latitude of the location longitude Float Yes Longitude of the location horizontal_accuracy Float Optional The radius of uncertainty for the location, measured in meters; 0-1500 live_period Integer Optional Period in seconds during which the location will be updated (see Live Locations , should be between 60 and 86400, or 0x7FFFFFFF for live locations that can be edited indefinitely. heading Integer Optional For live locations, a direction in which the user is moving, in degrees. Must be between 1 and 360 if specified. proximity_alert_radius Integer Optional For live locations, a maximum distance for proximity alerts about approaching another chat member, in meters. Must be between 1 and 100000 if specified. disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user sendVenue Use this method to send information about a venue. On success, the sent Message is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only latitude Float Yes Latitude of the venue longitude Float Yes Longitude of the venue title String Yes Name of the venue address String Yes Address of the venue foursquare_id String Optional Foursquare identifier of the venue foursquare_type String Optional Foursquare type of the venue, if known. (For example, “arts_entertainment/default”, “arts_entertainment/aquarium” or “food/icecream”.) google_place_id String Optional Google Places identifier of the venue google_place_type String Optional Google Places type of the venue. (See supported types .) disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user sendContact Use this method to send phone contacts. On success, the sent Message is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only phone_number String Yes Contact's phone number first_name String Yes Contact's first name last_name String Optional Contact's last name vcard String Optional Additional data about the contact in the form of a vCard , 0-2048 bytes disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user sendPoll Use this method to send a native poll. On success, the sent Message is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only question String Yes Poll question, 1-300 characters question_parse_mode String Optional Mode for parsing entities in the question. See formatting options for more details. Currently, only custom emoji entities are allowed question_entities Array of MessageEntity Optional A JSON-serialized list of special entities that appear in the poll question. It can be specified instead of question_parse_mode options Array of InputPollOption Yes A JSON-serialized list of 2-10 answer options is_anonymous Boolean Optional True , if the poll needs to be anonymous, defaults to True type String Optional Poll type, “quiz” or “regular”, defaults to “regular” allows_multiple_answers Boolean Optional True , if the poll allows multiple answers, ignored for polls in quiz mode, defaults to False correct_option_id Integer Optional 0-based identifier of the correct answer option, required for polls in quiz mode explanation String Optional Text that is shown when a user chooses an incorrect answer or taps on the lamp icon in a quiz-style poll, 0-200 characters with at most 2 line feeds after entities parsing explanation_parse_mode String Optional Mode for parsing entities in the explanation. See formatting options for more details. explanation_entities Array of MessageEntity Optional A JSON-serialized list of special entities that appear in the poll explanation. It can be specified instead of explanation_parse_mode open_period Integer Optional Amount of time in seconds the poll will be active after creation, 5-600. Can't be used together with close_date . close_date Integer Optional Point in time (Unix timestamp) when the poll will be automatically closed. Must be at least 5 and no more than 600 seconds in the future. Can't be used together with open_period . is_closed Boolean Optional Pass True if the poll needs to be immediately closed. This can be useful for poll preview. disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user sendDice Use this method to send an animated emoji that will display a random value. On success, the sent Message is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only emoji String Optional Emoji on which the dice throw animation is based. Currently, must be one of “ ”, “ ”, “ ”, “ ”, “ ”, or “ ”. Dice can have values 1-6 for “ ”, “ ” and “ ”, values 1-5 for “ ” and “ ”, and values 1-64 for “ ”. Defaults to “ ” disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user sendChatAction Use this method when you need to tell the user that something is happening on the bot's side. The status is set for 5 seconds or less (when a message arrives from your bot, Telegram clients clear its typing status). Returns True on success. Example: The ImageBot needs some time to process a request and upload the image. Instead of sending a text message along the lines of “Retrieving image, please wait…”, the bot may use sendChatAction with action = upload_photo . The user will see a “sending photo” status for the bot. We only recommend using this method when a response from the bot will take a noticeable amount of time to arrive. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the action will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread; for supergroups only action String Yes Type of action to broadcast. Choose one, depending on what the user is about to receive: typing for text messages , upload_photo for photos , record_video or upload_video for videos , record_voice or upload_voice for voice notes , upload_document for general files , choose_sticker for stickers , find_location for location data , record_video_note or upload_video_note for video notes . setMessageReaction Use this method to change the chosen reactions on a message. Service messages can't be reacted to. Automatically forwarded messages from a channel to its discussion group have the same available reactions as messages in the channel. Bots can't use paid reactions. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_id Integer Yes Identifier of the target message. If the message belongs to a media group, the reaction is set to the first non-deleted message in the group instead. reaction Array of ReactionType Optional A JSON-serialized list of reaction types to set on the message. Currently, as non-premium users, bots can set up to one reaction per message. A custom emoji reaction can be used if it is either already present on the message or explicitly allowed by chat administrators. Paid reactions can't be used by bots. is_big Boolean Optional Pass True to set the reaction with a big animation getUserProfilePhotos Use this method to get a list of profile pictures for a user. Returns a UserProfilePhotos object. Parameter Type Required Description user_id Integer Yes Unique identifier of the target user offset Integer Optional Sequential number of the first photo to be returned. By default, all photos are returned. limit Integer Optional Limits the number of photos to be retrieved. Values between 1-100 are accepted. Defaults to 100. setUserEmojiStatus Changes the emoji status for a given user that previously allowed the bot to manage their emoji status via the Mini App method requestEmojiStatusAccess . Returns True on success. Parameter Type Required Description user_id Integer Yes Unique identifier of the target user emoji_status_custom_emoji_id String Optional Custom emoji identifier of the emoji status to set. Pass an empty string to remove the status. emoji_status_expiration_date Integer Optional Expiration date of the emoji status, if any getFile Use this method to get basic information about a file and prepare it for downloading. For the moment, bots can download files of up to 20MB in size. On success, a File object is returned. The file can then be downloaded via the link https://api.telegram.org/file/bot<token>/<file_path> , where <file_path> is taken from the response. It is guaranteed that the link will be valid for at least 1 hour. When the link expires, a new one can be requested by calling getFile again. Parameter Type Required Description file_id String Yes File identifier to get information about Note: This function may not preserve the original file name and MIME type. You should save the file's MIME type and name (if available) when the File object is received. banChatMember Use this method to ban a user in a group, a supergroup or a channel. In the case of supergroups and channels, the user will not be able to return to the chat on their own using invite links, etc., unless unbanned first. The bot must be an administrator in the chat for this to work and must have the appropriate administrator rights. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target group or username of the target supergroup or channel (in the format @channelusername ) user_id Integer Yes Unique identifier of the target user until_date Integer Optional Date when the user will be unbanned; Unix time. If user is banned for more than 366 days or less than 30 seconds from the current time they are considered to be banned forever. Applied for supergroups and channels only. revoke_messages Boolean Optional Pass True to delete all messages from the chat for the user that is being removed. If False , the user will be able to see messages in the group that were sent before the user was removed. Always True for supergroups and channels. unbanChatMember Use this method to unban a previously banned user in a supergroup or channel. The user will not return to the group or channel automatically, but will be able to join via link, etc. The bot must be an administrator for this to work. By default, this method guarantees that after the call the user is not a member of the chat, but will be able to join it. So if the user is a member of the chat they will also be removed from the chat. If you don't want this, use the parameter only_if_banned . Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target group or username of the target supergroup or channel (in the format @channelusername ) user_id Integer Yes Unique identifier of the target user only_if_banned Boolean Optional Do nothing if the user is not banned restrictChatMember Use this method to restrict a user in a supergroup. The bot must be an administrator in the supergroup for this to work and must have the appropriate administrator rights. Pass True for all permissions to lift restrictions from a user. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) user_id Integer Yes Unique identifier of the target user permissions ChatPermissions Yes A JSON-serialized object for new user permissions use_independent_chat_permissions Boolean Optional Pass True if chat permissions are set independently. Otherwise, the can_send_other_messages and can_add_web_page_previews permissions will imply the can_send_messages , can_send_audios , can_send_documents , can_send_photos , can_send_videos , can_send_video_notes , and can_send_voice_notes permissions; the can_send_polls permission will imply the can_send_messages permission. until_date Integer Optional Date when restrictions will be lifted for the user; Unix time. If user is restricted for more than 366 days or less than 30 seconds from the current time, they are considered to be restricted forever promoteChatMember Use this method to promote or demote a user in a supergroup or a channel. The bot must be an administrator in the chat for this to work and must have the appropriate administrator rights. Pass False for all boolean parameters to demote a user. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) user_id Integer Yes Unique identifier of the target user is_anonymous Boolean Optional Pass True if the administrator's presence in the chat is hidden can_manage_chat Boolean Optional Pass True if the administrator can access the chat event log, get boost list, see hidden supergroup and channel members, report spam messages and ignore slow mode. Implied by any other administrator privilege. can_delete_messages Boolean Optional Pass True if the administrator can delete messages of other users can_manage_video_chats Boolean Optional Pass True if the administrator can manage video chats can_restrict_members Boolean Optional Pass True if the administrator can restrict, ban or unban chat members, or access supergroup statistics can_promote_members Boolean Optional Pass True if the administrator can add new administrators with a subset of their own privileges or demote administrators that they have promoted, directly or indirectly (promoted by administrators that were appointed by him) can_change_info Boolean Optional Pass True if the administrator can change chat title, photo and other settings can_invite_users Boolean Optional Pass True if the administrator can invite new users to the chat can_post_stories Boolean Optional Pass True if the administrator can post stories to the chat can_edit_stories Boolean Optional Pass True if the administrator can edit stories posted by other users, post stories to the chat page, pin chat stories, and access the chat's story archive can_delete_stories Boolean Optional Pass True if the administrator can delete stories posted by other users can_post_messages Boolean Optional Pass True if the administrator can post messages in the channel, or access channel statistics; for channels only can_edit_messages Boolean Optional Pass True if the administrator can edit messages of other users and can pin messages; for channels only can_pin_messages Boolean Optional Pass True if the administrator can pin messages; for supergroups only can_manage_topics Boolean Optional Pass True if the user is allowed to create, rename, close, and reopen forum topics; for supergroups only setChatAdministratorCustomTitle Use this method to set a custom title for an administrator in a supergroup promoted by the bot. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) user_id Integer Yes Unique identifier of the target user custom_title String Yes New custom title for the administrator; 0-16 characters, emoji are not allowed banChatSenderChat Use this method to ban a channel chat in a supergroup or a channel. Until the chat is unbanned , the owner of the banned chat won't be able to send messages on behalf of any of their channels . The bot must be an administrator in the supergroup or channel for this to work and must have the appropriate administrator rights. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) sender_chat_id Integer Yes Unique identifier of the target sender chat unbanChatSenderChat Use this method to unban a previously banned channel chat in a supergroup or channel. The bot must be an administrator for this to work and must have the appropriate administrator rights. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) sender_chat_id Integer Yes Unique identifier of the target sender chat setChatPermissions Use this method to set default chat permissions for all members. The bot must be an administrator in the group or a supergroup for this to work and must have the can_restrict_members administrator rights. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) permissions ChatPermissions Yes A JSON-serialized object for new default chat permissions use_independent_chat_permissions Boolean Optional Pass True if chat permissions are set independently. Otherwise, the can_send_other_messages and can_add_web_page_previews permissions will imply the can_send_messages , can_send_audios , can_send_documents , can_send_photos , can_send_videos , can_send_video_notes , and can_send_voice_notes permissions; the can_send_polls permission will imply the can_send_messages permission. exportChatInviteLink Use this method to generate a new primary invite link for a chat; any previously generated primary link is revoked. The bot must be an administrator in the chat for this to work and must have the appropriate administrator rights. Returns the new invite link as String on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) Note: Each administrator in a chat generates their own invite links. Bots can't use invite links generated by other administrators. If you want your bot to work with invite links, it will need to generate its own link using exportChatInviteLink or by calling the getChat method. If your bot needs to generate a new primary invite link replacing its previous one, use exportChatInviteLink again. createChatInviteLink Use this method to create an additional invite link for a chat. The bot must be an administrator in the chat for this to work and must have the appropriate administrator rights. The link can be revoked using the method revokeChatInviteLink . Returns the new invite link as ChatInviteLink object. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) name String Optional Invite link name; 0-32 characters expire_date Integer Optional Point in time (Unix timestamp) when the link will expire member_limit Integer Optional The maximum number of users that can be members of the chat simultaneously after joining the chat via this invite link; 1-99999 creates_join_request Boolean Optional True , if users joining the chat via the link need to be approved by chat administrators. If True , member_limit can't be specified editChatInviteLink Use this method to edit a non-primary invite link created by the bot. The bot must be an administrator in the chat for this to work and must have the appropriate administrator rights. Returns the edited invite link as a ChatInviteLink object. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) invite_link String Yes The invite link to edit name String Optional Invite link name; 0-32 characters expire_date Integer Optional Point in time (Unix timestamp) when the link will expire member_limit Integer Optional The maximum number of users that can be members of the chat simultaneously after joining the chat via this invite link; 1-99999 creates_join_request Boolean Optional True , if users joining the chat via the link need to be approved by chat administrators. If True , member_limit can't be specified createChatSubscriptionInviteLink Use this method to create a subscription invite link for a channel chat. The bot must have the can_invite_users administrator rights. The link can be edited using the method editChatSubscriptionInviteLink or revoked using the method revokeChatInviteLink . Returns the new invite link as a ChatInviteLink object. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target channel chat or username of the target channel (in the format @channelusername ) name String Optional Invite link name; 0-32 characters subscription_period Integer Yes The number of seconds the subscription will be active for before the next payment. Currently, it must always be 2592000 (30 days). subscription_price Integer Yes The amount of Telegram Stars a user must pay initially and after each subsequent subscription period to be a member of the chat; 1-2500 editChatSubscriptionInviteLink Use this method to edit a subscription invite link created by the bot. The bot must have the can_invite_users administrator rights. Returns the edited invite link as a ChatInviteLink object. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) invite_link String Yes The invite link to edit name String Optional Invite link name; 0-32 characters revokeChatInviteLink Use this method to revoke an invite link created by the bot. If the primary link is revoked, a new link is automatically generated. The bot must be an administrator in the chat for this to work and must have the appropriate administrator rights. Returns the revoked invite link as ChatInviteLink object. Parameter Type Required Description chat_id Integer or String Yes Unique identifier of the target chat or username of the target channel (in the format @channelusername ) invite_link String Yes The invite link to revoke approveChatJoinRequest Use this method to approve a chat join request. The bot must be an administrator in the chat for this to work and must have the can_invite_users administrator right. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) user_id Integer Yes Unique identifier of the target user declineChatJoinRequest Use this method to decline a chat join request. The bot must be an administrator in the chat for this to work and must have the can_invite_users administrator right. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) user_id Integer Yes Unique identifier of the target user setChatPhoto Use this method to set a new profile photo for the chat. Photos can't be changed for private chats. The bot must be an administrator in the chat for this to work and must have the appropriate administrator rights. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) photo InputFile Yes New chat photo, uploaded using multipart/form-data deleteChatPhoto Use this method to delete a chat photo. Photos can't be changed for private chats. The bot must be an administrator in the chat for this to work and must have the appropriate administrator rights. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) setChatTitle Use this method to change the title of a chat. Titles can't be changed for private chats. The bot must be an administrator in the chat for this to work and must have the appropriate administrator rights. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) title String Yes New chat title, 1-128 characters setChatDescription Use this method to change the description of a group, a supergroup or a channel. The bot must be an administrator in the chat for this to work and must have the appropriate administrator rights. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) description String Optional New chat description, 0-255 characters pinChatMessage Use this method to add a message to the list of pinned messages in a chat. If the chat is not a private chat, the bot must be an administrator in the chat for this to work and must have the 'can_pin_messages' administrator right in a supergroup or 'can_edit_messages' administrator right in a channel. Returns True on success. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be pinned chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_id Integer Yes Identifier of a message to pin disable_notification Boolean Optional Pass True if it is not necessary to send a notification to all chat members about the new pinned message. Notifications are always disabled in channels and private chats. unpinChatMessage Use this method to remove a message from the list of pinned messages in a chat. If the chat is not a private chat, the bot must be an administrator in the chat for this to work and must have the 'can_pin_messages' administrator right in a supergroup or 'can_edit_messages' administrator right in a channel. Returns True on success. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be unpinned chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_id Integer Optional Identifier of the message to unpin. Required if business_connection_id is specified. If not specified, the most recent pinned message (by sending date) will be unpinned. unpinAllChatMessages Use this method to clear the list of pinned messages in a chat. If the chat is not a private chat, the bot must be an administrator in the chat for this to work and must have the 'can_pin_messages' administrator right in a supergroup or 'can_edit_messages' administrator right in a channel. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) leaveChat Use this method for your bot to leave a group, supergroup or channel. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup or channel (in the format @channelusername ) getChat Use this method to get up-to-date information about the chat. Returns a ChatFullInfo object on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup or channel (in the format @channelusername ) getChatAdministrators Use this method to get a list of administrators in a chat, which aren't bots. Returns an Array of ChatMember objects. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup or channel (in the format @channelusername ) getChatMemberCount Use this method to get the number of members in a chat. Returns Int on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup or channel (in the format @channelusername ) getChatMember Use this method to get information about a member of a chat. The method is only guaranteed to work for other users if the bot is an administrator in the chat. Returns a ChatMember object on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup or channel (in the format @channelusername ) user_id Integer Yes Unique identifier of the target user setChatStickerSet Use this method to set a new group sticker set for a supergroup. The bot must be an administrator in the chat for this to work and must have the appropriate administrator rights. Use the field can_set_sticker_set optionally returned in getChat requests to check if the bot can use this method. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) sticker_set_name String Yes Name of the sticker set to be set as the group sticker set deleteChatStickerSet Use this method to delete a group sticker set from a supergroup. The bot must be an administrator in the chat for this to work and must have the appropriate administrator rights. Use the field can_set_sticker_set optionally returned in getChat requests to check if the bot can use this method. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) getForumTopicIconStickers Use this method to get custom emoji stickers, which can be used as a forum topic icon by any user. Requires no parameters. Returns an Array of Sticker objects. createForumTopic Use this method to create a topic in a forum supergroup chat. The bot must be an administrator in the chat for this to work and must have the can_manage_topics administrator rights. Returns information about the created topic as a ForumTopic object. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) name String Yes Topic name, 1-128 characters icon_color Integer Optional Color of the topic icon in RGB format. Currently, must be one of 7322096 (0x6FB9F0), 16766590 (0xFFD67E), 13338331 (0xCB86DB), 9367192 (0x8EEE98), 16749490 (0xFF93B2), or 16478047 (0xFB6F5F) icon_custom_emoji_id String Optional Unique identifier of the custom emoji shown as the topic icon. Use getForumTopicIconStickers to get all allowed custom emoji identifiers. editForumTopic Use this method to edit name and icon of a topic in a forum supergroup chat. The bot must be an administrator in the chat for this to work and must have the can_manage_topics administrator rights, unless it is the creator of the topic. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) message_thread_id Integer Yes Unique identifier for the target message thread of the forum topic name String Optional New topic name, 0-128 characters. If not specified or empty, the current name of the topic will be kept icon_custom_emoji_id String Optional New unique identifier of the custom emoji shown as the topic icon. Use getForumTopicIconStickers to get all allowed custom emoji identifiers. Pass an empty string to remove the icon. If not specified, the current icon will be kept closeForumTopic Use this method to close an open topic in a forum supergroup chat. The bot must be an administrator in the chat for this to work and must have the can_manage_topics administrator rights, unless it is the creator of the topic. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) message_thread_id Integer Yes Unique identifier for the target message thread of the forum topic reopenForumTopic Use this method to reopen a closed topic in a forum supergroup chat. The bot must be an administrator in the chat for this to work and must have the can_manage_topics administrator rights, unless it is the creator of the topic. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) message_thread_id Integer Yes Unique identifier for the target message thread of the forum topic deleteForumTopic Use this method to delete a forum topic along with all its messages in a forum supergroup chat. The bot must be an administrator in the chat for this to work and must have the can_delete_messages administrator rights. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) message_thread_id Integer Yes Unique identifier for the target message thread of the forum topic unpinAllForumTopicMessages Use this method to clear the list of pinned messages in a forum topic. The bot must be an administrator in the chat for this to work and must have the can_pin_messages administrator right in the supergroup. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) message_thread_id Integer Yes Unique identifier for the target message thread of the forum topic editGeneralForumTopic Use this method to edit the name of the 'General' topic in a forum supergroup chat. The bot must be an administrator in the chat for this to work and must have the can_manage_topics administrator rights. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) name String Yes New topic name, 1-128 characters closeGeneralForumTopic Use this method to close an open 'General' topic in a forum supergroup chat. The bot must be an administrator in the chat for this to work and must have the can_manage_topics administrator rights. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) reopenGeneralForumTopic Use this method to reopen a closed 'General' topic in a forum supergroup chat. The bot must be an administrator in the chat for this to work and must have the can_manage_topics administrator rights. The topic will be automatically unhidden if it was hidden. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) hideGeneralForumTopic Use this method to hide the 'General' topic in a forum supergroup chat. The bot must be an administrator in the chat for this to work and must have the can_manage_topics administrator rights. The topic will be automatically closed if it was open. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) unhideGeneralForumTopic Use this method to unhide the 'General' topic in a forum supergroup chat. The bot must be an administrator in the chat for this to work and must have the can_manage_topics administrator rights. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) unpinAllGeneralForumTopicMessages Use this method to clear the list of pinned messages in a General forum topic. The bot must be an administrator in the chat for this to work and must have the can_pin_messages administrator right in the supergroup. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target supergroup (in the format @supergroupusername ) answerCallbackQuery Use this method to send answers to callback queries sent from inline keyboards . The answer will be displayed to the user as a notification at the top of the chat screen or as an alert. On success, True is returned. Alternatively, the user can be redirected to the specified Game URL. For this option to work, you must first create a game for your bot via @BotFather and accept the terms. Otherwise, you may use links like t.me/your_bot?start=XXXX that open your bot with a parameter. Parameter Type Required Description callback_query_id String Yes Unique identifier for the query to be answered text String Optional Text of the notification. If not specified, nothing will be shown to the user, 0-200 characters show_alert Boolean Optional If True , an alert will be shown by the client instead of a notification at the top of the chat screen. Defaults to false . url String Optional URL that will be opened by the user's client. If you have created a Game and accepted the conditions via @BotFather , specify the URL that opens your game - note that this will only work if the query comes from a callback_game button. Otherwise, you may use links like t.me/your_bot?start=XXXX that open your bot with a parameter. cache_time Integer Optional The maximum amount of time in seconds that the result of the callback query may be cached client-side. Telegram apps will support caching starting in version 3.14. Defaults to 0. getUserChatBoosts Use this method to get the list of boosts added to a chat by a user. Requires administrator rights in the chat. Returns a UserChatBoosts object. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the chat or username of the channel (in the format @channelusername ) user_id Integer Yes Unique identifier of the target user getBusinessConnection Use this method to get information about the connection of the bot with a business account. Returns a BusinessConnection object on success. Parameter Type Required Description business_connection_id String Yes Unique identifier of the business connection setMyCommands Use this method to change the list of the bot's commands. See this manual for more details about bot commands. Returns True on success. Parameter Type Required Description commands Array of BotCommand Yes A JSON-serialized list of bot commands to be set as the list of the bot's commands. At most 100 commands can be specified. scope BotCommandScope Optional A JSON-serialized object, describing scope of users for which the commands are relevant. Defaults to BotCommandScopeDefault . language_code String Optional A two-letter ISO 639-1 language code. If empty, commands will be applied to all users from the given scope, for whose language there are no dedicated commands deleteMyCommands Use this method to delete the list of the bot's commands for the given scope and user language. After deletion, higher level commands will be shown to affected users. Returns True on success. Parameter Type Required Description scope BotCommandScope Optional A JSON-serialized object, describing scope of users for which the commands are relevant. Defaults to BotCommandScopeDefault . language_code String Optional A two-letter ISO 639-1 language code. If empty, commands will be applied to all users from the given scope, for whose language there are no dedicated commands getMyCommands Use this method to get the current list of the bot's commands for the given scope and user language. Returns an Array of BotCommand objects. If commands aren't set, an empty list is returned. Parameter Type Required Description scope BotCommandScope Optional A JSON-serialized object, describing scope of users. Defaults to BotCommandScopeDefault . language_code String Optional A two-letter ISO 639-1 language code or an empty string setMyName Use this method to change the bot's name. Returns True on success. Parameter Type Required Description name String Optional New bot name; 0-64 characters. Pass an empty string to remove the dedicated name for the given language. language_code String Optional A two-letter ISO 639-1 language code. If empty, the name will be shown to all users for whose language there is no dedicated name. getMyName Use this method to get the current bot name for the given user language. Returns BotName on success. Parameter Type Required Description language_code String Optional A two-letter ISO 639-1 language code or an empty string setMyDescription Use this method to change the bot's description, which is shown in the chat with the bot if the chat is empty. Returns True on success. Parameter Type Required Description description String Optional New bot description; 0-512 characters. Pass an empty string to remove the dedicated description for the given language. language_code String Optional A two-letter ISO 639-1 language code. If empty, the description will be applied to all users for whose language there is no dedicated description. getMyDescription Use this method to get the current bot description for the given user language. Returns BotDescription on success. Parameter Type Required Description language_code String Optional A two-letter ISO 639-1 language code or an empty string setMyShortDescription Use this method to change the bot's short description, which is shown on the bot's profile page and is sent together with the link when users share the bot. Returns True on success. Parameter Type Required Description short_description String Optional New short description for the bot; 0-120 characters. Pass an empty string to remove the dedicated short description for the given language. language_code String Optional A two-letter ISO 639-1 language code. If empty, the short description will be applied to all users for whose language there is no dedicated short description. getMyShortDescription Use this method to get the current bot short description for the given user language. Returns BotShortDescription on success. Parameter Type Required Description language_code String Optional A two-letter ISO 639-1 language code or an empty string setChatMenuButton Use this method to change the bot's menu button in a private chat, or the default menu button. Returns True on success. Parameter Type Required Description chat_id Integer Optional Unique identifier for the target private chat. If not specified, default bot's menu button will be changed menu_button MenuButton Optional A JSON-serialized object for the bot's new menu button. Defaults to MenuButtonDefault getChatMenuButton Use this method to get the current value of the bot's menu button in a private chat, or the default menu button. Returns MenuButton on success. Parameter Type Required Description chat_id Integer Optional Unique identifier for the target private chat. If not specified, default bot's menu button will be returned setMyDefaultAdministratorRights Use this method to change the default administrator rights requested by the bot when it's added as an administrator to groups or channels. These rights will be suggested to users, but they are free to modify the list before adding the bot. Returns True on success. Parameter Type Required Description rights ChatAdministratorRights Optional A JSON-serialized object describing new default administrator rights. If not specified, the default administrator rights will be cleared. for_channels Boolean Optional Pass True to change the default administrator rights of the bot in channels. Otherwise, the default administrator rights of the bot for groups and supergroups will be changed. getMyDefaultAdministratorRights Use this method to get the current default administrator rights of the bot. Returns ChatAdministratorRights on success. Parameter Type Required Description for_channels Boolean Optional Pass True to get default administrator rights of the bot in channels. Otherwise, default administrator rights of the bot for groups and supergroups will be returned. Inline mode methods Methods and objects used in the inline mode are described in the Inline mode section . Updating messages The following methods allow you to change an existing message in the message history instead of sending a new one with a result of an action. This is most useful for messages with inline keyboards using callback queries, but can also help reduce clutter in conversations with regular chat bots. Please note, that it is currently only possible to edit messages without reply_markup or with inline keyboards . editMessageText Use this method to edit text and game messages. On success, if the edited message is not an inline message, the edited Message is returned, otherwise True is returned. Note that business messages that were not sent by the bot and do not contain an inline keyboard can only be edited within 48 hours from the time they were sent. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message to be edited was sent chat_id Integer or String Optional Required if inline_message_id is not specified. Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_id Integer Optional Required if inline_message_id is not specified. Identifier of the message to edit inline_message_id String Optional Required if chat_id and message_id are not specified. Identifier of the inline message text String Yes New text of the message, 1-4096 characters after entities parsing parse_mode String Optional Mode for parsing entities in the message text. See formatting options for more details. entities Array of MessageEntity Optional A JSON-serialized list of special entities that appear in message text, which can be specified instead of parse_mode link_preview_options LinkPreviewOptions Optional Link preview generation options for the message reply_markup InlineKeyboardMarkup Optional A JSON-serialized object for an inline keyboard . editMessageCaption Use this method to edit captions of messages. On success, if the edited message is not an inline message, the edited Message is returned, otherwise True is returned. Note that business messages that were not sent by the bot and do not contain an inline keyboard can only be edited within 48 hours from the time they were sent. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message to be edited was sent chat_id Integer or String Optional Required if inline_message_id is not specified. Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_id Integer Optional Required if inline_message_id is not specified. Identifier of the message to edit inline_message_id String Optional Required if chat_id and message_id are not specified. Identifier of the inline message caption String Optional New caption of the message, 0-1024 characters after entities parsing parse_mode String Optional Mode for parsing entities in the message caption. See formatting options for more details. caption_entities Array of MessageEntity Optional A JSON-serialized list of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional Pass True , if the caption must be shown above the message media. Supported only for animation, photo and video messages. reply_markup InlineKeyboardMarkup Optional A JSON-serialized object for an inline keyboard . editMessageMedia Use this method to edit animation, audio, document, photo, or video messages, or to add media to text messages. If a message is part of a message album, then it can be edited only to an audio for audio albums, only to a document for document albums and to a photo or a video otherwise. When an inline message is edited, a new file can't be uploaded; use a previously uploaded file via its file_id or specify a URL. On success, if the edited message is not an inline message, the edited Message is returned, otherwise True is returned. Note that business messages that were not sent by the bot and do not contain an inline keyboard can only be edited within 48 hours from the time they were sent. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message to be edited was sent chat_id Integer or String Optional Required if inline_message_id is not specified. Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_id Integer Optional Required if inline_message_id is not specified. Identifier of the message to edit inline_message_id String Optional Required if chat_id and message_id are not specified. Identifier of the inline message media InputMedia Yes A JSON-serialized object for a new media content of the message reply_markup InlineKeyboardMarkup Optional A JSON-serialized object for a new inline keyboard . editMessageLiveLocation Use this method to edit live location messages. A location can be edited until its live_period expires or editing is explicitly disabled by a call to stopMessageLiveLocation . On success, if the edited message is not an inline message, the edited Message is returned, otherwise True is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message to be edited was sent chat_id Integer or String Optional Required if inline_message_id is not specified. Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_id Integer Optional Required if inline_message_id is not specified. Identifier of the message to edit inline_message_id String Optional Required if chat_id and message_id are not specified. Identifier of the inline message latitude Float Yes Latitude of new location longitude Float Yes Longitude of new location live_period Integer Optional New period in seconds during which the location can be updated, starting from the message send date. If 0x7FFFFFFF is specified, then the location can be updated forever. Otherwise, the new value must not exceed the current live_period by more than a day, and the live location expiration date must remain within the next 90 days. If not specified, then live_period remains unchanged horizontal_accuracy Float Optional The radius of uncertainty for the location, measured in meters; 0-1500 heading Integer Optional Direction in which the user is moving, in degrees. Must be between 1 and 360 if specified. proximity_alert_radius Integer Optional The maximum distance for proximity alerts about approaching another chat member, in meters. Must be between 1 and 100000 if specified. reply_markup InlineKeyboardMarkup Optional A JSON-serialized object for a new inline keyboard . stopMessageLiveLocation Use this method to stop updating a live location message before live_period expires. On success, if the message is not an inline message, the edited Message is returned, otherwise True is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message to be edited was sent chat_id Integer or String Optional Required if inline_message_id is not specified. Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_id Integer Optional Required if inline_message_id is not specified. Identifier of the message with live location to stop inline_message_id String Optional Required if chat_id and message_id are not specified. Identifier of the inline message reply_markup InlineKeyboardMarkup Optional A JSON-serialized object for a new inline keyboard . editMessageReplyMarkup Use this method to edit only the reply markup of messages. On success, if the edited message is not an inline message, the edited Message is returned, otherwise True is returned. Note that business messages that were not sent by the bot and do not contain an inline keyboard can only be edited within 48 hours from the time they were sent. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message to be edited was sent chat_id Integer or String Optional Required if inline_message_id is not specified. Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_id Integer Optional Required if inline_message_id is not specified. Identifier of the message to edit inline_message_id String Optional Required if chat_id and message_id are not specified. Identifier of the inline message reply_markup InlineKeyboardMarkup Optional A JSON-serialized object for an inline keyboard . stopPoll Use this method to stop a poll which was sent by the bot. On success, the stopped Poll is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message to be edited was sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_id Integer Yes Identifier of the original message with the poll reply_markup InlineKeyboardMarkup Optional A JSON-serialized object for a new message inline keyboard . deleteMessage Use this method to delete a message, including service messages, with the following limitations: - A message can only be deleted if it was sent less than 48 hours ago. - Service messages about a supergroup, channel, or forum topic creation can't be deleted. - A dice message in a private chat can only be deleted if it was sent more than 24 hours ago. - Bots can delete outgoing messages in private chats, groups, and supergroups. - Bots can delete incoming messages in private chats. - Bots granted can_post_messages permissions can delete outgoing messages in channels. - If the bot is an administrator of a group, it can delete any message there. - If the bot has can_delete_messages permission in a supergroup or a channel, it can delete any message there. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_id Integer Yes Identifier of the message to delete deleteMessages Use this method to delete multiple messages simultaneously. If some of the specified messages can't be found, they are skipped. Returns True on success. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_ids Array of Integer Yes A JSON-serialized list of 1-100 identifiers of messages to delete. See deleteMessage for limitations on which messages can be deleted Stickers The following methods and objects allow your bot to handle stickers and sticker sets. Sticker This object represents a sticker. Field Type Description file_id String Identifier for this file, which can be used to download or reuse the file file_unique_id String Unique identifier for this file, which is supposed to be the same over time and for different bots. Can't be used to download or reuse the file. type String Type of the sticker, currently one of “regular”, “mask”, “custom_emoji”. The type of the sticker is independent from its format, which is determined by the fields is_animated and is_video . width Integer Sticker width height Integer Sticker height is_animated Boolean True , if the sticker is animated is_video Boolean True , if the sticker is a video sticker thumbnail PhotoSize Optional . Sticker thumbnail in the .WEBP or .JPG format emoji String Optional . Emoji associated with the sticker set_name String Optional . Name of the sticker set to which the sticker belongs premium_animation File Optional . For premium regular stickers, premium animation for the sticker mask_position MaskPosition Optional . For mask stickers, the position where the mask should be placed custom_emoji_id String Optional . For custom emoji stickers, unique identifier of the custom emoji needs_repainting True Optional . True , if the sticker must be repainted to a text color in messages, the color of the Telegram Premium badge in emoji status, white color on chat photos, or another appropriate color in other places file_size Integer Optional . File size in bytes StickerSet This object represents a sticker set. Field Type Description name String Sticker set name title String Sticker set title sticker_type String Type of stickers in the set, currently one of “regular”, “mask”, “custom_emoji” stickers Array of Sticker List of all set stickers thumbnail PhotoSize Optional . Sticker set thumbnail in the .WEBP, .TGS, or .WEBM format MaskPosition This object describes the position on faces where a mask should be placed by default. Field Type Description point String The part of the face relative to which the mask should be placed. One of “forehead”, “eyes”, “mouth”, or “chin”. x_shift Float Shift by X-axis measured in widths of the mask scaled to the face size, from left to right. For example, choosing -1.0 will place mask just to the left of the default mask position. y_shift Float Shift by Y-axis measured in heights of the mask scaled to the face size, from top to bottom. For example, 1.0 will place the mask just below the default mask position. scale Float Mask scaling coefficient. For example, 2.0 means double size. InputSticker This object describes a sticker to be added to a sticker set. Field Type Description sticker InputFile or String The added sticker. Pass a file_id as a String to send a file that already exists on the Telegram servers, pass an HTTP URL as a String for Telegram to get a file from the Internet, upload a new one using multipart/form-data, or pass “attach://<file_attach_name>” to upload a new one using multipart/form-data under <file_attach_name> name. Animated and video stickers can't be uploaded via HTTP URL. More information on Sending Files » format String Format of the added sticker, must be one of “static” for a .WEBP or .PNG image, “animated” for a .TGS animation, “video” for a WEBM video emoji_list Array of String List of 1-20 emoji associated with the sticker mask_position MaskPosition Optional . Position where the mask should be placed on faces. For “mask” stickers only. keywords Array of String Optional . List of 0-20 search keywords for the sticker with total length of up to 64 characters. For “regular” and “custom_emoji” stickers only. sendSticker Use this method to send static .WEBP, animated .TGS, or video .WEBM stickers. On success, the sent Message is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only sticker InputFile or String Yes Sticker to send. Pass a file_id as String to send a file that exists on the Telegram servers (recommended), pass an HTTP URL as a String for Telegram to get a .WEBP sticker from the Internet, or upload a new .WEBP, .TGS, or .WEBM sticker using multipart/form-data. More information on Sending Files » . Video and animated stickers can't be sent via an HTTP URL. emoji String Optional Emoji associated with the sticker; only for just uploaded stickers disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup or ReplyKeyboardMarkup or ReplyKeyboardRemove or ForceReply Optional Additional interface options. A JSON-serialized object for an inline keyboard , custom reply keyboard , instructions to remove a reply keyboard or to force a reply from the user getStickerSet Use this method to get a sticker set. On success, a StickerSet object is returned. Parameter Type Required Description name String Yes Name of the sticker set getCustomEmojiStickers Use this method to get information about custom emoji stickers by their identifiers. Returns an Array of Sticker objects. Parameter Type Required Description custom_emoji_ids Array of String Yes A JSON-serialized list of custom emoji identifiers. At most 200 custom emoji identifiers can be specified. uploadStickerFile Use this method to upload a file with a sticker for later use in the createNewStickerSet , addStickerToSet , or replaceStickerInSet methods (the file can be used multiple times). Returns the uploaded File on success. Parameter Type Required Description user_id Integer Yes User identifier of sticker file owner sticker InputFile Yes A file with the sticker in .WEBP, .PNG, .TGS, or .WEBM format. See https://core.telegram.org/stickers for technical requirements. More information on Sending Files » sticker_format String Yes Format of the sticker, must be one of “static”, “animated”, “video” createNewStickerSet Use this method to create a new sticker set owned by a user. The bot will be able to edit the sticker set thus created. Returns True on success. Parameter Type Required Description user_id Integer Yes User identifier of created sticker set owner name String Yes Short name of sticker set, to be used in t.me/addstickers/ URLs (e.g., animals ). Can contain only English letters, digits and underscores. Must begin with a letter, can't contain consecutive underscores and must end in \"_by_<bot_username>\" . <bot_username> is case insensitive. 1-64 characters. title String Yes Sticker set title, 1-64 characters stickers Array of InputSticker Yes A JSON-serialized list of 1-50 initial stickers to be added to the sticker set sticker_type String Optional Type of stickers in the set, pass “regular”, “mask”, or “custom_emoji”. By default, a regular sticker set is created. needs_repainting Boolean Optional Pass True if stickers in the sticker set must be repainted to the color of text when used in messages, the accent color if used as emoji status, white on chat photos, or another appropriate color based on context; for custom emoji sticker sets only addStickerToSet Use this method to add a new sticker to a set created by the bot. Emoji sticker sets can have up to 200 stickers. Other sticker sets can have up to 120 stickers. Returns True on success. Parameter Type Required Description user_id Integer Yes User identifier of sticker set owner name String Yes Sticker set name sticker InputSticker Yes A JSON-serialized object with information about the added sticker. If exactly the same sticker had already been added to the set, then the set isn't changed. setStickerPositionInSet Use this method to move a sticker in a set created by the bot to a specific position. Returns True on success. Parameter Type Required Description sticker String Yes File identifier of the sticker position Integer Yes New sticker position in the set, zero-based deleteStickerFromSet Use this method to delete a sticker from a set created by the bot. Returns True on success. Parameter Type Required Description sticker String Yes File identifier of the sticker replaceStickerInSet Use this method to replace an existing sticker in a sticker set with a new one. The method is equivalent to calling deleteStickerFromSet , then addStickerToSet , then setStickerPositionInSet . Returns True on success. Parameter Type Required Description user_id Integer Yes User identifier of the sticker set owner name String Yes Sticker set name old_sticker String Yes File identifier of the replaced sticker sticker InputSticker Yes A JSON-serialized object with information about the added sticker. If exactly the same sticker had already been added to the set, then the set remains unchanged. setStickerEmojiList Use this method to change the list of emoji assigned to a regular or custom emoji sticker. The sticker must belong to a sticker set created by the bot. Returns True on success. Parameter Type Required Description sticker String Yes File identifier of the sticker emoji_list Array of String Yes A JSON-serialized list of 1-20 emoji associated with the sticker setStickerKeywords Use this method to change search keywords assigned to a regular or custom emoji sticker. The sticker must belong to a sticker set created by the bot. Returns True on success. Parameter Type Required Description sticker String Yes File identifier of the sticker keywords Array of String Optional A JSON-serialized list of 0-20 search keywords for the sticker with total length of up to 64 characters setStickerMaskPosition Use this method to change the mask position of a mask sticker. The sticker must belong to a sticker set that was created by the bot. Returns True on success. Parameter Type Required Description sticker String Yes File identifier of the sticker mask_position MaskPosition Optional A JSON-serialized object with the position where the mask should be placed on faces. Omit the parameter to remove the mask position. setStickerSetTitle Use this method to set the title of a created sticker set. Returns True on success. Parameter Type Required Description name String Yes Sticker set name title String Yes Sticker set title, 1-64 characters setStickerSetThumbnail Use this method to set the thumbnail of a regular or mask sticker set. The format of the thumbnail file must match the format of the stickers in the set. Returns True on success. Parameter Type Required Description name String Yes Sticker set name user_id Integer Yes User identifier of the sticker set owner thumbnail InputFile or String Optional A .WEBP or .PNG image with the thumbnail, must be up to 128 kilobytes in size and have a width and height of exactly 100px, or a .TGS animation with a thumbnail up to 32 kilobytes in size (see https://core.telegram.org/stickers#animation-requirements for animated sticker technical requirements), or a WEBM video with the thumbnail up to 32 kilobytes in size; see https://core.telegram.org/stickers#video-requirements for video sticker technical requirements. Pass a file_id as a String to send a file that already exists on the Telegram servers, pass an HTTP URL as a String for Telegram to get a file from the Internet, or upload a new one using multipart/form-data. More information on Sending Files » . Animated and video sticker set thumbnails can't be uploaded via HTTP URL. If omitted, then the thumbnail is dropped and the first sticker is used as the thumbnail. format String Yes Format of the thumbnail, must be one of “static” for a .WEBP or .PNG image, “animated” for a .TGS animation, or “video” for a WEBM video setCustomEmojiStickerSetThumbnail Use this method to set the thumbnail of a custom emoji sticker set. Returns True on success. Parameter Type Required Description name String Yes Sticker set name custom_emoji_id String Optional Custom emoji identifier of a sticker from the sticker set; pass an empty string to drop the thumbnail and use the first sticker as the thumbnail. deleteStickerSet Use this method to delete a sticker set that was created by the bot. Returns True on success. Parameter Type Required Description name String Yes Sticker set name Gift This object represents a gift that can be sent by the bot. Field Type Description id String Unique identifier of the gift sticker Sticker The sticker that represents the gift star_count Integer The number of Telegram Stars that must be paid to send the sticker total_count Integer Optional . The total number of the gifts of this type that can be sent; for limited gifts only remaining_count Integer Optional . The number of remaining gifts of this type that can be sent; for limited gifts only Gifts This object represent a list of gifts. Field Type Description gifts Array of Gift The list of gifts getAvailableGifts Returns the list of gifts that can be sent by the bot to users. Requires no parameters. Returns a Gifts object. sendGift Sends a gift to the given user. The gift can't be converted to Telegram Stars by the user. Returns True on success. Parameter Type Required Description user_id Integer Yes Unique identifier of the target user that will receive the gift gift_id String Yes Identifier of the gift text String Optional Text that will be shown along with the gift; 0-255 characters text_parse_mode String Optional Mode for parsing entities in the text. See formatting options for more details. Entities other than “bold”, “italic”, “underline”, “strikethrough”, “spoiler”, and “custom_emoji” are ignored. text_entities Array of MessageEntity Optional A JSON-serialized list of special entities that appear in the gift text. It can be specified instead of text_parse_mode . Entities other than “bold”, “italic”, “underline”, “strikethrough”, “spoiler”, and “custom_emoji” are ignored. Inline mode The following methods and objects allow your bot to work in inline mode . Please see our Introduction to Inline bots for more details. To enable this option, send the /setinline command to @BotFather and provide the placeholder text that the user will see in the input field after typing your bot's name. InlineQuery This object represents an incoming inline query. When the user sends an empty query, your bot could return some default or trending results. Field Type Description id String Unique identifier for this query from User Sender query String Text of the query (up to 256 characters) offset String Offset of the results to be returned, can be controlled by the bot chat_type String Optional . Type of the chat from which the inline query was sent. Can be either “sender” for a private chat with the inline query sender, “private”, “group”, “supergroup”, or “channel”. The chat type should be always known for requests sent from official clients and most third-party clients, unless the request was sent from a secret chat location Location Optional . Sender location, only for bots that request user location answerInlineQuery Use this method to send answers to an inline query. On success, True is returned. No more than 50 results per query are allowed. Parameter Type Required Description inline_query_id String Yes Unique identifier for the answered query results Array of InlineQueryResult Yes A JSON-serialized array of results for the inline query cache_time Integer Optional The maximum amount of time in seconds that the result of the inline query may be cached on the server. Defaults to 300. is_personal Boolean Optional Pass True if results may be cached on the server side only for the user that sent the query. By default, results may be returned to any user who sends the same query. next_offset String Optional Pass the offset that a client should send in the next query with the same text to receive more results. Pass an empty string if there are no more results or if you don't support pagination. Offset length can't exceed 64 bytes. button InlineQueryResultsButton Optional A JSON-serialized object describing a button to be shown above inline query results InlineQueryResultsButton This object represents a button to be shown above inline query results. You must use exactly one of the optional fields. Field Type Description text String Label text on the button web_app WebAppInfo Optional . Description of the Web App that will be launched when the user presses the button. The Web App will be able to switch back to the inline mode using the method switchInlineQuery inside the Web App. start_parameter String Optional . Deep-linking parameter for the /start message sent to the bot when a user presses the button. 1-64 characters, only A-Z , a-z , 0-9 , _ and - are allowed. Example: An inline bot that sends YouTube videos can ask the user to connect the bot to their YouTube account to adapt search results accordingly. To do this, it displays a 'Connect your YouTube account' button above the results, or even before showing any. The user presses the button, switches to a private chat with the bot and, in doing so, passes a start parameter that instructs the bot to return an OAuth link. Once done, the bot can offer a switch_inline button so that the user can easily return to the chat where they wanted to use the bot's inline capabilities. InlineQueryResult This object represents one result of an inline query. Telegram clients currently support results of the following 20 types: InlineQueryResultCachedAudio InlineQueryResultCachedDocument InlineQueryResultCachedGif InlineQueryResultCachedMpeg4Gif InlineQueryResultCachedPhoto InlineQueryResultCachedSticker InlineQueryResultCachedVideo InlineQueryResultCachedVoice InlineQueryResultArticle InlineQueryResultAudio InlineQueryResultContact InlineQueryResultGame InlineQueryResultDocument InlineQueryResultGif InlineQueryResultLocation InlineQueryResultMpeg4Gif InlineQueryResultPhoto InlineQueryResultVenue InlineQueryResultVideo InlineQueryResultVoice Note: All URLs passed in inline query results will be available to end users and therefore must be assumed to be public . InlineQueryResultArticle Represents a link to an article or web page. Field Type Description type String Type of the result, must be article id String Unique identifier for this result, 1-64 Bytes title String Title of the result input_message_content InputMessageContent Content of the message to be sent reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message url String Optional . URL of the result hide_url Boolean Optional . Pass True if you don't want the URL to be shown in the message description String Optional . Short description of the result thumbnail_url String Optional . Url of the thumbnail for the result thumbnail_width Integer Optional . Thumbnail width thumbnail_height Integer Optional . Thumbnail height InlineQueryResultPhoto Represents a link to a photo. By default, this photo will be sent by the user with optional caption. Alternatively, you can use input_message_content to send a message with the specified content instead of the photo. Field Type Description type String Type of the result, must be photo id String Unique identifier for this result, 1-64 bytes photo_url String A valid URL of the photo. Photo must be in JPEG format. Photo size must not exceed 5MB thumbnail_url String URL of the thumbnail for the photo photo_width Integer Optional . Width of the photo photo_height Integer Optional . Height of the photo title String Optional . Title for the result description String Optional . Short description of the result caption String Optional . Caption of the photo to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the photo caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional . Pass True , if the caption must be shown above the message media reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the photo InlineQueryResultGif Represents a link to an animated GIF file. By default, this animated GIF file will be sent by the user with optional caption. Alternatively, you can use input_message_content to send a message with the specified content instead of the animation. Field Type Description type String Type of the result, must be gif id String Unique identifier for this result, 1-64 bytes gif_url String A valid URL for the GIF file. File size must not exceed 1MB gif_width Integer Optional . Width of the GIF gif_height Integer Optional . Height of the GIF gif_duration Integer Optional . Duration of the GIF in seconds thumbnail_url String URL of the static (JPEG or GIF) or animated (MPEG4) thumbnail for the result thumbnail_mime_type String Optional . MIME type of the thumbnail, must be one of “image/jpeg”, “image/gif”, or “video/mp4”. Defaults to “image/jpeg” title String Optional . Title for the result caption String Optional . Caption of the GIF file to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional . Pass True , if the caption must be shown above the message media reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the GIF animation InlineQueryResultMpeg4Gif Represents a link to a video animation (H.264/MPEG-4 AVC video without sound). By default, this animated MPEG-4 file will be sent by the user with optional caption. Alternatively, you can use input_message_content to send a message with the specified content instead of the animation. Field Type Description type String Type of the result, must be mpeg4_gif id String Unique identifier for this result, 1-64 bytes mpeg4_url String A valid URL for the MPEG4 file. File size must not exceed 1MB mpeg4_width Integer Optional . Video width mpeg4_height Integer Optional . Video height mpeg4_duration Integer Optional . Video duration in seconds thumbnail_url String URL of the static (JPEG or GIF) or animated (MPEG4) thumbnail for the result thumbnail_mime_type String Optional . MIME type of the thumbnail, must be one of “image/jpeg”, “image/gif”, or “video/mp4”. Defaults to “image/jpeg” title String Optional . Title for the result caption String Optional . Caption of the MPEG-4 file to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional . Pass True , if the caption must be shown above the message media reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the video animation InlineQueryResultVideo Represents a link to a page containing an embedded video player or a video file. By default, this video file will be sent by the user with an optional caption. Alternatively, you can use input_message_content to send a message with the specified content instead of the video. If an InlineQueryResultVideo message contains an embedded video (e.g., YouTube), you must replace its content using input_message_content . Field Type Description type String Type of the result, must be video id String Unique identifier for this result, 1-64 bytes video_url String A valid URL for the embedded video player or video file mime_type String MIME type of the content of the video URL, “text/html” or “video/mp4” thumbnail_url String URL of the thumbnail (JPEG only) for the video title String Title for the result caption String Optional . Caption of the video to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the video caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional . Pass True , if the caption must be shown above the message media video_width Integer Optional . Video width video_height Integer Optional . Video height video_duration Integer Optional . Video duration in seconds description String Optional . Short description of the result reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the video. This field is required if InlineQueryResultVideo is used to send an HTML-page as a result (e.g., a YouTube video). InlineQueryResultAudio Represents a link to an MP3 audio file. By default, this audio file will be sent by the user. Alternatively, you can use input_message_content to send a message with the specified content instead of the audio. Field Type Description type String Type of the result, must be audio id String Unique identifier for this result, 1-64 bytes audio_url String A valid URL for the audio file title String Title caption String Optional . Caption, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the audio caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode performer String Optional . Performer audio_duration Integer Optional . Audio duration in seconds reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the audio InlineQueryResultVoice Represents a link to a voice recording in an .OGG container encoded with OPUS. By default, this voice recording will be sent by the user. Alternatively, you can use input_message_content to send a message with the specified content instead of the the voice message. Field Type Description type String Type of the result, must be voice id String Unique identifier for this result, 1-64 bytes voice_url String A valid URL for the voice recording title String Recording title caption String Optional . Caption, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the voice message caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode voice_duration Integer Optional . Recording duration in seconds reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the voice recording InlineQueryResultDocument Represents a link to a file. By default, this file will be sent by the user with an optional caption. Alternatively, you can use input_message_content to send a message with the specified content instead of the file. Currently, only .PDF and .ZIP files can be sent using this method. Field Type Description type String Type of the result, must be document id String Unique identifier for this result, 1-64 bytes title String Title for the result caption String Optional . Caption of the document to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the document caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode document_url String A valid URL for the file mime_type String MIME type of the content of the file, either “application/pdf” or “application/zip” description String Optional . Short description of the result reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the file thumbnail_url String Optional . URL of the thumbnail (JPEG only) for the file thumbnail_width Integer Optional . Thumbnail width thumbnail_height Integer Optional . Thumbnail height InlineQueryResultLocation Represents a location on a map. By default, the location will be sent by the user. Alternatively, you can use input_message_content to send a message with the specified content instead of the location. Field Type Description type String Type of the result, must be location id String Unique identifier for this result, 1-64 Bytes latitude Float Location latitude in degrees longitude Float Location longitude in degrees title String Location title horizontal_accuracy Float Optional . The radius of uncertainty for the location, measured in meters; 0-1500 live_period Integer Optional . Period in seconds during which the location can be updated, should be between 60 and 86400, or 0x7FFFFFFF for live locations that can be edited indefinitely. heading Integer Optional . For live locations, a direction in which the user is moving, in degrees. Must be between 1 and 360 if specified. proximity_alert_radius Integer Optional . For live locations, a maximum distance for proximity alerts about approaching another chat member, in meters. Must be between 1 and 100000 if specified. reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the location thumbnail_url String Optional . Url of the thumbnail for the result thumbnail_width Integer Optional . Thumbnail width thumbnail_height Integer Optional . Thumbnail height InlineQueryResultVenue Represents a venue. By default, the venue will be sent by the user. Alternatively, you can use input_message_content to send a message with the specified content instead of the venue. Field Type Description type String Type of the result, must be venue id String Unique identifier for this result, 1-64 Bytes latitude Float Latitude of the venue location in degrees longitude Float Longitude of the venue location in degrees title String Title of the venue address String Address of the venue foursquare_id String Optional . Foursquare identifier of the venue if known foursquare_type String Optional . Foursquare type of the venue, if known. (For example, “arts_entertainment/default”, “arts_entertainment/aquarium” or “food/icecream”.) google_place_id String Optional . Google Places identifier of the venue google_place_type String Optional . Google Places type of the venue. (See supported types .) reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the venue thumbnail_url String Optional . Url of the thumbnail for the result thumbnail_width Integer Optional . Thumbnail width thumbnail_height Integer Optional . Thumbnail height InlineQueryResultContact Represents a contact with a phone number. By default, this contact will be sent by the user. Alternatively, you can use input_message_content to send a message with the specified content instead of the contact. Field Type Description type String Type of the result, must be contact id String Unique identifier for this result, 1-64 Bytes phone_number String Contact's phone number first_name String Contact's first name last_name String Optional . Contact's last name vcard String Optional . Additional data about the contact in the form of a vCard , 0-2048 bytes reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the contact thumbnail_url String Optional . Url of the thumbnail for the result thumbnail_width Integer Optional . Thumbnail width thumbnail_height Integer Optional . Thumbnail height InlineQueryResultGame Represents a Game . Field Type Description type String Type of the result, must be game id String Unique identifier for this result, 1-64 bytes game_short_name String Short name of the game reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message InlineQueryResultCachedPhoto Represents a link to a photo stored on the Telegram servers. By default, this photo will be sent by the user with an optional caption. Alternatively, you can use input_message_content to send a message with the specified content instead of the photo. Field Type Description type String Type of the result, must be photo id String Unique identifier for this result, 1-64 bytes photo_file_id String A valid file identifier of the photo title String Optional . Title for the result description String Optional . Short description of the result caption String Optional . Caption of the photo to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the photo caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional . Pass True , if the caption must be shown above the message media reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the photo InlineQueryResultCachedGif Represents a link to an animated GIF file stored on the Telegram servers. By default, this animated GIF file will be sent by the user with an optional caption. Alternatively, you can use input_message_content to send a message with specified content instead of the animation. Field Type Description type String Type of the result, must be gif id String Unique identifier for this result, 1-64 bytes gif_file_id String A valid file identifier for the GIF file title String Optional . Title for the result caption String Optional . Caption of the GIF file to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional . Pass True , if the caption must be shown above the message media reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the GIF animation InlineQueryResultCachedMpeg4Gif Represents a link to a video animation (H.264/MPEG-4 AVC video without sound) stored on the Telegram servers. By default, this animated MPEG-4 file will be sent by the user with an optional caption. Alternatively, you can use input_message_content to send a message with the specified content instead of the animation. Field Type Description type String Type of the result, must be mpeg4_gif id String Unique identifier for this result, 1-64 bytes mpeg4_file_id String A valid file identifier for the MPEG4 file title String Optional . Title for the result caption String Optional . Caption of the MPEG-4 file to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional . Pass True , if the caption must be shown above the message media reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the video animation InlineQueryResultCachedSticker Represents a link to a sticker stored on the Telegram servers. By default, this sticker will be sent by the user. Alternatively, you can use input_message_content to send a message with the specified content instead of the sticker. Field Type Description type String Type of the result, must be sticker id String Unique identifier for this result, 1-64 bytes sticker_file_id String A valid file identifier of the sticker reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the sticker InlineQueryResultCachedDocument Represents a link to a file stored on the Telegram servers. By default, this file will be sent by the user with an optional caption. Alternatively, you can use input_message_content to send a message with the specified content instead of the file. Field Type Description type String Type of the result, must be document id String Unique identifier for this result, 1-64 bytes title String Title for the result document_file_id String A valid file identifier for the file description String Optional . Short description of the result caption String Optional . Caption of the document to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the document caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the file InlineQueryResultCachedVideo Represents a link to a video file stored on the Telegram servers. By default, this video file will be sent by the user with an optional caption. Alternatively, you can use input_message_content to send a message with the specified content instead of the video. Field Type Description type String Type of the result, must be video id String Unique identifier for this result, 1-64 bytes video_file_id String A valid file identifier for the video file title String Title for the result description String Optional . Short description of the result caption String Optional . Caption of the video to be sent, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the video caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode show_caption_above_media Boolean Optional . Pass True , if the caption must be shown above the message media reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the video InlineQueryResultCachedVoice Represents a link to a voice message stored on the Telegram servers. By default, this voice message will be sent by the user. Alternatively, you can use input_message_content to send a message with the specified content instead of the voice message. Field Type Description type String Type of the result, must be voice id String Unique identifier for this result, 1-64 bytes voice_file_id String A valid file identifier for the voice message title String Voice message title caption String Optional . Caption, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the voice message caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the voice message InlineQueryResultCachedAudio Represents a link to an MP3 audio file stored on the Telegram servers. By default, this audio file will be sent by the user. Alternatively, you can use input_message_content to send a message with the specified content instead of the audio. Field Type Description type String Type of the result, must be audio id String Unique identifier for this result, 1-64 bytes audio_file_id String A valid file identifier for the audio file caption String Optional . Caption, 0-1024 characters after entities parsing parse_mode String Optional . Mode for parsing entities in the audio caption. See formatting options for more details. caption_entities Array of MessageEntity Optional . List of special entities that appear in the caption, which can be specified instead of parse_mode reply_markup InlineKeyboardMarkup Optional . Inline keyboard attached to the message input_message_content InputMessageContent Optional . Content of the message to be sent instead of the audio InputMessageContent This object represents the content of a message to be sent as a result of an inline query. Telegram clients currently support the following 5 types: InputTextMessageContent InputLocationMessageContent InputVenueMessageContent InputContactMessageContent InputInvoiceMessageContent InputTextMessageContent Represents the content of a text message to be sent as the result of an inline query. Field Type Description message_text String Text of the message to be sent, 1-4096 characters parse_mode String Optional . Mode for parsing entities in the message text. See formatting options for more details. entities Array of MessageEntity Optional . List of special entities that appear in message text, which can be specified instead of parse_mode link_preview_options LinkPreviewOptions Optional . Link preview generation options for the message InputLocationMessageContent Represents the content of a location message to be sent as the result of an inline query. Field Type Description latitude Float Latitude of the location in degrees longitude Float Longitude of the location in degrees horizontal_accuracy Float Optional . The radius of uncertainty for the location, measured in meters; 0-1500 live_period Integer Optional . Period in seconds during which the location can be updated, should be between 60 and 86400, or 0x7FFFFFFF for live locations that can be edited indefinitely. heading Integer Optional . For live locations, a direction in which the user is moving, in degrees. Must be between 1 and 360 if specified. proximity_alert_radius Integer Optional . For live locations, a maximum distance for proximity alerts about approaching another chat member, in meters. Must be between 1 and 100000 if specified. InputVenueMessageContent Represents the content of a venue message to be sent as the result of an inline query. Field Type Description latitude Float Latitude of the venue in degrees longitude Float Longitude of the venue in degrees title String Name of the venue address String Address of the venue foursquare_id String Optional . Foursquare identifier of the venue, if known foursquare_type String Optional . Foursquare type of the venue, if known. (For example, “arts_entertainment/default”, “arts_entertainment/aquarium” or “food/icecream”.) google_place_id String Optional . Google Places identifier of the venue google_place_type String Optional . Google Places type of the venue. (See supported types .) InputContactMessageContent Represents the content of a contact message to be sent as the result of an inline query. Field Type Description phone_number String Contact's phone number first_name String Contact's first name last_name String Optional . Contact's last name vcard String Optional . Additional data about the contact in the form of a vCard , 0-2048 bytes InputInvoiceMessageContent Represents the content of an invoice message to be sent as the result of an inline query. Field Type Description title String Product name, 1-32 characters description String Product description, 1-255 characters payload String Bot-defined invoice payload, 1-128 bytes. This will not be displayed to the user, use it for your internal processes. provider_token String Optional . Payment provider token, obtained via @BotFather . Pass an empty string for payments in Telegram Stars . currency String Three-letter ISO 4217 currency code, see more on currencies . Pass “XTR” for payments in Telegram Stars . prices Array of LabeledPrice Price breakdown, a JSON-serialized list of components (e.g. product price, tax, discount, delivery cost, delivery tax, bonus, etc.). Must contain exactly one item for payments in Telegram Stars . max_tip_amount Integer Optional . The maximum accepted amount for tips in the smallest units of the currency (integer, not float/double). For example, for a maximum tip of US$ 1.45 pass max_tip_amount = 145 . See the exp parameter in currencies.json , it shows the number of digits past the decimal point for each currency (2 for the majority of currencies). Defaults to 0. Not supported for payments in Telegram Stars . suggested_tip_amounts Array of Integer Optional . A JSON-serialized array of suggested amounts of tip in the smallest units of the currency (integer, not float/double). At most 4 suggested tip amounts can be specified. The suggested tip amounts must be positive, passed in a strictly increased order and must not exceed max_tip_amount . provider_data String Optional . A JSON-serialized object for data about the invoice, which will be shared with the payment provider. A detailed description of the required fields should be provided by the payment provider. photo_url String Optional . URL of the product photo for the invoice. Can be a photo of the goods or a marketing image for a service. photo_size Integer Optional . Photo size in bytes photo_width Integer Optional . Photo width photo_height Integer Optional . Photo height need_name Boolean Optional . Pass True if you require the user's full name to complete the order. Ignored for payments in Telegram Stars . need_phone_number Boolean Optional . Pass True if you require the user's phone number to complete the order. Ignored for payments in Telegram Stars . need_email Boolean Optional . Pass True if you require the user's email address to complete the order. Ignored for payments in Telegram Stars . need_shipping_address Boolean Optional . Pass True if you require the user's shipping address to complete the order. Ignored for payments in Telegram Stars . send_phone_number_to_provider Boolean Optional . Pass True if the user's phone number should be sent to the provider. Ignored for payments in Telegram Stars . send_email_to_provider Boolean Optional . Pass True if the user's email address should be sent to the provider. Ignored for payments in Telegram Stars . is_flexible Boolean Optional . Pass True if the final price depends on the shipping method. Ignored for payments in Telegram Stars . ChosenInlineResult Represents a result of an inline query that was chosen by the user and sent to their chat partner. Field Type Description result_id String The unique identifier for the result that was chosen from User The user that chose the result location Location Optional . Sender location, only for bots that require user location inline_message_id String Optional . Identifier of the sent inline message. Available only if there is an inline keyboard attached to the message. Will be also received in callback queries and can be used to edit the message. query String The query that was used to obtain the result Note: It is necessary to enable inline feedback via @BotFather in order to receive these objects in updates. answerWebAppQuery Use this method to set the result of an interaction with a Web App and send a corresponding message on behalf of the user to the chat from which the query originated. On success, a SentWebAppMessage object is returned. Parameter Type Required Description web_app_query_id String Yes Unique identifier for the query to be answered result InlineQueryResult Yes A JSON-serialized object describing the message to be sent SentWebAppMessage Describes an inline message sent by a Web App on behalf of a user. Field Type Description inline_message_id String Optional . Identifier of the sent inline message. Available only if there is an inline keyboard attached to the message. savePreparedInlineMessage Stores a message that can be sent by a user of a Mini App. Returns a PreparedInlineMessage object. Parameter Type Required Description user_id Integer Yes Unique identifier of the target user that can use the prepared message result InlineQueryResult Yes A JSON-serialized object describing the message to be sent allow_user_chats Boolean Optional Pass True if the message can be sent to private chats with users allow_bot_chats Boolean Optional Pass True if the message can be sent to private chats with bots allow_group_chats Boolean Optional Pass True if the message can be sent to group and supergroup chats allow_channel_chats Boolean Optional Pass True if the message can be sent to channel chats PreparedInlineMessage Describes an inline message to be sent by a user of a Mini App. Field Type Description id String Unique identifier of the prepared message expiration_date Integer Expiration date of the prepared message, in Unix time. Expired prepared messages can no longer be used Payments Your bot can accept payments from Telegram users. Please see the introduction to payments for more details on the process and how to set up payments for your bot. sendInvoice Use this method to send invoices. On success, the sent Message is returned. Parameter Type Required Description chat_id Integer or String Yes Unique identifier for the target chat or username of the target channel (in the format @channelusername ) message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only title String Yes Product name, 1-32 characters description String Yes Product description, 1-255 characters payload String Yes Bot-defined invoice payload, 1-128 bytes. This will not be displayed to the user, use it for your internal processes. provider_token String Optional Payment provider token, obtained via @BotFather . Pass an empty string for payments in Telegram Stars . currency String Yes Three-letter ISO 4217 currency code, see more on currencies . Pass “XTR” for payments in Telegram Stars . prices Array of LabeledPrice Yes Price breakdown, a JSON-serialized list of components (e.g. product price, tax, discount, delivery cost, delivery tax, bonus, etc.). Must contain exactly one item for payments in Telegram Stars . max_tip_amount Integer Optional The maximum accepted amount for tips in the smallest units of the currency (integer, not float/double). For example, for a maximum tip of US$ 1.45 pass max_tip_amount = 145 . See the exp parameter in currencies.json , it shows the number of digits past the decimal point for each currency (2 for the majority of currencies). Defaults to 0. Not supported for payments in Telegram Stars . suggested_tip_amounts Array of Integer Optional A JSON-serialized array of suggested amounts of tips in the smallest units of the currency (integer, not float/double). At most 4 suggested tip amounts can be specified. The suggested tip amounts must be positive, passed in a strictly increased order and must not exceed max_tip_amount . start_parameter String Optional Unique deep-linking parameter. If left empty, forwarded copies of the sent message will have a Pay button, allowing multiple users to pay directly from the forwarded message, using the same invoice. If non-empty, forwarded copies of the sent message will have a URL button with a deep link to the bot (instead of a Pay button), with the value used as the start parameter provider_data String Optional JSON-serialized data about the invoice, which will be shared with the payment provider. A detailed description of required fields should be provided by the payment provider. photo_url String Optional URL of the product photo for the invoice. Can be a photo of the goods or a marketing image for a service. People like it better when they see what they are paying for. photo_size Integer Optional Photo size in bytes photo_width Integer Optional Photo width photo_height Integer Optional Photo height need_name Boolean Optional Pass True if you require the user's full name to complete the order. Ignored for payments in Telegram Stars . need_phone_number Boolean Optional Pass True if you require the user's phone number to complete the order. Ignored for payments in Telegram Stars . need_email Boolean Optional Pass True if you require the user's email address to complete the order. Ignored for payments in Telegram Stars . need_shipping_address Boolean Optional Pass True if you require the user's shipping address to complete the order. Ignored for payments in Telegram Stars . send_phone_number_to_provider Boolean Optional Pass True if the user's phone number should be sent to the provider. Ignored for payments in Telegram Stars . send_email_to_provider Boolean Optional Pass True if the user's email address should be sent to the provider. Ignored for payments in Telegram Stars . is_flexible Boolean Optional Pass True if the final price depends on the shipping method. Ignored for payments in Telegram Stars . disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup Optional A JSON-serialized object for an inline keyboard . If empty, one 'Pay total price ' button will be shown. If not empty, the first button must be a Pay button. createInvoiceLink Use this method to create a link for an invoice. Returns the created invoice link as String on success. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the link will be created. For payments in Telegram Stars only. title String Yes Product name, 1-32 characters description String Yes Product description, 1-255 characters payload String Yes Bot-defined invoice payload, 1-128 bytes. This will not be displayed to the user, use it for your internal processes. provider_token String Optional Payment provider token, obtained via @BotFather . Pass an empty string for payments in Telegram Stars . currency String Yes Three-letter ISO 4217 currency code, see more on currencies . Pass “XTR” for payments in Telegram Stars . prices Array of LabeledPrice Yes Price breakdown, a JSON-serialized list of components (e.g. product price, tax, discount, delivery cost, delivery tax, bonus, etc.). Must contain exactly one item for payments in Telegram Stars . subscription_period Integer Optional The number of seconds the subscription will be active for before the next payment. The currency must be set to “XTR” (Telegram Stars) if the parameter is used. Currently, it must always be 2592000 (30 days) if specified. Any number of subscriptions can be active for a given bot at the same time, including multiple concurrent subscriptions from the same user. Subscription price must no exceed 2500 Telegram Stars. max_tip_amount Integer Optional The maximum accepted amount for tips in the smallest units of the currency (integer, not float/double). For example, for a maximum tip of US$ 1.45 pass max_tip_amount = 145 . See the exp parameter in currencies.json , it shows the number of digits past the decimal point for each currency (2 for the majority of currencies). Defaults to 0. Not supported for payments in Telegram Stars . suggested_tip_amounts Array of Integer Optional A JSON-serialized array of suggested amounts of tips in the smallest units of the currency (integer, not float/double). At most 4 suggested tip amounts can be specified. The suggested tip amounts must be positive, passed in a strictly increased order and must not exceed max_tip_amount . provider_data String Optional JSON-serialized data about the invoice, which will be shared with the payment provider. A detailed description of required fields should be provided by the payment provider. photo_url String Optional URL of the product photo for the invoice. Can be a photo of the goods or a marketing image for a service. photo_size Integer Optional Photo size in bytes photo_width Integer Optional Photo width photo_height Integer Optional Photo height need_name Boolean Optional Pass True if you require the user's full name to complete the order. Ignored for payments in Telegram Stars . need_phone_number Boolean Optional Pass True if you require the user's phone number to complete the order. Ignored for payments in Telegram Stars . need_email Boolean Optional Pass True if you require the user's email address to complete the order. Ignored for payments in Telegram Stars . need_shipping_address Boolean Optional Pass True if you require the user's shipping address to complete the order. Ignored for payments in Telegram Stars . send_phone_number_to_provider Boolean Optional Pass True if the user's phone number should be sent to the provider. Ignored for payments in Telegram Stars . send_email_to_provider Boolean Optional Pass True if the user's email address should be sent to the provider. Ignored for payments in Telegram Stars . is_flexible Boolean Optional Pass True if the final price depends on the shipping method. Ignored for payments in Telegram Stars . answerShippingQuery If you sent an invoice requesting a shipping address and the parameter is_flexible was specified, the Bot API will send an Update with a shipping_query field to the bot. Use this method to reply to shipping queries. On success, True is returned. Parameter Type Required Description shipping_query_id String Yes Unique identifier for the query to be answered ok Boolean Yes Pass True if delivery to the specified address is possible and False if there are any problems (for example, if delivery to the specified address is not possible) shipping_options Array of ShippingOption Optional Required if ok is True . A JSON-serialized array of available shipping options. error_message String Optional Required if ok is False . Error message in human readable form that explains why it is impossible to complete the order (e.g. \"Sorry, delivery to your desired address is unavailable'). Telegram will display this message to the user. answerPreCheckoutQuery Once the user has confirmed their payment and shipping details, the Bot API sends the final confirmation in the form of an Update with the field pre_checkout_query . Use this method to respond to such pre-checkout queries. On success, True is returned. Note: The Bot API must receive an answer within 10 seconds after the pre-checkout query was sent. Parameter Type Required Description pre_checkout_query_id String Yes Unique identifier for the query to be answered ok Boolean Yes Specify True if everything is alright (goods are available, etc.) and the bot is ready to proceed with the order. Use False if there are any problems. error_message String Optional Required if ok is False . Error message in human readable form that explains the reason for failure to proceed with the checkout (e.g. \"Sorry, somebody just bought the last of our amazing black T-shirts while you were busy filling out your payment details. Please choose a different color or garment!\"). Telegram will display this message to the user. getStarTransactions Returns the bot's Telegram Star transactions in chronological order. On success, returns a StarTransactions object. Parameter Type Required Description offset Integer Optional Number of transactions to skip in the response limit Integer Optional The maximum number of transactions to be retrieved. Values between 1-100 are accepted. Defaults to 100. refundStarPayment Refunds a successful payment in Telegram Stars . Returns True on success. Parameter Type Required Description user_id Integer Yes Identifier of the user whose payment will be refunded telegram_payment_charge_id String Yes Telegram payment identifier editUserStarSubscription Allows the bot to cancel or re-enable extension of a subscription paid in Telegram Stars. Returns True on success. Parameter Type Required Description user_id Integer Yes Identifier of the user whose subscription will be edited telegram_payment_charge_id String Yes Telegram payment identifier for the subscription is_canceled Boolean Yes Pass True to cancel extension of the user subscription; the subscription must be active up to the end of the current subscription period. Pass False to allow the user to re-enable a subscription that was previously canceled by the bot. LabeledPrice This object represents a portion of the price for goods or services. Field Type Description label String Portion label amount Integer Price of the product in the smallest units of the currency (integer, not float/double). For example, for a price of US$ 1.45 pass amount = 145 . See the exp parameter in currencies.json , it shows the number of digits past the decimal point for each currency (2 for the majority of currencies). Invoice This object contains basic information about an invoice. Field Type Description title String Product name description String Product description start_parameter String Unique bot deep-linking parameter that can be used to generate this invoice currency String Three-letter ISO 4217 currency code, or “XTR” for payments in Telegram Stars total_amount Integer Total price in the smallest units of the currency (integer, not float/double). For example, for a price of US$ 1.45 pass amount = 145 . See the exp parameter in currencies.json , it shows the number of digits past the decimal point for each currency (2 for the majority of currencies). ShippingAddress This object represents a shipping address. Field Type Description country_code String Two-letter ISO 3166-1 alpha-2 country code state String State, if applicable city String City street_line1 String First line for the address street_line2 String Second line for the address post_code String Address post code OrderInfo This object represents information about an order. Field Type Description name String Optional . User name phone_number String Optional . User's phone number email String Optional . User email shipping_address ShippingAddress Optional . User shipping address ShippingOption This object represents one shipping option. Field Type Description id String Shipping option identifier title String Option title prices Array of LabeledPrice List of price portions SuccessfulPayment This object contains basic information about a successful payment. Field Type Description currency String Three-letter ISO 4217 currency code, or “XTR” for payments in Telegram Stars total_amount Integer Total price in the smallest units of the currency (integer, not float/double). For example, for a price of US$ 1.45 pass amount = 145 . See the exp parameter in currencies.json , it shows the number of digits past the decimal point for each currency (2 for the majority of currencies). invoice_payload String Bot-specified invoice payload subscription_expiration_date Integer Optional . Expiration date of the subscription, in Unix time; for recurring payments only is_recurring True Optional . True, if the payment is a recurring payment for a subscription is_first_recurring True Optional . True, if the payment is the first payment for a subscription shipping_option_id String Optional . Identifier of the shipping option chosen by the user order_info OrderInfo Optional . Order information provided by the user telegram_payment_charge_id String Telegram payment identifier provider_payment_charge_id String Provider payment identifier RefundedPayment This object contains basic information about a refunded payment. Field Type Description currency String Three-letter ISO 4217 currency code, or “XTR” for payments in Telegram Stars . Currently, always “XTR” total_amount Integer Total refunded price in the smallest units of the currency (integer, not float/double). For example, for a price of US$ 1.45 , total_amount = 145 . See the exp parameter in currencies.json , it shows the number of digits past the decimal point for each currency (2 for the majority of currencies). invoice_payload String Bot-specified invoice payload telegram_payment_charge_id String Telegram payment identifier provider_payment_charge_id String Optional . Provider payment identifier ShippingQuery This object contains information about an incoming shipping query. Field Type Description id String Unique query identifier from User User who sent the query invoice_payload String Bot-specified invoice payload shipping_address ShippingAddress User specified shipping address PreCheckoutQuery This object contains information about an incoming pre-checkout query. Field Type Description id String Unique query identifier from User User who sent the query currency String Three-letter ISO 4217 currency code, or “XTR” for payments in Telegram Stars total_amount Integer Total price in the smallest units of the currency (integer, not float/double). For example, for a price of US$ 1.45 pass amount = 145 . See the exp parameter in currencies.json , it shows the number of digits past the decimal point for each currency (2 for the majority of currencies). invoice_payload String Bot-specified invoice payload shipping_option_id String Optional . Identifier of the shipping option chosen by the user order_info OrderInfo Optional . Order information provided by the user PaidMediaPurchased This object contains information about a paid media purchase. Field Type Description from User User who purchased the media paid_media_payload String Bot-specified paid media payload RevenueWithdrawalState This object describes the state of a revenue withdrawal operation. Currently, it can be one of RevenueWithdrawalStatePending RevenueWithdrawalStateSucceeded RevenueWithdrawalStateFailed RevenueWithdrawalStatePending The withdrawal is in progress. Field Type Description type String Type of the state, always “pending” RevenueWithdrawalStateSucceeded The withdrawal succeeded. Field Type Description type String Type of the state, always “succeeded” date Integer Date the withdrawal was completed in Unix time url String An HTTPS URL that can be used to see transaction details RevenueWithdrawalStateFailed The withdrawal failed and the transaction was refunded. Field Type Description type String Type of the state, always “failed” AffiliateInfo Contains information about the affiliate that received a commission via this transaction. Field Type Description affiliate_user User Optional . The bot or the user that received an affiliate commission if it was received by a bot or a user affiliate_chat Chat Optional . The chat that received an affiliate commission if it was received by a chat commission_per_mille Integer The number of Telegram Stars received by the affiliate for each 1000 Telegram Stars received by the bot from referred users amount Integer Integer amount of Telegram Stars received by the affiliate from the transaction, rounded to 0; can be negative for refunds nanostar_amount Integer Optional . The number of 1/1000000000 shares of Telegram Stars received by the affiliate; from -999999999 to 999999999; can be negative for refunds TransactionPartner This object describes the source of a transaction, or its recipient for outgoing transactions. Currently, it can be one of TransactionPartnerUser TransactionPartnerAffiliateProgram TransactionPartnerFragment TransactionPartnerTelegramAds TransactionPartnerTelegramApi TransactionPartnerOther TransactionPartnerUser Describes a transaction with a user. Field Type Description type String Type of the transaction partner, always “user” user User Information about the user affiliate AffiliateInfo Optional . Information about the affiliate that received a commission via this transaction invoice_payload String Optional . Bot-specified invoice payload subscription_period Integer Optional . The duration of the paid subscription paid_media Array of PaidMedia Optional . Information about the paid media bought by the user paid_media_payload String Optional . Bot-specified paid media payload gift Gift Optional . The gift sent to the user by the bot TransactionPartnerAffiliateProgram Describes the affiliate program that issued the affiliate commission received via this transaction. Field Type Description type String Type of the transaction partner, always “affiliate_program” sponsor_user User Optional . Information about the bot that sponsored the affiliate program commission_per_mille Integer The number of Telegram Stars received by the bot for each 1000 Telegram Stars received by the affiliate program sponsor from referred users TransactionPartnerFragment Describes a withdrawal transaction with Fragment. Field Type Description type String Type of the transaction partner, always “fragment” withdrawal_state RevenueWithdrawalState Optional . State of the transaction if the transaction is outgoing TransactionPartnerTelegramAds Describes a withdrawal transaction to the Telegram Ads platform. Field Type Description type String Type of the transaction partner, always “telegram_ads” TransactionPartnerTelegramApi Describes a transaction with payment for paid broadcasting . Field Type Description type String Type of the transaction partner, always “telegram_api” request_count Integer The number of successful requests that exceeded regular limits and were therefore billed TransactionPartnerOther Describes a transaction with an unknown source or recipient. Field Type Description type String Type of the transaction partner, always “other” StarTransaction Describes a Telegram Star transaction. Field Type Description id String Unique identifier of the transaction. Coincides with the identifier of the original transaction for refund transactions. Coincides with SuccessfulPayment.telegram_payment_charge_id for successful incoming payments from users. amount Integer Integer amount of Telegram Stars transferred by the transaction nanostar_amount Integer Optional . The number of 1/1000000000 shares of Telegram Stars transferred by the transaction; from 0 to 999999999 date Integer Date the transaction was created in Unix time source TransactionPartner Optional . Source of an incoming transaction (e.g., a user purchasing goods or services, Fragment refunding a failed withdrawal). Only for incoming transactions receiver TransactionPartner Optional . Receiver of an outgoing transaction (e.g., a user for a purchase refund, Fragment for a withdrawal). Only for outgoing transactions StarTransactions Contains a list of Telegram Star transactions. Field Type Description transactions Array of StarTransaction The list of transactions Telegram Passport Telegram Passport is a unified authorization method for services that require personal identification. Users can upload their documents once, then instantly share their data with services that require real-world ID (finance, ICOs, etc.). Please see the manual for details. PassportData Describes Telegram Passport data shared with the bot by the user. Field Type Description data Array of EncryptedPassportElement Array with information about documents and other Telegram Passport elements that was shared with the bot credentials EncryptedCredentials Encrypted credentials required to decrypt the data PassportFile This object represents a file uploaded to Telegram Passport. Currently all Telegram Passport files are in JPEG format when decrypted and don't exceed 10MB. Field Type Description file_id String Identifier for this file, which can be used to download or reuse the file file_unique_id String Unique identifier for this file, which is supposed to be the same over time and for different bots. Can't be used to download or reuse the file. file_size Integer File size in bytes file_date Integer Unix time when the file was uploaded EncryptedPassportElement Describes documents or other Telegram Passport elements shared with the bot by the user. Field Type Description type String Element type. One of “personal_details”, “passport”, “driver_license”, “identity_card”, “internal_passport”, “address”, “utility_bill”, “bank_statement”, “rental_agreement”, “passport_registration”, “temporary_registration”, “phone_number”, “email”. data String Optional . Base64-encoded encrypted Telegram Passport element data provided by the user; available only for “personal_details”, “passport”, “driver_license”, “identity_card”, “internal_passport” and “address” types. Can be decrypted and verified using the accompanying EncryptedCredentials . phone_number String Optional . User's verified phone number; available only for “phone_number” type email String Optional . User's verified email address; available only for “email” type files Array of PassportFile Optional . Array of encrypted files with documents provided by the user; available only for “utility_bill”, “bank_statement”, “rental_agreement”, “passport_registration” and “temporary_registration” types. Files can be decrypted and verified using the accompanying EncryptedCredentials . front_side PassportFile Optional . Encrypted file with the front side of the document, provided by the user; available only for “passport”, “driver_license”, “identity_card” and “internal_passport”. The file can be decrypted and verified using the accompanying EncryptedCredentials . reverse_side PassportFile Optional . Encrypted file with the reverse side of the document, provided by the user; available only for “driver_license” and “identity_card”. The file can be decrypted and verified using the accompanying EncryptedCredentials . selfie PassportFile Optional . Encrypted file with the selfie of the user holding a document, provided by the user; available if requested for “passport”, “driver_license”, “identity_card” and “internal_passport”. The file can be decrypted and verified using the accompanying EncryptedCredentials . translation Array of PassportFile Optional . Array of encrypted files with translated versions of documents provided by the user; available if requested for “passport”, “driver_license”, “identity_card”, “internal_passport”, “utility_bill”, “bank_statement”, “rental_agreement”, “passport_registration” and “temporary_registration” types. Files can be decrypted and verified using the accompanying EncryptedCredentials . hash String Base64-encoded element hash for using in PassportElementErrorUnspecified EncryptedCredentials Describes data required for decrypting and authenticating EncryptedPassportElement . See the Telegram Passport Documentation for a complete description of the data decryption and authentication processes. Field Type Description data String Base64-encoded encrypted JSON-serialized data with unique user's payload, data hashes and secrets required for EncryptedPassportElement decryption and authentication hash String Base64-encoded data hash for data authentication secret String Base64-encoded secret, encrypted with the bot's public RSA key, required for data decryption setPassportDataErrors Informs a user that some of the Telegram Passport elements they provided contains errors. The user will not be able to re-submit their Passport to you until the errors are fixed (the contents of the field for which you returned the error must change). Returns True on success. Use this if the data submitted by the user doesn't satisfy the standards your service requires for any reason. For example, if a birthday date seems invalid, a submitted document is blurry, a scan shows evidence of tampering, etc. Supply some details in the error message to make sure the user knows how to correct the issues. Parameter Type Required Description user_id Integer Yes User identifier errors Array of PassportElementError Yes A JSON-serialized array describing the errors PassportElementError This object represents an error in the Telegram Passport element which was submitted that should be resolved by the user. It should be one of: PassportElementErrorDataField PassportElementErrorFrontSide PassportElementErrorReverseSide PassportElementErrorSelfie PassportElementErrorFile PassportElementErrorFiles PassportElementErrorTranslationFile PassportElementErrorTranslationFiles PassportElementErrorUnspecified PassportElementErrorDataField Represents an issue in one of the data fields that was provided by the user. The error is considered resolved when the field's value changes. Field Type Description source String Error source, must be data type String The section of the user's Telegram Passport which has the error, one of “personal_details”, “passport”, “driver_license”, “identity_card”, “internal_passport”, “address” field_name String Name of the data field which has the error data_hash String Base64-encoded data hash message String Error message PassportElementErrorFrontSide Represents an issue with the front side of a document. The error is considered resolved when the file with the front side of the document changes. Field Type Description source String Error source, must be front_side type String The section of the user's Telegram Passport which has the issue, one of “passport”, “driver_license”, “identity_card”, “internal_passport” file_hash String Base64-encoded hash of the file with the front side of the document message String Error message PassportElementErrorReverseSide Represents an issue with the reverse side of a document. The error is considered resolved when the file with reverse side of the document changes. Field Type Description source String Error source, must be reverse_side type String The section of the user's Telegram Passport which has the issue, one of “driver_license”, “identity_card” file_hash String Base64-encoded hash of the file with the reverse side of the document message String Error message PassportElementErrorSelfie Represents an issue with the selfie with a document. The error is considered resolved when the file with the selfie changes. Field Type Description source String Error source, must be selfie type String The section of the user's Telegram Passport which has the issue, one of “passport”, “driver_license”, “identity_card”, “internal_passport” file_hash String Base64-encoded hash of the file with the selfie message String Error message PassportElementErrorFile Represents an issue with a document scan. The error is considered resolved when the file with the document scan changes. Field Type Description source String Error source, must be file type String The section of the user's Telegram Passport which has the issue, one of “utility_bill”, “bank_statement”, “rental_agreement”, “passport_registration”, “temporary_registration” file_hash String Base64-encoded file hash message String Error message PassportElementErrorFiles Represents an issue with a list of scans. The error is considered resolved when the list of files containing the scans changes. Field Type Description source String Error source, must be files type String The section of the user's Telegram Passport which has the issue, one of “utility_bill”, “bank_statement”, “rental_agreement”, “passport_registration”, “temporary_registration” file_hashes Array of String List of base64-encoded file hashes message String Error message PassportElementErrorTranslationFile Represents an issue with one of the files that constitute the translation of a document. The error is considered resolved when the file changes. Field Type Description source String Error source, must be translation_file type String Type of element of the user's Telegram Passport which has the issue, one of “passport”, “driver_license”, “identity_card”, “internal_passport”, “utility_bill”, “bank_statement”, “rental_agreement”, “passport_registration”, “temporary_registration” file_hash String Base64-encoded file hash message String Error message PassportElementErrorTranslationFiles Represents an issue with the translated version of a document. The error is considered resolved when a file with the document translation change. Field Type Description source String Error source, must be translation_files type String Type of element of the user's Telegram Passport which has the issue, one of “passport”, “driver_license”, “identity_card”, “internal_passport”, “utility_bill”, “bank_statement”, “rental_agreement”, “passport_registration”, “temporary_registration” file_hashes Array of String List of base64-encoded file hashes message String Error message PassportElementErrorUnspecified Represents an issue in an unspecified place. The error is considered resolved when new data is added. Field Type Description source String Error source, must be unspecified type String Type of element of the user's Telegram Passport which has the issue element_hash String Base64-encoded element hash message String Error message Games Your bot can offer users HTML5 games to play solo or to compete against each other in groups and one-on-one chats. Create games via @BotFather using the /newgame command. Please note that this kind of power requires responsibility: you will need to accept the terms for each game that your bots will be offering. Games are a new type of content on Telegram, represented by the Game and InlineQueryResultGame objects. Once you've created a game via BotFather , you can send games to chats as regular messages using the sendGame method, or use inline mode with InlineQueryResultGame . If you send the game message without any buttons, it will automatically have a 'Play GameName ' button. When this button is pressed, your bot gets a CallbackQuery with the game_short_name of the requested game. You provide the correct URL for this particular user and the app opens the game in the in-app browser. You can manually add multiple buttons to your game message. Please note that the first button in the first row must always launch the game, using the field callback_game in InlineKeyboardButton . You can add extra buttons according to taste: e.g., for a description of the rules, or to open the game's official community. To make your game more attractive, you can upload a GIF animation that demonstrates the game to the users via BotFather (see Lumberjack for example). A game message will also display high scores for the current chat. Use setGameScore to post high scores to the chat with the game, add the disable_edit_message parameter to disable automatic update of the message with the current scoreboard. Use getGameHighScores to get data for in-game high score tables. You can also add an extra sharing button for users to share their best score to different chats. For examples of what can be done using this new stuff, check the @gamebot and @gamee bots. sendGame Use this method to send a game. On success, the sent Message is returned. Parameter Type Required Description business_connection_id String Optional Unique identifier of the business connection on behalf of which the message will be sent chat_id Integer Yes Unique identifier for the target chat message_thread_id Integer Optional Unique identifier for the target message thread (topic) of the forum; for forum supergroups only game_short_name String Yes Short name of the game, serves as the unique identifier for the game. Set up your games via @BotFather . disable_notification Boolean Optional Sends the message silently . Users will receive a notification with no sound. protect_content Boolean Optional Protects the contents of the sent message from forwarding and saving allow_paid_broadcast Boolean Optional Pass True to allow up to 1000 messages per second, ignoring broadcasting limits for a fee of 0.1 Telegram Stars per message. The relevant Stars will be withdrawn from the bot's balance message_effect_id String Optional Unique identifier of the message effect to be added to the message; for private chats only reply_parameters ReplyParameters Optional Description of the message to reply to reply_markup InlineKeyboardMarkup Optional A JSON-serialized object for an inline keyboard . If empty, one 'Play game_title' button will be shown. If not empty, the first button must launch the game. Game This object represents a game. Use BotFather to create and edit games, their short names will act as unique identifiers. Field Type Description title String Title of the game description String Description of the game photo Array of PhotoSize Photo that will be displayed in the game message in chats. text String Optional . Brief description of the game or high scores included in the game message. Can be automatically edited to include current high scores for the game when the bot calls setGameScore , or manually edited using editMessageText . 0-4096 characters. text_entities Array of MessageEntity Optional . Special entities that appear in text , such as usernames, URLs, bot commands, etc. animation Animation Optional . Animation that will be displayed in the game message in chats. Upload via BotFather CallbackGame A placeholder, currently holds no information. Use BotFather to set up your game. setGameScore Use this method to set the score of the specified user in a game message. On success, if the message is not an inline message, the Message is returned, otherwise True is returned. Returns an error, if the new score is not greater than the user's current score in the chat and force is False . Parameter Type Required Description user_id Integer Yes User identifier score Integer Yes New score, must be non-negative force Boolean Optional Pass True if the high score is allowed to decrease. This can be useful when fixing mistakes or banning cheaters disable_edit_message Boolean Optional Pass True if the game message should not be automatically edited to include the current scoreboard chat_id Integer Optional Required if inline_message_id is not specified. Unique identifier for the target chat message_id Integer Optional Required if inline_message_id is not specified. Identifier of the sent message inline_message_id String Optional Required if chat_id and message_id are not specified. Identifier of the inline message getGameHighScores Use this method to get data for high score tables. Will return the score of the specified user and several of their neighbors in a game. Returns an Array of GameHighScore objects. This method will currently return scores for the target user, plus two of their closest neighbors on each side. Will also return the top three users if the user and their neighbors are not among them. Please note that this behavior is subject to change. Parameter Type Required Description user_id Integer Yes Target user id chat_id Integer Optional Required if inline_message_id is not specified. Unique identifier for the target chat message_id Integer Optional Required if inline_message_id is not specified. Identifier of the sent message inline_message_id String Optional Required if chat_id and message_id are not specified. Identifier of the inline message GameHighScore This object represents one row of the high scores table for a game. Field Type Description position Integer Position in high score table for the game user User User score Integer Score And that's about all we've got for now. If you've got any questions, please check out our Bot FAQ » Telegram Telegram is a cloud-based mobile and desktop messaging app with a focus on security and speed. About FAQ Privacy Press Mobile Apps iPhone/iPad Android Mobile Web Desktop Apps PC/Mac/Linux macOS Web-browser Platform API Translations Instant View About Blog Press Moderation",
            "meta_description": "",
            "links": [
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/api",
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/faq",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/api-changelog",
                "https://core.telegram.org/bots/self-signed",
                "https://core.telegram.org/bots/self-signed",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/inline",
                "https://core.telegram.org/bots/inline",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/payments/currencies.json",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/payments",
                "https://core.telegram.org/bots/payments/currencies.json",
                "https://core.telegram.org/bots/payments/currencies.json",
                "https://core.telegram.org/bots/payments/currencies.json",
                "https://core.telegram.org/bots/payments/currencies.json",
                "https://core.telegram.org/bots/payments/currencies.json",
                "https://core.telegram.org/bots/payments/currencies.json",
                "https://core.telegram.org/bots/payments/currencies.json",
                "https://core.telegram.org/bots/faq"
            ],
            "timestamp": "2024-12-27T22:28:14.419904",
            "status_code": 200
        },
        {
            "url": "https://core.telegram.org/bots/faq",
            "title": "Bots FAQ",
            "text_content": "Bots FAQ Twitter Home FAQ Apps API Protocol Schema Telegram Bots Bots FAQ Bots FAQ If you are new to Telegram bots, we recommend checking out our Introduction to Bots first. You may also find the Bot API Manual useful. General How do I create a bot? Where can I get some code examples? I have a feature request! What messages will my bot get? Why doesn't my bot see messages from other bots? Getting Updates How do I get updates? Long polling problems Webhook problems Using self-signed certificates How can I make sure webhook requests come from Telegram? Handling Media Downloading files Uploading large files Can I count of file_ids to be persistent? Broadcasting to Users How do I avoid hitting limits? How do I message all my subscribers? General Questions How do I create a bot? Creating Telegram bots is super-easy, but you will need at least some skills at computer programming. In order for a bot to work, set up a bot account with @BotFather , then connect it to your backend server via our API . Unfortunately, there are no out-of-the-box ways to create a working bot if you are not a developer. But we're sure you'll soon find plenty of bots created by other people to play with. I'm a developer. Where can I find some examples? Here are two sample bots, both written in PHP: Hello Bot demonstrates the basics of the Telegram bot API. Simple Poll bot is a more complete example, it supports both long-polling and Webhooks for updates. Many members of our community are building bots and publishing sources. We're collecting them on this page » Ping us on @BotSupport if you've built a bot and would like to share it with others. Will you add X to the Bot API? The bot API is still pretty young. There are many potential features to consider and implement. We'll be studying what people do with their bots for a while to see which directions will be most important for the platform. All bot developers are welcome to share ideas for our Bot API with our @BotSupport account. What messages will my bot get? 1. All bots , regardless of settings, will receive: All service messages. All messages from private chats with users. All messages from channels where they are a member. 2. Bot admins and bots with privacy mode disabled will receive all messages except messages sent by other bots. 3. Bots with privacy mode enabled will receive: Commands explicitly meant for them (e.g., /command@this_bot). General commands from users (e.g. /start) if the bot was the last bot to send a message to the group. Messages sent via this bot. Replies to any messages implicitly or explicitly meant for this bot. Note that each particular message can only be available to one privacy-enabled bot at a time, i.e., a reply to bot A containing an explicit command for bot B or sent via bot C will only be available to bot A. Replies have the highest priority. Why doesn't my bot see messages from other bots? Bots talking to each other could potentially get stuck in unwelcome loops. To avoid this, we decided that bots will not be able to see messages from other bots regardless of mode. Getting Updates How do I get updates? There are currently two ways of getting updates. You can either use long polling or Webhooks . Please note that it's not possible to get updates via long polling while an outgoing Webhook is set. Long polling gives me the same updates again and again! The getUpdates method returns the earliest 100 unconfirmed updates. To confirm an update, use the offset parameter when calling getUpdates like this: offset = update_id of last processed update + 1 All updates with update_id less than or equal to offset will be marked as confirmed on the server and will no longer be returned. I'm having problems with Webhooks. If you've set up your webhook successfully, but are not getting any updates, please remember: You need a valid SSL certificate for webhooks to work. To use a self-signed certificate, you need to upload your public key certificate using the certificate parameter in setWebhook . Please upload as InputFile, sending a String will not work. Ports currently supported for Webhooks: 443 , 80 , 88 , 8443 . Wildcard certificates may not be supported. Redirects are not supported. CN must exactly match your domain. Please check out this new WEBHOOK GUIDE to learn all there is to know about webhooks! I'm having trouble with my self-signed certificate! Please take a look at this self-signed certificate guide we made just for you. If you've read it and still have a question, ping us on botsupport. How can I make sure that Webhook requests are coming from Telegram? If you'd like to make sure that the Webhook request comes from Telegram, we recommend using a secret path in the URL you give us, e.g. www.example.com/your_token. Since nobody else knows your bot's token, you can be pretty sure it's us. How can I make requests in response to updates? This is possible if you're using webhooks. The upside is that you need less requests, the downside — that in this case it's not possible to know that such a request was successful or get its result. Whenever you receive a webhook update, you have two options: 1. Issue POST to https://api.telegram.org/bot<token>/method 2. Reply directly and give method as JSON payload in the reply You may also want to look at our sample HelloBot , it offers a PHP implementation of this. Handling Media How do I download files? Use the getFile method. Please note that this will only work with files of up to 20 MB in size. How do I upload a large file? Bots can currently send files of any type of up to 50 MB in size, so yes, very large files won't work for now. Sorry. This limit may be changed in the future. Can I count on file_ids to be persistent? Yes, file_ids can be treated as persistent. Broadcasting to Users My bot is hitting limits, how do I avoid this? By default, bots are able to message their users at no cost – but have limitations on the number of messages they can broadcast in a single interval: In a single chat, avoid sending more than one message per second. We may allow short bursts that go over this limit, but eventually you'll begin receiving 429 errors. In a group, bots are not be able to send more than 20 messages per minute. For bulk notifications, bots are not able to broadcast more than about 30 messages per second, unless they enable paid broadcasts to increase the limit. How can I message all of my bot's subscribers at once? Enabling paid broadcasts in @BotFather allows a bot to broadcast up to 1000 messages per second . Each message broadcasted over the free amount of 30 per second incurs a cost of 0.1 Stars per message , paid with Telegram Stars from the bot's balance. In order to enable this feature, a bot must have at least 100,000 Stars on its balance and at least 100,000 monthly active users. Bots with increased limits are only charged for messages that are broadcasted successfully. If you do not wish to enable paid broadcasts, consider spreading them over longer intervals (e.g. 8-12 hours) to avoid hitting the limit. The API will not allow bulk notifications to more than ~30 users per second – if you go over that, you'll start getting 429 errors. If you've got questions that are not answered on this page, ping us at @BotSupport in Telegram. We welcome any suggestions for the Bot Platform and API. Telegram Telegram is a cloud-based mobile and desktop messaging app with a focus on security and speed. About FAQ Privacy Press Mobile Apps iPhone/iPad Android Mobile Web Desktop Apps PC/Mac/Linux macOS Web-browser Platform API Translations Instant View About Blog Press Moderation",
            "meta_description": "",
            "links": [
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/faq",
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/api",
                "https://core.telegram.org/bots/api",
                "https://core.telegram.org/bots/samples/hellobot",
                "https://core.telegram.org/bots/samples",
                "https://core.telegram.org/bots/self-signed",
                "https://core.telegram.org/bots/samples/hellobot"
            ],
            "timestamp": "2024-12-27T22:28:15.021323",
            "status_code": 200
        },
        {
            "url": "https://core.telegram.org/bots/webapps",
            "title": "Telegram Mini Apps",
            "text_content": "Telegram Mini Apps Twitter Home FAQ Apps API Protocol Schema Bots Telegram Mini Apps Telegram Mini Apps With Mini Apps developers can use JavaScript to create infinitely flexible interfaces that can be launched right inside Telegram — and can completely replace any website . Like bots, Mini Apps support seamless authorization , integrated payments via 20 payment providers (with Google Pay and Apple Pay out of the box), delivering tailored push notifications to users, and much more . To see a Mini App in action, try our sample @DurgerKingBot . Recent changes November 17, 2024 Bot API 8.0 This is the largest update in the history of Telegram mini apps – adding more than 10 new features and monetization options for developers. To read more about all these changes, check out this dedicated blog post . Full-screen Mode Mini Apps are now able to become full-screen in both portrait and landscape mode – allowing them to host more games , play widescreen media and support immersive user experiences. Added the methods requestFullscreen and exitFullscreen to toggle full-screen mode. Added the fields safeAreaInset and contentSafeAreaInset , allowing Mini Apps to ensure that their content properly respects the device's safe area margins. Further added the fields isActive and isFullscreen to the class WebApp . Added the events activated , deactivated , safeAreaChanged , contentSafeAreaChanged , fullscreenChanged and fullscreenFailed . Homescreen Shortcuts Mini Apps can now be accessed via direct shortcuts added to the home screen of mobile devices. Added the method addToHomeScreen to create a shortcut for users to add to their home screens. Added the method checkHomeScreenStatus to determine the status and support of the home screen shortcut for the Mini App on the current device. Added the events homeScreenAdded and homeScreenChecked . Emoji Status Mini Apps can now prompt users to set their emoji status – or request access to later sync it automatically with in-game badges, third-party APIs and more. Added the method setEmojiStatus to let users manually confirm a custom emoji as their new status via a native dialog. Added the method requestEmojiStatusAccess for obtaining permission to later update a user's emoji status via the Bot API method setUserEmojiStatus . Added the events emojiStatusSet , emojiStatusFailed and emojiStatusAccessRequested . Media Sharing and File Downloads Users can now share media directly from Mini Apps – sending referral codes , custom memes, artwork and more to any chat or posting them as a story . Added the method shareMessage to share media from Mini Apps to Telegram chats. Also see PreparedInlineMessage . Added the method downloadFile , introducing support for a native popup that prompts users to download files from the Mini App. Added the events shareMessageSent , shareMessageFailed and fileDownloadRequested . Geolocation Access Mini Apps can now request geolocation access to users, allowing them to build virtually any location-based service, from games with dynamic points of interest to interactive maps for events. Added the field LocationManager to the class WebApp . Added the events locationManagerUpdated and locationRequested . Device Motion Tracking Mini Apps can now track detailed device motion data , allowing them to implement better productivity tools, immersive VR experiences and more. Added the fields isOrientationLocked , Accelerometer , DeviceOrientation and Gyroscope to the class WebApp . Added the methods lockOrientation and unlockOrientation to control the screen orientation. Added the events accelerometerStarted , accelerometerStopped , accelerometerChanged , accelerometerFailed , deviceOrientationStarted , deviceOrientationStopped , deviceOrientationChanged , deviceOrientationFailed , gyroscopeStarted , gyroscopeStopped , gyroscopeChanged , gyroscopeFailed . Subscription Plans and Gifts for Telegram Stars Mini Apps now support paid subscriptions powered by Telegram Stars – monetizing their efforts with multiple tiers of content and features. Mini Apps can use their balance of Telegram Stars to send gifts to their users. You can read more on implementing Paid Subscriptions and Gifts in our Bot API documentation . Loading Screen Customization Mini Apps can customize their loading screen, adding their own icon and specific colors for light and dark themes. You can access these customization settings in @BotFather via /mybots > Select Bot > Bot Settings > Configure Mini App > Enable Mini App Hardware-specific Optimizations Mini Apps running on Android can now receive basic information about a device's processing hardware, allowing them to optimize user experience based on the device's capabilities. This information includes the OS, App and SDK's respective versions as well as the device's model and performance class. General The field photo_url in the class WebAppUser is now available to all Mini Apps, allowing them to access a user's profile photo if their privacy settings allow for it. Third parties (e.g., Mini App builders, external SDKs etc.) that receive or process data on behalf of Mini Apps are now able to validate it without knowing the App's bot token . Debugging options have been expanded to include full support for iOS devices . You can use these tools to find app-specific issues in your Mini App. September 6, 2024 Bot API 7.10 Added the field SecondaryButton to the class WebApp . Added the event secondaryButtonClicked . Renamed the class MainButton to the class BottomButton . Added the field bottomBarColor and the method setBottomBarColor to the class WebApp . Added the field bottom_bar_bg_color to the class ThemeParams . July 31, 2024 Bot API 7.8 Added the option for bots to set a Main Mini App , which can be previewed and launched directly from a button in the bot's profile or a link. Added the method shareToStory to the class WebApp . July 7, 2024 Bot API 7.7 Added the field isVerticalSwipesEnabled and the methods enableVerticalSwipes , disableVerticalSwipes to the class WebApp . Added the event scanQrPopupClosed . July 1, 2024 Bot API 7.6 Added the field section_separator_color to the class ThemeParams . Changed the default opening mode for Direct Link Mini Apps . March 31, 2024 Bot API 7.2 Added the field BiometricManager to the class WebApp . December 29, 2023 Bot API 7.0 Added the field SettingsButton to the class WebApp . Added the fields header_bg_color , accent_text_color , section_bg_color , section_header_text_color , subtitle_text_color , destructive_text_color to the class ThemeParams . Mini Apps no longer close when the method WebApp.openTelegramLink is called. September 22, 2023 Bot API 6.9 Added the field CloudStorage to the class WebApp . Added the methods requestWriteAccess and requestContact to the class WebApp . Added the fields added_to_attachment_menu and allows_write_to_pm to the class WebAppUser . Added the events writeAccessRequested and contactRequested . Added the ability to set any header color using the setHeaderColor method. April 21, 2023 Bot API 6.7 Added support for launching Mini Apps from inline query results and from a direct link. Added the method switchInlineQuery to the class WebApp . December 30, 2022 Bot API 6.4 Added the field platform , the optional parameter options to the method openLink and the methods showScanQrPopup , closeScanQrPopup , readTextFromClipboard to the class WebApp . Added the events qrTextReceived , clipboardTextReceived . August 12, 2022 Bot API 6.2 Added the field isClosingConfirmationEnabled and the methods enableClosingConfirmation , disableClosingConfirmation , showPopup , showAlert , showConfirm to the class WebApp . Added the field is_premium to the class WebAppUser . Added the event popupClosed . June 20, 2022 Bot API 6.1 Added the ability to use bots added to the attachment menu in group, supergroup and channel chats. Added support for t.me links that can be used to select the chat in which the attachment menu with the bot will be opened. Added the fields version , headerColor , backgroundColor , BackButton , HapticFeedback and the methods isVersionAtLeast , setHeaderColor , setBackgroundColor , openLink , openTelegramLink , openInvoice to the class WebApp . Added the field secondary_bg_color to the class ThemeParams . Added the method offClick to the class MainButton . Added the fields chat , can_send_after to the class WebAppInitData . Added the events backButtonClicked , settingsButtonClicked , invoiceClosed . Designing Mini Apps Color Schemes Mini Apps always receive data about the user's current color theme in real time, so you can adjust the appearance of your interfaces to match it. For example, when users switch between Day and Night modes or use various custom themes . Jump to technical information Design Guidelines Telegram apps are known for being snappy, smooth and following a consistent cross-platform design. Your Mini App should ideally reflect these principles. All elements should be responsive and designed with a mobile-first approach. Interactive elements should mimic the style, behavior, and intent of UI components that already exist. All included animations should be smooth, ideally 60fps. All inputs and images should contain labels for accessibility purposes. The app should deliver a seamless experience by monitoring the dynamic theme-based colors provided by the API and using them accordingly. Ensure that the app’s interface respects the safe area and content safe area to avoid overlapping with control elements, especially when using fullscreen mode. For Android devices, consider the additional information in the User-Agent (see User-Agent details ) and adjust for the device’s performance class, minimizing animations and visual effects on low-performance devices to ensure smooth performance. Implementing Mini Apps Telegram currently supports seven different ways of launching Mini Apps: the main Mini App from a profile button , from a keyboard button , from an inline button , from the bot menu button , via inline mode , from a direct link – and even from the attachment menu . Keyboard Button Mini Apps TL;DR: Mini Apps launched from a web_app type keyboard button can send data back to the bot in a service message using Telegram.WebApp.sendData . This makes it possible for the bot to produce a response without communicating with any external servers. Users can interact with bots using custom keyboards , buttons under bot messages , as well as by sending freeform text messages or any of the attachment types supported by Telegram: photos and videos, files, locations, contacts and polls. For even more flexibility, bots can utilize the full power of HTML5 to create user-friendly input interfaces. You can send a web_app type KeyboardButton that opens a Mini App from the specified URL. To transmit data from the user back to the bot, the Mini App can call the Telegram.WebApp.sendData method. Data will be transmitted to the bot as a String in a service message. The bot can continue communicating with the user after receiving it. Good for: Сustom data input interfaces (a personalized calendar for selecting dates; selecting data from a list with advanced search options; a randomizer that lets the user “spin a wheel” and chooses one of the available options, etc.) Reusable components that do not depend on a particular bot. Inline Button Mini Apps TL;DR: For more interactive Mini Apps like @DurgerKingBot , use a web_app type Inline KeyboardButton , which gets basic user information and can be used to send a message on behalf of the user to the chat with the bot. If receiving text data alone is insufficient or you need a more advanced and personalized interface, you can open a Mini App using a web_app type Inline KeyboardButton . From the button, a Mini App will open with the URL specified in the button. In addition to the user's theme settings , it will receive basic user information ( ID , name , username , language_code ) and a unique identifier for the session, query_id , which allows messages on behalf of the user to be sent back to the bot. The bot can call the Bot API method answerWebAppQuery to send an inline message from the user back to the bot and close the Mini App. After receiving the message, the bot can continue communicating with the user. Good for: Fully-fledged web services and integrations of any kind. The use cases are effectively unlimited . Launching Mini Apps from the Menu Button TL;DR: Mini Apps can be launched from a customized menu button. This simply offers a quicker way to access the app and is otherwise identical to launching a mini app from an inline button . By default, chats with bots always show a convenient menu button that provides quick access to all listed commands . With Bot API 6.0 , this button can be used to launch a Mini App instead. To configure the menu button, you must specify the text it should show and the Mini App URL. There are two ways to set these parameters: To customize the button for all users , use @BotFather (the /setmenubutton command or Bot Settings > Menu Button ). To customize the button for both all users and specific users , use the setChatMenuButton method in the Bot API. For example, change the button text according to the user's language, or show links to different Mini Apps based on a user's settings in your bot. Apart from this, Mini Apps opened via the menu button work in the exact same way as when using inline buttons . @DurgerKingBot allows launching its Mini App both from an inline button and from the menu button. Launching the main Mini App TL;DR: If your bot is a mini app, you can add a prominent Launch app button as well as high-quality demo videos and screenshots to the bot’s profile. To do this, go to @BotFather and set up your bot's Main Mini App . If your bot is a mini app, you can unlock a number of features that streamline and simplify the way in which users view and interact with it. To do this, go to @BotFather and set up your bot's Main Mini App . After setting a main mini app, you'll be able to upload detailed media preview demos to publicly highlight your app's key features on its profile. A Launch app button will also appear, allowing users to open your app directly from its profile. Bots that enabled a main mini app will be displayed in the Apps tab of the search for users who have launched them. Media previews support multiple languages – so you can upload translated versions of your previews that will be shown to users based on their app language . A bot's main Mini App can also be opened in the current chat by direct link in the format https://t.me/botusername?startapp . If a non-empty startapp parameter is included in the link, it will be passed to the Mini App in the start_param field and in the GET parameter tgWebAppStartParam . Examples https://t.me/botusername?startapp https://t.me/botusername?startapp=command https://t.me/botusername?startapp=command&mode=compact In this mode, Mini Apps can use the chat_type and chat_instance parameters to keep track of the current chat context. This introduces support for concurrent and shared usage by multiple chat members – to create live whiteboards, group orders, multiplayer games and similar apps. By default, the main Mini App opens to full-screen height, and users cannot reduce them to half-height. However, you can change this behavior via @BotFather or by including the parameter mode=compact in the link to the Mini App, in which case it will open to half-screen height by default. Good for: Fully-fledged web services and integrations that any user can open in one tap. Cooperative, multiplayer or teamwork-oriented services within a chat context. The use cases are effectively unlimited . Successful bots which enable a main Mini App and accept payments in Telegram Stars may be featured in the Telegram Mini App Store . To increase the chances of being featured, we recommend uploading high-quality media showcasing your app on your bot's profile and following our design guidelines . Inline Mode Mini Apps TL;DR: Mini Apps launched via web_app type InlineQueryResultsButton can be used anywhere in inline mode. Users can create content in a web interface and then seamlessly send it to the current chat via inline mode. You can use the button parameter in the answerInlineQuery method to display a special 'Switch to Mini App' button either above or in place of the inline results. This button will open a Mini App from the specified URL. Once done, you can call the Telegram.WebApp.switchInlineQuery method to send the user back to inline mode. Inline Mini Apps have no access to the chat – they can't read messages or send new ones on behalf of the user. To send messages, the user must be redirected to inline mode and actively pick a result. Good for: Fully-fledged web services and integrations in inline mode. Direct Link Mini Apps TL;DR: Mini App Bots can be launched from a direct link in any chat. They support a startapp parameter and are aware of the current chat context. You can use direct links to open a Mini App directly in the current chat. If a non-empty startapp parameter is included in the link, it will be passed to the Mini App in the start_param field and in the GET parameter tgWebAppStartParam . In this mode, Mini Apps can use the chat_type and chat_instance parameters to keep track of the current chat context. This introduces support for concurrent and shared usage by multiple chat members – to create live whiteboards, group orders, multiplayer games and similar apps. Mini Apps opened from a direct link have no access to the chat – they can't read messages or send new ones on behalf of the user. To send messages, the user must be redirected to inline mode and actively pick a result. Starting from Bot API 7.6 , by default, Mini Apps of this type open to full-screen height, and users cannot reduce them to half-height. However, you can change this behavior by including the parameter mode=compact in the link to the Mini App, in which case it will open to half-screen height by default. Examples https://t.me/botusername/appname https://t.me/botusername/appname?startapp=command https://t.me/botusername/appname?startapp=command&mode=compact Good for: Fully-fledged web services and integrations that any user can open in one tap. Cooperative, multiplayer or teamwork-oriented services within a chat context. The use cases are effectively unlimited . Launching Mini Apps from the Attachment Menu TL;DR: Mini App Bots can request to be added directly to a user's attachment menu, allowing them to be quickly launched from any chat. To try this mode, open this attachment menu link for @DurgerKingBot , then use the menu in any type of chat . Mini App Bots can request to be added directly to a user's attachment menu, allowing them to be quickly launched from any type of chat . You can configure in which types of chats your mini app can be started from the attachment menu (private, groups, supergroups or channels). Attachment menu integration is currently only available for major advertisers on the Telegram Ad Platform . However, all bots can use it in the test server environment . To enable this feature for your bot, open @BotFather from an account on the test server and send the /setattach command – or go to Bot Settings > Configure Attachment Menu . Then specify the URL that will be opened to launch the bot's Mini App via its icon in the attachment menu. You can add a 'Settings' item to the context menu of your Mini App using @BotFather . When users select this option from the menu, your bot will receive a settingsButtonClicked event. In addition to the user's theme settings , the bot will receive basic user information ( ID , name , username , language_code , photo ), as well as public info about the chat partner ( ID , name , username , photo ) or the chat info ( ID , type , title , username , photo ) and a unique identifier for the web view session query_id , which allows messages of any type to be sent to the chat on behalf of the user that opened the bot. The bot can call the Bot API method answerWebAppQuery , which sends an inline message from the user via the bot to the chat where it was launched and closes the Mini App. You can read more about adding bots to the attachment menu here . Initializing Mini Apps To connect your Mini App to the Telegram client, place the script telegram-web-app.js in the <head> tag before any other scripts, using this code: <script src=\"https://telegram.org/js/telegram-web-app.js?56\"></script> Once the script is connected, a window.Telegram.WebApp object will become available with the following fields: Field Type Description initData String A string with raw data transferred to the Mini App, convenient for validating data . WARNING: Validate data from this field before using it on the bot's server. initDataUnsafe WebAppInitData An object with input data transferred to the Mini App. WARNING: Data from this field should not be trusted. You should only use data from initData on the bot's server and only after it has been validated . version String The version of the Bot API available in the user's Telegram app. platform String The name of the platform of the user's Telegram app. colorScheme String The color scheme currently used in the Telegram app. Either “light” or “dark”. Also available as the CSS variable var(--tg-color-scheme) . themeParams ThemeParams An object containing the current theme settings used in the Telegram app. isActive NEW Boolean Bot API 8.0+ True , if the Mini App is currently active. False , if the Mini App is minimized. isExpanded Boolean True , if the Mini App is expanded to the maximum available height. False, if the Mini App occupies part of the screen and can be expanded to the full height using the expand() method. viewportHeight Float The current height of the visible area of the Mini App. Also available in CSS as the variable var(--tg-viewport-height) . The application can display just the top part of the Mini App, with its lower part remaining outside the screen area. From this position, the user can “pull” the Mini App to its maximum height, while the bot can do the same by calling the expand() method. As the position of the Mini App changes, the current height value of the visible area will be updated in real time. Please note that the refresh rate of this value is not sufficient to smoothly follow the lower border of the window. It should not be used to pin interface elements to the bottom of the visible area. It's more appropriate to use the value of the viewportStableHeight field for this purpose. viewportStableHeight Float The height of the visible area of the Mini App in its last stable state. Also available in CSS as a variable var(--tg-viewport-stable-height) . The application can display just the top part of the Mini App, with its lower part remaining outside the screen area. From this position, the user can “pull” the Mini App to its maximum height, while the bot can do the same by calling the expand() method. Unlike the value of viewportHeight , the value of viewportStableHeight does not change as the position of the Mini App changes with user gestures or during animations. The value of viewportStableHeight will be updated after all gestures and animations are completed and the Mini App reaches its final size. Note the event viewportChanged with the passed parameter isStateStable=true , which will allow you to track when the stable state of the height of the visible area changes. headerColor String Current header color in the #RRGGBB format. backgroundColor String Current background color in the #RRGGBB format. bottomBarColor String Current bottom bar color in the #RRGGBB format. isClosingConfirmationEnabled Boolean True , if the confirmation dialog is enabled while the user is trying to close the Mini App. False , if the confirmation dialog is disabled. isVerticalSwipesEnabled Boolean True , if vertical swipes to close or minimize the Mini App are enabled. False , if vertical swipes to close or minimize the Mini App are disabled. In any case, the user will still be able to minimize and close the Mini App by swiping the Mini App's header. isFullscreen NEW Boolean True , if the Mini App is currently being displayed in fullscreen mode. isOrientationLocked NEW Boolean True , if the Mini App’s orientation is currently locked. False , if orientation changes freely based on the device’s rotation. safeAreaInset NEW SafeAreaInset An object representing the device's safe area insets, accounting for system UI elements like notches or navigation bars. contentSafeAreaInset NEW ContentSafeAreaInset An object representing the safe area for displaying content within the app, free from overlapping Telegram UI elements. BackButton BackButton An object for controlling the back button which can be displayed in the header of the Mini App in the Telegram interface. MainButton BottomButton An object for controlling the main button, which is displayed at the bottom of the Mini App in the Telegram interface. SecondaryButton BottomButton An object for controlling the secondary button, which is displayed at the bottom of the Mini App in the Telegram interface. SettingsButton SettingsButton An object for controlling the Settings item in the context menu of the Mini App in the Telegram interface. HapticFeedback HapticFeedback An object for controlling haptic feedback. CloudStorage CloudStorage An object for controlling cloud storage. BiometricManager BiometricManager An object for controlling biometrics on the device. Accelerometer NEW Accelerometer An object for accessing accelerometer data on the device. DeviceOrientation NEW DeviceOrientation An object for accessing device orientation data on the device. Gyroscope NEW Gyroscope An object for accessing gyroscope data on the device. LocationManager NEW LocationManager An object for controlling location on the device. isVersionAtLeast(version) Function Returns true if the user's app supports a version of the Bot API that is equal to or higher than the version passed as the parameter. setHeaderColor(color) Function Bot API 6.1+ A method that sets the app header color in the #RRGGBB format. You can also use keywords bg_color and secondary_bg_color . Up to Bot API 6.9 You can only pass Telegram.WebApp.themeParams.bg_color or Telegram.WebApp.themeParams.secondary_bg_color as a color or bg_color , secondary_bg_color keywords. setBackgroundColor(color) Function Bot API 6.1+ A method that sets the app background color in the #RRGGBB format. You can also use keywords bg_color and secondary_bg_color . setBottomBarColor(color) Function Bot API 7.10+ A method that sets the app's bottom bar color in the #RRGGBB format. You can also use the keywords bg_color , secondary_bg_color , and bottom_bar_bg_color . This color is also applied to the navigation bar on Android. enableClosingConfirmation() Function Bot API 6.2+ A method that enables a confirmation dialog while the user is trying to close the Mini App. disableClosingConfirmation() Function Bot API 6.2+ A method that disables the confirmation dialog while the user is trying to close the Mini App. enableVerticalSwipes() Function Bot API 7.7+ A method that enables vertical swipes to close or minimize the Mini App. For user convenience, it is recommended to always enable swipes unless they conflict with the Mini App's own gestures. disableVerticalSwipes() Function Bot API 7.7+ A method that disables vertical swipes to close or minimize the Mini App. This method is useful if your Mini App uses swipe gestures that may conflict with the gestures for minimizing and closing the app. requestFullscreen() NEW Function Bot API 8.0+ A method that requests opening the Mini App in fullscreen mode. Although the header is transparent in fullscreen mode, it is recommended that the Mini App sets the header color using the setHeaderColor method. This color helps determine a contrasting color for the status bar and other UI controls. exitFullscreen() NEW Function Bot API 8.0+ A method that requests exiting fullscreen mode. lockOrientation() NEW Function Bot API 8.0+ A method that locks the Mini App’s orientation to its current mode (either portrait or landscape). Once locked, the orientation remains fixed, regardless of device rotation. This is useful if a stable orientation is needed during specific interactions. unlockOrientation() NEW Function Bot API 8.0+ A method that unlocks the Mini App’s orientation, allowing it to follow the device's rotation freely. Use this to restore automatic orientation adjustments based on the device orientation. addToHomeScreen() NEW Function Bot API 8.0+ A method that prompts the user to add the Mini App to the home screen. After successfully adding the icon, the homeScreenAdded event will be triggered if supported by the device. Note that if the device cannot determine the installation status, the event may not be received even if the icon has been added. checkHomeScreenStatus([callback]) NEW Function Bot API 8.0+ A method that checks if adding to the home screen is supported and if the Mini App has already been added. If an optional callback parameter is provided, the callback function will be called with a single argument status , which is a string indicating the home screen status. Possible values for status are: - unsupported – the feature is not supported, and it is not possible to add the icon to the home screen, - unknown – the feature is supported, and the icon can be added, but it is not possible to determine if the icon has already been added, - added – the icon has already been added to the home screen, - missed – the icon has not been added to the home screen. onEvent(eventType, eventHandler) Function A method that sets the app event handler. Check the list of available events . offEvent(eventType, eventHandler) Function A method that deletes a previously set event handler. sendData(data) Function A method used to send data to the bot. When this method is called, a service message is sent to the bot containing the data data of the length up to 4096 bytes, and the Mini App is closed. See the field web_app_data in the class Message . This method is only available for Mini Apps launched via a Keyboard button . switchInlineQuery(query[, choose_chat_types]) Function Bot API 6.7+ A method that inserts the bot's username and the specified inline query in the current chat's input field. Query may be empty, in which case only the bot's username will be inserted. If an optional choose_chat_types parameter was passed, the client prompts the user to choose a specific chat, then opens that chat and inserts the bot's username and the specified inline query in the input field. You can specify which types of chats the user will be able to choose from. It can be one or more of the following types: users , bots , groups , channels . openLink(url[, options]) Function A method that opens a link in an external browser. The Mini App will not be closed. Bot API 6.4+ If the optional options parameter is passed with the field try_instant_view=true , the link will be opened in Instant View mode if possible. Note that this method can be called only in response to user interaction with the Mini App interface (e.g. a click inside the Mini App or on the main button) openTelegramLink(url) Function A method that opens a telegram link inside the Telegram app. The Mini App will not be closed after this method is called. Up to Bot API 7.0 The Mini App will be closed after this method is called. openInvoice(url[, callback]) Function Bot API 6.1+ A method that opens an invoice using the link url . The Mini App will receive the event invoiceClosed when the invoice is closed. If an optional callback parameter was passed, the callback function will be called and the invoice status will be passed as the first argument. shareToStory(media_url[, params]) Function Bot API 7.8+ A method that opens the native story editor with the media specified in the media_url parameter as an HTTPS URL. An optional params argument of the type StoryShareParams describes additional sharing settings. shareMessage(msg_id[, callback]) NEW Function Bot API 8.0+ A method that opens a dialog allowing the user to share a message provided by the bot. If an optional callback parameter is provided, the callback function will be called with a boolean as the first argument, indicating whether the message was successfully sent. The message id passed to this method must belong to a PreparedInlineMessage previously obtained via the Bot API method savePreparedInlineMessage . setEmojiStatus(custom_emoji_id[, params, callback]) Function Bot API 8.0+ A method that opens a dialog allowing the user to set the specified custom emoji as their status. An optional params argument of type EmojiStatusParams specifies additional settings, such as duration. If an optional callback parameter is provided, the callback function will be called with a boolean as the first argument, indicating whether the status was set. Note: this method opens a native dialog and cannot be used to set the emoji status without manual user interaction. For fully programmatic changes, you should instead use the Bot API method setUserEmojiStatus after obtaining authorization to do so via the Mini App method requestEmojiStatusAccess. requestEmojiStatusAccess([callback]) NEW Function Bot API 8.0+ A method that shows a native popup requesting permission for the bot to manage user's emoji status. If an optional callback parameter was passed, the callback function will be called when the popup is closed and the first argument will be a boolean indicating whether the user granted this access. downloadFile(params[, callback]) NEW Function Bot API 8.0+ A method that displays a native popup prompting the user to download a file specified by the params argument of type DownloadFileParams . If an optional callback parameter is provided, the callback function will be called when the popup is closed, with the first argument as a boolean indicating whether the user accepted the download request. showPopup(params[, callback]) Function Bot API 6.2+ A method that shows a native popup described by the params argument of the type PopupParams . The Mini App will receive the event popupClosed when the popup is closed. If an optional callback parameter was passed, the callback function will be called and the field id of the pressed button will be passed as the first argument. showAlert(message[, callback]) Function Bot API 6.2+ A method that shows message in a simple alert with a 'Close' button. If an optional callback parameter was passed, the callback function will be called when the popup is closed. showConfirm(message[, callback]) Function Bot API 6.2+ A method that shows message in a simple confirmation window with 'OK' and 'Cancel' buttons. If an optional callback parameter was passed, the callback function will be called when the popup is closed and the first argument will be a boolean indicating whether the user pressed the 'OK' button. showScanQrPopup(params[, callback]) Function Bot API 6.4+ A method that shows a native popup for scanning a QR code described by the params argument of the type ScanQrPopupParams . The Mini App will receive the event qrTextReceived every time the scanner catches a code with text data. If an optional callback parameter was passed, the callback function will be called and the text from the QR code will be passed as the first argument. Returning true inside this callback function causes the popup to be closed. Starting from Bot API 7.7 , the Mini App will receive the scanQrPopupClosed event if the user closes the native popup for scanning a QR code. closeScanQrPopup() Function Bot API 6.4+ A method that closes the native popup for scanning a QR code opened with the showScanQrPopup method. Run it if you received valid data in the event qrTextReceived . readTextFromClipboard([callback]) Function Bot API 6.4+ A method that requests text from the clipboard. The Mini App will receive the event clipboardTextReceived . If an optional callback parameter was passed, the callback function will be called and the text from the clipboard will be passed as the first argument. Note: this method can be called only for Mini Apps launched from the attachment menu and only in response to a user interaction with the Mini App interface (e.g. a click inside the Mini App or on the main button). requestWriteAccess([callback]) Function Bot API 6.9+ A method that shows a native popup requesting permission for the bot to send messages to the user. If an optional callback parameter was passed, the callback function will be called when the popup is closed and the first argument will be a boolean indicating whether the user granted this access. requestContact([callback]) Function Bot API 6.9+ A method that shows a native popup prompting the user for their phone number. If an optional callback parameter was passed, the callback function will be called when the popup is closed and the first argument will be a boolean indicating whether the user shared its phone number. ready() Function A method that informs the Telegram app that the Mini App is ready to be displayed. It is recommended to call this method as early as possible, as soon as all essential interface elements are loaded. Once this method is called, the loading placeholder is hidden and the Mini App is shown. If the method is not called, the placeholder will be hidden only when the page is fully loaded. expand() Function A method that expands the Mini App to the maximum available height. To find out if the Mini App is expanded to the maximum height, refer to the value of the Telegram.WebApp.isExpanded parameter close() Function A method that closes the Mini App. ThemeParams Mini Apps can adjust the appearance of the interface to match the Telegram user's app in real time. This object contains the user's current theme settings: Field Type Description bg_color String Optional . Background color in the #RRGGBB format. Also available as the CSS variable var(--tg-theme-bg-color) . text_color String Optional . Main text color in the #RRGGBB format. Also available as the CSS variable var(--tg-theme-text-color) . hint_color String Optional . Hint text color in the #RRGGBB format. Also available as the CSS variable var(--tg-theme-hint-color) . link_color String Optional . Link color in the #RRGGBB format. Also available as the CSS variable var(--tg-theme-link-color) . button_color String Optional . Button color in the #RRGGBB format. Also available as the CSS variable var(--tg-theme-button-color) . button_text_color String Optional . Button text color in the #RRGGBB format. Also available as the CSS variable var(--tg-theme-button-text-color) . secondary_bg_color String Optional . Bot API 6.1+ Secondary background color in the #RRGGBB format. Also available as the CSS variable var(--tg-theme-secondary-bg-color) . header_bg_color String Optional . Bot API 7.0+ Header background color in the #RRGGBB format. Also available as the CSS variable var(--tg-theme-header-bg-color) . bottom_bar_bg_color String Optional . Bot API 7.10+ Bottom background color in the #RRGGBB format. Also available as the CSS variable var(--tg-theme-bottom-bar-bg-color) . accent_text_color String Optional . Bot API 7.0+ Accent text color in the #RRGGBB format. Also available as the CSS variable var(--tg-theme-accent-text-color) . section_bg_color String Optional . Bot API 7.0+ Background color for the section in the #RRGGBB format. It is recommended to use this in conjunction with secondary_bg_color . Also available as the CSS variable var(--tg-theme-section-bg-color) . section_header_text_color String Optional . Bot API 7.0+ Header text color for the section in the #RRGGBB format. Also available as the CSS variable var(--tg-theme-section-header-text-color) . section_separator_color String Optional . Bot API 7.6+ Section separator color in the #RRGGBB format. Also available as the CSS variable var(--tg-theme-section-separator-color) . subtitle_text_color String Optional . Bot API 7.0+ Subtitle text color in the #RRGGBB format. Also available as the CSS variable var(--tg-theme-subtitle-text-color) . destructive_text_color String Optional . Bot API 7.0+ Text color for destructive actions in the #RRGGBB format. Also available as the CSS variable var(--tg-theme-destructive-text-color) . StoryShareParams This object describes additional sharing settings for the native story editor. Field Type Description text String Optional . The caption to be added to the media, 0-200 characters for regular users and 0-2048 characters for premium subscribers. widget_link StoryWidgetLink Optional . An object that describes a widget link to be included in the story. Note that only premium subscribers can post stories with links. StoryWidgetLink This object describes a widget link to be included in the story. Field Type Description url String The URL to be included in the story. name String Optional . The name to be displayed for the widget link, 0-48 characters. ScanQrPopupParams This object describes the native popup for scanning QR codes. Field Type Description text String Optional . The text to be displayed under the 'Scan QR' heading, 0-64 characters. PopupParams This object describes the native popup. Field Type Description title String Optional . The text to be displayed in the popup title, 0-64 characters. message String The message to be displayed in the body of the popup, 1-256 characters. buttons Array of PopupButton Optional . List of buttons to be displayed in the popup, 1-3 buttons. Set to [{“type”:“close”}] by default. PopupButton This object describes the native popup button. Field Type Description id String Optional . Identifier of the button, 0-64 characters. Set to empty string by default. If the button is pressed, its id is returned in the callback and the popupClosed event. type String Optional . Type of the button. Set to default by default. Can be one of these values: - default , a button with the default style, - ok , a button with the localized text “OK”, - close , a button with the localized text “Close”, - cancel , a button with the localized text “Cancel”, - destructive , a button with a style that indicates a destructive action (e.g. “Remove”, “Delete”, etc.). text String Optional . The text to be displayed on the button, 0-64 characters. Required if type is default or destructive . Irrelevant for other types. EmojiStatusParams This object describes additional settings for setting an emoji status. Field Type Description duration Integer Optional . The duration for which the status will remain set, in seconds. DownloadFileParams This object describes the parameters for the file download request. Note: To ensure consistent file download behavior across platforms, it is recommended to include the HTTP header Content-Disposition: attachment; filename=\"<file_name>\" in the server response. This header helps prompt the download action and suggests a file name for the downloaded file, especially on web platforms where forced downloads cannot always be guaranteed. Field Type Description url String The HTTPS URL of the file to be downloaded. file_name String The suggested name for the downloaded file. SafeAreaInset This object represents the system-defined safe area insets, providing padding values to ensure content remains within visible boundaries, avoiding overlap with system UI elements like notches or navigation bars. Field Type Description top Integer The top inset in pixels, representing the space to avoid at the top of the screen. Also available as the CSS variable var(--tg-safe-area-inset-top) . bottom Integer The bottom inset in pixels, representing the space to avoid at the bottom of the screen. Also available as the CSS variable var(--tg-safe-area-inset-bottom) . left Integer The left inset in pixels, representing the space to avoid on the left side of the screen. Also available as the CSS variable var(--tg-safe-area-inset-left) . right Integer The right inset in pixels, representing the space to avoid on the right side of the screen. Also available as the CSS variable var(--tg-safe-area-inset-right) . ContentSafeAreaInset This object represents the content-defined safe area insets, providing padding values to ensure content remains within visible boundaries, avoiding overlap with Telegram UI elements. Field Type Description top Integer The top inset in pixels, representing the space to avoid at the top of the content area. Also available as the CSS variable var(--tg-content-safe-area-inset-top) . bottom Integer The bottom inset in pixels, representing the space to avoid at the bottom of the content area. Also available as the CSS variable var(--tg-content-safe-area-inset-bottom) . left Integer The left inset in pixels, representing the space to avoid on the left side of the content area. Also available as the CSS variable var(--tg-content-safe-area-inset-left) . right Integer The right inset in pixels, representing the space to avoid on the right side of the content area. Also available as the CSS variable var(--tg-content-safe-area-inset-right) . BackButton This object controls the back button, which can be displayed in the header of the Mini App in the Telegram interface. Field Type Description isVisible Boolean Shows whether the button is visible. Set to false by default. onClick(callback) Function Bot API 6.1+ A method that sets the button press event handler. An alias for Telegram.WebApp.onEvent('backButtonClicked', callback) offClick(callback) Function Bot API 6.1+ A method that removes the button press event handler. An alias for Telegram.WebApp.offEvent('backButtonClicked', callback) show() Function Bot API 6.1+ A method to make the button active and visible. hide() Function Bot API 6.1+ A method to hide the button. All these methods return the BackButton object so they can be chained. BottomButton This object controls the button that is displayed at the bottom of the Mini App in the Telegram interface. Field Type Description type String Readonly. Type of the button. It can be either main for the main button or secondary for the secondary button. text String Current button text. Set to Continue for the main button and Cancel for the secondary button by default. color String Current button color. Set to themeParams.button_color for the main button and themeParams.bottom_bar_bg_color for the secondary button by default. textColor String Current button text color. Set to themeParams.button_text_color for the main button and themeParams.button_color for the secondary button by default. isVisible Boolean Shows whether the button is visible. Set to false by default. isActive Boolean Shows whether the button is active. Set to true by default. hasShineEffect Boolean Bot API 7.10+ Shows whether the button has a shine effect. Set to false by default. position String Bot API 7.10+ Position of the secondary button. Not defined for the main button. It applies only if both the main and secondary buttons are visible. Set to left by default. Supported values: - left , displayed to the left of the main button, - right , displayed to the right of the main button, - top , displayed above the main button, - bottom , displayed below the main button. isProgressVisible Boolean Readonly. Shows whether the button is displaying a loading indicator. setText(text) Function A method to set the button text. onClick(callback) Function A method that sets the button's press event handler. An alias for Telegram.WebApp.onEvent('mainButtonClicked', callback) offClick(callback) Function A method that removes the button's press event handler. An alias for Telegram.WebApp.offEvent('mainButtonClicked', callback) show() Function A method to make the button visible. Note that opening the Mini App from the attachment menu hides the main button until the user interacts with the Mini App interface. hide() Function A method to hide the button. enable() Function A method to enable the button. disable() Function A method to disable the button. showProgress(leaveActive) Function A method to show a loading indicator on the button. It is recommended to display loading progress if the action tied to the button may take a long time. By default, the button is disabled while the action is in progress. If the parameter leaveActive=true is passed, the button remains enabled. hideProgress() Function A method to hide the loading indicator. setParams(params) Function A method to set the button parameters. The params parameter is an object containing one or several fields that need to be changed: text - button text; color - button color; text_color - button text color; has_shine_effect - Bot API 7.10+ enable shine effect; position - position of the secondary button; is_active - enable the button; is_visible - show the button. All these methods return the BottomButton object so they can be chained. SettingsButton This object controls the Settings item in the context menu of the Mini App in the Telegram interface. Field Type Description isVisible Boolean Shows whether the context menu item is visible. Set to false by default. onClick(callback) Function Bot API 7.0+ A method that sets the press event handler for the Settings item in the context menu. An alias for Telegram.WebApp.onEvent('settingsButtonClicked', callback) offClick(callback) Function Bot API 7.0+ A method that removes the press event handler from the Settings item in the context menu. An alias for Telegram.WebApp.offEvent('settingsButtonClicked', callback) show() Function Bot API 7.0+ A method to make the Settings item in the context menu visible. hide() Function Bot API 7.0+ A method to hide the Settings item in the context menu. All these methods return the SettingsButton object so they can be chained. HapticFeedback This object controls haptic feedback. Field Type Description impactOccurred(style) Function Bot API 6.1+ A method tells that an impact occurred. The Telegram app may play the appropriate haptics based on style value passed. Style can be one of these values: - light , indicates a collision between small or lightweight UI objects, - medium , indicates a collision between medium-sized or medium-weight UI objects, - heavy , indicates a collision between large or heavyweight UI objects, - rigid , indicates a collision between hard or inflexible UI objects, - soft , indicates a collision between soft or flexible UI objects. notificationOccurred(type) Function Bot API 6.1+ A method tells that a task or action has succeeded, failed, or produced a warning. The Telegram app may play the appropriate haptics based on type value passed. Type can be one of these values: - error , indicates that a task or action has failed, - success , indicates that a task or action has completed successfully, - warning , indicates that a task or action produced a warning. selectionChanged() Function Bot API 6.1+ A method tells that the user has changed a selection. The Telegram app may play the appropriate haptics. Do not use this feedback when the user makes or confirms a selection; use it only when the selection changes. All these methods return the HapticFeedback object so they can be chained. CloudStorage This object controls the cloud storage. Each bot can store up to 1024 items per user in the cloud storage. Field Type Description setItem(key, value[, callback]) Function Bot API 6.9+ A method that stores a value in the cloud storage using the specified key. The key should contain 1-128 characters, only A-Z , a-z , 0-9 , _ and - are allowed. The value should contain 0-4096 characters. You can store up to 1024 keys in the cloud storage. If an optional callback parameter was passed, the callback function will be called. In case of an error, the first argument will contain the error. In case of success, the first argument will be null and the second argument will be a boolean indicating whether the value was stored. getItem(key, callback) Function Bot API 6.9+ A method that receives a value from the cloud storage using the specified key. The key should contain 1-128 characters, only A-Z , a-z , 0-9 , _ and - are allowed. In case of an error, the callback function will be called and the first argument will contain the error. In case of success, the first argument will be null and the value will be passed as the second argument. getItems(keys, callback) Function Bot API 6.9+ A method that receives values from the cloud storage using the specified keys. The keys should contain 1-128 characters, only A-Z , a-z , 0-9 , _ and - are allowed. In case of an error, the callback function will be called and the first argument will contain the error. In case of success, the first argument will be null and the values will be passed as the second argument. removeItem(key[, callback]) Function Bot API 6.9+ A method that removes a value from the cloud storage using the specified key. The key should contain 1-128 characters, only A-Z , a-z , 0-9 , _ and - are allowed. If an optional callback parameter was passed, the callback function will be called. In case of an error, the first argument will contain the error. In case of success, the first argument will be null and the second argument will be a boolean indicating whether the value was removed. removeItems(keys[, callback]) Function Bot API 6.9+ A method that removes values from the cloud storage using the specified keys. The keys should contain 1-128 characters, only A-Z , a-z , 0-9 , _ and - are allowed. If an optional callback parameter was passed, the callback function will be called. In case of an error, the first argument will contain the error. In case of success, the first argument will be null and the second argument will be a boolean indicating whether the values were removed. getKeys(callback) Function Bot API 6.9+ A method that receives the list of all keys stored in the cloud storage. In case of an error, the callback function will be called and the first argument will contain the error. In case of success, the first argument will be null and the list of keys will be passed as the second argument. All these methods return the CloudStorage object, so they can be chained. BiometricManager This object controls biometrics on the device. Before the first use of this object, it needs to be initialized using the init method. Field Type Description isInited Boolean Shows whether biometrics object is initialized. isBiometricAvailable Boolean Shows whether biometrics is available on the current device. biometricType String The type of biometrics currently available on the device. Can be one of these values: - finger , fingerprint-based biometrics, - face , face-based biometrics, - unknown , biometrics of an unknown type. isAccessRequested Boolean Shows whether permission to use biometrics has been requested. isAccessGranted Boolean Shows whether permission to use biometrics has been granted. isBiometricTokenSaved Boolean Shows whether the token is saved in secure storage on the device. deviceId String A unique device identifier that can be used to match the token to the device. init([callback]) Function Bot API 7.2+ A method that initializes the BiometricManager object. It should be called before the object's first use. If an optional callback parameter was passed, the callback function will be called when the object is initialized. requestAccess(params[, callback]) Function Bot API 7.2+ A method that requests permission to use biometrics according to the params argument of type BiometricRequestAccessParams . If an optional callback parameter was passed, the callback function will be called and the first argument will be a boolean indicating whether the user granted access. authenticate(params[, callback]) Function Bot API 7.2+ A method that authenticates the user using biometrics according to the params argument of type BiometricAuthenticateParams . If an optional callback parameter was passed, the callback function will be called and the first argument will be a boolean indicating whether the user authenticated successfully. If so, the second argument will be a biometric token. updateBiometricToken(token, [callback]) Function Bot API 7.2+ A method that updates the biometric token in secure storage on the device. To remove the token, pass an empty string. If an optional callback parameter was passed, the callback function will be called and the first argument will be a boolean indicating whether the token was updated. openSettings() Function Bot API 7.2+ A method that opens the biometric access settings for bots. Useful when you need to request biometrics access to users who haven't granted it yet. Note that this method can be called only in response to user interaction with the Mini App interface (e.g. a click inside the Mini App or on the main button) All these methods return the BiometricManager object so they can be chained. BiometricRequestAccessParams This object describes the native popup for requesting permission to use biometrics. Field Type Description reason String Optional . The text to be displayed to a user in the popup describing why the bot needs access to biometrics, 0-128 characters. BiometricAuthenticateParams This object describes the native popup for authenticating the user using biometrics. Field Type Description reason String Optional . The text to be displayed to a user in the popup describing why you are asking them to authenticate and what action you will be taking based on that authentication, 0-128 characters. Accelerometer This object provides access to accelerometer data on the device. Field Type Description isStarted Boolean Indicates whether accelerometer tracking is currently active. x Float The current acceleration in the X-axis, measured in m/s². y Float The current acceleration in the Y-axis, measured in m/s². z Float The current acceleration in the Z-axis, measured in m/s². start(params[, callback]) Function Bot API 8.0+ Starts tracking accelerometer data using params of type AccelerometerStartParams . If an optional callback parameter is provided, the callback function will be called with a boolean indicating whether tracking was successfully started. stop([callback]) Function Bot API 8.0+ Stops tracking accelerometer data. If an optional callback parameter is provided, the callback function will be called with a boolean indicating whether tracking was successfully stopped. All these methods return the Accelerometer object so they can be chained. AccelerometerStartParams This object defines the parameters for starting accelerometer tracking. Field Type Description refresh_rate Integer Optional. The refresh rate in milliseconds, with acceptable values ranging from 20 to 1000. Set to 1000 by default. Note that refresh_rate may not be supported on all platforms, so the actual tracking frequency may differ from the specified value. DeviceOrientation This object provides access to orientation data on the device. Field Type Description isStarted Boolean Indicates whether device orientation tracking is currently active. absolute Boolean A boolean that indicates whether or not the device is providing orientation data in absolute values. alpha Float The rotation around the Z-axis, measured in radians. beta Float The rotation around the X-axis, measured in radians. gamma Float The rotation around the Y-axis, measured in radians. start(params[, callback]) Function Bot API 8.0+ Starts tracking device orientation data using params of type DeviceOrientationStartParams . If an optional callback parameter is provided, the callback function will be called with a boolean indicating whether tracking was successfully started. stop([callback]) Function Bot API 8.0+ Stops tracking device orientation data. If an optional callback parameter is provided, the callback function will be called with a boolean indicating whether tracking was successfully stopped. All these methods return the DeviceOrientation object so they can be chained. DeviceOrientationStartParams This object defines the parameters for starting device orientation tracking. Field Type Description refresh_rate Integer Optional. The refresh rate in milliseconds, with acceptable values ranging from 20 to 1000. Set to 1000 by default. Note that refresh_rate may not be supported on all platforms, so the actual tracking frequency may differ from the specified value. need_absolute Boolean Optional. Pass true to receive absolute orientation data, allowing you to determine the device's attitude relative to magnetic north. Use this option if implementing features like a compass in your app. If relative data is sufficient, pass false . Set to false by default. Note: Keep in mind that some devices may not support absolute orientation data. In such cases, you will receive relative data even if need_absolute=true is passed. Check the DeviceOrientation.absolute parameter to determine whether the data provided is absolute or relative. Gyroscope This object provides access to gyroscope data on the device. Field Type Description isStarted Boolean Indicates whether gyroscope tracking is currently active. x Float The current rotation rate around the X-axis, measured in rad/s. y Float The current rotation rate around the Y-axis, measured in rad/s. z Float The current rotation rate around the Z-axis, measured in rad/s. start(params[, callback]) Function Bot API 8.0+ Starts tracking gyroscope data using params of type GyroscopeStartParams . If an optional callback parameter is provided, the callback function will be called with a boolean indicating whether tracking was successfully started. stop([callback]) Function Bot API 8.0+ Stops tracking gyroscope data. If an optional callback parameter is provided, the callback function will be called with a boolean indicating whether tracking was successfully stopped. All these methods return the Gyroscope object so they can be chained. GyroscopeStartParams This object defines the parameters for starting gyroscope tracking. Field Type Description refresh_rate Integer Optional. The refresh rate in milliseconds, with acceptable values ranging from 20 to 1000. Set to 1000 by default. Note that refresh_rate may not be supported on all platforms, so the actual tracking frequency may differ from the specified value. LocationManager This object controls location access on the device. Before the first use of this object, it needs to be initialized using the init method. Field Type Description isInited Boolean Shows whether the LocationManager object has been initialized. isLocationAvailable Boolean Shows whether location services are available on the current device. isAccessRequested Boolean Shows whether permission to use location has been requested. isAccessGranted Boolean Shows whether permission to use location has been granted. init([callback]) Function Bot API 8.0+ A method that initializes the LocationManager object. It should be called before the object's first use. If an optional callback parameter is provided, the callback function will be called when the object is initialized. getLocation(callback) Function Bot API 8.0+ A method that requests location data. The callback function will be called with null as the first argument if access to location was not granted, or an object of type LocationData as the first argument if access was successful. openSettings() Function Bot API 8.0+ A method that opens the location access settings for bots. Useful when you need to request location access from users who haven't granted it yet. Note that this method can be called only in response to user interaction with the Mini App interface (e.g., a click inside the Mini App or on the main button). All these methods return the LocationManager object so they can be chained. LocationData This object contains data about the current location. Field Type Description latitude Float Latitude in degrees. longitude Float Longitude in degrees. altitude Float Altitude above sea level in meters. null if altitude data is not available on the device. course Float The direction the device is moving in degrees (0 = North, 90 = East, 180 = South, 270 = West). null if course data is not available on the device. speed Float The speed of the device in m/s. null if speed data is not available on the device. horizontal_accuracy Float Accuracy of the latitude and longitude values in meters. null if horizontal accuracy data is not available on the device. vertical_accuracy Float Accuracy of the altitude value in meters. null if vertical accuracy data is not available on the device. course_accuracy Float Accuracy of the course value in degrees. null if course accuracy data is not available on the device. speed_accuracy Float Accuracy of the speed value in m/s. null if speed accuracy data is not available on the device. WebAppInitData This object contains data that is transferred to the Mini App when it is opened. It is empty if the Mini App was launched from a keyboard button or from inline mode . Field Type Description query_id String Optional. A unique identifier for the Mini App session, required for sending messages via the answerWebAppQuery method. user WebAppUser Optional. An object containing data about the current user. receiver WebAppUser Optional. An object containing data about the chat partner of the current user in the chat where the bot was launched via the attachment menu. Returned only for private chats and only for Mini Apps launched via the attachment menu. chat WebAppChat Optional. An object containing data about the chat where the bot was launched via the attachment menu. Returned for supergroups, channels and group chats – only for Mini Apps launched via the attachment menu. chat_type String Optional. Type of the chat from which the Mini App was opened. Can be either “sender” for a private chat with the user opening the link, “private”, “group”, “supergroup”, or “channel”. Returned only for Mini Apps launched from direct links. chat_instance String Optional. Global identifier, uniquely corresponding to the chat from which the Mini App was opened. Returned only for Mini Apps launched from a direct link. start_param String Optional. The value of the startattach parameter, passed via link . Only returned for Mini Apps when launched from the attachment menu via link. The value of the start_param parameter will also be passed in the GET-parameter tgWebAppStartParam , so the Mini App can load the correct interface right away. can_send_after Integer Optional. Time in seconds, after which a message can be sent via the answerWebAppQuery method. auth_date Integer Unix time when the form was opened. hash String A hash of all passed parameters, which the bot server can use to check their validity . signature NEW String A signature of all passed parameters (except hash ), which the third party can use to check their validity . WebAppUser This object contains the data of the Mini App user. Field Type Description id Integer A unique identifier for the user or bot. This number may have more than 32 significant bits and some programming languages may have difficulty/silent defects in interpreting it. It has at most 52 significant bits, so a 64-bit integer or a double-precision float type is safe for storing this identifier. is_bot Boolean Optional . True , if this user is a bot. Returns in the receiver field only. first_name String First name of the user or bot. last_name String Optional . Last name of the user or bot. username String Optional . Username of the user or bot. language_code String Optional . IETF language tag of the user's language. Returns in user field only. is_premium True Optional . True , if this user is a Telegram Premium user. added_to_attachment_menu True Optional . True , if this user added the bot to the attachment menu. allows_write_to_pm True Optional . True , if this user allowed the bot to message them. photo_url String Optional . URL of the user’s profile photo. The photo can be in .jpeg or .svg formats. WebAppChat This object represents a chat. Field Type Description id Integer Unique identifier for this chat. This number may have more than 32 significant bits and some programming languages may have difficulty/silent defects in interpreting it. But it has at most 52 significant bits, so a signed 64-bit integer or double-precision float type are safe for storing this identifier. type String Type of chat, can be either “group”, “supergroup” or “channel” title String Title of the chat username String Optional . Username of the chat photo_url String Optional . URL of the chat’s photo. The photo can be in .jpeg or .svg formats. Only returned for Mini Apps launched from the attachment menu. Validating data received via the Mini App To validate data received via the Mini App, one should send the data from the Telegram.WebApp.initData field to the bot's backend. The data is a query string, which is composed of a series of field-value pairs. You can verify the integrity of the data received by comparing the received hash parameter with the hexadecimal representation of the HMAC-SHA-256 signature of the data-check-string with the secret key, which is the HMAC-SHA-256 signature of the bot's token with the constant string WebAppData used as a key. Data-check-string is a chain of all received fields, sorted alphabetically, in the format key=<value> with a line feed character ('\\n', 0x0A) used as separator – e.g., 'auth_date=<auth_date>\\nquery_id=<query_id>\\nuser=<user>' . The full check might look like: data_check_string = ...\nsecret_key = HMAC_SHA256(<bot_token>, \"WebAppData\")\nif (hex(HMAC_SHA256(data_check_string, secret_key)) == hash) {\n  // data is from Telegram\n} To prevent the use of outdated data, you can additionally check the auth_date field, which contains a Unix timestamp of when it was received by the Mini App. Once validated, the data may be used on your server. Complex data types are represented as JSON-serialized objects. Validating data for Third-Party Use NEW If you need to share the data with a third party, they can validate the data without requiring access to your bot's token . Simply provide them with the data from the Telegram.WebApp.initData field and your bot_id . The integrity of the data can be verified by validating the received signature parameter, which is the base64url-encoded representation of the Ed25519 signature of the data-check-string . The verification is performed using the public key provided by Telegram. Data-check-string is constructed as follows: 1. Prepend the bot_id , followed by : and the constant string WebAppData . 2. Add a line feed character ( '\\n' , 0x0A). 3. Append all received fields (except hash and signature ), sorted alphabetically, in the format key=<value> . 4. Separate each key-value pair with a line feed character ( '\\n' , 0x0A). Example: '12345678:WebAppData\\nauth_date=<auth_date>\\nquery_id=<query_id>\\nuser=<user>' The verification process might look like this: data_check_string = ...\npublic_key = \"<Telegram_public_key>\"\nif (Ed25519_verify(public_key, data_check_string, signature)) {\n  // data is valid and originated from Telegram\n} Telegram provides the following Ed25519 public keys for signature verification: Test environment: 40055058a4ee38156a06562e52eece92a771bcd8346a8c4615cb7376eddf72ec (hex) Production: e7bf03a2fa4602af4580703d88dda5bb59f32ed8b02a56c187fe7d34caed242d (hex) To prevent the use of outdated data, the third party should additionally validate the auth_date field. This field contains a Unix timestamp indicating when the data was received by the Mini App. Once validated, the data may be used. Complex data types are represented as JSON-serialized objects. Events Available for Mini Apps The Mini App can receive events from the Telegram app, onto which a handler can be attached using the Telegram.WebApp.onEvent(eventType, eventHandler) method. Inside eventHandler the this object refers to Telegram.WebApp , the set of parameters sent to the handler depends on the event type. Below is a list of possible events: eventType Description activated NEW Bot API 8.0+ Occurs when the Mini App becomes active (e.g., opened from minimized state or selected among tabs). eventHandler receives no parameters. deactivated NEW Bot API 8.0+ Occurs when the Mini App becomes inactive (e.g., minimized or moved to an inactive tab). eventHandler receives no parameters. themeChanged Occurs whenever theme settings are changed in the user's Telegram app (including switching to night mode). eventHandler receives no parameters, new theme settings and color scheme can be received via this.themeParams and this.colorScheme respectively. viewportChanged Occurs when the visible section of the Mini App is changed. eventHandler receives an object with the single field isStateStable . If isStateStable is true, the resizing of the Mini App is finished. If it is false, the resizing is ongoing (the user is expanding or collapsing the Mini App or an animated object is playing). The current value of the visible section’s height is available in this.viewportHeight . safeAreaChanged NEW Bot API 8.0+ Occurs when the device's safe area insets change (e.g., due to orientation change or screen adjustments). eventHandler receives no parameters. The current inset values can be accessed via this.safeAreaInset . contentSafeAreaChanged NEW Bot API 8.0+ Occurs when the safe area for content changes (e.g., due to orientation change or screen adjustments). eventHandler receives no parameters. The current inset values can be accessed via this.contentSafeAreaInset . mainButtonClicked Occurs when the main button is pressed. eventHandler receives no parameters. secondaryButtonClicked Bot API 7.10+ Occurs when the secondary button is pressed. eventHandler receives no parameters. backButtonClicked Bot API 6.1+ Occurrs when the back button is pressed. eventHandler receives no parameters. settingsButtonClicked Bot API 6.1+ Occurrs when the Settings item in context menu is pressed. eventHandler receives no parameters. invoiceClosed Bot API 6.1+ Occurrs when the opened invoice is closed. eventHandler receives an object with the two fields: url – invoice link provided and status – one of the invoice statuses: - paid – invoice was paid successfully, - cancelled – user closed this invoice without paying, - failed – user tried to pay, but the payment was failed, - pending – the payment is still processing. The bot will receive a service message about a successful payment when the payment is successfully paid. popupClosed Bot API 6.2+ Occurrs when the opened popup is closed. eventHandler receives an object with the single field button_id – the value of the field id of the pressed button. If no buttons were pressed, the field button_id will be null . qrTextReceived Bot API 6.4+ Occurs when the QR code scanner catches a code with text data. eventHandler receives an object with the single field data containing text data from the QR code. scanQrPopupClosed Bot API 7.7+ Occurs when the QR code scanner popup is closed by the user. eventHandler receives no parameters. clipboardTextReceived Bot API 6.4+ Occurrs when the readTextFromClipboard method is called. eventHandler receives an object with the single field data containing text data from the clipboard. If the clipboard contains non-text data, the field data will be an empty string. If the Mini App has no access to the clipboard, the field data will be null . writeAccessRequested Bot API 6.9+ Occurs when the write permission was requested. eventHandler receives an object with the single field status containing one of the statuses: - allowed – user granted write permission to the bot, - cancelled – user declined this request. contactRequested Bot API 6.9+ Occurrs when the user's phone number was requested. eventHandler receives an object with the single field status containing one of the statuses: - sent – user shared their phone number with the bot, - cancelled – user declined this request. biometricManagerUpdated Bot API 7.2+ Occurs whenever BiometricManager object is changed. eventHandler receives no parameters. biometricAuthRequested Bot API 7.2+ Occurs whenever biometric authentication was requested. eventHandler receives an object with the field isAuthenticated containing a boolean indicating whether the user was authenticated successfully. If isAuthenticated is true, the field biometricToken will contain the biometric token stored in secure storage on the device. biometricTokenUpdated Bot API 7.2+ Occurs whenever the biometric token was updated. eventHandler receives an object with the single field isUpdated , containing a boolean indicating whether the token was updated. fullscreenChanged NEW Bot API 8.0+ Occurs whenever the Mini App enters or exits fullscreen mode. eventHandler receives no parameters. The current fullscreen state can be checked via this.isFullscreen . fullscreenFailed NEW Bot API 8.0+ Occurs if a request to enter fullscreen mode fails. eventHandler receives an object with the single field error , describing the reason for the failure. Possible values for error are: UNSUPPORTED – Fullscreen mode is not supported on this device or platform. ALREADY_FULLSCREEN – The Mini App is already in fullscreen mode. homeScreenAdded NEW Bot API 8.0+ Occurs when the Mini App is successfully added to the home screen. eventHandler receives no parameters. homeScreenChecked NEW Bot API 8.0+ Occurs after checking the home screen status. eventHandler receives an object with the field status , which is a string indicating the current home screen status. Possible values for status are: - unsupported – the feature is not supported, and it is not possible to add the icon to the home screen, - unknown – the feature is supported, and the icon can be added, but it is not possible to determine if the icon has already been added, - added – the icon has already been added to the home screen, - missed – the icon has not been added to the home screen. accelerometerStarted NEW Bot API 8.0+ Occurs when accelerometer tracking has started successfully. eventHandler receives no parameters. accelerometerStopped NEW Bot API 8.0+ Occurs when accelerometer tracking has stopped. eventHandler receives no parameters. accelerometerChanged NEW Bot API 8.0+ Occurs with the specified frequency after calling the start method, sending the current accelerometer data. eventHandler receives no parameters, the current acceleration values can be received via this.x , this.y and this.z respectively. accelerometerFailed NEW Bot API 8.0+ Occurs if a request to start accelerometer tracking fails. eventHandler receives an object with the single field error , describing the reason for the failure. Possible values for error are: UNSUPPORTED – Accelerometer tracking is not supported on this device or platform. deviceOrientationStarted NEW Bot API 8.0+ Occurs when device orientation tracking has started successfully. eventHandler receives no parameters. deviceOrientationStopped NEW Bot API 8.0+ Occurs when device orientation tracking has stopped. eventHandler receives no parameters. deviceOrientationChanged NEW Bot API 8.0+ Occurs with the specified frequency after calling the start method, sending the current orientation data. eventHandler receives no parameters, the current device orientation values can be received via this.alpha , this.beta and this.gamma respectively. deviceOrientationFailed NEW Bot API 8.0+ Occurs if a request to start device orientation tracking fails. eventHandler receives an object with the single field error , describing the reason for the failure. Possible values for error are: UNSUPPORTED – Device orientation tracking is not supported on this device or platform. gyroscopeStarted NEW Bot API 8.0+ Occurs when gyroscope tracking has started successfully. eventHandler receives no parameters. gyroscopeStopped NEW Bot API 8.0+ Occurs when gyroscope tracking has stopped. eventHandler receives no parameters. gyroscopeChanged NEW Bot API 8.0+ Occurs with the specified frequency after calling the start method, sending the current gyroscope data. eventHandler receives no parameters, the current rotation rates can be received via this.x , this.y and this.z respectively. gyroscopeFailed NEW Bot API 8.0+ Occurs if a request to start gyroscope tracking fails. eventHandler receives an object with the single field error , describing the reason for the failure. Possible values for error are: UNSUPPORTED – Gyroscope tracking is not supported on this device or platform. locationManagerUpdated NEW Bot API 8.0+ Occurs whenever LocationManager object is changed. eventHandler receives no parameters. locationRequested NEW Bot API 8.0+ Occurs when location data is requested. eventHandler receives an object with the single field locationData of type LocationData , containing the current location information. shareMessageSent NEW Bot API 8.0+ Occurs when the message is successfully shared by the user. eventHandler receives no parameters. shareMessageFailed NEW Bot API 8.0+ Occurs if sharing the message fails. eventHandler receives an object with the single field error , describing the reason for the failure. Possible values for error are: UNSUPPORTED – The feature is not supported by the client. MESSAGE_EXPIRED – The message could not be retrieved because it has expired. MESSAGE_SEND_FAILED – An error occurred while attempting to send the message. USER_DECLINED – The user closed the dialog without sharing the message. UNKNOWN_ERROR – An unknown error occurred. emojiStatusSet NEW Bot API 8.0+ Occurs when the emoji status is successfully set. eventHandler receives no parameters. emojiStatusFailed NEW Bot API 8.0+ Occurs if setting the emoji status fails. eventHandler receives an object with the single field error , describing the reason for the failure. Possible values for error are: UNSUPPORTED – The feature is not supported by the client. SUGGESTED_EMOJI_INVALID – One or more emoji identifiers are invalid. DURATION_INVALID – The specified duration is invalid. USER_DECLINED – The user closed the dialog without setting a status. SERVER_ERROR – A server error occurred when attempting to set the status. UNKNOWN_ERROR – An unknown error occurred. emojiStatusAccessRequested NEW Bot API 8.0+ Occurs when the write permission was requested. eventHandler receives an object with the single field status containing one of the statuses: - allowed – user granted emoji status permission to the bot, - cancelled – user declined this request. fileDownloadRequested NEW Bot API 8.0+ Occurs when the user responds to the file download request. eventHandler receives an object with the single field status containing one of the statuses: - downloading – the file download has started, - cancelled – user declined this request. Adding Bots to the Attachment Menu Attachment menu integration is currently only available for major advertisers on the Telegram Ad Platform . However, all bots can use it in the test server environment . Talk to Botfather on the test server to set up the integration . A special link is used to add bots to the attachment menu: https://t.me/botusername?startattach or https://t.me/botusername?startattach=command For example, open this attachment menu link for @DurgerKingBot , then use the menu in any private chat . Opening the link prompts the user to add the bot to their attachment menu. If the bot has already been added, the attachment menu will open in the current chat and redirect to the bot there (if the link is opened from a 1-on-1 chat). If a non-empty startattach parameter was included in the link, it will be passed to the Mini App in the start_param field and in the GET parameter tgWebAppStartParam . The following link formats are also supported: https://t.me/username?attach=botusername https://t.me/username?attach=botusername&startattach=command https://t.me/+1234567890?attach=botusername https://t.me/+1234567890?attach=botusername&startattach=command These links open the Mini App in the attachment menu in the chat with a specific user. If the bot wasn't already added to the attachment menu, the user will be prompted to do so. If a non-empty startattach parameter was included in the link, it will be passed to the Mini App in the start_param field and in the GET parameter tgWebAppStartParam . Bot API 6.1+ supports a new link format: https://t.me/botusername?startattach&choose=users+bots https://t.me/botusername?startattach=command&choose=groups+channels Opening such a link prompts the user to choose a specific chat and opens the attachment menu in that chat. If the bot wasn't already added to the attachment menu, the user will be prompted to do so. You can specify which types of chats the user will be able to choose  from. It can be one or more of the following types: users , bots , groups , channels separated by a + sign. If a non-empty startattach parameter was included in the link, it will be passed to the Mini App in the start_param field and in the GET parameter tgWebAppStartParam . Additional Data in User-Agent When the Mini App is running on Android, additional information is appended to the User-Agent string to provide more context about the app environment. This information includes the app version, device model, Android version, SDK version, and device performance class, formatted as follows: Telegram-Android/{app_version} ({manufacturer} {model}; Android {android_version}; SDK {sdk_version}; {performance_class}) where: {app_version} is the version of the Telegram app (e.g., 11.3.3 ), {manufacturer} {model} represents the device’s manufacturer and model (e.g., Google sdk_gphone64_arm64 ), {android_version} is the Android OS version running on the device (e.g., 14 ), {sdk_version} indicates the Android SDK version (e.g., 34 ), {performance_class} specifies the device performance class as LOW , AVERAGE , or HIGH , indicating the device's performance capacity. Example Mozilla/5.0 (Linux; Android 14; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.5672.136 Mobile Safari/537.36 Telegram-Android/11.3.3 (Google sdk_gphone64_arm64; Android 14; SDK 34; LOW) We recommend using this information to optimize your Mini App based on the device's capabilities. For instance, you can adjust animations and visual effects in games on low-performance devices to ensure a smooth experience for all users, regardless of device specifications. Testing Mini Apps Using bots in the test environment To log in to the test environment, use either of the following: iOS: tap 10 times on the Settings icon > Accounts > Login to another account > Test. Telegram Desktop: open ☰ Settings > Shift + Alt + Right click ‘Add Account’ and select ‘Test Server’. macOS: click the Settings icon 10 times to open the Debug Menu, ⌘ + click ‘Add Account’ and log in via phone number. The test environment is completely separate from the main environment, so you will need to create a new user account and a new bot with @BotFather. After receiving your bot token, you can send requests to the Bot API in this format: https://api.telegram.org/bot<token>/test/METHOD_NAME Note: When working with the test environment, you may use HTTP links without TLS to test your Mini App. Debug Mode for Mini Apps Use these tools to find app-specific issues in your Mini App: iOS In Telegram tap 10 times on the Settings icon and toggle on Allow Web View Inspection . Connect your phone to your computer using a USB cable. Open Safari on your Mac, then go to Develop > [Your Device Name] in the menu bar. Launch your Mini App on the iOS device – it will appear in the Develop menu under your device. Android Enable USB-Debugging on your device. In Telegram Settings, scroll all the way down, press and hold on the version number two times. Choose Enable WebView Debug in the Debug Settings. Connect your phone to your computer and open chrome://inspect/#devices in Chrome – you will see your Mini App there when you launch it on your phone. Telegram Desktop on Windows and Linux Download and launch the Beta Version of Telegram Desktop on Windows or Linux (not supported on Telegram Desktop for macOS yet). Go to Settings > Advanced > Experimental settings > Enable webview inspection . Right click in the WebView and choose Inspect . Telegram macOS Download and launch the Beta Version of Telegram macOS. Quickly click 5 times on the Settings icon to open the debug menu and enable “Debug Mini Apps”. Right click in the Mini App and choose Inspect Element . Telegram Telegram is a cloud-based mobile and desktop messaging app with a focus on security and speed. About FAQ Privacy Press Mobile Apps iPhone/iPad Android Mobile Web Desktop Apps PC/Mac/Linux macOS Web-browser Platform API Translations Instant View About Blog Press Moderation",
            "meta_description": "",
            "links": [
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/payments",
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/payments-stars"
            ],
            "timestamp": "2024-12-27T22:28:17.123901",
            "status_code": 200
        },
        {
            "url": "https://core.telegram.org/bots/api-changelog",
            "title": "Bot API changelog",
            "text_content": "Bot API changelog Twitter Home FAQ Apps API Protocol Schema Bot API changelog The Bot API is an HTTP-based interface created for developers keen on building bots for Telegram. To learn how to create and set up a bot, please consult our Introduction to Bots » You will find all changes to our Bot API on this page. Recent changes Subscribe to @BotNews to be the first to know about the latest updates and join the discussion in @BotTalk 2024 December 4, 2024 Bot API 8.1 Added the field nanostar_amount to the class StarTransaction . Added the class TransactionPartnerAffiliateProgram for transactions pertaining to incoming affiliate commissions. Added the class AffiliateInfo and the field affiliate to the class TransactionPartnerUser , allowing bots to identify the relevant affiliate in transactions with an affiliate commission. November 17, 2024 Bot API 8.0 Bot API 8.0 introduces 10 powerful new features for Mini Apps - including the ability to enter full-screen mode , launch from home screen shortcuts , offer subscription plans and more. Check out all the details in our dedicated blog and Mini App documentation . Star Subscriptions Bots now support paid subscriptions powered by Telegram Stars - monetizing their efforts with multiple tiers of content and features. Added the parameter subscription_period to the method createInvoiceLink to support the creation of links that are billed periodically. Added the parameter business_connection_id to the method createInvoiceLink to support the creation of invoice links on behalf of business accounts. Added the fields subscription_expiration_date , is_recurring and is_first_recurring to the class SuccessfulPayment . Added the method editUserStarSubscription . Added the field subscription_period to the class TransactionPartnerUser . Full-screen Mode Mini Apps are now able to become full-screen in both portrait and landscape mode - allowing them to host more games , play widescreen media and support immersive user experiences. Added the methods requestFullscreen and exitFullscreen to the class WebApp to toggle full-screen mode. Added the fields safeAreaInset and contentSafeAreaInset to the class WebApp , allowing Mini Apps to ensure that their content properly respects the device's safe area margins. Further added the fields isActive and isFullscreen to the class WebApp . Added the events activated , deactivated , safeAreaChanged , contentSafeAreaChanged , fullscreenChanged and fullscreenFailed for Mini Apps. Homescreen Shortcuts Mini Apps can now be accessed via direct shortcuts added to the home screen of mobile devices. Added the method addToHomeScreen to the class WebApp to create a shortcut for users to add to their home screens. Added the method checkHomeScreenStatus to the class WebApp to determine the status and support of the home screen shortcut for the Mini App on the current device. Added the events homeScreenAdded and homeScreenChecked for Mini Apps. Emoji Status Mini Apps can now prompt users to set their emoji status - or request access to later sync it automatically with in-game badges, third-party APIs and more. Added the method setUserEmojiStatus . The user must allow the bot to manage their emoji status. Added the method setEmojiStatus to the class WebApp to let users manually confirm a custom emoji as their new status via a native dialog. Added the method requestEmojiStatusAccess to the class WebApp for obtaining permission to later update a user's emoji status via the Bot API method setUserEmojiStatus . Added the events emojiStatusSet , emojiStatusFailed and emojiStatusAccessRequested for Mini Apps. Media Sharing and File Downloads Users can now share media directly from Mini Apps - sending referral codes , custom memes, artwork and more to any chat or posting them as a story . Added the class PreparedInlineMessage and the method savePreparedInlineMessage , allowing bots to suggest users to send a specific message from a Mini App via the method shareMessage . Added the method shareMessage to the class WebApp to share media from Mini Apps to Telegram chats. Added the method downloadFile to the class WebApp , introducing support for a native popup that prompts users to download files from the Mini App. Added the events shareMessageSent , shareMessageFailed and fileDownloadRequested for Mini Apps. Geolocation Access Mini Apps can now request geolocation access to users, allowing them to build virtually any location-based service, from games with dynamic points of interest to interactive maps for events. Added the field LocationManager to the class WebApp . Added the events locationManagerUpdated and locationRequested for Mini Apps. Device Motion Tracking Mini Apps can now track detailed device motion data , allowing them to implement better productivity tools, immersive VR experiences and more. Added the fields isOrientationLocked , Accelerometer , DeviceOrientation and Gyroscope to the class WebApp . Added the methods lockOrientation and unlockOrientation to the class WebApp to control the screen orientation. Added the events accelerometerStarted , accelerometerStopped , accelerometerChanged , accelerometerFailed , deviceOrientationStarted , deviceOrientationStopped , deviceOrientationChanged , deviceOrientationFailed , gyroscopeStarted , gyroscopeStopped , gyroscopeChanged , gyroscopeFailed for Mini Apps. Gifts Bots can now send Paid Gifts to users in exchange for Telegram Stars. Added the classes Gift and Gifts and the method getAvailableGifts , allowing bots to get all gifts available for sending. Added the method sendGift , allowing bots to send gifts to users. Added the field gift to the class TransactionPartnerUser . Loading Screen Customization Mini Apps can customize their loading screen, adding their own icon and specific colors for light and dark themes. You can access these customization settings in @BotFather via /mybots > Select Bot > Bot Settings > Configure Mini App > Enable Mini App Hardware-specific Optimizations Mini Apps running on Android can now receive basic information about a device's processing hardware, allowing them to optimize user experience based on the device's capabilities. This information includes the OS, App and SDK's respective versions as well as the device's model and performance class. General Added the field photo_url to the class WebAppUser for all bots, allowing Mini Apps to access a user's profile photo if their privacy settings allow for it. Third parties (e.g., Mini App builders) that receive or process data on behalf of Mini Apps are now able to validate it without knowing the App's bot token . Debugging options have been expanded to include full support for iOS devices . You can use these tools to find app-specific issues in your Mini App. WARNING! Starting December 1, 2024 messages with video that are sent, copied or forwarded to groups and channels with a sufficiently large audience can be automatically scheduled by the server until the respective video is reencoded. Such messages will have 0 as their message identifier and can't be used before they are actually sent. October 31, 2024 Bot API 7.11 Added the class CopyTextButton and the field copy_text in the class InlineKeyboardButton allowing bots to send and receive inline buttons that copy arbitrary text. Added the parameter allow_paid_broadcast to the methods sendMessage , sendPhoto , sendVideo , sendAnimation , sendAudio , sendDocument , sendPaidMedia , sendSticker , sendVideoNote , sendVoice , sendLocation , sendVenue , sendContact , sendPoll , sendDice , sendInvoice , sendGame , sendMediaGroup and copyMessage . Added the class TransactionPartnerTelegramApi for transactions related to paid broadcasted messages. Introduced the ability to add media to existing text messages using the method editMessageMedia . Added support for hashtag and cashtag entities with a specified chat username that opens a search for the relevant tag within the specified chat. WARNING! Starting December 1, 2024 messages with video that are sent, copied or forwarded to groups and channels with a sufficiently large audience can be automatically scheduled by the server until the respective video is reencoded. Such messages will have 0 as their message identifier and can't be used before they are actually sent. September 6, 2024 Bot API 7.10 Added updates about purchased paid media, represented by the class PaidMediaPurchased and the field purchased_paid_media in the class Update . Added the ability to specify a payload in sendPaidMedia that is received back by the bot in TransactionPartnerUser and purchased_paid_media updates. Added the field prize_star_count to the classes GiveawayCreated , Giveaway , GiveawayWinners and ChatBoostSourceGiveaway . Added the field is_star_giveaway to the class GiveawayCompleted . Added the field SecondaryButton to the class WebApp . Added the event secondaryButtonClicked for Mini Apps. Added the field bottomBarColor and the method setBottomBarColor to the class WebApp . Added the field bottom_bar_bg_color to the class ThemeParams . August 14, 2024 Bot API 7.9 Added support for Super Channels , allowing received channel messages to have users or other channels as their senders. Added the ability to send paid media to any chat. Added the parameter business_connection_id to the method sendPaidMedia , allowing bots to send paid media on behalf of a business account. Added the field paid_media to the class TransactionPartnerUser for transactions involving paid media. Added the fields subscription_period and subscription_price to the class ChatInviteLink . Added the method createChatSubscriptionInviteLink , allowing bots to create subscription invite links. Added the method editChatSubscriptionInviteLink , allowing bots to edit the name of subscription invite links. Added the field until_date to the class ChatMemberMember for members with an active subscription. Added support for paid reactions and the class ReactionTypePaid . July 31, 2024 Bot API 7.8 Added the option for bots to set a Main Mini App , which can be previewed and launched directly from a button in the bot's profile or a link. Added the method shareToStory to the class WebApp . Added the field has_main_web_app to the class User , which is returned in the response to getMe . Added the parameter business_connection_id to the methods pinChatMessage and unpinChatMessage , allowing bots to manage pinned messages on behalf of a business account. July 7, 2024 Bot API 7.7 Added the class RefundedPayment , containing information about a refunded payment. Added the field refunded_payment to the class Message , describing a service message about a refunded payment. Added the field isVerticalSwipesEnabled and the methods enableVerticalSwipes , disableVerticalSwipes to the class WebApp . Added the event scanQrPopupClosed for Mini Apps. July 1, 2024 Bot API 7.6 Added the classes PaidMedia , PaidMediaInfo , PaidMediaPreview , PaidMediaPhoto and PaidMediaVideo , containing information about paid media. Added the method sendPaidMedia and the classes InputPaidMedia , InputPaidMediaPhoto and InputPaidMediaVideo , to support sending paid media. Documented that the methods copyMessage and copyMessages cannot be used to copy paid media. Added the field can_send_paid_media to the class ChatFullInfo . Added the field paid_media to the classes Message and ExternalReplyInfo . Added the class TransactionPartnerTelegramAds , containing information about Telegram Star transactions involving the Telegram Ads Platform. Added the field invoice_payload to the class TransactionPartnerUser , containing the bot-specified invoice payload. Changed the default opening mode for Direct Link Mini Apps . Added support for launching Web Apps via t.me link in the class MenuButtonWebApp . Added the field section_separator_color to the class ThemeParams . June 18, 2024 Bot API 7.5 Added the classes StarTransactions , StarTransaction , TransactionPartner and RevenueWithdrawalState , containing information about Telegram Star transactions involving the bot. Added the method getStarTransactions that can be used to get the list of all Telegram Star transactions for the bot. Added support for callback buttons in InlineKeyboardMarkup for messages sent on behalf of a business account. Added support for callback queries originating from a message sent on behalf of a business account. Added the parameter business_connection_id to the methods editMessageText , editMessageMedia , editMessageCaption , editMessageLiveLocation , stopMessageLiveLocation and editMessageReplyMarkup , allowing the bot to edit business messages. Added the parameter business_connection_id to the method stopPoll , allowing the bot to stop polls it sent on behalf of a business account. May 28, 2024 Bot API 7.4 Added support for payments in Telegram Stars by introducing the new currency “XTR”. The parameter provider_token of the methods sendInvoice and createInvoiceLink must be omitted for payments in Telegram Stars . The field provider_token in the class InputInvoiceMessageContent must be omitted for payments in Telegram Stars . Added the method refundStarPayment . Added the field effect_id to the class Message . Added the parameter message_effect_id to the methods sendMessage , sendPhoto , sendVideo , sendAnimation , sendAudio , sendDocument , sendSticker , sendVideoNote , sendVoice , sendLocation , sendVenue , sendContact , sendPoll , sendDice , sendInvoice , sendGame , and sendMediaGroup . Added the field show_caption_above_media to the classes Message , InputMediaAnimation , InputMediaPhoto , InputMediaVideo , InlineQueryResultGif , InlineQueryResultMpeg4Gif , InlineQueryResultPhoto , InlineQueryResultVideo , InlineQueryResultCachedGif , InlineQueryResultCachedMpeg4Gif , InlineQueryResultCachedPhoto , and InlineQueryResultCachedVideo . Added the parameter show_caption_above_media to the methods sendAnimation , sendPhoto , sendVideo , copyMessage , and editMessageCaption . Added support for “expandable_blockquote” entities in received messages. Added support for “expandable_blockquote” entity parsing in “MarkdownV2” and “HTML” parse modes. Allowed to explicitly specify “expandable_blockquote” entities in formatted texts. May 6, 2024 Bot API 7.3 Added support for InlineKeyboardMarkup with url , login_url , and callback_game buttons for messages sent on behalf of a business account. Added the field via_join_request to the class ChatMemberUpdated . Added support for live locations that can be edited indefinitely, allowing 0x7FFFFFFF to be used as live_period . Added the parameter live_period to the method editMessageLiveLocation . Added the field question_entities to the class Poll . Added the field text_entities to the class PollOption . Added the parameters question_parse_mode and question_entities to the method sendPoll . Added the class InputPollOption and changed the type of the parameter options in the method sendPoll to Array of InputPollOption . Added the classes ChatBackground , BackgroundType , BackgroundFill and the field chat_background_set of type ChatBackground to the class Message , describing service messages about background changes. Split out the class ChatFullInfo from the class Chat and changed the return type of the method getChat to ChatFullInfo . Added the field max_reaction_count to the class ChatFullInfo . Documented that .MP3 and .M4A files can be used as voice messages. March 31, 2024 Bot API 7.2 Integration with Business Accounts Added the class BusinessConnection and updates about the connection or disconnection of the bot to a business account, represented by the field business_connection in the class Update . Added updates about new messages in a business account connected to the bot, represented by the field business_message in the class Update . Added updates about message edits in a business account connected to the bot, represented by the field edited_business_message in the class Update . Added updates about message deletion in a business account connected to the bot, represented by the class BusinessMessagesDeleted and the field deleted_business_messages in the class Update . Added the method getBusinessConnection . Working on Behalf of Business Accounts Added the parameter business_connection_id to the methods sendMessage , sendPhoto , sendVideo , sendAnimation , sendAudio , sendDocument , sendSticker , sendVideoNote , sendVoice , sendLocation , sendVenue , sendContact , sendPoll , sendDice , sendGame , and sendMediaGroup . Added the parameter business_connection_id to the method sendChatAction . Added the field business_connection_id to the class Message . Added the field sender_business_bot to the class Message . Information about Business Accounts Added the class BusinessIntro and the field business_intro to the class Chat . Added the class BusinessLocation and the field business_location to the class Chat . Added the classes BusinessOpeningHours and BusinessOpeningHoursInterval and the field business_opening_hours to the class Chat . Mixed-Format Sticker Packs Removed the fields is_animated and is_video from the class StickerSet . Added the field format to the class InputSticker . Removed the parameter sticker_format from the method createNewStickerSet . Added the parameter format to the method setStickerSetThumbnail . Increased the maximum number of stickers in any regular and mask sticker set to 120. Allowed to upload WEBM stickers using SendSticker . Request Chat Improvements Added the fields request_name , request_username , and request_photo to the class KeyboardButtonRequestUsers . Added the fields request_title , request_username , and request_photo to the class KeyboardButtonRequestChat . Added the class SharedUser and replaced the field user_ids in the class UsersShared with the field users . Added the fields title , username , and photo to the class ChatShared . Other Changes Added the field is_from_offline to the class Message . Added the field can_connect_to_business to the class User . Added the field personal_chat to the class Chat . Added the method replaceStickerInSet , Added the class Birthdate and the field birthdate to the class Chat . Added the field BiometricManager to the class WebApp . February 16, 2024 Bot API 7.1 Added support for the administrator rights can_post_stories , can_edit_stories , can_delete_stories in supergroups. Added the class ChatBoostAdded and the field boost_added to the class Message for service messages about a user boosting a chat. Added the field sender_boost_count to the class Message . Added the field reply_to_story to the class Message . Added the fields chat and id to the class Story . Added the field unrestrict_boost_count to the class Chat . Added the field custom_emoji_sticker_set_name to the class Chat . 2023 December 29, 2023 Bot API 7.0 Reactions Added the classes ReactionTypeEmoji and ReactionTypeCustomEmoji representing different types of reaction. Added updates about a reaction change on a message with non-anonymous reactions, represented by the class MessageReactionUpdated and the field message_reaction in the class Update . The bot must explicitly allow the update to receive it. Added updates about reaction changes on a message with anonymous reactions, represented by the class MessageReactionCountUpdated and the field message_reaction_count in the class Update . The bot must explicitly allow the update to receive it. Added the method setMessageReaction that allows bots to react to messages. Added the field available_reactions to the class Chat . Replies 2.0 Added the ability to reply to messages in other chats or forum topics. Added the class ExternalReplyInfo and the field external_reply of type ExternalReplyInfo to the class Message , containing information about a message that is replied to by the current message, but can be from another chat or forum topic. Added the ability to quote a part of the replied message. Added the class TextQuote and the field quote of type TextQuote to the class Message , which contains the part of the replied message text or caption that is quoted in the current message. Added the class ReplyParameters and replaced parameters reply_to_message_id and allow_sending_without_reply in the methods copyMessage , sendMessage , sendPhoto , sendVideo , sendAnimation , sendAudio , sendDocument , sendSticker , sendVideoNote , sendVoice , sendLocation , sendVenue , sendContact , sendPoll , sendDice , sendInvoice , sendGame , and sendMediaGroup with the field reply_parameters of type ReplyParameters . Link Preview Customization Allowed to explicitly specify the URL that will be used for link preview generation in outgoing text messages. Allowed to position link previews above the message text. Allowed to choose media size in link previews. Added the class LinkPreviewOptions and replaced the parameter disable_web_page_preview with link_preview_options in the methods sendMessage and editMessageText . Replaced the field disable_web_page_preview with link_preview_options in the class InputTextMessageContent . Added the field link_preview_options to the class Message with information about the link preview options used to send the message. Block Quotation Added support for “blockquote” entities in received messages. Added support for “blockquote” entity parsing in “MarkdownV2” and “HTML” parse modes. Allowed to explicitly specify “blockquote” entities in formatted texts. Multiple Message Actions Added the method deleteMessages to allow the deletion of multiple messages in a single request. Added the method forwardMessages for forwarding of multiple messages in a single request. Added the method copyMessages for copying of multiple messages in a single request. Request for multiple users Renamed the class KeyboardButtonRequestUser to KeyboardButtonRequestUsers and added the field max_quantity to it. Renamed the field request_user in the class KeyboardButton to request_users . The old name will still work for backward compatibility. Added the class UsersShared . Replaced the field user_shared in the class Message with the field users_shared . Chat Boost Added updates about chat boost changes, represented by the classes ChatBoostUpdated and ChatBoostRemoved and the fields chat_boost and removed_chat_boost in the class Update . The bot must be an administrator in the chat to receive these updates. Added the classes ChatBoostSourcePremium , ChatBoostSourceGiftCode and ChatBoostSourceGiveaway , representing different sources of a chat boost. Added the method getUserChatBoosts for obtaining the list of all active boosts a user has contributed to a chat. Giveaway Added the class Giveaway and the field giveaway to the class Message for messages about scheduled giveaways. Added the class GiveawayCreated and the field giveaway_created to the class Message for service messages about the creation of a scheduled giveaway. Added the class GiveawayWinners and the field giveaway_winners to the class Message for messages about the completion of a giveaway with public winners. Added the class GiveawayCompleted and the field giveaway_completed to the class Message for service messages about the completion of a giveaway without public winners. Web App Changes Added the field SettingsButton to the class WebApp . Added the fields header_bg_color , accent_text_color , section_bg_color , section_header_text_color , subtitle_text_color , destructive_text_color to the class ThemeParams . Web Apps no longer close when the method WebApp.openTelegramLink is called. Other Changes Added support for the fields emoji_status_custom_emoji_id and emoji_status_expiration_date in the class Chat for non-private chats. Added the fields accent_color_id , background_custom_emoji_id , profile_accent_color_id , and profile_background_custom_emoji_id to the class Chat . Added the field has_visible_history to the class Chat . Added the class MessageOrigin and replaced the fields forward_from , forward_from_chat , forward_from_message_id , forward_signature , forward_sender_name , and forward_date with the field forward_origin of type MessageOrigin in the class Message . Improved documentation for the field message of the class callbackQuery and the field pinned_message of the class Message by adding the classes MaybeInaccessibleMessage and InaccessibleMessage . September 22, 2023 Bot API 6.9 Added the new administrator privileges can_post_stories , can_edit_stories and can_delete_stories to the classes ChatMemberAdministrator and ChatAdministratorRights . Added the parameters can_post_stories , can_edit_stories and can_delete_stories to the method promoteChatMember . Currently, bots have no use for these privileges besides assigning them to other administrators. Added the ability to set any header color for Web App using the method setHeaderColor . Added the field CloudStorage to the class WebApp . Added the methods requestWriteAccess and requestContact to the class WebApp . Added Web App events writeAccessRequested and contactRequested . Added the fields from_request and from_attachment_menu to the class WriteAccessAllowed . Added the fields added_to_attachment_menu and allows_write_to_pm to the class WebAppUser . August 18, 2023 Bot API 6.8 Added the field story to the class Message for messages with forwarded stories. Currently, it holds no information. Added the field voter_chat to the class PollAnswer , to contain channel chat voters in Polls . For backward compatibility, the field user in such objects will contain the user 136817688 ( @Channel_Bot ). Added the field emoji_status_expiration_date to the class Chat . Added the method unpinAllGeneralForumTopicMessages . Increased to 512 characters the maximum length of the startapp parameter in direct Web App links. April 21, 2023 Bot API 6.7 Added support for launching Web Apps from inline query results by replacing the parameters switch_pm_text and switch_pm_parameter of the method answerInlineQuery with the parameter button of type InlineQueryResultsButton . Added the field web_app_name to the class WriteAccessAllowed . Added the field switch_inline_query_chosen_chat of the type SwitchInlineQueryChosenChat to the class InlineKeyboardButton , which allows bots to switch to inline mode in a chosen chat of the given type. Added the field via_chat_folder_invite_link to the class ChatMemberUpdated . Added the ability to set different bot names for different user languages using the method setMyName . Added the ability to get the current bot name in the given language as the class BotName using the method getMyName . Added the ability to change bot settings from the bot's profile in official Telegram apps, including the ability to set animated profile photos. Added the ability to specify custom emoji entities using HTML and MarkdownV2 formatting options for bots that purchased additional usernames on Fragment . March 9, 2023 Bot API 6.6 Added the ability to set different bot descriptions for different user languages using the method setMyDescription . Added the ability to get the current bot description in the given language as the class BotDescription using the method getMyDescription . Added the ability to set different bot short descriptions for different user languages using the method setMyShortDescription . Added the ability to get the current bot short description in the given language as the class BotShortDescription using the method getMyShortDescription . Added the parameter emoji to the method sendSticker to specify an emoji for just uploaded stickers. Added support for the creation of custom emoji sticker sets in createNewStickerSet . Added the parameter needs_repainting to the method createNewStickerSet to automatically change the color of emoji based on context (e.g., use text color in messages, accent color in statuses, etc.). Added the field needs_repainting to the class Sticker . Replaced the parameters png_sticker , tgs_sticker , webm_sticker , emojis and mask_position in the method addStickerToSet with the parameter sticker of the type InputSticker . Added support for the creation of sticker sets with multiple initial stickers in createNewStickerSet by replacing the parameters png_sticker , tgs_sticker , webm_sticker , emojis and mask_position with the parameters stickers and sticker_format . Added support for .WEBP files in createNewStickerSet and addStickerToSet . Added support for .WEBP, .TGS, and .WEBM files in uploadStickerFile by replacing the parameter png_sticker in the method uploadStickerFile with the parameters sticker and sticker_format . Added the ability to specify search keywords for stickers added to sticker sets. Added the method setCustomEmojiStickerSetThumbnail for editing the thumbnail of custom emoji sticker sets created by the bot. Added the method setStickerSetTitle for editing the title of sticker sets created by the bot. Added the method deleteStickerSet for complete deletion of a given sticker set that was created by the bot. Added the method setStickerEmojiList for changing the list of emoji associated with a sticker. Added the method setStickerKeywords for changing the search keywords assigned to a sticker. Added the method setStickerMaskPosition for changing the mask position of a mask sticker. Renamed the field thumb in the classes Animation , Audio , Document , Sticker , Video , VideoNote , InputMediaAnimation , InputMediaAudio , InputMediaDocument , InputMediaVideo , StickerSet to thumbnail . Renamed the parameter thumb in the methods sendAnimation , sendAudio , sendDocument , sendVideo , sendVideoNote to thumbnail . Renamed the method setStickerSetThumb to setStickerSetThumbnail and its parameter thumb to thumbnail . Renamed the fields thumb_url , thumb_width , and thumb_height in the classes InlineQueryResultArticle , InlineQueryResultContact , InlineQueryResultDocument , InlineQueryResultLocation , and InlineQueryResultVenue to thumbnail_url , thumbnail_width , and thumbnail_height respectively. Renamed the field thumb_url in the classes InlineQueryResultPhoto and InlineQueryResultVideo to thumbnail_url . Renamed the fields thumb_url and thumb_mime_type in the classes InlineQueryResultGif , and InlineQueryResultMpeg4Gif to thumbnail_url and thumbnail_mime_type respectively. February 3, 2023 Bot API 6.5 Added requests for users and chats and support for granular media permissions . Added the class KeyboardButtonRequestUser and the field request_user to the class KeyboardButton . Added the class KeyboardButtonRequestChat and the field request_chat to the class KeyboardButton . Added the classes UserShared , ChatShared and the fields user_shared , and chat_shared to the class Message . Replaced the fields can_send_media_messages in the classes ChatMemberRestricted and ChatPermissions with separate fields can_send_audios , can_send_documents , can_send_photos , can_send_videos , can_send_video_notes , and can_send_voice_notes for different media types. Added the parameter use_independent_chat_permissions to the methods restrictChatMember and setChatPermissions . Added the field user_chat_id to the class ChatJoinRequest . 2022 December 30, 2022 Bot API 6.4 Added the field is_persistent to the class ReplyKeyboardMarkup , allowing to control when the keyboard is shown. Added the parameter has_spoiler to the methods sendPhoto , sendVideo , and sendAnimation . Added the field has_spoiler to the classes InputMediaPhoto , InputMediaVideo , and InputMediaAnimation . Added the field has_media_spoiler to the class Message . The parameters name and icon_custom_emoji_id of the method editForumTopic are now optional. If they are omitted, the existing values are kept. Added the classes ForumTopicEdited , GeneralForumTopicHidden , GeneralForumTopicUnhidden , and WriteAccessAllowed and the fields forum_topic_edited , general_forum_topic_hidden , general_forum_topic_unhidden , and write_access_allowed to the class Message . Added the methods editGeneralForumTopic , closeGeneralForumTopic , reopenGeneralForumTopic , hideGeneralForumTopic , unhideGeneralForumTopic for managing the General topic in forums. Added the parameter message_thread_id to the method sendChatAction for sending chat actions to a specific message thread or a forum topic. Added the field has_hidden_members to the class Chat . Note that the method getChatMember is only guaranteed to work if the bot is an administrator in the chat. Added the field has_aggressive_anti_spam_enabled to the class Chat . Added Web App events qrTextReceived and clipboardTextReceived . Added the field platform to the class WebApp . Added the methods showScanQrPopup , closeScanQrPopup , and readTextFromClipboard to the class WebApp . Added the parameter options to the method openLink of the class WebApp . November 5, 2022 Bot API 6.3 Added support for Topics in Groups . Added the field is_forum to the class Chat . Added the fields is_topic_message and message_thread_id to the class Message to allow detection of messages belonging to a forum topic and their message thread identifier. Added the classes ForumTopicCreated , ForumTopicClosed , and ForumTopicReopened and the fields forum_topic_created , forum_topic_closed , and forum_topic_reopened to the class Message . Note that service messages about forum topic creation can't be deleted with the deleteMessage method. Added the field can_manage_topics to the classes ChatAdministratorRights , ChatPermissions , ChatMemberAdministrator , and ChatMemberRestricted . Added the parameter can_manage_topics to the method promoteChatMember . Added the methods createForumTopic , editForumTopic , closeForumTopic , reopenForumTopic , deleteForumTopic , unpinAllForumTopicMessages , and getForumTopicIconStickers for forum topic management. Added the parameter message_thread_id to the methods sendMessage , sendPhoto , sendVideo , sendAnimation , sendAudio , sendDocument , sendSticker , sendVideoNote , sendVoice , sendLocation , sendVenue , sendContact , sendPoll , sendDice , sendInvoice , sendGame , sendMediaGroup , copyMessage , forwardMessage to support sending of messages to a forum topic. Added support for Multiple Usernames via the field active_usernames in the class Chat . Added the field emoji_status_custom_emoji_id to the class Chat . August 12, 2022 Bot API 6.2 Custom Emoji Support Added the MessageEntity type “custom_emoji”. Added the field custom_emoji_id to the class MessageEntity for “custom_emoji” entities. Added the method getCustomEmojiStickers . Added the fields type and custom_emoji_id to the class Sticker . Added the field sticker_type to the class StickerSet , describing the type of stickers in the set. The field contains_masks has been removed from the documentation of the class StickerSet . The field is still returned in the object for backward compatibility, but new bots should use the field sticker_type instead. Added the parameter sticker_type to the method createNewStickerSet . The parameter contains_masks has been removed from the documentation of the method createNewStickerSet . The parameter will still work for backward compatibility, but new bots should use the parameter sticker_type instead. Web App Improvements Added the field isClosingConfirmationEnabled and the methods enableClosingConfirmation , disableClosingConfirmation , showPopup , showAlert , showConfirm to the class WebApp . Added the field is_premium to the class WebAppUser . Added the event popupClosed . Other Changes Added the field has_restricted_voice_and_video_messages to the class Chat to support the new setting . June 20, 2022 Bot API 6.1 Media in Descriptions Added support for photos and videos in the 'What can this bot do?' section (shown on the bot's start screen). Use BotFather to set up media. Web App Improvements Added the fields version , headerColor , backgroundColor , BackButton , HapticFeedback and the methods isVersionAtLeast , setHeaderColor , setBackgroundColor , openLink , openTelegramLink , openInvoice to the class WebApp . Added the field secondary_bg_color to the class ThemeParams . Added the method offClick to the class MainButton . Added the fields chat , can_send_after to the class WebAppInitData . Added the events backButtonClicked , settingsButtonClicked , invoiceClosed . Join Requests & Payments Added the fields join_to_send_messages and join_by_request to the class Chat . Added the ability to process join requests which were created without an invite link . Bots will receive a “chat_join_request” update as usual. Added the method createInvoiceLink to generate an HTTP link for an invoice. Telegram Premium Support ( more info ) The maximum value of the field file_size in the classes Animation , Audio , Document , Video , Voice , and File can no longer be stored in a signed 32-bit integer type. This change is necessary to support 4GB files uploaded by premium accounts . Added the field is_premium to the class User . Added the field premium_animation to the class Sticker . Attachment Menu Integration Added the field added_to_attachment_menu to the class User . Bots integrated in the attachment menu can now be used in groups, supergroups and channels. Added support for t.me links that can be used to select the chat in which the attachment menu with the bot will be opened. Other Changes Added the parameter secret_token to the method setWebhook . As previously announced, only HTTPS links are now allowed in login_url inline keyboard buttons. April 16, 2022 Bot API 6.0 Added support for Web Apps , see the detailed manual here . ( blog announcement ) Added the class WebAppInfo and the fields web_app to the classes KeyboardButton and InlineKeyboardButton . Added the class SentWebAppMessage and the method answerWebAppQuery for sending an answer to a Web App query, which originated from an inline button of the 'web_app' type. Added the class WebAppData and the field web_app_data to the class Message . Added the class MenuButton and the methods setChatMenuButton and getChatMenuButton for managing the behavior of the bot's menu button in private chats. Added the class ChatAdministratorRights and the methods setMyDefaultAdministratorRights and getMyDefaultAdministratorRights for managing the bot's default administrator rights. Added support for t.me links that can be used to add the bot to groups and channels as an administrator. Added the field last_synchronization_error_date to the class WebhookInfo . Renamed the field can_manage_voice_chats to can_manage_video_chats in the class ChatMemberAdministrator . The old field will remain temporarily available. Renamed the parameter can_manage_voice_chats to can_manage_video_chats in the method promoteChatMember . The old parameter will remain temporarily available. Renamed the fields voice_chat_scheduled , voice_chat_started , voice_chat_ended , and voice_chat_participants_invited to video_chat_scheduled , video_chat_started , video_chat_ended , and video_chat_participants_invited in the class Message . The old fields will remain temporarily available. WARNING! After the next update, only HTTPS links will be allowed in login_url inline keyboard buttons. January 31, 2022 Bot API 5.7 Added support for Video Stickers . Added the field is_video to the classes Sticker and StickerSet . Added the parameter webm_sticker to the methods createNewStickerSet and addStickerToSet . 2021 December 30, 2021 Bot API 5.6 Improved support for Protected Content . Added the parameter protect_content to the methods sendMessage , sendPhoto , sendVideo , sendAnimation , sendAudio , sendDocument , sendSticker , sendVideoNote , sendVoice , sendLocation , sendVenue , sendContact , sendPoll , sendDice , sendInvoice , sendGame , sendMediaGroup , copyMessage , forwardMessage to allow sending messages with protected content to any chat. Added support for spoiler entities , which will work in Telegram versions released after December 30, 2021. Older clients will display unsupported message . Added new MessageEntity type “spoiler”. Added the ability to specify spoiler entities using HTML and MarkdownV2 formatting options. December 7, 2021 Bot API 5.5 Bots are now allowed to contact users who sent a join request to a chat where the bot is an administrator with the can_invite_users administrator right – even if the user never interacted with the bot before. Added support for mentioning users by their ID in inline keyboards. This will only work in Telegram versions released after December 7, 2021. Older clients will display unsupported message . Added the methods banChatSenderChat and unbanChatSenderChat for banning and unbanning channel chats in supergroups and channels. Added the field has_private_forwards to the class Chat for private chats, which can be used to check the possibility of mentioning the user by their ID. Added the field has_protected_content to the classes Chat and Message . Added the field is_automatic_forward to the class Message . Note: After this update it will become impossible to forward messages from some chats. Use the fields has_protected_content in the classes Message and Chat to check this. Note: After this update users are able to send messages on behalf of channels they own. Bots are expected to use the field sender_chat in the class Message to correctly support such messages. Note: As previously announced, user identifiers can now have up to 52 significant bits and require a 64-bit integer or double-precision float type to be stored safely. November 5, 2021 Bot API 5.4 Added the the parameter creates_join_request to the methods createChatInviteLink and editChatInviteLink for managing chat invite links that create join requests (read more about this on our blog ). Added the fields creates_join_request and pending_join_request_count to the class ChatInviteLink . Added the field name to the class ChatInviteLink and the parameters name to the methods createChatInviteLink and editChatInviteLink for managing invite link names . Added updates about new requests to join the chat, represented by the class ChatJoinRequest and the field chat_join_request in the Update class. The bot must be an administrator in the chat with the can_invite_users administrator right to receive these updates. Added the methods approveChatJoinRequest and declineChatJoinRequest for managing requests to join the chat. Added support for the choose_sticker action in the method sendChatAction . WARNING! User identifiers will become bigger than 2^31 - 1 before the end of this year and it will be no longer possible to store them in a signed 32-bit integer type. User identifiers will have up to 52 significant bits, so a 64-bit integer or double-precision float type would still be safe for storing them. Please make sure that your code can correctly handle such user identifiers. June 25, 2021 Bot API 5.3 Personalized Commands Bots can now show lists of commands tailored to specific situations - including localized commands for users with different languages, as well as different commands based on chat type or for specific chats, and special lists of commands for chat admins. Added the class BotCommandScope , describing the scope to which bot commands apply. Added the parameters scope and language_code to the method setMyCommands to allow bots specify different commands for different chats and users. Added the parameters scope and language_code to the method getMyCommands . Added the method deleteMyCommands to allow deletion of the bot's commands for the given scope and user language. Improved visibility of bot commands in Telegram apps with the new 'Menu' button in chats with bots, read more on the blog . Custom Placeholders Added the ability to specify a custom input field placeholder in the classes ReplyKeyboardMarkup and ForceReply . And More Improved documentation of the class ChatMember by splitting it into 6 subclasses. Renamed the method kickChatMember to banChatMember . The old method name can still be used. Renamed the method getChatMembersCount to getChatMemberCount . The old method name can still be used. Values of the field file_unique_id in objects of the type PhotoSize and of the fields small_file_unique_id and big_file_unique_id in objects of the type ChatPhoto were changed. WARNING! After one of the upcoming Bot API updates, user identifiers will become bigger than 2^31 - 1 and it will be no longer possible to store them in a signed 32-bit integer type. User identifiers will have up to 52 significant bits, so a 64-bit integer or double-precision float type would still be safe for storing them. Please make sure that your code can correctly handle such user identifiers. April 26, 2021 Bot API 5.2 Support for Payments 2.0 , see this manual for more details about the Bot Payments API . Added the type InputInvoiceMessageContent to support sending invoices as inline query results. Allowed sending invoices to group, supergroup and channel chats. Added the fields max_tip_amount and suggested_tip_amounts to the method sendInvoice to allow adding optional tips to the payment. The parameter start_parameter of the method sendInvoice became optional. If the parameter isn't specified, the invoice can be paid directly from forwarded messages. Added the field chat_type to the class InlineQuery , containing the type of the chat, from which the inline request was sent. Added the type VoiceChatScheduled and the field voice_chat_scheduled to the class Message . Fixed an error in sendChatAction documentation to correctly mention “record_voice” and “upload_voice” instead of “record_audio” and “upload_audio” for related to voice note actions. Old action names will still work for backward compatibility. WARNING! After the next Bot API update (Bot API 5.3) there will be a one-time change of the value of the field file_unique_id in objects of the type PhotoSize and of the fields small_file_unique_id and big_file_unique_id in objects of the type ChatPhoto . WARNING! Service messages about non-bot users joining the chat will be soon removed from large groups. We recommend using the “chat_member” update as a replacement. WARNING! After one of the upcoming Bot API updates, user identifiers will become bigger than 2^31 - 1 and it will be no longer possible to store them in a signed 32-bit integer type. User identifiers will have up to 52 significant bits, so a 64-bit integer or double-precision float type would still be safe for storing them. Please make sure that your code can correctly handle such user identifiers. March 9, 2021 Bot API 5.1 Added two new update types Added updates about member status changes in chats, represented by the class ChatMemberUpdated and the fields my_chat_member and chat_member in the Update class. The bot must be an administrator in the chat to receive chat_member updates about other chat members. By default, only my_chat_member updates about the bot itself are received. Improved Invite Links Added the class ChatInviteLink , representing an invite link to a chat. Added the method createChatInviteLink , which can be used to create new invite links in addition to the primary invite link. Added the method editChatInviteLink , which can be used to edit non-primary invite links created by the bot. Added the method revokeChatInviteLink , which can be used to revoke invite links created by the bot. Voice Chat Info Added the type VoiceChatStarted and the field voice_chat_started to the class Message . Added the type VoiceChatEnded and the field voice_chat_ended to the class Message . Added the type VoiceChatParticipantsInvited and the field voice_chat_participants_invited to the class Message . Added the new administrator privilege can_manage_voice_chats to the class ChatMember and parameter can_manage_voice_chats to the method promoteChatMember . For now, bots can use this privilege only for passing to other administrators. And More Added the type MessageAutoDeleteTimerChanged and the field message_auto_delete_timer_changed to the class Message . Added the parameter revoke_messages to the method kickChatMember , allowing to delete all messages from a group for the user who is being removed. Added the new administrator privilege can_manage_chat to the class ChatMember and parameter can_manage_chat to the method promoteChatMember . This administrator right is implied by any other administrator privilege. Supported the new bowling animation for the random dice . Choose between different animations (dice, darts, basketball, football, bowling, slot machine) by specifying the emoji parameter in the method sendDice . WARNING! After one of the upcoming Bot API updates, some user identifiers will become bigger than 2^31 - 1 and it will be no longer possible to store them in a signed 32-bit integer type. User identifiers will have up to 52 significant bits, so a 64-bit integer or double-precision float type would still be safe for storing them. Please make sure that your code can correctly handle such user identifiers. 2020 November 4, 2020 Introducing Bot API 5.0 Run Your Own Bot API Server Bot API source code is now available at telegram-bot-api . You can now run your own Bot API server locally, boosting your bots' performance. Added the method logOut , which can be used to log out from the cloud Bot API server before launching your bot locally. You must log out the bot before running it locally, otherwise there is no guarantee that the bot will receive all updates. Added the method close , which can be used to close the bot instance before moving it from one local server to another. Transfer Bot Ownership You can now use @BotFather to transfer your existing bots to another Telegram account. Webhooks Added the parameter ip_address to the method setWebhook , allowing to bypass DNS resolving and use the specified fixed IP address to send webhook requests. Added the field ip_address to the class WebhookInfo , containing the current IP address used for webhook connections creation. Added the ability to drop all pending updates when changing webhook URL using the parameter drop_pending_updates in the methods setWebhook and deleteWebhook . Working with Groups The getChat request now returns the user's bio for private chats if available. The getChat request now returns the identifier of the linked chat for supergroups and channels, i.e. the discussion group identifier for a channel and vice versa. The getChat request now returns the location to which the supergroup is connected (see Local Groups ). Added the class ChatLocation to represent the location. Added the parameter only_if_banned to the method unbanChatMember to allow safe unban. Working with Files Added the field file_name to the classes Audio and Video , containing the name of the original file. Added the ability to disable server-side file content type detection using the parameter disable_content_type_detection in the method sendDocument and the class inputMediaDocument . Multiple Pinned Messages Added the ability to pin messages in private chats . Added the parameter message_id to the method unpinChatMessage to allow unpinning of the specific pinned message. Added the method unpinAllChatMessages , which can be used to unpin all pinned messages in a chat. File Albums Added support for sending and receiving audio and document albums in the method sendMediaGroup . Live Locations Added the field live_period to the class Location , representing a maximum period for which the live location can be updated. Added support for live location heading : added the field heading to the classes Location , InlineQueryResultLocation , InputLocationMessageContent and the parameter heading to the methods sendLocation and editMessageLiveLocation . Added support for proximity alerts in live locations: added the field proximity_alert_radius to the classes Location , InlineQueryResultLocation , InputLocationMessageContent and the parameter proximity_alert_radius to the methods sendLocation and editMessageLiveLocation . Added the type ProximityAlertTriggered and the field proximity_alert_triggered to the class Message . Added possibility to specify the horizontal accuracy of a location. Added the field horizontal_accuracy to the classes Location , InlineQueryResultLocation , InputLocationMessageContent and the parameter horizontal_accuracy to the methods sendLocation and editMessageLiveLocation . Anonymous Admins Added the field sender_chat to the class Message , containing the sender of a message which is a chat (group or channel). For backward compatibility in non-channel chats, the field from in such messages will contain the user 777000 for messages automatically forwarded to the discussion group and the user 1087968824 ( @GroupAnonymousBot ) for messages from anonymous group administrators. Added the field is_anonymous to the class chatMember , which can be used to distinguish anonymous chat administrators. Added the parameter is_anonymous to the method promoteChatMember , which allows to promote anonymous chat administrators. The bot itself should have the is_anonymous right to do this. Despite the fact that bots can have the is_anonymous right, they will never appear as anonymous in the chat. Bots can use the right only for passing to other administrators. Added the custom title of an anonymous message sender to the class Message as author_signature . And More Added the method copyMessage , which sends a copy of any message. Maximum poll question length increased to 300. Added the ability to manually specify text entities instead of specifying the parse_mode in the classes InputMediaPhoto , InputMediaVideo , InputMediaAnimation , InputMediaAudio , InputMediaDocument , InlineQueryResultPhoto , InlineQueryResultGif , InlineQueryResultMpeg4Gif , InlineQueryResultVideo , InlineQueryResultAudio , InlineQueryResultVoice , InlineQueryResultDocument , InlineQueryResultCachedPhoto , InlineQueryResultCachedGif , InlineQueryResultCachedMpeg4Gif , InlineQueryResultCachedVideo , InlineQueryResultCachedAudio , InlineQueryResultCachedVoice , InlineQueryResultCachedDocument , InputTextMessageContent and the methods sendMessage , sendPhoto , sendVideo , sendAnimation , sendAudio , sendDocument , sendVoice , sendPoll , editMessageText , editMessageCaption . Added the fields google_place_id and google_place_type to the classes Venue , InlineQueryResultVenue , InputVenueMessageContent and the optional parameters google_place_id and google_place_type to the method sendVenue to support Google Places as a venue API provider. Added the field allow_sending_without_reply to the methods sendMessage , sendPhoto , sendVideo , sendAnimation , sendAudio , sendDocument , sendSticker , sendVideoNote , sendVoice , sendLocation , sendVenue , sendContact , sendPoll , sendDice , sendInvoice , sendGame , sendMediaGroup to allow sending messages not a as reply if the replied-to message has already been deleted. And Last but not Least Supported the new football and slot machine animations for the random dice . Choose between different animations (dice, darts, basketball, football, slot machine) by specifying the emoji parameter in the method sendDice . June 4, 2020 Bot API 4.9 Added the new field via_bot to the Message object. You can now know which bot was used to send a message. Supported video thumbnails for inline GIF and MPEG4 animations. Supported the new basketball animation for the random dice . Choose between different animations (dice, darts, basketball) by specifying the emoji parameter in the method sendDice . April 24, 2020 Bot API 4.8 Supported explanations for Quizzes 2.0 . Add explanations by specifying the parameters explanation and explanation_parse_mode in the method sendPoll . Added the fields explanation and explanation_entities to the Poll object. Supported timed polls that automatically close at a certain date and time. Set up by specifying the parameter open_period or close_date in the method sendPoll . Added the fields open_period and close_date to the Poll object. Supported the new darts animation for the dice mini-game. Choose between the default dice animation and darts animation by specifying the parameter emoji in the method sendDice . Added the field emoji to the Dice object. March 30, 2020 Bot API 4.7 Added the method sendDice for sending a dice message, which will have a random value from 1 to 6. (Yes, we're aware of the “proper” singular of die . But it's awkward, and we decided to help it change. One dice at a time!) Added the field dice to the Message object. Added the method getMyCommands for getting the current list of the bot's commands. Added the method setMyCommands for changing the list of the bot's commands through the Bot API instead of @BotFather . Added the ability to create animated sticker sets by specifying the parameter tgs_sticker instead of png_sticker in the method createNewStickerSet . Added the ability to add animated stickers to sets created by the bot by specifying the parameter tgs_sticker instead of png_sticker in the method addStickerToSet . Added the field thumb to the StickerSet object. Added the ability to change thumbnails of sticker sets created by the bot using the method setStickerSetThumb . January 23, 2020 Bot API 4.6 Supported Polls 2.0 . Added the ability to send non-anonymous, multiple answer, and quiz-style polls: added the parameters is_anonymous , type , allows_multiple_answers , correct_option_id , is_closed options to the method sendPoll . Added the object KeyboardButtonPollType and the field request_poll to the object KeyboardButton . Added updates about changes of user answers in non-anonymous polls, represented by the object PollAnswer and the field poll_answer in the Update object. Added the fields total_voter_count , is_anonymous , type , allows_multiple_answers , correct_option_id to the Poll object. Bots can now send polls to private chats. Added more information about the bot in response to the getMe request: added the fields can_join_groups , can_read_all_group_messages and supports_inline_queries to the User object. Added the optional field language to the MessageEntity object. 2019 December 31, 2019 Bot API 4.5 Added support for two new MessageEntity types, underline and strikethrough . Added support for nested MessageEntity objects. Entities can now contain other entities. If two entities have common characters then one of them is fully contained inside the other. Added support for nested entities and the new tags <u>/<ins> (for underlined text) and <s>/<strike>/<del> (for strikethrough text) in parse mode HTML. Added a new parse mode, MarkdownV2 , which supports nested entities and two new entities __ (for underlined text) and ~ (for strikethrough text). Parse mode Markdown remains unchanged for backward compatibility. Added the field file_unique_id to the objects Animation , Audio , Document , PassportFile , PhotoSize , Sticker , Video , VideoNote , Voice , File and the fields small_file_unique_id and big_file_unique_id to the object ChatPhoto . The new fields contain a unique file identifier, which is supposed to be the same over time and for different bots, but can't be used to download or reuse the file. Added the field custom_title to the ChatMember object. Added the new method setChatAdministratorCustomTitle to manage the custom titles of administrators promoted by the bot. Added the field slow_mode_delay to the Chat object. July 29, 2019 Bot API 4.4 Added support for animated stickers . New field is_animated in Sticker and StickerSet objects, animated stickers can now be used in sendSticker and InlineQueryResultCachedSticker . Added support for default permissions in groups. New object ChatPermissions , containing actions which a member can take in a chat. New field permissions in the Chat object; new method setChatPermissions . The field all_members_are_administrators has been removed from the documentation for the Chat object. The field is still returned in the object for backward compatibility, but new bots should use the permissions field instead. Added support for more permissions for group and supergroup members: added the new field can_send_polls to ChatMember object, added can_change_info , can_invite_users , can_pin_messages in ChatMember object for restricted users (previously available only for administrators). The method restrictChatMember now takes the new user permissions in a single argument of the type ChatPermissions . The old way of passing parameters will keep working for a while for backward compatibility. Added description support for basic groups (previously available in supergroups and channel chats). You can pass a group's chat_id to setChatDescription and receive the group's description in the Chat object in the response to getChat method. Added invite_link support for basic groups (previously available in supergroups and channel chats). You can pass a group's chat_id to exportChatInviteLink and receive the group's invite link in the Chat object in the response to getChat method. File identifiers from the ChatPhoto object are now invalidated and can no longer be used whenever the photo is changed. All webhook requests from the Bot API are now coming from the subnets 149.154.160.0/20 and 91.108.4.0/22 . Most users won't need to do anything to continue receiving webhooks. If you control inbound access with a firewall, you may need to update your configuration. You can always find the list of actual IP addresses of servers used to send webhooks there: https://core.telegram.org/bots/webhooks . As of the next Bot API update ( version 4.5 ), nested MessageEntity objects will be allowed in message texts and captions. Please make sure that your code can correctly handle such entities. May 31, 2019 Bot API 4.3 Added support for Seamless Telegram Login on external websites. Added the new object LoginUrl and the new field login_url to the InlineKeyboardButton object which allows to automatically authorize users before they go to a URL specified by the bot. Users will be asked to confirm authorization in their Telegram app (needs version 5.7 or higher) when they press the button: Also in this update: Added the field reply_markup to the Message object, containing the inline keyboard attached to the message. If a message with an inline keyboard is forwarded, the forwarded message will now have an inline keyboard if the keyboard contained only url and login_url buttons or if the message was sent via a bot and the keyboard contained only url , login_url , switch_inline_query or switch_inline_query_current_chat buttons. In the latter case, switch_inline_query_current_chat buttons are replaced with switch_inline_query buttons. Bots now receive the edited_message Update even if only Message.reply_markup has changed. Bots that have the can_edit_messages right in a channel can now use the method editMessageReplyMarkup for messages written by other administrators forever without the 48 hours limit. Don't forget that starting in July 2019 , webhook requests from Bot API will be coming from the subnets 149.154.160.0/20 and 91.108.4.0/22 . Most users won't need to do anything to continue receiving webhooks. If you control inbound access with a firewall, you may need to update your configuration. You can always find the list of actual IP addresses of servers used to send webhooks there: https://core.telegram.org/bots/webhooks . April 14, 2019 Bot API 4.2 Added support for native polls: added the object Poll , the methods sendPoll and stopPoll and the field poll in the Message and Update objects. The method deleteMessage can now be used to delete messages sent by a user to the bot in private chats within 48 hours. Added support for pinned messages in basic groups in addition to supergroups and channel chats: you can pass group's chat_id to pinChatMessage and unpinChatMessage , and receive the pinned group message in Chat object. Added the field is_member to the ChatMember object, which can be used to find whether a restricted user is a member of the chat. Added the field forward_sender_name to the Message object, containing name of the sender who has opted to hide their account. Starting in July 2019, webhook requests from Bot API will be coming from the subnets 149.154.160.0/20 and 91.108.4.0/22 . Most users won't need to do anything to continue receiving webhooks. If you control inbound access with a firewall, you may need to update your configuration. You can always find the list of actual IP addresses of servers used to send webhooks there: https://core.telegram.org/bots/webhooks . Document thumbnails now should be inscribed in a 320x320 square instead of 90x90. 2018 August 27, 2018 Bot API 4.1 Added support for translated versions of documents in Telegram Passport . New field translation in EncryptedPassportElement . New errors: PassportElementErrorTranslationFile and PassportElementErrorTranslationFiles and PassportElementErrorUnspecified . July 26, 2018 Bot API 4.0 . Added support for Telegram Passport . See the official announcement on the blog and the manual for details. Added support for editing the media content of messages : added the method editMessageMedia and new types InputMediaAnimation , InputMediaAudio , and InputMediaDocument . Added the field thumb to the Audio object to contain the thumbnail of the album cover to which the music file belongs. Added support for attaching custom thumbnails to uploaded files. For animations, audios, videos and video notes, which are less than 10 MB in size, thumbnails are generated automatically. tg:// URLs now can be used in inline keyboard url buttons and text_link message entities. Added the method sendAnimation , which can be used instead of sendDocument to send animations, specifying their duration, width and height. Added the field animation to the Message object. For backward compatibility, when this field is set, the document field will be also set. Added two new MessageEntity types: cashtag and phone_number . Added support for Foursquare venues: added the new field foursquare_type to the objects Venue , InlineQueryResultVenue and InputVenueMessageContent , and the parameter foursquare_type to the sendVenue method. You can now create inline mentions of users, who have pressed your bot's callback buttons. You can now use the Retry-After response header to configure the delay after which the Bot API will retry the request after an unsuccessful response from a webhook. If a webhook returns the HTTP error 410 Gone for all requests for more than 23 hours successively, it can be automatically removed. Added vCard support when sharing contacts: added the field vcard to the objects Contact , InlineQueryResultContact , InputContactMessageContent and the method sendContact . February 13, 2018 Bot API 3.6 . Supported text formatting in media captions. Specify the desired parse_mode ( Markdown or HTML ) when you provide a caption. In supergroups, if the bot receives a message that is a reply, it will also receive the message to which that message is replying – even if the original message is inaccessible due to the bot's privacy settings. (In other words, replying to any message in a supergroup with a message that mentions the bot or features a command for it acts as forwarding the original message to the bot). Added the new field connected_website to Message . The bot will receive a message with this field in a private chat when a user logs in on the bot's connected website using the Login Widget and allows sending messages from your bot. Added the new parameter supports_streaming to the sendVideo method and a field with the same name to the InputMediaVideo object. 2017 November 17, 2017 Bot API 3.5 . Added the new method sendMediaGroup and two kinds of InputMedia objects to support the new albums feature . Added support for pinning messages in channels. pinChatMessage and unpinChatMessage accept channels. Added the new fields provider_data , send_phone_number_to_provider , send_email_to_provider to sendInvoice for sharing information about the invoice with the payment provider. October 11, 2017 Bot API 3.4 . Bots can now send and receive Live Locations . Added new field live_period to the sendLocation method and the editMessageLiveLocation and stopMessageLiveLocation methods as well as the necessary objects for inline bots. Bots can use the new setChatStickerSet and deleteChatStickerSet methods to manage group sticker sets . The getChat request now returns the group's sticker set for supergroups if available. Bots now receive entities from media captions in the new field caption_entities in Message . August 23, 2017 Bot API 3.3 . Bots can now mention users via inline mentions , without using usernames. getChat now also returns pinned messages in supergroups, if present. Added the new field pinned_message to the Chat object. Added the new fields author_signature and forward_signature to the Message object. Added the new field is_bot to the User object. July 21, 2017 Bot API 3.2 . Teach your bot to handle stickers and sticker sets . Added new methods for working with stickers: getStickerSet , uploadStickerFile , createNewStickerSet , addStickerToSet , setStickerPositionInSet , and deleteStickerFromSet . Added the fields set_name and mask_position to the Sticker object, plus two new objects, StickerSet , and MaskPosition . June 30, 2017 Bot API 3.1 . Build your own robotic police force for supergoups with these new methods for admin bots: Added new methods restrictChatMember and promoteChatMember to manage users and admins, added new parameter until_date to kickChatMember for temporary bans. Added new methods exportChatInviteLink , setChatPhoto , deleteChatPhoto , setChatTitle , setChatDescription , pinChatMessage and unpinChatMessage to manage groups and channels. Added the new fields photo , description and invite_link to the Chat object. Added the new fields until_date , can_be_edited , can_change_info , can_post_messages , can_edit_messages , can_delete_messages , can_invite_users , can_restrict_members , can_pin_messages , can_promote_members , can_send_messages , can_send_media_messages , can_send_other_messages and can_add_web_page_previews to the ChatMember object. May 18, 2017 Introducing Bot API 3.0 . NEW Payment Platform See Introduction to Bot Payments for a brief overview. If you're not a developer, you may like this user-friendly blog post better. Your bot can now accept payments for goods and services via Telegram. Added new kinds of updates , shipping_query and pre_checkout_query , and new types of message content, invoice and successful_payment . Added new methods for payments: sendInvoice , answerShippingQuery , and answerPreCheckoutQuery . Added a new type of button, the pay button to InlineKeyboardButton . NEW Video Messages As of Telegram v.4.0, users can send short rounded video messages , using an interface similar to that of voice notes. Added the sendVideoNote method, the new field video_note to Message , the fields record_video_note or upload_video_note to sendChatAction . NEW Multilingual Bots The User object now may have a language_code field that contains the IETF language tag of the user's language. Thanks to this, your bot can now offer localized responses to users that speak different languages. More power to admin bots unbanChatMemeber now also works in channels! New method deleteMessage that allows the bot to delete its own messages, as well as messages posted by other in groups and channels where the bot is an administrator. Minor Changes Replaced the field new_chat_member in Message with new_chat_members (the old field will still be available for a while for compatibility purposes). Inline keyboards with switch_inline_query and switch_inline_query_current_chat can no longer be sent to channels because they are useless there. New fields gif_duration in InlineQueryResultGif and mpeg4_duration in InlineQueryResultMpeg4Gif . 2016 December 4, 2016 Introducing Bot API 2.3.1 , a nifty little update that will give you more control over how your bot gets its updates. Use the new field max_connections in setWebhook to optimize your bot's server load Use allowed_updates in setWebhook and getUpdates to selectively subscribe to updates of a certain type. Among other things, this allows you to stop getting updates about new posts in channels where your bot is an admin. deleteWebhook moved out of setWebhook to get a whole separate method for itself. November 21, 2016 Bot API 2.3 Modified bot privacy mode for the sake of consistency. Your bot can now get updates about posts in channels . Added new fields channel_post and edited_channel_post to Update . You can now update high scores to a lower value by using the new force parameter in setGameScore . Handy for punishing cheaters or fixing errors in your game's High Score table. Starting today, messages with high scores will be updated with new high scores by default. Use disable_edit_message in setGameScore if you don't want this. The edit_message parameter from setGameScore is no longer in use. For backward compatibility, it will be taken into account for a while, unless disable_edit_message is passed explicitly. Added the new field forward_from_message_id to Message . Added the new parameter cache_time to answerCallbackQuery . Will eventually work in Telegram apps — somewhere after version 3.14, maybe 3.15. Renamed hide_keyboard to remove_keyboard in ReplyKeyboardRemove for clarity. hide_keyboard will still work for a while for backward compatibility. October 3, 2016 Bot API 2.2. Introducing a new Gaming Platform! See this introduction for a brief overview. If you're not a developer, you may like this user-friendly blog post better. New tools for building HTML5 games . New method sendGame , new object InlineQueryResultGame , new field game in Message . New parameter url in answerCallbackQuery . Create a game and accept the conditions using Botfather to send custom urls that open your games for the user. New field callback_game in InlineKeyboardButton , new fields game_short_name and chat_instance in CallbackQuery , new object CallbackGame . New methods setGameScore and getGameHighScores . Other changes Making life easier for webhook users. Added a detailed Guide to All Things Webhook that describes every pothole you can run into on the webhook road. New method getWebhookInfo to check current webhook status. Added the option to specify an HTTP URL for a file in all methods where InputFile or file_id can be used (except voice messages). Telegram will get the file from the specified URL and send it to the user. Files must be smaller than 5 MB for photos and smaller than 20 MB for all other types of content. Use the new url parameter in answerCallbackQuery to create buttons that open your bot with user-specific parameters. Added new field switch_inline_query_current_chat in InlineKeyboardButton . Added caption fields to sendAudio , sendVoice , InlineQueryResultAudio , InlineQueryResultVoice , InlineQueryResultCachedAudio , and InlineQueryResultCachedVoice . New field all_members_are_administrators in the Chat object. Certain server responses may now contain the new parameters field with expanded info on errors that occurred while processing your requests. May 25, 2016 Inline keyboards may now be used in group chats. Channels coming soon. Check out @vote and @like for examples. May 22, 2016 Bot API 2.1. Added more tools for group administrator bots. Your bot can now get a list of administrators and members count in a group, check a user's current status (administrator, creator, left the group, kicked from the group), and leave a group. Added new methods: getChat , leaveChat , getChatAdministrators , getChatMember , getChatMembersCount . Added support for edited messages and new mentions from Telegram v.3.9. New fields: edited_message in Update , edit_date in Message , user in MessageEntity . New value text_mention for the type field in MessageEntity . May 12, 2016 Added consistency to what messages bots get in groups and supergroups. See updated FAQ for details » May 6, 2016 Added the field emoji to the Sticker object. Your bot can now know the emoji a sticker corresponds to. Added the field forward_from_chat to the Message object for messages forwarded from channels. April 9, 2016 Introducing Bot API 2.0 . Check out this page for a review of this major update. New inline keyboards with callback and URL buttons . Added new objects InlineKeyboardMarkup , InlineKeyboardButton and CallbackQuery , added reply_markup fields to all InlineQueryResult objects. Added field callback_query to the Update object, new method answerCallbackQuery . Bots can now edit their messages . Added methods editMessageText , editMessageCaption , editMessageReplyMarkup . Bots can request location and phone number from the user. The keyboard field in the object ReplyKeyboardMarkup now supports KeyboardButton , a new object that can have the fields request_location and request_contact . Inline bots Added support for all content types available on Telegram. 19 types of InlineQueryResult objects are now supported. Inline bots can now substitute all kinds of content with text. Added 4 types of InputMessageContent objects. Your inline bot can also ask users for permission to use their location. Added the new Botfather command /setinlinegeo , added field location to the InlineQuery object, added fields location and inline_message_id to the ChosenInlineResult object. Added an easy way to switch between inline mode and a private chat with the bot – useful for settings, establishing external connections and teaching users how to use your bot in inline mode. Added parameters switch_pm_text and switch_pm_parameter to the method answerInlineQuery . Miscellaneous Added group administration tools. New methods kickChatMember and unbanChatMember . Added fields venue , pinned_message and entities to the Message object. Added new objects MessageEntity and Venue , new methods sendVenue and sendContact . Renamed the fields new_chat_participant and left_chat_participant of the Message object to new_chat_member and left_chat_member . February 20, 2016 Added the disable_notification parameter to all methods that send messages or any kind. Removed backward compatibility from the method sendAudio . Voice messages now must be sent using the method sendVoice . There is no more need to specify a non-empty title or performer while sending the audio by file_id . January 20, 2016 By the way, you can use both HTML-style and markdown-style formatting in your bot's messages to send bold, italic or fixed-width text and inline links. All official Telegram clients support this. See Formatting options for details. January 14, 2016 You can now collect feedback on which results provided by your inline bot get chosen by the users. Added the setinlinefeedback command for Botfather, new type ChosenInlineResult , new field chosen_inline_result to the Update object. January 4, 2016 Added support for Inline Mode , a new way for people to contact your bot by typing its username and a query in the text input field in any chat. Enable by sending /setinline to @BotFather . New optional field inline_query added to the Update object. Added new method answerInlineQuery and new objects InlineQuery , InlineQueryResultArticle , InlineQueryResultPhoto , InlineQueryResultGif , InlineQueryResultMpeg4Gif and InlineQueryResultVideo . 2015 November, 2015 Added support for supergroups . The Type field in the Chat object can now contain 'supergroup'. New optional fields added to the Message object: supergroup_chat_created , migrate_to_chat_id , migrate_from_chat_id and channel_chat_created . October 8, 2015 Added initial channel support for bots (no Telegram clients support this at the moment, please wait for updates): The Chat field in the Message is now of the new type Chat . You can now pass a channel username (in the format @channelusername ) in the place of chat_id in all methods (and instead of from_chat_id in forwardMessage ). For this to work, the bot must be an administrator in the channel (and that's exactly what Telegram clients don't support yet — adding bots as administrators coming soon). September 18, 2015 Bots can now download files and media sent by users. Added getFile and File . September 7, 2015 You can now pass parameters using application/json (please note that this doesn't work for file uploads: use multipart/form-data to upload files). Added very basic markdown support . New field parse_mode added to sendMessage . For the moment messages with markdown will be displayed correctly only in Telegram for Android . Other official apps will catch up soon. August 29, 2015 Added support for self-signed certificates: upload your certificate using the certificate parameter in the setWebhook method. You can now make new requests when responding to webhook updates. August 15, 2015 Added new type Voice and new method sendVoice for sending voice messages. Earlier Audio and sendAudio should now be used for sending music files. Telegram clients will show such files in the in-app music player. If you were using sendAudio for your bot to send voice messages, please use sendVoice instead. Added optional fields performer , title to the Audio object and sendAudio method. Added optional field voice to the Message object. July 2015 The thumb field is now optional for Video , Sticker and Document objects The API now supports both video and photo captions. The caption field has been removed from the Video object and added to the Message object instead. caption and duration optional fields have been added to the sendVideo method. Fixed typo: user_id in the Contact object is now correctly labeled as Integer, not String June 24, 2015 The bot platform is officially launched . Back to the Bot API Manual » Telegram Telegram is a cloud-based mobile and desktop messaging app with a focus on security and speed. About FAQ Privacy Press Mobile Apps iPhone/iPad Android Mobile Web Desktop Apps PC/Mac/Linux macOS Web-browser Platform API Translations Instant View About Blog Press Moderation",
            "meta_description": "",
            "links": [
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/api",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/payments",
                "https://core.telegram.org/bots/payments",
                "https://core.telegram.org/bots/games",
                "https://core.telegram.org/bots/games",
                "https://core.telegram.org/bots/2-0-intro",
                "https://core.telegram.org/bots/inline",
                "https://core.telegram.org/bots/api"
            ],
            "timestamp": "2024-12-27T22:28:20.816390",
            "status_code": 200
        },
        {
            "url": "https://core.telegram.org/bots/self-signed",
            "title": "Using self-signed certificates",
            "text_content": "Using self-signed certificates Twitter Home FAQ Apps API Protocol Schema Telegram Bots FAQ Using self-signed certificates Using self-signed certificates Upload your certificate using the certificate parameter in the setWebhook method. The certificate supplied should be PEM encoded (ASCII BASE64), the pem file should only contain the public key (including BEGIN and END portions). When converting from a bundle format, please split the file to only include the public key. Generating a self-signed certificate pair (PEM): Openssl Windows binaries for Openssl are available online openssl req -newkey rsa:2048 -sha256 -nodes -keyout YOURPRIVATE.key -x509 -days 365 -out YOURPUBLIC.pem -subj \"/C=US/ST=New York/L=Brooklyn/O=Example Brooklyn Company/CN=YOURDOMAIN.EXAMPLE\" YOURPUBLIC.pem has to be used as input for setting the self-signed webhook. You can inspect the generated certificate with: openssl x509 -text -noout -in YOURPUBLIC.pem Converting from a previously generated DER: openssl x509 -inform der -in YOURDER.der -out YOURPEM.pem Converting from a previously generated PKCS12: openssl pkcs12 -in YOURPKCS.p12 -out YOURPEM.pem More information: https://www.openssl.org/ Java keystore Generate self-signed JKS: keytool -genkey -keyalg RSA -alias YOURDOMAIN.EXAMPLE -keystore YOURJKS.jks -storepass YOURPASSWORD -validity 360 -keysize 2048 Converting JKS to pkcs12 (intermediate step for conversion to PEM): keytool -importkeystore -srckeystore YOURJKS.jks -destkeystore YOURPKCS.p12 -srcstoretype jks -deststoretype pkcs12 Convert PKCS12 to PEM (requires openssl): openssl pkcs12 -in YOURPKCS.p12 -out YOURPEM.pem More information: https://docs.oracle.com Windows Creating a self-signed certificate using Windows native utilities is also possible, although OpenSSL binaries for Windows are available online. On the commandline: certreq -new TEMPLATE.txt RequestFileOut TEMPLATE.txt example file: [NewRequest]\n\n; At least one value must be set in this section\nSubject = \"CN=DOMAIN.EXAMPLE\"\nKeyLength = 2048\nKeyAlgorithm = RSA\nHashAlgorithm = sha256\n;MachineKeySet = true\nRequestType = Cert\nUseExistingKeySet=false ;generates a new private key (for export)\nExportable = true ;makes the private key exportable with the PFX A self-signed certificate will be generated and installed, to view the certificate: certutil -store -user my To export in DER format (intermediate step for conversion to PEM) certutil -user -store -split my SERIALNUMBER YOURDER.crt Converting to PEM (used for setting the webhook) certutil -encode YOURDER.crt YOURPEM.cer To delete a certificate from your store: certutil -delstore -user my SERIALNUMBER (from view) To export in PFX(PKCS12) format certutil -exportpfx -user YOURDOMAIN.EXAMPLE YOURPKCS.pfx NoChain More information: https://technet.microsoft.com Converting YOURPKCS.pfx to PEM including the private key is best done with OpenSSL: openssl pkcs12 -in YOURPKCS.pfx -out YOURPEM.cer Remember that only the public key is needed as input for the self-signed webhook certificate parameter. certmgr.msc can also be used as a GUI to export the public part of self-signed certificates to PEM. Telegram Telegram is a cloud-based mobile and desktop messaging app with a focus on security and speed. About FAQ Privacy Press Mobile Apps iPhone/iPad Android Mobile Web Desktop Apps PC/Mac/Linux macOS Web-browser Platform API Translations Instant View About Blog Press Moderation",
            "meta_description": "",
            "links": [
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/faq",
                "https://core.telegram.org/bots/self-signed"
            ],
            "timestamp": "2024-12-27T22:28:21.131485",
            "status_code": 200
        },
        {
            "url": "https://core.telegram.org/bots/inline",
            "title": "Inline Bots",
            "text_content": "Inline Bots Twitter Home FAQ Apps API Protocol Schema Telegram Bots Inline Bots Inline Bots Beyond sending commands in private messages or groups, users can interact with your bot via inline queries . If inline queries are enabled, users can call your bot by typing its username and a query in the text input field in any chat. The query is sent to your bot in an update. This way, people can request content from your bot in any of their chats, groups, or channels without sending any messages at all. To enable this option, send the /setinline command to @BotFather and provide the placeholder text that the user will see in the input field after typing your bot’s name. See the Bot API Manual for the relevant methods and objects. Inline results Inline bots support all types of content available in Telegram (20 in all). They are capable of sending stickers, videos, music, locations, documents and more. Clients can display the results with vertical or horizontal scrolling, depending on the type of content: As soon as the user taps on an item, it's immediately sent to the recipient, and the input field is cleared. Switching inline/PM modes Some inline bots can benefit from an initial setup process, like connecting them to an account on an external service (e.g., YouTube). We've added an easy way of switching between the private chat with a bot and whatever chat the user wants to share inline results in. You can display a special 'Switch to PM' button above the inline results (or instead of them). This button will open a private chat with the bot and pass a parameter of your choosing, so that you can prompt the user for the relevant setup actions. Once done, you can use an inline keyboard with a switch_inline_query button to send the user back to the original chat. Sample bots @youtube – Shows a 'Sign in to YouTube' button, then suggests personalized results. Manual: Switch to PM Location-based results Inline bots can request location data from their users. Use the /setinlinegeo command with @BotFather to enable this. Your bot will ask the user for permission to access their location whenever they send an inline request. Sample bot @foursquare – This bot will ask for permission to access the user's location, then provide geo-targeted results. Spreading virally Messages sent with the help of your bot will show its username next to the sender's name. When a user taps on the bot username in the message header, the mention is automatically inserted into the input field. Entering the @ symbol in the input field brings up a list of suggestions, featuring recently used inline bots. Collecting feedback To know which of the provided results your users are sending to their chat partners, send @Botfather the /setinlinefeedback command. With this enabled, you will receive updates on the results chosen by your users. Please note that this can create load issues for popular bots –  you may receive more results than actual requests due to caching (see the cache_time parameter in answerInlineQuery ). For these cases, we recommend adjusting the probability setting to receive 1/10, 1/100 or 1/1000 of the results. Inline bot samples Here are some sample inline bots, in case you’re curious to see one in action. Try any of these: @gif – GIF search @vid – Video search @pic – Yandex image search @bing – Bing image search @wiki – Wikipedia search @imdb – IMDB search @bold – Make bold, italic or fixed sys text NEW @youtube - Connect your account for personalized results @music - Search and send classical music @foursquare – Find and send venue addresses @sticker – Find and send stickers based on emoji Telegram Telegram is a cloud-based mobile and desktop messaging app with a focus on security and speed. About FAQ Privacy Press Mobile Apps iPhone/iPad Android Mobile Web Desktop Apps PC/Mac/Linux macOS Web-browser Platform API Translations Instant View About Blog Press Moderation",
            "meta_description": "",
            "links": [
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/inline"
            ],
            "timestamp": "2024-12-27T22:28:21.314134",
            "status_code": 200
        },
        {
            "url": "https://core.telegram.org/bots/payments/currencies.json",
            "title": "No title",
            "text_content": "{\"AED\":{\"code\":\"AED\",\"title\":\"United Arab Emirates Dirham\",\"symbol\":\"AED\",\"native\":\"د.إ.‏\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"367\",\"max_amount\":\"3672982\"},\"AFN\":{\"code\":\"AFN\",\"title\":\"Afghan Afghani\",\"symbol\":\"AFN\",\"native\":\"؋\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"7025\",\"max_amount\":\"70246674\"},\"ALL\":{\"code\":\"ALL\",\"title\":\"Albanian Lek\",\"symbol\":\"ALL\",\"native\":\"Lek\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"9369\",\"max_amount\":\"93691324\"},\"AMD\":{\"code\":\"AMD\",\"title\":\"Armenian Dram\",\"symbol\":\"AMD\",\"native\":\"դր.\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"39346\",\"max_amount\":\"393457623\"},\"ARS\":{\"code\":\"ARS\",\"title\":\"Argentine Peso\",\"symbol\":\"ARS\",\"native\":\"$\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"102057\",\"max_amount\":\"1020573009\"},\"AUD\":{\"code\":\"AUD\",\"title\":\"Australian Dollar\",\"symbol\":\"AU$\",\"native\":\"$\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"158\",\"max_amount\":\"1578395\"},\"AZN\":{\"code\":\"AZN\",\"title\":\"Azerbaijani Manat\",\"symbol\":\"AZN\",\"native\":\"ман.\",\"thousands_sep\":\" \",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"170\",\"max_amount\":\"1695694\"},\"BAM\":{\"code\":\"BAM\",\"title\":\"Bosnia & Herzegovina Convertible Mark\",\"symbol\":\"BAM\",\"native\":\"KM\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"186\",\"max_amount\":\"1864158\"},\"BDT\":{\"code\":\"BDT\",\"title\":\"Bangladeshi Taka\",\"symbol\":\"BDT\",\"native\":\"৳\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"11950\",\"max_amount\":\"119495596\"},\"BGN\":{\"code\":\"BGN\",\"title\":\"Bulgarian Lev\",\"symbol\":\"BGN\",\"native\":\"лв.\",\"thousands_sep\":\" \",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"186\",\"max_amount\":\"1863175\"},\"BHD\":{\"code\":\"BHD\",\"title\":\"Bahraini dinar\",\"symbol\":\"BHD\",\"native\":\"د.ب.‏\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":3,\"min_amount\":\"377\",\"max_amount\":\"3770580\"},\"BND\":{\"code\":\"BND\",\"title\":\"Brunei Dollar\",\"symbol\":\"BND\",\"native\":\"$\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"135\",\"max_amount\":\"1350871\"},\"BOB\":{\"code\":\"BOB\",\"title\":\"Bolivian Boliviano\",\"symbol\":\"BOB\",\"native\":\"Bs\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"691\",\"max_amount\":\"6910224\"},\"BRL\":{\"code\":\"BRL\",\"title\":\"Brazilian Real\",\"symbol\":\"R$\",\"native\":\"R$\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"609\",\"max_amount\":\"6094403\"},\"BYN\":{\"code\":\"BYN\",\"title\":\"Belarusian ruble\",\"symbol\":\"BYN\",\"native\":\"BYN\",\"thousands_sep\":\" \",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"327\",\"max_amount\":\"3272523\"},\"CAD\":{\"code\":\"CAD\",\"title\":\"Canadian Dollar\",\"symbol\":\"CA$\",\"native\":\"$\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"143\",\"max_amount\":\"1431090\"},\"CHF\":{\"code\":\"CHF\",\"title\":\"Swiss Franc\",\"symbol\":\"CHF\",\"native\":\"CHF\",\"thousands_sep\":\"'\",\"decimal_sep\":\".\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"89\",\"max_amount\":\"892499\"},\"CLP\":{\"code\":\"CLP\",\"title\":\"Chilean Peso\",\"symbol\":\"CLP\",\"native\":\"$\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":0,\"min_amount\":\"983\",\"max_amount\":\"9826095\"},\"CNY\":{\"code\":\"CNY\",\"title\":\"Chinese Renminbi Yuan\",\"symbol\":\"CN¥\",\"native\":\"CN¥\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"728\",\"max_amount\":\"7284801\"},\"COP\":{\"code\":\"COP\",\"title\":\"Colombian Peso\",\"symbol\":\"COP\",\"native\":\"$\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"434553\",\"max_amount\":\"4345530000\"},\"CRC\":{\"code\":\"CRC\",\"title\":\"Costa Rican Colón\",\"symbol\":\"CRC\",\"native\":\"₡\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"50336\",\"max_amount\":\"503355065\"},\"CZK\":{\"code\":\"CZK\",\"title\":\"Czech Koruna\",\"symbol\":\"CZK\",\"native\":\"Kč\",\"thousands_sep\":\" \",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"2392\",\"max_amount\":\"23916017\"},\"DKK\":{\"code\":\"DKK\",\"title\":\"Danish Krone\",\"symbol\":\"DKK\",\"native\":\"kr\",\"thousands_sep\":\"\",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"711\",\"max_amount\":\"7111510\"},\"DOP\":{\"code\":\"DOP\",\"title\":\"Dominican Peso\",\"symbol\":\"DOP\",\"native\":\"$\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"6070\",\"max_amount\":\"60696176\"},\"DZD\":{\"code\":\"DZD\",\"title\":\"Algerian Dinar\",\"symbol\":\"DZD\",\"native\":\"د.ج.‏\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"13392\",\"max_amount\":\"133920121\"},\"EGP\":{\"code\":\"EGP\",\"title\":\"Egyptian Pound\",\"symbol\":\"EGP\",\"native\":\"ج.م.‏\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"5078\",\"max_amount\":\"50783001\"},\"ETB\":{\"code\":\"ETB\",\"title\":\"Ethiopian Birr\",\"symbol\":\"ETB\",\"native\":\"ብር\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"12700\",\"max_amount\":\"126995596\"},\"EUR\":{\"code\":\"EUR\",\"title\":\"Euro\",\"symbol\":\"€\",\"native\":\"€\",\"thousands_sep\":\" \",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"95\",\"max_amount\":\"953305\"},\"GBP\":{\"code\":\"GBP\",\"title\":\"British Pound\",\"symbol\":\"£\",\"native\":\"£\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"79\",\"max_amount\":\"786640\"},\"GEL\":{\"code\":\"GEL\",\"title\":\"Georgian Lari\",\"symbol\":\"GEL\",\"native\":\"GEL\",\"thousands_sep\":\" \",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"281\",\"max_amount\":\"2810083\"},\"GHS\":{\"code\":\"GHS\",\"title\":\"Ghanaian cedi\",\"symbol\":\"GHS\",\"native\":\"GHS\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"1470\",\"max_amount\":\"14699168\"},\"GTQ\":{\"code\":\"GTQ\",\"title\":\"Guatemalan Quetzal\",\"symbol\":\"GTQ\",\"native\":\"Q\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"770\",\"max_amount\":\"7702279\"},\"HKD\":{\"code\":\"HKD\",\"title\":\"Hong Kong Dollar\",\"symbol\":\"HK$\",\"native\":\"$\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"777\",\"max_amount\":\"7769310\"},\"HNL\":{\"code\":\"HNL\",\"title\":\"Honduran Lempira\",\"symbol\":\"HNL\",\"native\":\"L\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"2538\",\"max_amount\":\"25379351\"},\"HRK\":{\"code\":\"HRK\",\"title\":\"Croatian Kuna\",\"symbol\":\"HRK\",\"native\":\"kn\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"717\",\"max_amount\":\"7172906\"},\"HUF\":{\"code\":\"HUF\",\"title\":\"Hungarian Forint\",\"symbol\":\"HUF\",\"native\":\"Ft\",\"thousands_sep\":\" \",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"39050\",\"max_amount\":\"390499501\"},\"IDR\":{\"code\":\"IDR\",\"title\":\"Indonesian Rupiah\",\"symbol\":\"IDR\",\"native\":\"Rp\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"1605000\",\"max_amount\":\"16050000000\"},\"ILS\":{\"code\":\"ILS\",\"title\":\"Israeli New Sheqel\",\"symbol\":\"₪\",\"native\":\"₪\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"359\",\"max_amount\":\"3592601\"},\"INR\":{\"code\":\"INR\",\"title\":\"Indian Rupee\",\"symbol\":\"₹\",\"native\":\"₹\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"8489\",\"max_amount\":\"84894802\"},\"IQD\":{\"code\":\"IQD\",\"title\":\"Iraqi dinar\",\"symbol\":\"IQD\",\"native\":\"د.ع.‏\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":3,\"min_amount\":\"1310000\",\"max_amount\":\"13100003810\"},\"IRR\":{\"code\":\"IRR\",\"title\":\"Iranian rial\",\"symbol\":\"IRR\",\"native\":\"ريال\",\"thousands_sep\":\",\",\"decimal_sep\":\"\\/\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"4208750\",\"max_amount\":\"42087504320\"},\"ISK\":{\"code\":\"ISK\",\"title\":\"Icelandic Króna\",\"symbol\":\"ISK\",\"native\":\"kr\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":0,\"min_amount\":\"138\",\"max_amount\":\"1379502\"},\"JMD\":{\"code\":\"JMD\",\"title\":\"Jamaican Dollar\",\"symbol\":\"JMD\",\"native\":\"$\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"15651\",\"max_amount\":\"156506157\"},\"JOD\":{\"code\":\"JOD\",\"title\":\"Jordanian dinar\",\"symbol\":\"JOD\",\"native\":\"JOD\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":3,\"min_amount\":\"709\",\"max_amount\":\"7093970\"},\"JPY\":{\"code\":\"JPY\",\"title\":\"Japanese Yen\",\"symbol\":\"¥\",\"native\":\"￥\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":0,\"min_amount\":\"153\",\"max_amount\":\"1533675\"},\"KES\":{\"code\":\"KES\",\"title\":\"Kenyan Shilling\",\"symbol\":\"KES\",\"native\":\"Ksh\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"12925\",\"max_amount\":\"129250172\"},\"KGS\":{\"code\":\"KGS\",\"title\":\"Kyrgyzstani Som\",\"symbol\":\"KGS\",\"native\":\"KGS\",\"thousands_sep\":\" \",\"decimal_sep\":\"-\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"8700\",\"max_amount\":\"86999788\"},\"KRW\":{\"code\":\"KRW\",\"title\":\"South Korean Won\",\"symbol\":\"₩\",\"native\":\"₩\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":0,\"min_amount\":\"1438\",\"max_amount\":\"14380650\"},\"KZT\":{\"code\":\"KZT\",\"title\":\"Kazakhstani Tenge\",\"symbol\":\"KZT\",\"native\":\"₸\",\"thousands_sep\":\" \",\"decimal_sep\":\"-\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"52411\",\"max_amount\":\"524107696\"},\"LBP\":{\"code\":\"LBP\",\"title\":\"Lebanese Pound\",\"symbol\":\"LBP\",\"native\":\"ل.ل.‏\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"8954811\",\"max_amount\":\"89548114682\"},\"LKR\":{\"code\":\"LKR\",\"title\":\"Sri Lankan Rupee\",\"symbol\":\"LKR\",\"native\":\"රු.\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"29115\",\"max_amount\":\"291147203\"},\"MAD\":{\"code\":\"MAD\",\"title\":\"Moroccan Dirham\",\"symbol\":\"MAD\",\"native\":\"د.م.‏\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"1000\",\"max_amount\":\"10001906\"},\"MDL\":{\"code\":\"MDL\",\"title\":\"Moldovan Leu\",\"symbol\":\"MDL\",\"native\":\"MDL\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"1830\",\"max_amount\":\"18304167\"},\"MMK\":{\"code\":\"MMK\",\"title\":\"Myanmar kyat\",\"symbol\":\"MMK\",\"native\":\"MMK\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"324796\",\"max_amount\":\"3247960992\"},\"MNT\":{\"code\":\"MNT\",\"title\":\"Mongolian Tögrög\",\"symbol\":\"MNT\",\"native\":\"MNT\",\"thousands_sep\":\" \",\"decimal_sep\":\",\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"339800\",\"max_amount\":\"3397999870\"},\"MOP\":{\"code\":\"MOP\",\"title\":\"Macanese pataca\",\"symbol\":\"MOP\",\"native\":\"MOP\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"800\",\"max_amount\":\"8002021\"},\"MUR\":{\"code\":\"MUR\",\"title\":\"Mauritian Rupee\",\"symbol\":\"MUR\",\"native\":\"MUR\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"4650\",\"max_amount\":\"46502094\"},\"MVR\":{\"code\":\"MVR\",\"title\":\"Maldivian Rufiyaa\",\"symbol\":\"MVR\",\"native\":\"MVR\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"1540\",\"max_amount\":\"15398678\"},\"MXN\":{\"code\":\"MXN\",\"title\":\"Mexican Peso\",\"symbol\":\"MX$\",\"native\":\"$\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"2022\",\"max_amount\":\"20223170\"},\"MYR\":{\"code\":\"MYR\",\"title\":\"Malaysian Ringgit\",\"symbol\":\"MYR\",\"native\":\"RM\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"447\",\"max_amount\":\"4466500\"},\"MZN\":{\"code\":\"MZN\",\"title\":\"Mozambican Metical\",\"symbol\":\"MZN\",\"native\":\"MTn\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"6390\",\"max_amount\":\"63901804\"},\"NGN\":{\"code\":\"NGN\",\"title\":\"Nigerian Naira\",\"symbol\":\"NGN\",\"native\":\"₦\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"155309\",\"max_amount\":\"1553090322\"},\"NIO\":{\"code\":\"NIO\",\"title\":\"Nicaraguan Córdoba\",\"symbol\":\"NIO\",\"native\":\"C$\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"3680\",\"max_amount\":\"36800343\"},\"NOK\":{\"code\":\"NOK\",\"title\":\"Norwegian Krone\",\"symbol\":\"NOK\",\"native\":\"kr\",\"thousands_sep\":\" \",\"decimal_sep\":\",\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"1120\",\"max_amount\":\"11203525\"},\"NPR\":{\"code\":\"NPR\",\"title\":\"Nepalese Rupee\",\"symbol\":\"NPR\",\"native\":\"नेरू\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"13592\",\"max_amount\":\"135915399\"},\"NZD\":{\"code\":\"NZD\",\"title\":\"New Zealand Dollar\",\"symbol\":\"NZ$\",\"native\":\"$\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"174\",\"max_amount\":\"1737815\"},\"PAB\":{\"code\":\"PAB\",\"title\":\"Panamanian Balboa\",\"symbol\":\"PAB\",\"native\":\"B\\/.\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"100\",\"max_amount\":\"999933\"},\"PEN\":{\"code\":\"PEN\",\"title\":\"Peruvian Nuevo Sol\",\"symbol\":\"PEN\",\"native\":\"S\\/.\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"374\",\"max_amount\":\"3736894\"},\"PHP\":{\"code\":\"PHP\",\"title\":\"Philippine Peso\",\"symbol\":\"PHP\",\"native\":\"₱\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"5899\",\"max_amount\":\"58991005\"},\"PKR\":{\"code\":\"PKR\",\"title\":\"Pakistani Rupee\",\"symbol\":\"PKR\",\"native\":\"₨\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"27819\",\"max_amount\":\"278186125\"},\"PLN\":{\"code\":\"PLN\",\"title\":\"Polish Złoty\",\"symbol\":\"PLN\",\"native\":\"zł\",\"thousands_sep\":\" \",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"407\",\"max_amount\":\"4070220\"},\"PYG\":{\"code\":\"PYG\",\"title\":\"Paraguayan Guaraní\",\"symbol\":\"PYG\",\"native\":\"₲\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":0,\"min_amount\":\"7817\",\"max_amount\":\"78174592\"},\"QAR\":{\"code\":\"QAR\",\"title\":\"Qatari Riyal\",\"symbol\":\"QAR\",\"native\":\"ر.ق.‏\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"365\",\"max_amount\":\"3645964\"},\"RON\":{\"code\":\"RON\",\"title\":\"Romanian Leu\",\"symbol\":\"RON\",\"native\":\"RON\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"474\",\"max_amount\":\"4742803\"},\"RSD\":{\"code\":\"RSD\",\"title\":\"Serbian Dinar\",\"symbol\":\"RSD\",\"native\":\"дин.\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"11149\",\"max_amount\":\"111488498\"},\"RUB\":{\"code\":\"RUB\",\"title\":\"Russian Ruble\",\"symbol\":\"RUB\",\"native\":\"₽\",\"thousands_sep\":\" \",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"10982\",\"max_amount\":\"109823763\"},\"SAR\":{\"code\":\"SAR\",\"title\":\"Saudi Riyal\",\"symbol\":\"SAR\",\"native\":\"ر.س.‏\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"376\",\"max_amount\":\"3757671\"},\"SEK\":{\"code\":\"SEK\",\"title\":\"Swedish Krona\",\"symbol\":\"SEK\",\"native\":\"kr\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"1097\",\"max_amount\":\"10966495\"},\"SGD\":{\"code\":\"SGD\",\"title\":\"Singapore Dollar\",\"symbol\":\"SGD\",\"native\":\"$\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"135\",\"max_amount\":\"1349795\"},\"SYP\":{\"code\":\"SYP\",\"title\":\"Syrian pound\",\"symbol\":\"SYP\",\"native\":\"ل.س.‏\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"251253\",\"max_amount\":\"2512530243\"},\"THB\":{\"code\":\"THB\",\"title\":\"Thai Baht\",\"symbol\":\"฿\",\"native\":\"฿\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"3418\",\"max_amount\":\"34180125\"},\"TJS\":{\"code\":\"TJS\",\"title\":\"Tajikistani Somoni\",\"symbol\":\"TJS\",\"native\":\"TJS\",\"thousands_sep\":\" \",\"decimal_sep\":\";\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"1092\",\"max_amount\":\"10924740\"},\"TRY\":{\"code\":\"TRY\",\"title\":\"Turkish Lira\",\"symbol\":\"TRY\",\"native\":\"TL\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"3499\",\"max_amount\":\"34988065\"},\"TTD\":{\"code\":\"TTD\",\"title\":\"Trinidad and Tobago Dollar\",\"symbol\":\"TTD\",\"native\":\"$\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"679\",\"max_amount\":\"6789612\"},\"TWD\":{\"code\":\"TWD\",\"title\":\"New Taiwan Dollar\",\"symbol\":\"NT$\",\"native\":\"NT$\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"3253\",\"max_amount\":\"32528502\"},\"TZS\":{\"code\":\"TZS\",\"title\":\"Tanzanian Shilling\",\"symbol\":\"TZS\",\"native\":\"TSh\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"235497\",\"max_amount\":\"2354966005\"},\"UAH\":{\"code\":\"UAH\",\"title\":\"Ukrainian Hryvnia\",\"symbol\":\"UAH\",\"native\":\"₴\",\"thousands_sep\":\" \",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"4187\",\"max_amount\":\"41872832\"},\"UGX\":{\"code\":\"UGX\",\"title\":\"Ugandan Shilling\",\"symbol\":\"UGX\",\"native\":\"USh\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":0,\"min_amount\":\"3640\",\"max_amount\":\"36400029\"},\"USD\":{\"code\":\"USD\",\"title\":\"United States Dollar\",\"symbol\":\"$\",\"native\":\"$\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":false,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"100\",\"max_amount\":1000000},\"UYU\":{\"code\":\"UYU\",\"title\":\"Uruguayan Peso\",\"symbol\":\"UYU\",\"native\":\"$\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"4458\",\"max_amount\":\"44578095\"},\"UZS\":{\"code\":\"UZS\",\"title\":\"Uzbekistani Som\",\"symbol\":\"UZS\",\"native\":\"UZS\",\"thousands_sep\":\" \",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"1286934\",\"max_amount\":\"12869343094\"},\"VND\":{\"code\":\"VND\",\"title\":\"Vietnamese Đồng\",\"symbol\":\"₫\",\"native\":\"₫\",\"thousands_sep\":\".\",\"decimal_sep\":\",\",\"symbol_left\":false,\"space_between\":true,\"drop_zeros\":false,\"exp\":0,\"min_amount\":\"25440\",\"max_amount\":\"254400000\"},\"YER\":{\"code\":\"YER\",\"title\":\"Yemeni Rial\",\"symbol\":\"YER\",\"native\":\"ر.ي.‏\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"25038\",\"max_amount\":\"250375009\"},\"ZAR\":{\"code\":\"ZAR\",\"title\":\"South African Rand\",\"symbol\":\"ZAR\",\"native\":\"R\",\"thousands_sep\":\",\",\"decimal_sep\":\".\",\"symbol_left\":true,\"space_between\":true,\"drop_zeros\":false,\"exp\":2,\"min_amount\":\"1809\",\"max_amount\":\"18092550\"}}",
            "meta_description": "",
            "links": [],
            "timestamp": "2024-12-27T22:28:21.514799",
            "status_code": 200
        },
        {
            "url": "https://core.telegram.org/bots/payments",
            "title": "Bot Payments API",
            "text_content": "Bot Payments API Twitter Home FAQ Apps API Protocol Schema Bot Payments API Telegram Bot Payments are a free and open platform that allows sellers to accept payments for goods and services from Telegram users. Telegram doesn't collect payment information and takes no commission . This page covers payments for physical goods and services – if you’re interested in selling digital goods and services , please check out our dedicated page . Note: This article is intended for bot developers and store owners. If you're looking for a general overview of Telegram Payments for physical goods and services, check out the Telegram blog . If you are new to Telegram bots and would like to learn how to create and set up a bot, please consult our Introduction to Bots and Bot FAQ . Payments for Physical Products Payments for physical goods and services were first added to Telegram in 2017 and allow bots to: Accept payments from over 200 countries using more than 20 providers. Send invoices to any chat , including groups and channels. Receive payments from users on mobile or desktop apps . Try @ShopBot to create a test invoice – or start a message with @ShopBot ... in any chat for an inline invoice . Check out Demo Shop for an example of a Telegram Channel used as virtual storefront . Check out @ShopBot for an example of a virtual storefront – then use our dedicated guides for digital and physical products to build your own. How does this work? You create a bot that offers goods and services to Telegram users. Merchant bots can send specially formatted invoice messages to users, groups or channels. If your bot supports inline mode , users can also send invoices to other chats via the bot , including to one-on-one chats with other users. Invoice messages feature a photo and description of the product along with a prominent Pay button. Tapping this button opens a special payment interface in the Telegram app. In this interface, users can choose a tip amount (if allowed by the merchant) and enter additional details like shipping info, phone number, or email address. The bot can offer several shipping options for physical goods based on the delivery address. When ready, users enter their credit card info or choose a saved card — and pay for the product. Telegram also supports Apple Pay and Google Pay . Once the transaction is done, the merchant bot can send a receipt message with payment details, shipping and delivery information. Detailed information and step-by-step instructions are available below. Payments Through Third-Party Providers Telegram does not process payments from users and instead allows developers to integrate directly with different third-party payment providers around the world. It is the payment providers that handle and store all sensitive information, like credit card details. Neither Telegram nor the bot developers have access to it. If you work for a company that provides services similar to standalone accounts in Stripe Connect , please let us know via @BotSupport (include the hashtag #paymentsprovider in your message). The Payments API This section explores payments via Telegram's Bot API in more detail. Connecting Payments To start accepting payments, you need a Telegram bot . Use BotFather to create a bot if you don't have one already. Now you have a merchant bot that can offer goods or services to Telegram users. Let's call it @merchantbot in this document. The first stop is to choose and connect a payment provider , you can find the list of supported providers above. Getting a Token Use the /mybots command in the chat with BotFather and choose the @merchantbot that will be offering goods or services. Go to Bot Settings > Payments . Choose a provider, and you will be redirected to the relevant bot. Enter the required details so that the payments provider is connected successfully, go back to the chat with Botfather. The message will now show available providers. Each will have a name, a token , and the date the provider was connected. You will use the token when working with the Bot API. Implementing Payments You will find the necessary methods for building your payment implementation in the Payments Section of the Bot API Manual . Testing Payments: the 'Stripe TEST MODE' Provider While you're still developing and testing payments for your bot, use the “Stripe TEST MODE” provider. When in this mode, you can make payments without actually billing any accounts. Real cards can't be used in test mode, but you can use test cards like 4242 4242 4242 4242 ( full list here ). You can switch between test mode and live mode as many times as you want, but please see the live checklist before you go live. Step-by-Step Process See Bot API: Payments for the complete list of available methods and objects. 1. Create Invoice The user contacts @merchantbot and requests to purchase something. The bot forms an invoice message with a description of the goods or service, amount to be paid, and requested shipping info. There are two ways of creating an invoice: A. Bot Invoice Use the sendInvoice method to generate an invoice and send it to a chat. The provider_token parameter is where you put the token value that you've obtained earlier via Botfather . It is possible for one merchant bot to use several different tokens for different users or different goods and services. Invoice messages with a pay button can be sent to chats of any type : private chats with the user, groups, or channels. The resulting invoice message will look like this: Bot Invoice B. Inline Invoice If @merchantbot supports inline mode , you can use inputInvoiceMessageContent to allow users to share invoices for your goods and services to their one-on-one chats with friends, or to their groups and channels. These invoices will have a Pay button that can be used multiple times. Inline Invoice 2. Choose Forwarding Behavior There are two ways for handling forwarded copies of your invoices, controlled by the parameter start_parameter in the sendInvoice method. A. Multi-chat invoice. Forwarded copies show a Pay button , which multiple users can press and attempt to pay for the goods or services. Inline invoices are always multi-chat invoices. B. Single-chat invoice. Invoice can only be paid from the chat to which it was sent, forwarded copies show a URL button with a deep link to the bot. The deep link can be used to generate a similar invoice in the chat with the bot, to show an error message, or for other purposes. More info on Deep Linking » If a single-chat invoice is sent to the chat with @merchantbot , it can only be paid once . If a single-chat invoice is sent to any other chat, it can be paid many times by many users. To get a better understanding of how this works, try toggling the “Pay from Forwards” parameter when creating invoices with our demo @ShopBot . Regardless of whether or not the Pay button is available in an invoice, the merchant bot always has the power to decide whether or not to accept new payments for a particular invoice. 3. Tips (optional step) If the max_tip_amount parameter is set to above 0 , users can add a tip to their payment. You can use the parameter suggested_tip_amounts to suggest particular amounts that you feel will be relevant for the invoice. Adding Tips 4. Shipping info and other details (optional step) The user specifies shipping information or other info requested by the bot. This could be the user's full name, an email address, a phone number in international format, or a full postal address for delivery. Shipping Info 5. Offer delivery options based on shipping address (optional step) If a shipping address was requested and you included the parameter is_flexible , the Bot API will send an Update with a shipping_query field to the bot. The bot must respond using answerShippingQuery either with a list of possible delivery options and the relevant delivery prices, or with an error (for example, if delivery to the specified address is not possible). Tip: It is recommended that the merchant bot confirms availability of the goods/services at this step – to let the user know in case they are no longer available. This is especially important if you are using multi-chat , inline or single-chat, multi-use invoices. 6. Select delivery option (optional step) The user selects a delivery option from the list (the overall amount to be paid may change at this point) and proceeds to checkout. Choose Delivery Option 7. Pre-Checkout The user enters their payment information and presses the final pay button. At this moment the Bot API sends an Update with the field pre_checkout_query to the bot that contains all the available information about the order. Your bot must reply using answerPrecheckoutQuery within 10 seconds after receiving this update or the transaction is canceled. The bot may return an error if it can't process the order for any reason. We highly recommend specifying a reason for failure to complete the order in human readable form (e.g. \"Sorry, we're all out of rubber ducks! Would you be interested in a cast iron bear instead?\" ). Telegram will display this reason to the user. Warning: It is critical to make sure your bot only accepts multiple payments when the order can be processed correctly. This is especially important if you are using multi-chat , inline or single-chat, multi-use invoices. 8. Checkout In case the bot confirms the order, Telegram requests the payment provider to complete the transaction. If the payment information was entered correctly and the payment goes through, the API will send a receipt message of the type successful_payment from the user. Once your bot receives this message, it should proceed with delivering the goods or services purchased by the user. If the invoice message was sent in the chat with @merchantbot , it becomes a Receipt in the UI for the user — they can open this receipt at any time and see all the details of the transaction: Receipt If the message was sent to any other chat, the Pay button remains and can be used again. It is up to the merchant bot whether to actually accept multiple payments. Going Live Once you've tested everything and confirmed that your payments implementation works, you're ready to switch to LIVE MODE. To do this, go to BotFather > /mybots > select @merchantbot > Bot Settings / Payments and enable Stripe LIVE MODE. You will get a token that has the string :LIVE: in the middle, e.g. 123:LIVE:XXXX . Do not give this token to any third parties! Before your merchant bot goes into live mode, please ensure the following: Live Checklist We highly recommend turning on 2-step verification for the Telegram account that controls your bot. You as the bot owner have full responsibility in case any conflicts or disputes arise. You must be prepared to correctly process disputes and chargebacks (in the case of Stripe, see here ). To prevent any misunderstandings and possible legal issues, make sure your bot can respond to a /terms command (or offers a similarly easy way of accessing your Terms and Conditions). Your Terms and Conditions should be written in a clear way and easy to understand for your users. The users must confirm that they have read and agree to your terms before they make the purchase. Your bot must provide support for its customers, either by responding to a /support command or by some other clearly communicated means. Users must have a clear way of contacting you about their purchases and you must process their support requests in a timely fashion. You must notify your users that Telegram support or bot support will not able to help them with purchases made via your bot. Make sure that your server hardware and software is stable. Use backups to make sure that you don't lose data about your users' payments. Make sure that you have completed the live checklist for your selected payments provider as well as this one. FAQ How do I join as a payment provider? If you work for a company that provides services similar to standalone accounts in Stripe Connect , please let us know via @BotSupport (kindly include the hashtag #paymentsprovider in your message). How much do you charge? Telegram does not charge any commission for using the Payments API. Note though, that most payment providers will have their own commissions. For example, Stripe in the US charges 2.9% + 30¢ per successful card charge (see the Stripe website for more details on pricing). Do I need a bot to accept payments? Yes. If you are not a developer, you will need to either hire someone to make a bot for you (recommended), or use a bot created by a third-party company. We advise extreme caution when using services of bots that process payments for you – Telegram doesn't maintain any such bots and doesn't endorse any of the third-party bots offering these services. What can my bot sell? Telegram does not impose any limits on what products or services your bot can offer. But please note that you must comply with the rules of the payments provider you choose in our system. E.g., Stripe has a special page for prohibited businesses – you may want to consult that one before you start selling harvested organs. Special Note: Due to Apple's limitations, bot developers are currently not allowed to accept payments for digital goods and virtual services from iOS users . UPD 2024: Thanks to recent changes in the Apple Review Guidelines, users will soon be able to pay for digital goods and services with Telegram Stars on all platforms. How are disputes handled? Telegram acts as a messenger between the paying user, the bot developer, and their chosen payment system. The user sends their credit card details directly to the payment system. Then the payment system's response and the shipping details entered by the user are passed to the bot developer so that they can process the order. Since Telegram doesn‘t process the payments, we don’t store and can‘t access any sensitive data. Due to this structure, it is impossible for Telegram to handle complaints or cashbacks – any disputed payments are the responsibility of the bot developers, payment providers, and banks that participated in the exchange. See also: Telegram Privacy Policy How do I support payments in my third-party app that uses the Telegram API? You are welcome to study the MTProto payment documentation . Supported Currencies Telegram payments currently support the currencies listed below (here's a JSON version in case you need it). If you're using Stripe as the payments provider, supported currencies may vary depending on the country you have specified in your Stripe account ( more info ). The minimum and maximum amounts for each of the currencies roughly correspond to the limit of US$ 1-10000 . The amount must be expressed in 12 digits or less, so the maximum value will be correspondingly lower for some lower-value currencies. Note that for each currency except USD these limits depend on exchange rates and may change over time (plan ahead for this when you implement limits in your code). Code Title Min amount Max amount AED United Arab Emirates Dirham AED 3.67 AED 36,729.82 AFN Afghan Afghani AFN70.25 AFN702,466.74 ALL Albanian Lek 93,69ALL 936.913,24ALL AMD Armenian Dram 393.46 AMD 3,934,576.23 AMD ARS Argentine Peso ARS 1.020,57 ARS 10.205.730,09 AUD Australian Dollar AU$1.58 AU$15,783.95 AZN Azerbaijani Manat 1,70 AZN 16 956,94 AZN BAM Bosnia & Herzegovina Convertible Mark 1,86 BAM 18.641,58 BAM BDT Bangladeshi Taka BDT 119.50 BDT 1,194,955.96 BGN Bulgarian Lev 1,86 BGN 18 631,75 BGN BHD Bahraini dinar BHD 0.377 BHD 3,770.580 BND Brunei Dollar BND1,35 BND13.508,71 BOB Bolivian Boliviano BOB 6,91 BOB 69.102,24 BRL Brazilian Real R$ 6,09 R$ 60.944,03 BYN Belarusian ruble 3,27 BYN 32 725,23 BYN CAD Canadian Dollar CA$1.43 CA$14,310.90 CHF Swiss Franc 0.89 CHF 8'924.99 CHF CLP Chilean Peso CLP 983 CLP 9.826.095 CNY Chinese Renminbi Yuan CN¥7.28 CN¥72,848.01 COP Colombian Peso COP 4.345,53 COP 43.455.300,00 CRC Costa Rican Colón CRC503,36 CRC5.033.550,65 CZK Czech Koruna 23,92 CZK 239 160,17 CZK DKK Danish Krone 7,11 DKK 71115,10 DKK DOP Dominican Peso DOP60.70 DOP606,961.76 DZD Algerian Dinar DZD 133.92 DZD 1,339,201.21 EGP Egyptian Pound EGP 50.78 EGP 507,830.01 ETB Ethiopian Birr ETB127.00 ETB1,269,955.96 EUR Euro 0,95 € 9 533,05 € GBP British Pound £0.79 £7,866.40 GEL Georgian Lari 2,81 GEL 28 100,83 GEL GHS Ghanaian cedi GHS14.70 GHS146,991.68 GTQ Guatemalan Quetzal GTQ7.70 GTQ77,022.79 HKD Hong Kong Dollar HK$7.77 HK$77,693.10 HNL Honduran Lempira HNL 25.38 HNL 253,793.51 HRK Croatian Kuna 7,17 HRK 71.729,06 HRK HUF Hungarian Forint 390,50 HUF 3 904 995,01 HUF IDR Indonesian Rupiah IDR16.050,00 IDR160.500.000,00 ILS Israeli New Sheqel ₪ 3.59 ₪ 35,926.01 INR Indian Rupee ₹84.89 ₹848,948.02 IQD Iraqi dinar IQD 1,310.000 IQD 13,100,003.810 IRR Iranian rial 42,087/50 IRR 420,875,043/20 IRR ISK Icelandic Króna 138 ISK 1.379.502 ISK JMD Jamaican Dollar JMD156.51 JMD1,565,061.57 JOD Jordanian dinar JOD0.709 JOD7,093.970 JPY Japanese Yen ¥153 ¥1,533,675 KES Kenyan Shilling KES129.25 KES1,292,501.72 KGS Kyrgyzstani Som 87-00 KGS 869 997-88 KGS KRW South Korean Won ₩1,438 ₩14,380,650 KZT Kazakhstani Tenge KZT524-11 KZT5 241 076-96 LBP Lebanese Pound LBP 89,548.11 LBP 895,481,146.82 LKR Sri Lankan Rupee LKR 291.15 LKR 2,911,472.03 MAD Moroccan Dirham MAD 10.00 MAD 100,019.06 MDL Moldovan Leu 18.30 MDL 183,041.67 MDL MMK Myanmar kyat MMK3,247.96 MMK32,479,609.92 MNT Mongolian Tögrög MNT3 398,00 MNT33 979 998,70 MOP Macanese pataca MOP8.00 MOP80,020.21 MUR Mauritian Rupee MUR46.50 MUR465,020.94 MVR Maldivian Rufiyaa 15.40 MVR 153,986.78 MVR MXN Mexican Peso MX$20.22 MX$202,231.70 MYR Malaysian Ringgit MYR4.47 MYR44,665.00 MZN Mozambican Metical MZN63.90 MZN639,018.04 NGN Nigerian Naira NGN1,553.09 NGN15,530,903.22 NIO Nicaraguan Córdoba NIO 36.80 NIO 368,003.43 NOK Norwegian Krone NOK 11,20 NOK 112 035,25 NPR Nepalese Rupee NPR135.92 NPR1,359,153.99 NZD New Zealand Dollar NZ$1.74 NZ$17,378.15 PAB Panamanian Balboa PAB 1.00 PAB 9,999.33 PEN Peruvian Nuevo Sol PEN 3.74 PEN 37,368.94 PHP Philippine Peso PHP58.99 PHP589,910.05 PKR Pakistani Rupee PKR278.19 PKR2,781,861.25 PLN Polish Złoty 4,07 PLN 40 702,20 PLN PYG Paraguayan Guaraní PYG 7.817 PYG 78.174.592 QAR Qatari Riyal QAR 3.65 QAR 36,459.64 RON Romanian Leu 4,74 RON 47.428,03 RON RSD Serbian Dinar 111,49 RSD 1.114.884,98 RSD RUB Russian Ruble 109,82 RUB 1 098 237,63 RUB SAR Saudi Riyal SAR 3.76 SAR 37,576.71 SEK Swedish Krona 10,97 SEK 109.664,95 SEK SGD Singapore Dollar SGD1.35 SGD13,497.95 SYP Syrian pound SYP 2,512.53 SYP 25,125,302.43 THB Thai Baht ฿34.18 ฿341,801.25 TJS Tajikistani Somoni 10;92 TJS 109 247;40 TJS TRY Turkish Lira 34,99 TRY 349.880,65 TRY TTD Trinidad and Tobago Dollar TTD6.79 TTD67,896.12 TWD New Taiwan Dollar NT$32.53 NT$325,285.02 TZS Tanzanian Shilling TZS2,354.97 TZS23,549,660.05 UAH Ukrainian Hryvnia 41,87UAH 418 728,32UAH UGX Ugandan Shilling UGX3,640 UGX36,400,029 USD United States Dollar $1.00 $10,000.00 UYU Uruguayan Peso UYU 44,58 UYU 445.780,95 UZS Uzbekistani Som 12 869,34 UZS 128 693 430,94 UZS VND Vietnamese Đồng 25.440 ₫ 254.400.000 ₫ YER Yemeni Rial YER 250.38 YER 2,503,750.09 ZAR South African Rand ZAR 18.09 ZAR 180,925.50 Telegram Telegram is a cloud-based mobile and desktop messaging app with a focus on security and speed. About FAQ Privacy Press Mobile Apps iPhone/iPad Android Mobile Web Desktop Apps PC/Mac/Linux macOS Web-browser Platform API Translations Instant View About Blog Press Moderation",
            "meta_description": "",
            "links": [
                "https://core.telegram.org/bots/payments-stars",
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/faq",
                "https://core.telegram.org/bots/payments-stars",
                "https://core.telegram.org/bots/payments",
                "https://core.telegram.org/bots/inline",
                "https://core.telegram.org/bots/inline",
                "https://core.telegram.org/bots/payments/currencies.json"
            ],
            "timestamp": "2024-12-27T22:28:22.038011",
            "status_code": 200
        },
        {
            "url": "https://core.telegram.org/bots/payments-stars",
            "title": "Bot Payments API for Digital Goods and Services",
            "text_content": "Bot Payments API for Digital Goods and Services Twitter Home FAQ Apps API Protocol Schema Bot Payments API for Digital Goods and Services Telegram Bot Payments are a free and open platform that allows sellers to accept payments for goods and services from Telegram users. This page covers payments for digital goods and services – if you’re interested in selling physical goods and services , please check out our dedicated page . Note: This article is intended for bot developers and store owners. If you're looking for a general overview of Telegram Payments, check out the Telegram blog . If you are new to Telegram bots and would like to learn how to create and set up a bot, please consult our Introduction to Bots and Bot FAQ . Overview Payments are seamlessly integrated into Telegram, allowing you to sell digital goods and services in exchange for Telegram Stars , an in-app virtual item that you can later convert to a reward. With the Payments API, you’ll be able to: Send invoices to any chat , including to groups and channels. Create invoices that can be forwarded and used by multiple buyers to order things. Use inline mode to help users show your goods and services to their friends and communities. Accept payments from users on mobile or desktop apps . Try @DurgerKingBot to create a test invoice – or start a message with @DurgerKingBot ... in any chat for an inline invoice . Check out Demo Shop for an example of a Telegram Channel used as virtual storefront . Star Pricing Here is a table showing a breakdown of how much a user pays to acquire certain amounts of Stars, and the net proceeds received by the developer. Note that these amounts may vary from user to user due to VAT and other applicable fees that are outside of Telegram’s control. How does this work? You create a bot that offers digital goods and services to Telegram users. Merchant bots can send specially formatted invoice messages to users, groups or channels. If your bot supports inline mode , users can also send invoices to other chats via the bot , including to 1-on-1 chats with other users. Invoice messages feature a photo and description of the product along with a prominent Pay button. Tapping this button opens a special payment interface in the Telegram app. Since your bot sells digital goods and services, all transactions must be carried out in Telegram Stars, with currency tag XTR . Users can acquire Stars from Telegram using standard Apple and Google in-app purchases or via @PremiumBot and then use them to buy digital goods and services from you. The invoice interface for digital goods and services is seamless and doesn’t require users to enter any personal information, like their full name, shipping address or credit card details. Detailed information and step-by-step instructions are available below. The Payments API This section explores payments via Telegram's Bot API in more detail. This section is dedicated to the sale of digital goods and services – if you’re interested in selling physical goods and services , please check out our dedicated page . Creating a Telegram Bot To start accepting payments, you need a Telegram bot . Use @BotFather to create a bot if you don't have one already. For the purposes of this document, we will assume you named your bot @merchantbot . Implementing Payments You will find the necessary methods for building your payment implementation in the Payments Section of the Bot API Manual . In short, you must: Send an invoice via sendInvoice ( currency :  “XTR”) Await an Update with the field pre_checkout_query Approve or cancel the order via answerPreCheckoutQuery Await an Update with the field successful_payment Store the SuccessfulPayment ’s telegram_payment_charge_id – it may be needed to issue a refund in the future Deliver the goods and services purchased by the user You may find that some API methods for Payments request a provider_token . This parameter is only needed for sales of physical goods and services – for digital ones, you can leave it empty. Testing Payments You can freely test payments in Telegram Stars by connecting your bot to Telegram’s dedicated test environment. To find out more about how you can access and use the test environment, please reference our documentation . Step-by-Step Process See Bot API: Payments for the complete list of available methods and objects. Be sure to provide the proper set of parameters to account for the type of goods you are selling, differentiating between digital and physical orders. 1. Create Invoice The user contacts @merchantbot and requests to purchase something. The bot forms an invoice message with a description of the goods or service and amount to be paid (expressed in Telegram Stars). There are two ways of creating an invoice: A. Bot Invoice Use the sendInvoice method to generate an invoice and send it to a chat. You can pass an empty string as the provider_token parameter, since the invoice is for digital goods and services. Invoice messages with a pay button can be sent to chats of any type : private chats with the user, groups, or channels. The resulting invoice message will look like this: Bot Invoice Remember to specify XTR in the currency field, since all sales of digital goods and services are carried out exclusively in Telegram Stars. B. Inline Invoice If @merchantbot supports inline mode , you can use inputInvoiceMessageContent to allow users to share invoices for your goods and services to their one-on-one chats with friends, or to their groups and channels. These invoices will have a Pay button that can be used multiple times. Inline Invoice 2. Choose Forwarding Behavior There are two ways for handling forwarded copies of your invoices, controlled by the parameter start_parameter in the sendInvoice method. A. Multi-chat invoice. Forwarded copies show a Pay button , which multiple users can press and attempt to pay for the goods or services. Inline invoices are always multi-chat invoices. B. Single-chat invoice. Invoice can only be paid from the chat to which it was sent, forwarded copies show a URL button with a deep link to the bot. The deep link can be used to generate a similar invoice in the chat with the bot, to show an error message, or for other purposes. More info on Deep Linking » If a single-chat invoice is sent to the chat with @merchantbot , it can only be paid once . If a single-chat invoice is sent to any other chat, it can be paid many times by many users. To get a better understanding of how this works, try toggling the “Pay from Forwards” parameter when creating invoices with our demo @ShopBot . Regardless of whether or not the Pay button is available in an invoice, the merchant bot always has the power to decide whether or not to accept new payments for a particular invoice. 3. Pre-Checkout The user purchases Stars from Telegram (if necessary), then presses the pay button. At this moment the Bot API sends an Update with the field pre_checkout_query that contains all the available information about the order to the bot. Your bot must reply using answerPrecheckoutQuery within 10 seconds after receiving this update or the transaction is canceled. The bot may return an error if it can't process the order for any reason. We highly recommend specifying a reason for failure to complete the order in human readable form (e.g. \"Sorry, we're all out of rubber duck digital posters! Would you be interested in a cast iron bear digital poster instead?\" ). Telegram will display this reason to the user. Warning: It is critical to make sure your bot only accepts multiple payments when the order can be processed correctly. This is especially important if you are using multi-chat , inline or single-chat, multi-use invoices. 4. Checkout If the bot confirms the order and the payment is successful, the API will send a receipt message of the type successful_payment from the user. Once your bot receives this message, it should proceed with sending the digital goods or services purchased by the user. Warning: You must always check that you received a successful_payment update before delivering the goods or services purchased by the user – simply answering a pre_checkout_query does not guarantee a successful order or payment. If the invoice message was sent in the chat with @merchantbot , it becomes a Receipt in the UI for the user – they can open this receipt at any time and see all the details of the transaction: Receipt If the message was sent to any other chat, the Pay button remains and can be used again. It is up to the merchant bot whether to actually accept multiple payments. Going Live Once you've tested everything and confirmed that your payments implementation works, you're ready to go live – to do so, simply switch to the production environment. Before your merchant bot goes into live mode, please ensure the following: Live Checklist We highly recommend turning on 2-step verification for the Telegram account that controls your bot. You as the bot owner have full responsibility in case any conflicts or disputes arise. You must be prepared to correctly process disputes and chargebacks. To prevent any misunderstandings and possible legal issues, make sure your bot can respond to a /terms command (or offers a similarly easy way of accessing your Terms and Conditions). Your Terms and Conditions should be written in a clear way and easy to understand for your users. The users must confirm that they have read and agree to your terms before they make the purchase. Your bot must provide support for its customers, either by responding to a /support command or by some other clearly communicated means. Users must have a clear way of contacting you about their purchases and you must process their support requests in a timely fashion. You must notify your users that Telegram support or bot support will not be able to help them with purchases made via your bot. Verify your server hardware and software are stable. Use backups to make sure that you don't lose data about your users' payments. Make sure that you have read, understood and agreed to the Telegram Terms of Service and, by extension, to the Telegram Bot Platform Developer Terms of Service . FAQ Q: Can I use a different currency or payment provider? Payments for digital goods and services must be carried out exclusively in Telegram Stars. App stores like the Play Store for Android and the App Store for iOS have specific rules for developers regarding the sale of digital goods and services. Due to these rules, Telegram cannot display your bot or mini-app to mobile users if you attempt to sell digital goods and services via other currencies or through third-party payment providers. If you’re selling physical goods or services, you may use different currencies or payment providers. Please check out our dedicated document to learn more. Q: I run my own website to process sales, does this mean I don’t have to use Telegram Stars? To remain in compliance with Google’s Payment Policies 1, 2 and 4, along with Apple’s Review Guidelines 3.1.1, 3.1.1(a) and 3.1.3(b), your bot or mini app must use Telegram Stars for the sale of digital goods and services inside Telegram apps, regardless of any other web portals, apps, services or payment providers you may have set up outside the Telegram ecosystem. Q: Can I accept payment in cryptocurrency instead of Telegram Stars? No. All payments via bots and mini apps for digital goods and services inside Telegram apps must exclusively use Telegram Stars, in order to comply with the guidelines of the App Store and Play Store. For more details on what terms regulate your engagement with Telegram Stars, please reference Section 6.2. Digital Goods and Services of the Telegram Bot Platform Developer Terms of Service. Q: Do I need a bot to accept payments? Yes. If you are not a developer, you will need to either hire someone to make a bot for you (recommended), or use a bot created by a third-party company. We advise extreme caution when using services of bots that process payments for you – Telegram doesn't maintain any such bots and doesn't endorse any of the third-party bots offering these services. Q: How are disputes handled? You are solely responsible for processing and rectifying legitimate user disputes for digital goods and services sold by your bots or mini apps. Your bots and mini apps must be able to respond to the command /paysupport and process user requests regarding payment issues , and you must provide customer support in a timely fashion. The Telegram Bot API also allows you to easily and independently refund purchases that were made on your bot or mini app with Telegram Stars. For more details on customer disputes, please reference Section 6.2.1. Payment Disputes for Digital Goods and Services of the Telegram Bot Platform Developer Terms of Service. Q: How can I issue refunds? Refunds for digital goods and services can be issued via the refundStarPayment method on the Telegram Bot API. Q: What can I do with Telegram Stars? Stars can be used to place Telegram Ads for your TPA or to accept a reward for the monetary value that Telegram assigns to each Star. For more details on what terms regulate your engagement with Telegram Stars, please reference Section 6.2. Digital Goods and Services of the Telegram Bot Platform Developer Terms of Service. Q: How do I support payments in my third-party app that uses the Telegram API? You are welcome to study the MTProto payment documentation . Telegram Telegram is a cloud-based mobile and desktop messaging app with a focus on security and speed. About FAQ Privacy Press Mobile Apps iPhone/iPad Android Mobile Web Desktop Apps PC/Mac/Linux macOS Web-browser Platform API Translations Instant View About Blog Press Moderation",
            "meta_description": "",
            "links": [
                "https://core.telegram.org/bots/payments",
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/faq",
                "https://core.telegram.org/bots/inline",
                "https://core.telegram.org/bots/inline",
                "https://core.telegram.org/bots/payments",
                "https://core.telegram.org/bots/inline",
                "https://core.telegram.org/bots/payments"
            ],
            "timestamp": "2024-12-27T22:28:22.330181",
            "status_code": 200
        },
        {
            "url": "https://core.telegram.org/bots/games",
            "title": "Gaming Platform",
            "text_content": "Gaming Platform Twitter Home FAQ Apps API Protocol Schema Telegram Bots Gaming Platform Gaming Platform Bots can offer their users HTML5 games to play solo or to compete against each other in groups and one-on-one chats. Games are a new type of content on Telegram that your bot can send to users. Check out the @gamebot and @gamee bots for examples of what you can do using the new Gaming Platform. Web Apps Since April 2022 , you can also use Web Apps to create powerful games using JavaScript . Check out the Web App Manual for details. Creating a Game To get started, send the /newgame command to @BotFather .You will be prompted for a description text and a photo. You can also upload an optional GIF animation that demostrates your game to the users to make messages with the game more attractive (check out Lumberjack or Corsairs for examples). Launching the Game Once the game is created, your bot can send it to chats as regular messages, or offer them via inline mode . The game message will always have an inline Play button . When this button is pressed, your bot gets a callback query that indicates the requested game. You provide the correct URL for this particular user and the app automatically opens the game in the in-app browser. Adding Buttons If you send the game message without any buttons, it will automatically have a 'Play GameName ' button. You can manually add multiple buttons to your game message. Please note that the first button in the first row must always be the one that launches the game. You can add more buttons: e.g., for a description of the rules, or a button that links to the game's official community. Tracking High Scores The message with your game will also display high scores for the current chat. When a new high score is set, a service message will be sent to the chat and the message with the current scoreboard will be updated. You can also request the necessary data for building in-game high score tables. Sharing Your Game to Telegram Chats There are many way for users to spread your game virally if they like it. The interface will always have the standard system button for sharing the game in the top right corner: You can also create an additional Share button inside your HTML page. Pressing this button will send the game to a desired chat along with the user's best score in the game. To add the sharing button, include this script at the end of the <body> tag on your page: <script src=\"https://telegram.org/js/games.js\"></script> Then use the method TelegramGameProxy.shareScore() to call the sharing option. Warning: Do not call this method without consent and direct action from the user. Example: <button onclick=\"TelegramGameProxy.shareScore()\">Share score</button> This library will only work when launched from inside Telegram, so please don't use it on ordinary web pages. Using URL Parameters If your URL is using a fragment identifier, please note that Telegram Apps could add certain service parameters to the fragment id. The names for such parameters will start in tg (you can check the code that adds them here ). Use the TelegramGameProxy.initParams object if you need to read your own parameters from the fragment id. Creating a Great HTML5 Experience Please make sure that your HTML5 page is responsive and works well on all Telegram apps and supported platforms. If you find it impossible to support certain conditions or platforms, don't leave your users hanging and at least provide a notification. See the Bot API Manual for the relevant methods and objects. Telegram Telegram is a cloud-based mobile and desktop messaging app with a focus on security and speed. About FAQ Privacy Press Mobile Apps iPhone/iPad Android Mobile Web Desktop Apps PC/Mac/Linux macOS Web-browser Platform API Translations Instant View About Blog Press Moderation",
            "meta_description": "",
            "links": [
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/games",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/webapps",
                "https://core.telegram.org/bots/inline"
            ],
            "timestamp": "2024-12-27T22:28:22.528320",
            "status_code": 200
        },
        {
            "url": "https://core.telegram.org/bots/tutorial",
            "title": "From BotFather to 'Hello World'",
            "text_content": "From BotFather to 'Hello World' Twitter Home FAQ Apps API Protocol Schema Telegram Bots From BotFather to 'Hello World' From BotFather to 'Hello World' This guide will walk you through everything you need to know to build your first Telegram Bot . If you already know your way around some of the basic steps, you can jump directly to the part you're missing. Equivalent examples are available in C# , Python , Go and TypeScript . Introduction Basic Tutorial Environment First Run Echo Bot Advanced Tutorial Commands Navigation Database Hosting Further Reading Introduction At its core, you can think of the Telegram Bot API as software that provides JSON-encoded responses to your queries. A bot, on the other hand, is essentially a routine, software or script that queries the API by means of an HTTPS request and waits for a response. There are several types of requests you can make, as well as many different objects that you can use and receive as responses. Since your browser is capable of sending HTTPS requests, you can use it to quickly try out the API. After obtaining your token , try pasting this string into your browser: https://api.telegram.org/bot<YOUR_BOT_TOKEN>/getMe In theory, you could interact with the API with basic requests like this, either via your browser or other tailor-made tools like cURL . While this can work for simple requests like the example above, it's not practical for larger applications and doesn't scale well. For that reason, this guide will show you how to use libraries and frameworks , along with some basic programming skills , to build a more robust and scalable project. If you know how to code, you'll fly right through each step in no time – and if you're just starting out, this guide will show you everything you need to learn. We will use Java throughout this guide as it's one of the most popular programming languages, however, you can follow along with any language as all the steps are fundamentally the same. Since Java is fully cross-platform, each code example will work with any operating system. If you pick another language, equivalent examples are available in C# , Python , Go and TypeScript . Getting Ready First, we will briefly cover how to create your first project , obtain your API token and download all necessary dependencies and libraries . For the purposes of this guide, a copy of the bot you will be creating is also live at @TutorialBot – feel free to check it out along the way to see how your own implementation should look after each step. Obtain Your Bot Token In this context, a token is a string that authenticates your bot (not your account) on the bot API. Each bot has a unique token which can also be revoked at any time via @BotFather . Obtaining a token is as simple as contacting @BotFather , issuing the /newbot command and following the steps until you're given a new token. You can find a step-by-step guide here . Your token will look something like this: 4839574812:AAFD39kkdpWt3ywyRZergyOLMaJhac60qc Make sure to save your token in a secure place, treat it like a password and don't share it with anyone . Download an IDE To program in Java you'll need an IDE – a special text editor that will let you write, compile and run your code. In this tutorial, we'll use IntelliJ – there are several free, open source alternatives like Eclipse or NetBeans which work in the exact same way. You will also need a JDK , a software kit that allows your Java code to run. Most IDEs don't include a JDK, so you should download a version compatible with your operating system separately. You can find a free, open source version here . If you use another language, the steps are identical. You will just have to download a different IDE and software development kit. Pick a Framework or Library You can think of a framework as software that handles all the low-level logic for you, including the API calls, and lets you focus on your bot-specific logic. In this tutorial, we'll use TelegramBots , but you can follow along with any equivalent implementation, since all the underlying methods are either similar or exactly the same. You can find many frameworks, along with code examples, in our dedicated list . Create Your Project In IntelliJ, go to File > New > Project . Fill in the fields accordingly: Name - The name of your project. For example, BotTutorial . Location - Where to store your project. You can use the default value. Language - Java Build System - The framework that will handle your dependencies. Pick Maven . JDK - Pick whichever version you downloaded. We'll be using version 17 . Add Sample Code - Leave this selected , it will generate some needed files for you. Advanced Settings > GroupId - We suggest tutorial . Advanced Settings > ArtifactId - You can use the default value. After hitting Create , if you did everything correctly, your Project view in the top left should show a project structure along these lines: BotTutorial\n├─ .idea\n├─ src\n│  └─ main\n│     └─ java\n│        └─ tutorial\n│           └─ Main\n└─ pom.xml Other IDEs will follow a similar pattern. Your dependency management system will have a different name (or no name at all if it's built-in) depending on the language you chose. If this looks scary, don't worry. We will only be using the Main file and the pom.xml file. In fact, to check that everything is working so far, double click on Main and click on the small green arrow on the left of public class Main , then select the first option. If you followed the steps correctly, Hello world! should appear in the console below. Add Framework Dependency We will now instruct the IDE to download and configure everything needed to work with the API. This is very easy and happens automatically behind the scenes. First, locate your pom.xml file on the left side of the screen. Open it by double-clicking and simply add: <dependencies>\n    <dependency>\n        <groupId>org.telegram</groupId>\n        <artifactId>telegrambots</artifactId>\n        <version>6.0.1</version>\n    </dependency>\n</dependencies> right after the </properties> tag. When you're done, your pom.xml should look something like this . Start Coding We are ready to start coding. If you're a beginner, consider that being familiar with your language of choice will greatly help. With this tutorial, you'll be able to teach your bot basic behaviors, though more advanced features will require some coding experience. Creating a Bot Class If you're familiar with object-oriented programming , you'll know what a class is. If you've never heard of it before, consider a class as a file where you write some logic. To create the class that will contain the bot logic, right click on tutorial from the project tree on the left and select New > Java Class . Name it Bot and hit enter. Now we have to connect this class to the bot framework. In other words, we must make sure it extends TelegramLongPollingBot . To do that, just add extends TelegramLongPollingBot right after Bot . A red line will appear – it simply means we're missing some important methods. To fix this, hover over the red line, click on implement methods , then hit OK. Depending on the IDE, this option may be called implement missing methods or something similar. You should end up with this – if something went wrong, feel free to copy it from here and paste it in your class: package tutorial;\nimport org.telegram.telegrambots.bots.TelegramLongPollingBot;\nimport org.telegram.telegrambots.meta.api.objects.Update;\n\npublic class Bot extends TelegramLongPollingBot {\n\n  @Override\n  public String getBotUsername() {\n      return null;\n  }\n\n  @Override\n  public String getBotToken() {\n      return null;\n  }\n\n  @Override\n  public void onUpdateReceived(Update update) {}\n\n} If you get a red line under TelegramLongPollingBot, it means you didn't set up your pom.xml correctly. If this is the case, restart from here . Available Methods Let's look into these 3 methods one by one. getBotUsername - This method must be edited to always return your bot's username. You should replace the null return value with it. getBotToken - This method will be used by the framework to retrieve your bot token. You should replace the null return value with the token. onUpdateReceived - This is the most important method. It will be called automatically whenever a new Update is available. Let's add a System.out.println(update); call in there to quickly show what we are getting. After you've replaced all the strings, you should end up with this: @Override\npublic String getBotUsername() {\n    return \"TutorialBot\";\n}\n\n@Override\npublic String getBotToken() {\n    return \"4839574812:AAFD39kkdpWt3ywyRZergyOLMaJhac60qc\";\n}\n\n@Override\npublic void onUpdateReceived(Update update) {\n    System.out.println(update);\n} At this point, the bot is configured and ready to go – time to register it on the API and start processing updates. In the future, you should consider storing your token in a dedicated settings file or in environment variables . Keeping it in the code is fine for the scope of this tutorial, however, it's not very versatile and is generally considered bad practice. Registering the Bot To register the bot on the API, simply add a couple of lines in the main method that will launch the application. If you named your class Bot , this is what your main method should look like: public static void main(String[] args) throws TelegramApiException {\n  TelegramBotsApi botsApi = new TelegramBotsApi(DefaultBotSession.class);\n  botsApi.registerBot(new Bot());\n} You can place this method in any class. Since we have an auto-generated main method in the Main class, we'll be using that one for this tutorial. First Run It's time to run your bot for the first time. Hit the green arrow to the left of public static void main and select the first option. And then there was nothing . Yes, a bit anticlimactic. This is because your bot has nothing to print – there are no new updates because nobody messaged it yet. If you try messaging the bot on Telegram, you'll then see new updates pop up in the console. At this point, you have your very own Telegram Bot – quite the achievement. Now, on to making it a bit more intelligent. If nothing pops up, make sure you messaged the right bot and that the token you pasted in the code is correct. Receiving Messages Every time someone sends a private message to your bot, your onUpdateReceived method will be called automatically and you'll be able to handle the update parameter, which contains the message , along with a great deal of other info which you can see detailed here . Let's focus on two values for now: The user - Who sent the message. Access it via update.getMessage().getFrom() . The message - What was sent. Access it via update.getMessage() . Knowing this, we can make it a bit more clear in the console output . @Override\npublic void onUpdateReceived(Update update) {\n    var msg = update.getMessage();\n    var user = msg.getFrom();\n\n    System.out.println(user.getFirstName() + \" wrote \" + msg.getText());\n} This is just a basic example – you can now play around with all the methods to see everything you can pull out of these objects. You can try getUsername , getLanguageCode , and dozens more. Knowing how to receive, process and print incoming messages , now it's time to learn how to answer them . Remember to stop and re-launch your bot after each change to the code. Sending Messages To send a private text message, you generally need three things : The user must have contacted your bot first. (Unless the user sent a join request to a group where your bot is an admin, but that's a more advanced scenario). You must have previously saved the User ID ( user.getId() ) A String object containing the message text, 1-4096 characters. With that out of the way, let's create a new method to send the first message: public void sendText(Long who, String what){\n   SendMessage sm = SendMessage.builder()\n                    .chatId(who.toString()) //Who are we sending a message to\n                    .text(what).build();    //Message content\n   try {\n        execute(sm);                        //Actually sending the message\n   } catch (TelegramApiException e) {\n        throw new RuntimeException(e);      //Any error will be printed here\n   }\n} And proceed to run this in the main method, right after registering the bot. For this example, we'll assume your User ID is 1234 . public static void main(String[] args) throws TelegramApiException {\n   TelegramBotsApi botsApi = new TelegramBotsApi(DefaultBotSession.class);\n   Bot bot = new Bot();                  //We moved this line out of the register method, to access it later\n   botsApi.registerBot(bot);            \n   bot.sendText(1234L, \"Hello World!\");  //The L just turns the Integer into a Long\n} If you did everything correctly, your bot should text you Hello World! every time you launch your code. Sending messages to groups or channels – assuming you have the relevant permissions – is as simple as replacing 1234 with the ID of the respective chat. Try experimenting with other types of messages, like SendPhoto, SendSticker, SendDice… A full list is available starting here . Echo Bot Let's practice everything we tried so far by coding an Echo Bot . Its functionality will be rather simple: every text message it receives will be sent right back to the user. Copying Text The most intuitive way of coding this is saving the User ID and calling sendText right after each update. In other words: @Override\npublic void onUpdateReceived(Update update) {\n    var msg = update.getMessage();\n    var user = msg.getFrom();\n    var id = user.getId();\n\n    sendText(id, msg.getText());\n} This works for text but can be extended to stickers, media and files. Copying Everything There are more specific functions that can be used to copy messages and send them back. Let's build a method to do just that: public void copyMessage(Long who, Integer msgId){\n   CopyMessage cm = CopyMessage.builder()\n              .fromChatId(who.toString())  //We copy from the user\n           .chatId(who.toString())      //And send it back to him\n           .messageId(msgId)            //Specifying what message\n           .build();\n    try {\n        execute(cm);\n    } catch (TelegramApiException e) {\n        throw new RuntimeException(e);\n    }\n} After replacing the method call in onUpdateReceived , running the code will result in a fully functional Echo Bot . This tutorial assumes that updates always contain messages for the sake of simplicity. This may not always be true – be sure to implement all the proper checks in your code to handle every type of update with the appropriate methods. Executing Commands To learn what a command is and how it works, we recommend reading this dedicated summary . In this guide, we'll focus on the technical side of things. Creating Your Command Begin by opening @BotFather . Type /mybots > Your_Bot_Name > Edit Bot > Edit Commands. Now send a new command, followed by a brief description. For the purpose of this tutorial, we'll implement two simple commands: scream - Speak, I'll scream right back \nwhisper - Shhhhhhh Command Logic We want the Echo Bot to reply in uppercase when it's in scream mode and normally otherwise. First, let's create a variable to store the current mode. public class Bot extends TelegramLongPollingBot {\n\n   private boolean screaming = false;\n\n   [...]\n} Then, let's change some logic to account for this mode . public void onUpdateReceived(Update update) {\n    [...]                                   //Same variables as the previous versions\n   if(screaming)                            //If we are screaming\n       scream(id, update.getMessage());     //Call a custom method\n   else\n       copyMessage(id, msg.getMessageId()); //Else proceed normally\n}\n\nprivate void scream(Long id, Message msg) {\n   if(msg.hasText())\n       sendText(id, msg.getText().toUpperCase());\n   else\n       copyMessage(id, msg.getMessageId());  //We can't really scream a sticker\n} Finally, let's add a couple more lines to the onUpdateReceived method to process each command before replying. if(msg.isCommand()){ \n   if(msg.getText().equals(\"/scream\"))         //If the command was /scream, we switch gears\n      screaming = true;\n   else if (msg.getText().equals(\"/whisper\"))  //Otherwise, we return to normal\n      screaming = false;\n\n   return;                                     //We don't want to echo commands, so we exit\n} As you can see, it checks if the message is a command . If it is, the bot enters scream mode . In the update method, we check which mode we are in and either copy the message or convert it to upper case before sending it back . And that's it. Now the bot can execute commands and change its behavior accordingly. Naturally, this simplified logic will change the bot's behavior for everyone – not just the person who sent the command. This can be fun for this tutorial but won't work in a production environment – consider using a Map, dictionary or equivalent data structure to assign settings for individual users. Remember to always implement a few basic global commands . You can practice by implementing a simple feedback to the /start command, which we intentionally left out. Buttons and Keyboards To streamline and simplify user interaction with your bot, you can replace many text-based exchanges with handy buttons. These buttons can perform a wide variety of actions and can be customized for each user. Button Types There are two main types of buttons: Reply Buttons - used to provide a list of predefined text reply options . Inline Buttons - used to offer quick navigation, shortcuts, URLs, games and so much more . Using these buttons is as easy as attaching a ReplyKeyboardMarkup or an InlineKeyboardMarkup to your SendMessage object. This guide will focus on inline buttons since they only require a few extra lines of code. Creating Buttons First of all, let's create some buttons. var next = InlineKeyboardButton.builder()\n            .text(\"Next\").callbackData(\"next\")           \n            .build();\n\n var back = InlineKeyboardButton.builder()\n            .text(\"Back\").callbackData(\"back\")\n            .build();\n\n var url = InlineKeyboardButton.builder()\n            .text(\"Tutorial\")\n            .url(\"https://core.telegram.org/bots/api\")\n            .build(); Let's go back through the fields we specified: Text - This is what the user will see, the text that appears on the button Callback Data - This will be sent back to the code instance as part of a new Update , so we can quickly identify what button was clicked. Url - A button that specifies a URL doesn't specify callbackdata since its behavior is predefined – it will open the given link when tapped. Creating Keyboards The buttons we created can be assembled into two keyboards , which will then be used to navigate back and forth between two sample menus . First, add two fields to store the necessary keyboards. private boolean screaming = false;\n\nprivate InlineKeyboardMarkup keyboardM1;\nprivate InlineKeyboardMarkup keyboardM2; Then, build and assign them. keyboardM1 = InlineKeyboardMarkup.builder()\n          .keyboardRow(List.of(next)).build();  \n\n//Buttons are wrapped in lists since each keyboard is a set of button rows\nkeyboardM2 = InlineKeyboardMarkup.builder()\n          .keyboardRow(List.of(back))\n          .keyboardRow(List.of(url))\n          .build(); You can place this code wherever you prefer, the important thing is making sure that keyboard variables are accessible from the method call that will send the new menu. If you're confused by this concept and don't know where to put them, just paste them above the command processing flow. Sending Keyboards Sending a keyboard only requires specifying a reply markup for the message. public void sendMenu(Long who, String txt, InlineKeyboardMarkup kb){\n    SendMessage sm = SendMessage.builder().chatId(who.toString())\n            .parseMode(\"HTML\").text(txt)\n            .replyMarkup(kb).build();\n\n    try {\n        execute(sm);\n    } catch (TelegramApiException e) {\n        throw new RuntimeException(e);\n    }\n} You may have noticed that we also added a new parameter, HTML . This is called a formatting option and will allow us to use HTML tags and add formatting to the text later on. Menu Trigger We could send a new menu for each new user, but for simplicity let's add a new command that will spawn a menu. We can achieve this by adding a new else clause to the previous command flow. var txt = msg.getText();\n if(msg.isCommand()) {\n        if (txt.equals(\"/scream\"))\n            screaming = true;\n        else if (txt.equals(\"/whisper\"))\n            screaming = false;\n        else if (txt.equals(\"/menu\"))\n            sendMenu(id, \"<b>Menu 1</b>\", keyboardM1);\n        return;\n } Try sending /menu to your bot now. If you did everything correctly, you should see a brand new menu pop up. In a production environment, commands should be handled with an appropriate design pattern that isolates them into different executor classes – modular and separated from the main logic. Navigation When building complex bots, navigation is essential. Your users must be able to move seamlessly from one menu to the next. In this example, we want the Next button to lead the user to the second menu. The Back button will send us back. To do that, we will start processing incoming CallbackQueries , which are the results we get after the user taps on a button. A CallbackQuery is essentially composed of three main parameters: queryId - Needed to close the query. You must always close new queries after processing them – if you don't, a loading symbol will keep showing on the user's side on top of each button. data - This identifies which button was pressed. from - The user who pressed the button. Processing in this context just means executing the action uniquely identified by the button, then closing the query . A very basic button handler could look something like: private void buttonTap(Long id, String queryId, String data, int msgId) {\n\n    EditMessageText newTxt = EditMessageText.builder()\n            .chatId(id.toString())\n            .messageId(msgId).text(\"\").build();\n\n    EditMessageReplyMarkup newKb = EditMessageReplyMarkup.builder()\n            .chatId(id.toString()).messageId(msgId).build();                           \n\n    if(data.equals(\"next\")) {\n        newTxt.setText(\"MENU 2\");\n        newKb.setReplyMarkup(keyboardM2);\n    } else if(data.equals(\"back\")) {\n        newTxt.setText(\"MENU 1\");\n        newKb.setReplyMarkup(keyboardM1);\n    }\n\n    AnswerCallbackQuery close = AnswerCallbackQuery.builder()\n            .callbackQueryId(queryId).build();\n\n    execute(close);\n    execute(newTxt);\n    execute(newKb);\n} With this handler, whenever a button is tapped, your bot will automatically navigate between inline menus. Expanding on this concept allows for endless combinations of navigable submenus, settings and dynamic pages. Database Telegram does not host an update database for you – once you process and consume an update, it will no longer be available. This means that features like user lists, message lists, current user inline menu, settings, etc. have to be implemented and maintained by bot developers. If your bot needs one of these features and you want to get started on data persistence , we recommend that you look into serialization practices and libraries for your language of choice, as well as available databases. Implementing a database is out of scope for this guide, however, several guides are available online for simple embedded open source software solutions like SQLite , HyperSQL , Derby and many more. Your language of choice will also influence which databases are available and supported – the list above assumes you followed this Java tutorial. Hosting So far, your bot has been running on your local machine – your PC. While this may be good for developing , testing and debugging , it is not ideal for a production environment. You'll want your bot to be available and responsive at all times, but your computer might not always be online. This can be done in four steps: Package your code Making your bot easy to move and runnable outside of an IDE is essential to host it elsewhere . If you followed this tutorial, this standard guide will work for you. If you didn't, look into export or packaging guides for your IDE and language of choice – procedures may vary but the end result is the same. Purchase a VPS or equivalent service A server is essentially a machine that is always online and running, without you having to worry about anything. To host your bot, you can opt for a VPS which serves this purpose and can be rented from several different providers. Another option would be to purchase a network-capable microcontroller , which come in all different specs and sizes depending on your needs. You should ensure that all user data remains heavily encrypted at all times in your database to guarantee the privacy of your users. The same concept applies to your local instance, however, this becomes especially important once you transfer your database to a remote server. Upload your executable/package Once you have a working ssh connection between your machine and your new server, you should upload your executable and all associated files. We will assume the runnable jar TutorialBot.jar and its database dbase.db are currently in the /TBot folder. $ scp -r /TBot/ username@server_ip:/bots/TBotRemote/ Run your application Depending on which language you chose, you might have to configure your server environment differently. If you chose Java, you just need to install a compatible JDK. $ apt install openjdk-17-jre\n$ java -version If you did everything correctly, you should see a Java version as the output, along with a few other values. This means you're ready to run your application. Now, to run the executable: $ cd /bots/TBotRemote/\n$ java -jar TutorialBot.jar Your bot is now online and users can interact with it at any time. To streamline and modularize this process, you could employ a specialized docker container or equivalent service. If you followed along in one of the equivalent examples ( C# , Python , Go and TypeScript ) you can find a detailed set of instructions to export and run your code here . Further Reading If you got this far, you might be interested in these additional guides and docs: General Bot Platform Overview Detailed List of Bot Features Full API Reference If you encounter any issues while following this guide, you can contact us on Telegram at @BotSupport . Telegram Telegram is a cloud-based mobile and desktop messaging app with a focus on security and speed. About FAQ Privacy Press Mobile Apps iPhone/iPad Android Mobile Web Desktop Apps PC/Mac/Linux macOS Web-browser Platform API Translations Instant View About Blog Press Moderation",
            "meta_description": "",
            "links": [
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/tutorial",
                "https://core.telegram.org/bots/api",
                "https://core.telegram.org/bots/samples",
                "https://core.telegram.org/bots/samples",
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/features",
                "https://core.telegram.org/bots/api"
            ],
            "timestamp": "2024-12-27T22:28:22.935974",
            "status_code": 200
        },
        {
            "url": "https://core.telegram.org/bots/samples",
            "title": "Bot API Library Examples",
            "text_content": "Bot API Library Examples Twitter Home FAQ Apps API Protocol Schema Telegram Bots Bot API Library Examples Bot API Library Examples A full tutorial covering everything from configuring your environment to deploying your finished bot is available here . This page lists some libraries and frameworks developed by the Telegram community – you should take care to report any bugs you may find to the respective developers, as these projects are not maintained by Telegram. Ping us on @BotSupport if you would like your library to appear on this page. PHP Nutgram . The Telegram bot framework that doesn't drive you nuts. https://github.com/nutgram/nutgram Telegraph . A Laravel package for fluently interacting with Telegram Bots. https://github.com/def-studio/telegraph tgWebValid . Library for Telegram Web App User Validation and Telegram Login Widget for PHP. https://github.com/CrazyTapok-bit/tgWebValid BPT . A simple library for working with Telegram Api. https://github.com/BPTproto/BPT https://github.com/BPTproto/BPT-Multi LaraGram . An advanced framework for Telegram Bot development. https://github.com/laraXgram/LaraGram BotAPI . SDK for the Telegram Bot API. https://github.com/TelegramSDK/BotAPI TuriBot . A simple way to communicate with Telegram APIs in PHP. https://github.com/davtur19/TuriBot TelegramBotsApi . SDK for Telegram Bot API. https://github.com/kuvardin/TelegramBotsApi Telegram Bot API for PHP . PHP library to interact with Telegram Bot API. https://github.com/vjik/telegram-bot-api/ PHP Telegram Bot . PHP Telegram Bot based on the official Telegram Bot API. https://github.com/php-telegram-bot/core Telegram Bot PHP . A library that makes using Telegram Bot API much easier. https://github.com/telegram-bot-php/core PHP Telegram API . A complete async capable Telegram Bot API implementation for PHP7. https://github.com/unreal4u/telegram-api Bot API PHP SDK . Telegram Bot API PHP SDK. Supports Laravel out of the box. https://github.com/irazasyed/telegram-bot-sdk klev-o/telegram-bot-api . Simple and convenient object-oriented implementation Telegram Bot API. https://github.com/klev-o/telegram-bot-api TeleBot . Easy way to create Telegram bots in PHP. Rich Laravel support out of the box. https://github.com/westacks/telebot TgBotLib . Telegram Bot Library for ncc. https://github.com/nosial/TgBotLib NeleBot X Framework . Framework for Telegram Bot API. https://github.com/NeleB54Gold/NeleBotX PHP Telegram Bot Api . Native PHP Wrapper for Telegram BOT API. https://github.com/TelegramBot/Api NovaGram . An Object-Oriented PHP library for Telegram Bots. https://github.com/skrtdev/NovaGram Go Golang Telegram Bot library . An autogenerated wrapper for the Telegram Bot API. https://github.com/paulsonoflars/gotgbot Telego . Telegram Bot API library. https://github.com/mymmrac/telego Golang Telegram Bot . Telegram Bot API Go framework. https://github.com/go-telegram/bot go-tg . Library for accessing Telegram Bot API, with batteries for building complex bots included. https://github.com/mr-linch/go-tg Telegram Bot API: Go implementation . A Telegram IM bots API implementation. https://github.com/temoon/telegram-bots-api echotron . An elegant and concurrent library for the Telegram Bot API. https://github.com/NicoNex/echotron Telegram Bot API helper for Golang . A Telegram Bot API wrapper. https://github.com/meinside/telegram-bot-go telebot . A Telegram bot framework. https://github.com/tucnak/telebot Telegrambot . Telegram Bot API in Go, but with more clean code. https://github.com/nickname76/telegrambot Python AIOGram . A pretty simple and fully asynchronous library for Telegram Bot API written with asyncio and aiohttp. https://github.com/aiogram/aiogram python-telegram-bot . A wrapper you can't refuse. https://github.com/python-telegram-bot/python-telegram-bot pyTelegramBotAPI . A simple, but extensible Python implementation for the Telegram Bot API. https://github.com/eternnoir/pyTelegramBotAPI telegram.py . An async API wrapper for the Telegram Bot API in Python. https://github.com/ilovetocode2019/telegram.py Telegrinder . Modern visionary telegram bot framework. https://github.com/timoniq/telegrinder Wonda . A light, blazing fast framework for developing capable bots. https://github.com/wondergram-org/wonda pure-teleapi . Pure declarative Telegram Bot API implementation with Pydantic models. https://github.com/AntonOvsyannikov/pure-teleapi telegram-text . A Python markup module, which can be used with other frameworks. https://github.com/SKY-ALIN/telegram-text OrigamiBot . A pythonic Telegram bot API library. https://github.com/cmd410/OrigamiBot Rust Frankenstein . A Telegram Bot API client. https://github.com/ayrat555/frankenstein Ferrisgram . An asynchronous autogenerated wrapper for the Telegram Bot API. https://github.com/ferrisgram/ferrisgram botapi-rs , A mildly competent autogenerated telegram api wrapper. https://github.com/fmeef/botapi-rs carapax . A Telegram Bot API framework. https://github.com/tg-rs/carapax teloxide . An elegant Telegram bots framework. https://github.com/teloxide/teloxide tgbotapi . A library for using the Telegram Bot API. https://github.com/Syfaro/tgbotapi-rs MOBOT . A Telegram Bot Library in Rust. https://github.com/0xfe/mobot .NET Telegram.bot . .NET Client for Telegram Bot API. https://github.com/TelegramBots/Telegram.Bot Telegram.BotAPI for NET . One of the most complete libraries available to interact with the Telegram Bot API. https://github.com/Eptagone/Telegram.BotAPI Telegram Bot Framework . A context based application framework for the C# TelegramBot library. https://github.com/MajMcCloud/TelegramBotFramework RxTelegram.Bot . RxTelegram uses a reactive approach to make Updates available. https://github.com/RxTelegram/RxTelegram.Bot Telegram.Bots . A .NET 5 wrapper for the Telegram Bot API. https://github.com/TelegramBotsAPI/Telegram.Bots Kotlin TelegramBotAPI . Type-safe library for work with Telegram Bot API. https://github.com/InsanusMokrassar/TelegramBotAPI Kotlin Telegram Bot . Telegram Bot API wrapper, with handy Kotlin DSL. https://github.com/vendelieu/telegram-bot Kotlin Telegram Bot . Library for creating scalable and expandable applications with hepful features. https://github.com/DEHuckaKpyT/telegram-bot Kotlin Telegram Bot . A wrapper for the Telegram Bot API. https://github.com/kotlin-telegram-bot/kotlin-telegram-bot TelegramKitty . Powerful and type-safe Telegram Bot API wrapper with built-in cat pic functionality. https://github.com/bezsahara/TelegramKitty Node.js Telegraf . Modern Telegram Bot Framework for Node.js. https://github.com/telegraf/telegraf Telenode . Lightweight Telegram API framework for Node.js. https://github.com/NivEz/telenode Node-Telegram-bot . Node.js module to interact with the official Telegram Bot API. https://github.com/yagop/node-telegram-bot-api Telegramsjs . A powerful library for interacting with the Telegram Bot API. https://github.com/telegramsjs/TelegramsJS TypeScript grammY . The Telegram Bot Framework. https://github.com/grammyjs/grammY puregram . Powerful and modern telegram bot api sdk for node.js and typescript. https://github.com/nitreojs/puregram typescript-telegram-bot-api . Telegram Bot API wrapper for Node.js written in TypeScript. https://github.com/Borodin/typescript-telegram-bot-api GramIO . Powerful, extensible and really type-safe Telegram Bot API framework. https://github.com/gramiojs/gramio Java TelegramBots . A simple to use library to create Telegram Bots. https://github.com/rubenlagus/TelegramBots Java API . Telegram Bot API for Java. https://github.com/pengrad/java-telegram-bot-api Teleight Bots . The most lightweight java telegram bot wrapper. https://github.com/Teleight/TeleightBots Telebof . Easy and modern Java Telegram Bot API. https://github.com/natanimn/Telebof C++ QTelegramBotAPI . Telegram Bot API on C++ and Qt. https://github.com/Modersi/TelegramBotAPI tgbot . A library for Telegram Bot API with generated API types and methods. https://github.com/egorpugin/tgbot tgbot-cpp . A library for Telegram Bot API. https://github.com/reo7sp/tgbot-cpp Ruby telegram-bot-ruby . Ruby wrapper for Telegram's Bot API. https://github.com/atipugin/telegram-bot-ruby Telegram::Bot . Ruby gem for building Telegram Bot with optional Rails integration. https://github.com/telegram-bot-rb/telegram-bot TelegramWorkflow . A simple utility to help you organize the code to create Telegram bots. https://github.com/rsamoilov/telegram_workflow Scala bot4s.telegram . Simple, extensible, strongly-typed wrapper for the Telegram Bot API. https://github.com/bot4s/telegram F[Tg] - Telegramium . Pure functional Telegram Bot API implementation. https://github.com/apimorphism/telegramium Lua ggram . Lua library for the Telegram bot API. You can even use it in Garry's Mod. https://github.com/TRIGONIM/ggram telegram-bot-lua . A feature-filled Telegram Bot API library. https://github.com/wrxck/telegram-bot-lua Dart Televerse . Your gateway to seamless Telegram Bot Development. https://github.com/xooniverse/televerse TeleDart . A library interfacing with Telegram Bot API. https://github.com/DinoLeung/TeleDart Other Languages Swift. Telegram Vapor Bot . The wrapper for the Telegram Bot API written in Swift with Vapor. https://github.com/nerzh/telegram-vapor-bot Elixir. ExGram . Telegram Bot API low level API and framework. https://github.com/rockneurotiko/ex_gram Pascal. TGBotMini . Telegram Bot Mini API. https://github.com/HemulGM/TGBotMini Clojure. telegrambot-lib . A library for interacting with the Telegram Bot API. https://github.com/wdhowe/telegrambot-lib OCaml. TelegraML . A library for creating bots for Telegram. https://github.com/nv-vn/TelegraML Haskell. haskell-telegram-api . High-level bindings to the Telegram Bot API based on servant library. https://github.com/klappvisor/haskell-telegram-api Perl. Telegram Bot . A genuine Perl 6 client for the Telegram's Bot API. https://github.com/GildedHonour/TelegramBot Telegram Telegram is a cloud-based mobile and desktop messaging app with a focus on security and speed. About FAQ Privacy Press Mobile Apps iPhone/iPad Android Mobile Web Desktop Apps PC/Mac/Linux macOS Web-browser Platform API Translations Instant View About Blog Press Moderation",
            "meta_description": "",
            "links": [
                "https://core.telegram.org/bots",
                "https://core.telegram.org/bots/samples",
                "https://core.telegram.org/bots/tutorial"
            ],
            "timestamp": "2024-12-27T22:28:23.202852",
            "status_code": 200
        }
    ]
}

